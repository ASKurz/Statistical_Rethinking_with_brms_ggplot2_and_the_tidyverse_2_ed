[["index.html", "Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition version 0.2.0 What and why My assumptions about you How to use and understand this project R setup We have updates Thank-you’s are in order License and citation You can do this, too", " Statistical rethinking with brms, ggplot2, and the tidyverse: Second edition version 0.2.0 A Solomon Kurz 2021-03-16 What and why This ebook is based on the second edition of Richard McElreath’s (2020a) text, Statistical rethinking: A Bayesian course with examples in R and Stan. My contributions show how to fit the models he covered with Paul Bürkner’s brms package (Bürkner, 2017, 2018, 2020), which makes it easy to fit Bayesian regression models in R (R Core Team, 2020) using Hamiltonian Monte Carlo. I also prefer plotting and data wrangling with the packages from the tidyverse (Wickham, 2019; Wickham et al., 2019). So we’ll be using those methods, too. My assumptions about you If you’re looking at this project, I’m guessing you’re either a graduate student, a post-graduate academic or a researcher of some sort, which suggests you have at least a 101-level foundation in statistics. If you’re rusty, consider checking out the free text books by Legler and Roback (2019) or Navarro (2019) before diving into Statistical rethinking. I’m also assuming you understand the rudiments of R and have at least a vague idea about what the tidyverse is. If you’re totally new to R, consider starting with Peng’s (2019) R programming for data science. For an introduction to the tidyvese-style of data analysis, the best source I’ve found is Grolemund and Wickham’s (2017) R for data science (R4DS), which I extensively link to throughout this project. Another nice alternative is Baumer, Kaplan, and Horton’s (2021), Modern data science with R. However, you do not need to be totally fluent in statistics or R. Otherwise why would you need this project, anyway? IMO, the most important things are curiosity, a willingness to try, and persistent tinkering. I love this stuff. Hopefully you will, too. How to use and understand this project This project is not meant to stand alone. It’s a supplement to the second edition of McElreath’s text. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse-style code. However, some of the sections in the text are composed entirely of equations or prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project will sometimes be blank or omitted, though I do highlight some of the important points in quotes and prose of my own. So I imagine students might reference this project as they progress through McElreath’s text. I also imagine working data analysts might use this project in conjunction with the text as they flip to the specific sections that seem relevant to solving their data challenges. I reproduce the bulk of the figures in the text, too. The plots in the first few chapters are the closest to those in the text. However, I’m passionate about data visualization and like to play around with color palettes, formatting templates, and other conventions quite a bit. As a result, the plots in each chapter have their own look and feel. For more on some of these topics, check out chapters 3, 7, and 28 in R4DS; Healy’s (2018) Data visualization: A practical introduction; Wilke’s (2019) Fundamentals of data visualization; or Wickham’s (2016) ggplot2: Elegant graphics for data analysis. In this project, I use a handful of formatting conventions gleaned from R4DS, The tidyverse style guide (Wickham, 2020), and R markdown: The definitive guide (Xie et al., 2020). R code blocks and their output appear in a gray background. E.g., 2 + 2 == 5 ## [1] FALSE R and the names of specific package (e.g., brms) are in boldface font. Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit the package a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidybayes::median_qi()). R objects, such as data or function arguments, are in typewriter font atop gray backgrounds (e.g., chimpanzees, .width = .5). You can detect hyperlinks by their typical blue-colored font. In the text, McElreath indexed his models with names like m4.1 (i.e., the first model of Chapter 4). I primarily followed that convention, but replaced the m with a b to stand for the brms package. R setup To get the full benefit from this ebook, you’ll need some software. Happily, everything will be free (provided you have access to a decent personal computer and an good internet connection). First, you’ll need to install R, which you can learn about at https://cran.r-project.org/. Though not necessary, your R experience might be more enjoyable if done through the free RStudio interface, which you can learn about at https://rstudio.com/products/rstudio/. Once you have installed R, execute the following to install the bulk of the add-on packages. This will probably take a few minutes to finish. Go make yourself a coffee. packages &lt;- c(&quot;ape&quot;, &quot;bayesplot&quot;, &quot;brms&quot;, &quot;broom&quot;, &quot;dagitty&quot;, &quot;devtools&quot;, &quot;flextable&quot;, &quot;GGally&quot;, &quot;ggdag&quot;, &quot;ggdark&quot;, &quot;ggmcmc&quot;, &quot;ggrepel&quot;, &quot;ggthemes&quot;, &quot;ggtree&quot;, &quot;ghibli&quot;, &quot;gtools&quot;, &quot;loo&quot;, &quot;patchwork&quot;, &quot;psych&quot;, &quot;rcartocolor&quot;, &quot;Rcpp&quot;, &quot;remotes&quot;, &quot;rstan&quot;, &quot;StanHeaders&quot;, &quot;statebins&quot;, &quot;tidybayes&quot;, &quot;tidyverse&quot;, &quot;viridis&quot;, &quot;viridisLite&quot;, &quot;wesanderson&quot;) install.packages(packages, dependencies = T) A few of the other packages are not officially available via the Comprehensive R Archive Network (CRAN; https://cran.r-project.org/). You can download them directly from GitHub by executing the following. devtools::install_github(&quot;stan-dev/cmdstanr&quot;) devtools::install_github(&quot;EdwinTh/dutchmasters&quot;) devtools::install_github(&quot;gadenbuie/ggpomological&quot;) devtools::install_github(&quot;rmcelreath/rethinking&quot;) devtools::install_github(&quot;UrbanInstitute/urbnmapr&quot;) remotes::install_github(&quot;stan-dev/posterior&quot;) It’s possible you’ll have problems installing some of these packages. Here are some likely suspects and where you can find help: for difficulties installing brms, go to https://github.com/paul-buerkner/brms#how-do-i-install-brms or search around in the brms section of the Stan forums; for difficulties installing cmdstanr, go to https://mc-stan.org/cmdstanr/articles/cmdstanr.html; for difficulties installing rethinking, go to https://github.com/rmcelreath/rethinking#quick-installation; and for difficulties installing rstan, go to https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started. We have updates For a brief rundown of the version history, we have: Version 0.1.0. I released the 0.1.0 version of this project in November 24, 2020. It was the first full-length and nearly complete draft including material from all the 17 chapters in McElreath’s source material. All brms models were fit with version 2.14.0+. Version 0.1.1. On December 2, 2020, I released a mini update designed to fix code breaks resulting from updates to the broom package (Robinson &amp; Hayes, 2020), caught by Jenny Bigman; replace the soon-to-be retired sample_n() code with slice_sample(), caught by Randall Pruim; and correct a few typos along the way. Version 0.2.0. Welcome to version 0.2.0! Major improvements include: a corrected workflow for fitting single-level b-spline models (Section 4.5.2), thanks to Stephen Wild; a refined workflow for fitting multilevel b-spline models using the s() function (Section 4.6), thanks to Gavin Simpson; a new bonus section (4.7) on grouping predictors within matrix columns, thanks to insights from Gelman et al. (2020) and hints from Paul-Christian Bürkner; a brms solution to the \\(\\sigma = 0.01\\) sixth-order polynomial model in Section 7.1.1, thanks again to Stephen Wild; a workflow to reproduce the Metropolis simulation for Figure 9.3, thanks to James Henegan; a corrected workflow for taking fitted()-based random draws for the bonus material in Section 13.5.2.1, thanks to an exchange with Ladislas Nalborczyk; a brms solution to McElreath’s bivariate differential equation model in Section 16.4, thanks to Markus Gesmann; better use of geom_area() throughout the book, thanks to insights from Randall Pruim; the adoption of a CONTRIBUTING section on GitHub, thanks to Brenton M. Wiernik; improved table workflow with the flextable package (Gohel, 2021a); and all models have been refit using brms version 2.15.0. We’re not done yet and I could use your help. Some areas of the book could use some fleshing out. The sections I’m particularly anxious to improve are 15.3, which may someday include a brms workflow for categorical missing data; and 16.2.3, which contains a mixture model that McElreath fit directly in Stan and I suspect may be possible in brms with a custom likelihood. If you have insights on how to improve these or any other sections, please share your thoughts on GitHub at https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/issues. The contribution guidelines for this book are listed at https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/blob/master/CONTRIBUTING.md. Thank-you’s are in order I’d like to thank the following for their helpful contributions: E. David Aja (@edavidaja), Monica Alexander (@MJAlexander), Shaan Amin (@Shaan-Amin), Malcolm Barrett (@malcolmbarrett), Adam Bear (@adambear91), Jenny Bigman (@jennybigman), Louis Bliard (@lbiard), Paul-Christian Bürkner (@paul-buerkner), Markus Gesmann (@mages), James Henegan (@jameshenegan), Sebastian Lobentanzer (@slobentanzer), Ed Merkle (@ecmerkle), Ladislas Nalborczyk (@lnalborczyk), Randall Pruim (@rpruim), Gavin Simpson (@gavinsimpson), Richard Torkar (@torkar), Brenton M. Wiernik (@bwiernik), Stephen Wild (@sjwild), and Donald R. Williams (@donaldRwilliams). Science is better when we work together. License and citation This book is licensed under the Creative Commons Zero v1.0 Universal license. You can learn the details, here. In short, you can use my work. Just make sure you give me the appropriate credit the same way you would for any other scholarly resource. Here’s the citation information: @book{kurzStatisticalRethinkingSecondEd2021, title = {Statistical rethinking with brms, ggplot2, and the tidyverse: {{Second}} edition}, author = {Kurz, A. Solomon}, year = {2021}, month = {mar}, edition = {version 0.2.0}, url = {https://bookdown.org/content/4857/} } You can do this, too This project is powered by Yihui Xie’s (2020) bookdown package, which makes it easy to turn R markdown files into HTML, PDF, and EPUB. Go here to learn more about bookdown. While you’re at it, also check out Xie, Allaire, and Grolemund’s R markdown: The definitive guide. And if you’re unacquainted with GitHub, check out Jenny Bryan’s (2020) Happy Git and GitHub for the useR. I’ve even blogged about what it was like putting together the first version of this project. The source code of the project is available on GitHub at https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed. "],["the-golem-of-prague.html", "1 The Golem of Prague Statistical golems Session info", " 1 The Golem of Prague Rabbi Loew and Golem by Mikoláš Aleš, 1899 As he opened the chapter, McElreath told us that ultimately Judah was forced to destroy the golem, as its combination of extraordinary power with clumsiness eventually led to innocent deaths. Wiping away one letter from the inscription emet to spell instead met, “death,” Rabbi Judah decommissioned the robot. Statistical golems Scientists also make golems. Our golems rarely have physical form, but they too are often made of clay, living in silicon as computer code. These golems are scientific models. But these golems have real effects on the world, through the predictions they make and the intuitions they challenge or inspire. A concern with “truth” enlivens these models, but just like a golem or a modern robot, scientific models are neither true nor false, neither prophets nor charlatans. Rather they are constructs engineered for some purpose. These constructs are incredibly powerful, dutifully conducting their programmed calculations. (McElreath, 2020a, p. 1, emphasis in the original) There are a lot of great points, themes, methods, and factoids in this text. For me, one of the most powerful themes interlaced throughout the pages is how we should be skeptical of our models. Yes, learn Bayes. Pore over this book. Fit models until late into the night. But please don’t fall into blind love with their elegance and power. If we all knew what we were doing, there’d be no need for science. For more wise deflation along these lines, do check out A personal essay on Bayes factors, Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection (D. J. Navarro, 2019) and Science, statistics and the problem of “pretty good inference”, a blog, paper and talk by the inimitable Danielle Navarro. Anyway, McElreath left us no code or figures to translate in this chapter. But before you skip off to the next one, why not invest a little time soaking in this chapter’s material by way of a lecture by McElreath, himself? He’s an engaging speaker and the material in his online lectures does not entirely overlap with that in the text. Here’s the first lecture from his Winter 2019 course: Session info At the end of every chapter, I use the sessionInfo() function to help make my results more reproducible. sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] bookdown_0.21 digest_0.6.27 assertthat_0.2.1 R6_2.5.0 ## [5] magrittr_2.0.1 evaluate_0.14 highr_0.8 httr_1.4.2 ## [9] rlang_0.4.10 stringi_1.5.3 curl_4.3 rmarkdown_2.7 ## [13] tools_4.0.4 stringr_1.4.0 glue_1.4.2 xfun_0.22 ## [17] yaml_2.2.1 compiler_4.0.4 vembedr_0.1.4 htmltools_0.5.1.1 ## [21] knitr_1.31 "],["small-worlds-and-large-worlds.html", "2 Small Worlds and Large Worlds 2.1 The garden of forking data 2.2 Building a model 2.3 Components of the model 2.4 Making the model go Session info", " 2 Small Worlds and Large Worlds A while back The Oatmeal put together an infographic on Christopher Columbus. I’m no historian and cannot vouch for its accuracy, so make of it what you will. McElreath described the thrust of this chapter this way: In this chapter, you will begin to build Bayesian models. The way that Bayesian models learn from evidence is arguably optimal in the small world. When their assumptions approximate reality, they also perform well in the large world. But large world performance has to be demonstrated rather than logically deduced. (McElreath, 2020a, p. 20) Indeed. 2.1 The garden of forking data Gelman and Loken (2013) wrote a great paper of a similar name and topic. The titles from this section and Gelman and Loken’s paper have their origins in the short story by Jorge Luis Borges (1941), The garden of forking paths. You can find copies of the original short story here or here. Here’s a snip: In all fictional works, each time a man is confronted with several alternatives, he chooses one and eliminates the others; in the fiction of Ts’ui Pên, he chooses–simultaneously–all of them. He creates, in this way, diverse futures, diverse times which themselves also proliferate and fork. The choices we make in our data analyses proliferate and fork in this way, too. 2.1.1 Counting possibilities. Throughout this project, we’ll make extensive use packages from the tidyverse for data wrangling and plotting. library(tidyverse) If you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., %&gt;%). I’m not going to explain the %&gt;% in this project, but you might learn more about in this brief clip, starting around minute 21:25 in this talk by Wickham, or in Section 5.6.1 from Grolemund and Wickham’s (2017) R for data science. Really, all of Chapter 5 of R4DS is just great for new R and new tidyverse users. And R4DS Chapter 3 is a nice introduction to plotting with ggplot2 (Wickham, 2016; Wickham et al., 2020). Other than the pipe, the other big thing to be aware of is tibbles (Müller &amp; Wickham, 2020). For our purposes, think of a tibble as a data object with two dimensions defined by rows and columns. Importantly, tibbles are just special types of data frames. So, whenever we talk about data frames, we’re usually talking about tibbles. For more on the topic, check out R4SD, Chapter 10. If we’re willing to code the marbles as 0 = “white” 1 = “blue,” we can arrange the possibility data in a tibble as follows. d &lt;- tibble(p1 = 0, p2 = rep(1:0, times = c(1, 3)), p3 = rep(1:0, times = c(2, 2)), p4 = rep(1:0, times = c(3, 1)), p5 = 1) head(d) ## # A tibble: 4 x 5 ## p1 p2 p3 p4 p5 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 1 1 1 1 ## 2 0 0 1 1 1 ## 3 0 0 0 1 1 ## 4 0 0 0 0 1 You might depict the possibility data in a plot. d %&gt;% set_names(1:5) %&gt;% mutate(x = 1:4) %&gt;% pivot_longer(-x, names_to = &quot;possibility&quot;) %&gt;% mutate(value = value %&gt;% as.character()) %&gt;% ggplot(aes(x = x, y = possibility, fill = value)) + geom_point(shape = 21, size = 5) + scale_fill_manual(values = c(&quot;white&quot;, &quot;navy&quot;)) + scale_x_discrete(NULL, breaks = NULL) + theme(legend.position = &quot;none&quot;) As a quick aside, check out Suzan Baert’s blog post Data wrangling part 2: Transforming your columns into the right shape for an extensive discussion on dplyr::mutate() and tidyr::gather(). The tidyr::pivot_longer() function is an updated variant of gather(), which we’ll be making extensive use of throughout this project. If you’re new to reshaping data with pivoting, check out the vignettes here and here (Pivot Data from Wide to Long Pivot_longer, 2020; Pivoting, 2020). Here’s the basic structure of the possibilities per marble draw. library(flextable) tibble(draw = 1:3, marbles = 4) %&gt;% mutate(possibilities = marbles ^ draw) %&gt;% flextable() .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-3a2c3808{border-collapse:collapse;}.cl-3a2639a8{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3a2655f0{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3a268c14{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a268c32{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3a268c3c{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}drawmarblespossibilities14424163464 Note our use of the flextable package (Gohel, 2021a, 2021b) to format the output into a nice table. We’ll get more practice with this throughout this chapter. If you walk that out a little, you can structure the data required to approach Figure 2.2. ( d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3)), fill = rep(c(&quot;b&quot;, &quot;w&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2))) ) ## # A tibble: 84 x 3 ## position draw fill ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 b ## 2 2 1 w ## 3 3 1 w ## 4 4 1 w ## 5 0.25 2 b ## 6 0.5 2 w ## 7 0.75 2 w ## 8 1 2 w ## 9 1.25 2 b ## 10 1.5 2 w ## # … with 74 more rows See what I did there with the parentheses? If you assign a value to an object in R (e.g., dog &lt;- 1) and just hit return, nothing will immediately pop up in the console. You have to actually execute dog before R will return 1. But if you wrap the code within parentheses (e.g., (dog &lt;- 1)), R will perform the assignment and return the value as if you had executed dog. But we digress. Here’s the initial plot. d %&gt;% ggplot(aes(x = position, y = draw, fill = fill)) + geom_point(shape = 21, size = 3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_y_continuous(breaks = 1:3) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) To my mind, the easiest way to connect the dots in the appropriate way is to make two auxiliary tibbles. # these will connect the dots from the first and second draws ( lines_1 &lt;- tibble(x = rep((1:4), each = 4), xend = ((1:4^2) / 4), y = 1, yend = 2) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.25 1 2 ## 2 1 0.5 1 2 ## 3 1 0.75 1 2 ## 4 1 1 1 2 ## 5 2 1.25 1 2 ## 6 2 1.5 1 2 ## 7 2 1.75 1 2 ## 8 2 2 1 2 ## 9 3 2.25 1 2 ## 10 3 2.5 1 2 ## 11 3 2.75 1 2 ## 12 3 3 1 2 ## 13 4 3.25 1 2 ## 14 4 3.5 1 2 ## 15 4 3.75 1 2 ## 16 4 4 1 2 # these will connect the dots from the second and third draws ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4), xend = (1:4^3) / (4^2), y = 2, yend = 3) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.25 0.0625 2 3 ## 2 0.25 0.125 2 3 ## 3 0.25 0.188 2 3 ## 4 0.25 0.25 2 3 ## 5 0.5 0.312 2 3 ## 6 0.5 0.375 2 3 ## 7 0.5 0.438 2 3 ## 8 0.5 0.5 2 3 ## 9 0.75 0.562 2 3 ## 10 0.75 0.625 2 3 ## # … with 54 more rows We can use the lines_1 and lines_2 data in the plot with two geom_segment() functions. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_y_continuous(breaks = 1:3) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) We’ve generated the values for position (i.e., the \\(x\\)-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that. d &lt;- d %&gt;% mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) d ## # A tibble: 84 x 4 ## position draw fill denominator ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0.5 1 b 0.5 ## 2 1.5 1 w 0.5 ## 3 2.5 1 w 0.5 ## 4 3.5 1 w 0.5 ## 5 0.125 2 b 0.125 ## 6 0.375 2 w 0.125 ## 7 0.625 2 w 0.125 ## 8 0.875 2 w 0.125 ## 9 1.12 2 b 0.125 ## 10 1.38 2 w 0.125 ## # … with 74 more rows We’ll follow the same logic for the lines_1 and lines_2 data. ( lines_1 &lt;- lines_1 %&gt;% mutate(x = x - 0.5, xend = xend - 0.5 / 4^1) ) ## # A tibble: 16 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 0.125 1 2 ## 2 0.5 0.375 1 2 ## 3 0.5 0.625 1 2 ## 4 0.5 0.875 1 2 ## 5 1.5 1.12 1 2 ## 6 1.5 1.38 1 2 ## 7 1.5 1.62 1 2 ## 8 1.5 1.88 1 2 ## 9 2.5 2.12 1 2 ## 10 2.5 2.38 1 2 ## 11 2.5 2.62 1 2 ## 12 2.5 2.88 1 2 ## 13 3.5 3.12 1 2 ## 14 3.5 3.38 1 2 ## 15 3.5 3.62 1 2 ## 16 3.5 3.88 1 2 ( lines_2 &lt;- lines_2 %&gt;% mutate(x = x - 0.5 / 4^1, xend = xend - 0.5 / 4^2) ) ## # A tibble: 64 x 4 ## x xend y yend ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.125 0.0312 2 3 ## 2 0.125 0.0938 2 3 ## 3 0.125 0.156 2 3 ## 4 0.125 0.219 2 3 ## 5 0.375 0.281 2 3 ## 6 0.375 0.344 2 3 ## 7 0.375 0.406 2 3 ## 8 0.375 0.469 2 3 ## 9 0.625 0.531 2 3 ## 10 0.625 0.594 2 3 ## # … with 54 more rows Now the plot’s looking closer. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 3) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_y_continuous(breaks = 1:3) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) For the final step, we’ll use coord_polar() to change the coordinate system, giving the plot a mandala-like feel. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend), size = 1/3) + geom_point(aes(fill = fill), shape = 21, size = 4) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + coord_polar() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) To make our version of Figure 2.3, we’ll have to add an index to tell us which paths remain logically valid after each choice. We’ll call the index remain. lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0:1, times = c(1, 3)), rep(0, times = 4 * 3))) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) d &lt;- d %&gt;% mutate(remain = c(rep(1:0, times = c(1, 3)), rep(0:1, times = c(1, 3)), rep(0, times = 4 * 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 12 * 4))) # finally, the plot: d %&gt;% ggplot(aes(x = position, y = draw)) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, alpha = remain %&gt;% as.character()), shape = 21, size = 4) + # it&#39;s the alpha parameter that makes elements semitransparent scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_alpha_manual(values = c(1/5, 1)) + scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) + coord_polar() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Letting “w” = a white dot and “b” = a blue dot, we might recreate the table in the middle of page 23 like so. # if we make two custom functions, here, # it will simplify the `mutate()` code, below n_blue &lt;- function(x) rowSums(x == &quot;b&quot;) n_white &lt;- function(x) rowSums(x == &quot;w&quot;) # make the data t &lt;- tibble(d1 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 4)), d2 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(2, 3)), d3 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 2)), d4 = rep(c(&quot;w&quot;, &quot;b&quot;), times = c(4, 1))) %&gt;% mutate(blue1 = n_blue(.), white = n_white(.), blue2 = n_blue(.)) %&gt;% mutate(product = blue1 * white * blue2) # format the table t %&gt;% transmute(conjecture = str_c(&quot;[&quot;, d1, &quot; &quot;, d2, &quot; &quot;, d3, &quot; &quot;, d4, &quot;]&quot;), `Ways to produce [w b w]` = str_c(blue1, &quot; * &quot;, white, &quot; * &quot;, blue2, &quot; = &quot;, product)) %&gt;% flextable() %&gt;% width(j = 1:2, width = c(1, 2)) %&gt;% align(align = &quot;center&quot;, part = &quot;all&quot;) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-3adef664{border-collapse:collapse;}.cl-3ad885f4{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3ad8942c{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3ad8baf6{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ad8bb0a{width:144pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ad8bb14{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ad8bb1e{width:144pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ad8bb28{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3ad8bb29{width:144pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}conjectureWays to produce [w b w][w w w w]0 * 4 * 0 = 0[b w w w]1 * 3 * 1 = 3[b b w w]2 * 2 * 2 = 8[b b b w]3 * 1 * 3 = 9[b b b b]4 * 0 * 4 = 0 We’ll need new data for Figure 2.4. Here’s the initial primary data, d. d &lt;- tibble(position = c((1:4^1) / 4^0, (1:4^2) / 4^1, (1:4^3) / 4^2), draw = rep(1:3, times = c(4^1, 4^2, 4^3))) ( d &lt;- d %&gt;% bind_rows( d, d ) %&gt;% # here are the fill colors mutate(fill = c(rep(c(&quot;w&quot;, &quot;b&quot;), times = c(1, 3)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), each = 2) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)), rep(c(&quot;w&quot;, &quot;b&quot;), times = c(3, 1)) %&gt;% rep(., times = c(4^0 + 4^1 + 4^2)))) %&gt;% # now we need to shift the positions over in accordance with draw, like before mutate(denominator = ifelse(draw == 1, .5, ifelse(draw == 2, .5 / 4, .5 / 4^2))) %&gt;% mutate(position = position - denominator) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for pie_index == &quot;b&quot; or &quot;c&quot;, we&#39;ll need to offset mutate(position = ifelse(pie_index == &quot;a&quot;, position, ifelse(pie_index == &quot;b&quot;, position + 4, position + 4 * 2))) ) ## # A tibble: 252 x 5 ## position draw fill denominator pie_index ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 1 w 0.5 a ## 2 1.5 1 b 0.5 a ## 3 2.5 1 b 0.5 a ## 4 3.5 1 b 0.5 a ## 5 0.125 2 w 0.125 a ## 6 0.375 2 b 0.125 a ## 7 0.625 2 b 0.125 a ## 8 0.875 2 b 0.125 a ## 9 1.12 2 w 0.125 a ## 10 1.38 2 b 0.125 a ## # … with 242 more rows Both lines_1 and lines_2 require adjustments for x and xend. Our current approach is a nested ifelse(). Rather than copy and paste that multi-line ifelse() code for all four, let’s wrap it in a compact function, which we’ll call move_over(). move_over &lt;- function(position, index) { ifelse( index == &quot;a&quot;, position, ifelse( index == &quot;b&quot;, position + 4, position + 4 * 2 ) ) } If you’re new to making your own R functions, check out Chapter 19 of R4DS or Chapter 14 of R programming for data science (Peng, 2019). Anyway, now we’ll make our new lines_1 and lines_2 data, for which we’ll use move_over() to adjust their x and xend positions to the correct spots. ( lines_1 &lt;- tibble(x = rep((1:4), each = 4) %&gt;% rep(., times = 3), xend = ((1:4^2) / 4) %&gt;% rep(., times = 3), y = 1, yend = 2) %&gt;% mutate(x = x - .5, xend = xend - .5 / 4^1) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 48 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.5 0.125 1 2 a ## 2 0.5 0.375 1 2 a ## 3 0.5 0.625 1 2 a ## 4 0.5 0.875 1 2 a ## 5 1.5 1.12 1 2 a ## 6 1.5 1.38 1 2 a ## 7 1.5 1.62 1 2 a ## 8 1.5 1.88 1 2 a ## 9 2.5 2.12 1 2 a ## 10 2.5 2.38 1 2 a ## # … with 38 more rows ( lines_2 &lt;- tibble(x = rep(((1:4^2) / 4), each = 4) %&gt;% rep(., times = 3), xend = (1:4^3 / 4^2) %&gt;% rep(., times = 3), y = 2, yend = 3) %&gt;% mutate(x = x - .5 / 4^1, xend = xend - .5 / 4^2) %&gt;% # here we&#39;ll add an index for which pie wedge we&#39;re working with mutate(pie_index = rep(letters[1:3], each = n()/3)) %&gt;% # to get the position axis correct for `pie_index == &quot;b&quot;` or `&quot;c&quot;`, we&#39;ll need to offset mutate(x = move_over(position = x, index = pie_index), xend = move_over(position = xend, index = pie_index)) ) ## # A tibble: 192 x 5 ## x xend y yend pie_index ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.125 0.0312 2 3 a ## 2 0.125 0.0938 2 3 a ## 3 0.125 0.156 2 3 a ## 4 0.125 0.219 2 3 a ## 5 0.375 0.281 2 3 a ## 6 0.375 0.344 2 3 a ## 7 0.375 0.406 2 3 a ## 8 0.375 0.469 2 3 a ## 9 0.625 0.531 2 3 a ## 10 0.625 0.594 2 3 a ## # … with 182 more rows For the last data wrangling step, we add the remain indices to help us determine which parts to make semitransparent. I’m not sure of a slick way to do this, so these are the result of brute force counting. d &lt;- d %&gt;% mutate(remain = c(#pie_index == &quot;a&quot; rep(0:1, times = c(1, 3)), rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), # pie_index == &quot;b&quot; rep(0:1, each = 2), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 2), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), # pie_index == &quot;c&quot;, rep(0:1, times = c(3, 1)), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) lines_1 &lt;- lines_1 %&gt;% mutate(remain = c(rep(0, times = 4), rep(1:0, times = c(1, 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 2), rep(1:0, each = 2) %&gt;% rep(., times = 2), rep(0, times = 4 * 3), rep(1:0, times = c(3, 1)) ) ) lines_2 &lt;- lines_2 %&gt;% mutate(remain = c(rep(0, times = 4 * 4), rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) %&gt;% rep(., times = 3), rep(0, times = 4 * 8), rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) %&gt;% rep(., times = 2), rep(0, times = 4 * 4 * 3), rep(0:1, times = c(3, 1)) %&gt;% rep(., times = 3), rep(0, times = 4) ) ) We’re finally ready to plot our Figure 2.4. d %&gt;% ggplot(aes(x = position, y = draw)) + geom_vline(xintercept = c(0, 4, 8), color = &quot;white&quot;, size = 2/3) + geom_segment(data = lines_1, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_segment(data = lines_2, aes(x = x, xend = xend, y = y, yend = yend, alpha = remain %&gt;% as.character()), size = 1/3) + geom_point(aes(fill = fill, size = draw, alpha = remain %&gt;% as.character()), shape = 21) + scale_fill_manual(values = c(&quot;navy&quot;, &quot;white&quot;)) + scale_size_continuous(range = c(3, 1.5)) + scale_alpha_manual(values = c(0.2, 1)) + scale_x_continuous(NULL, limits = c(0, 12), breaks = NULL) + scale_y_continuous(NULL, limits = c(0.75, 3.5), breaks = NULL) + coord_polar() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) 2.1.2 Combining other information. We may have additional information about the relative plausibility of each conjecture. This information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Whatever the source, it would help to have a way to combine different sources of information to update the plausibilities. Luckily there is a natural solution: Just multiply the counts. (p. 25) Here’s how to make a version of the table in the middle of page 25. # update t t &lt;- t %&gt;% mutate(nc = blue1 * product) # format the table t %&gt;% transmute(Conjecture = str_c(&quot;[&quot;, d1, &quot; &quot;, d2, &quot; &quot;, d3, &quot; &quot;, d4, &quot;]&quot;), `Ways to produce [b]` = blue1, `Prior counts` = product, `New count` = str_c(blue1, &quot; * &quot;, product, &quot; = &quot;, nc)) %&gt;% flextable() %&gt;% width(width = c(1, 1, 0.8, 1)) %&gt;% align(align = &quot;center&quot;, part = &quot;all&quot;) %&gt;% align(j = 4, align = &quot;left&quot;, part = &quot;all&quot;) %&gt;% valign(valign = &quot;bottom&quot;, part = &quot;header&quot;) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-3b34f2a8{border-collapse:collapse;}.cl-3b2f1892{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3b2f2878{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b2f288c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b2f2896{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b2f28a0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b2f63d8{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b2f63ec{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b2f63f6{width:57.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b2f6400{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b2f6401{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b2f640a{width:57.6pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b2f6414{width:72pt;background-color:transparent;vertical-align: bottom;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b2f6415{width:72pt;background-color:transparent;vertical-align: bottom;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b2f641e{width:57.6pt;background-color:transparent;vertical-align: bottom;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ConjectureWays to produce [b]Prior countsNew count[w w w w]000 * 0 = 0[b w w w]131 * 3 = 3[b b w w]282 * 8 = 16[b b b w]393 * 9 = 27[b b b b]404 * 0 = 0 We might update to reproduce the table a the top of page 26, like this. # update t t &lt;- t %&gt;% rename(pc = nc) %&gt;% mutate(fc = c(0, 3:0)) %&gt;% mutate(nc = pc * fc) # format the table t %&gt;% transmute(Conjecture = str_c(&quot;[&quot;, d1, &quot; &quot;, d2, &quot; &quot;, d3, &quot; &quot;, d4, &quot;]&quot;), `Prior count` = pc, `Factory count` = fc, `New count` = str_c(pc, &quot; * &quot;, fc, &quot; = &quot;, nc)) %&gt;% flextable() %&gt;% width(width = c(1, 1, 0.8, 1)) %&gt;% align(align = &quot;center&quot;, part = &quot;all&quot;) %&gt;% align(j = 4, align = &quot;left&quot;, part = &quot;all&quot;) %&gt;% valign(valign = &quot;bottom&quot;, part = &quot;header&quot;) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-3b4732d8{border-collapse:collapse;}.cl-3b417ab4{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3b418a04{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b418a18{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b418a22{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b418a2c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b41c56e{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b41c582{width:57.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b41c58c{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b41c596{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b41c5a0{width:57.6pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b41c5aa{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b41c5b4{width:72pt;background-color:transparent;vertical-align: bottom;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b41c5be{width:57.6pt;background-color:transparent;vertical-align: bottom;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b41c5bf{width:72pt;background-color:transparent;vertical-align: bottom;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}ConjecturePrior countFactory countNew count[w w w w]000 * 0 = 0[b w w w]333 * 3 = 9[b b w w]16216 * 2 = 32[b b b w]27127 * 1 = 27[b b b b]000 * 0 = 0 To learn more about dplyr::select() and dplyr::rename(), check out Baert’s exhaustive blog post, Data wrangling part 1: Basic to advanced ways to select columns. 2.1.2.1 Rethinking: Original ignorance. Which assumption should we use, when there is no previous information about the conjectures? The most common solution is to assign an equal number of ways that each conjecture could be correct, before seeing any data. This is sometimes known as the principle of indifference: When there is no reason to say that one conjecture is more plausible than another, weigh all of the conjectures equally. This book does not use nor endorse “ignorance” priors. As we’ll see in later chapters, the structure of the model and the scientific context always provide information that allows us to do better than ignorance. (p. 26, emphasis in the original) 2.1.3 From counts to probability. The opening sentences in this subsection are important: “It is helpful to think of this strategy as adhering to a principle of honest ignorance: When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible” (p. 26, emphasis in the original). We can define our updated plausibility as plausibility of after seeing \\(\\propto\\) ways can produce \\(\\times\\) prior plausibility of . In other words, plausibility of \\(p\\) after \\(D_\\text{new}\\) \\(\\propto\\) ways \\(p\\) can produce \\(D_\\text{new} \\times\\) prior plausibility of \\(p\\). But since we have to standardize the results to get them into a probability metric, the full equation is \\[\\text{plausibility of } p \\text{ after } D_\\text{new} = \\frac{\\text{ ways } p \\text{ can produce } D_\\text{new} \\times \\text{ prior plausibility of } p}{\\text{sum of the products}}.\\] You might make a version of the table in the middle of page 27 like this. # update t t %&gt;% rename(ways = product) %&gt;% mutate(p = blue1 / 4) %&gt;% mutate(pl = ways / sum(ways)) %&gt;% transmute(`Possible composition` = str_c(&quot;[&quot;, d1, &quot; &quot;, d2, &quot; &quot;, d3, &quot; &quot;, d4, &quot;]&quot;), p = p, `Ways to produce data` = ways, `Plausibility` = pl) %&gt;% # format for the table flextable() %&gt;% width(width = c(1.8, 1, 1.2, 1)) %&gt;% align(align = &quot;center&quot;, part = &quot;all&quot;) %&gt;% valign(valign = &quot;bottom&quot;, part = &quot;header&quot;) %&gt;% italic(j = 2, part = &quot;header&quot;) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-3b58ea3c{border-collapse:collapse;}.cl-3b531300{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3b531314{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3b5320b6{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b5320ca{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b5351c6{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b5351da{width:129.6pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b5351e4{width:86.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b5351ee{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b5351f8{width:129.6pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b5351f9{width:86.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b535202{width:72pt;background-color:transparent;vertical-align: bottom;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b53520c{width:129.6pt;background-color:transparent;vertical-align: bottom;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b535216{width:86.4pt;background-color:transparent;vertical-align: bottom;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}Possible compositionpWays to produce dataPlausibility[w w w w]0.0000.00[b w w w]0.2530.15[b b w w]0.5080.40[b b b w]0.7590.45[b b b b]1.0000.00 We just computed the plausibilities, but here’s McElreath’s R code 2.1. ways &lt;- c(0, 3, 8, 9, 0) ways / sum(ways) ## [1] 0.00 0.15 0.40 0.45 0.00 2.2 Building a model We might save our globe-tossing data in a tibble. (d &lt;- tibble(toss = c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;))) ## # A tibble: 9 x 1 ## toss ## &lt;chr&gt; ## 1 w ## 2 l ## 3 w ## 4 w ## 5 w ## 6 l ## 7 w ## 8 l ## 9 w 2.2.1 A data story. Bayesian data analysis usually means producing a story for how the data came to be. This story may be descriptive, specifying associations that can be used to predict outcomes, given observations. Or it may be causal, a theory of how some events produce other events. Typically, any story you intend to be causal may also be descriptive. But many descriptive stories are hard to interpret causally. But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data. (p. 28, emphasis in the original) 2.2.2 Bayesian updating. Here we’ll add the cumulative number of trials, n_trials, and the cumulative number of successes, n_successes (i.e., toss == \"w\"), to the data. ( d &lt;- d %&gt;% mutate(n_trials = 1:9, n_success = cumsum(toss == &quot;w&quot;)) ) ## # A tibble: 9 x 3 ## toss n_trials n_success ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 w 1 1 ## 2 l 2 1 ## 3 w 3 2 ## 4 w 4 3 ## 5 w 5 4 ## 6 l 6 4 ## 7 w 7 5 ## 8 l 8 5 ## 9 w 9 6 Fair warning: We don’t learn the skills for making Figure 2.5 until later in the chapter. So consider the data wrangling steps in this section as something of a preview. sequence_length &lt;- 50 d %&gt;% expand(nesting(n_trials, toss, n_success), p_water = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% group_by(p_water) %&gt;% # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html mutate(lagged_n_trials = lag(n_trials, k = 1), lagged_n_success = lag(n_success, k = 1)) %&gt;% ungroup() %&gt;% mutate(prior = ifelse(n_trials == 1, .5, dbinom(x = lagged_n_success, size = lagged_n_trials, prob = p_water)), likelihood = dbinom(x = n_success, size = n_trials, prob = p_water), strip = str_c(&quot;n = &quot;, n_trials)) %&gt;% # the next three lines allow us to normalize the prior and the likelihood, # putting them both in a probability metric group_by(n_trials) %&gt;% mutate(prior = prior / sum(prior), likelihood = likelihood / sum(likelihood)) %&gt;% # plot! ggplot(aes(x = p_water)) + geom_line(aes(y = prior), linetype = 2) + geom_line(aes(y = likelihood)) + scale_x_continuous(&quot;proportion water&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(&quot;plausibility&quot;, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ strip, scales = &quot;free_y&quot;) If it wasn’t clear in the code, the dashed curves are normalized prior densities. The solid ones are normalized likelihoods. If you don’t normalize (i.e., divide the density by the sum of the density), their respective heights don’t match up with those in the text. Furthermore, it’s the normalization that makes them directly comparable. To learn more about dplyr::group_by() and its opposite dplyr::ungroup(), check out R4DS, Chapter 5. To learn about tidyr::expand(), go here. 2.2.2.1 Rethinking: Sample size and reliable inference. It is common to hear that there is a minimum number of observations for a useful statistical estimate. For example, there is a widespread superstition that 30 observations are needed before one can use a Gaussian distribution. Why? In non-Bayesian statistical inference, procedures are often justified by the method’s behavior at very large sample sizes, so-called asymptotic behavior. As a result, performance at small samples sizes is questionable. In contrast, Bayesian estimates are valid for any sample size. This does not mean that more data isn’t helpful–it certainly is. Rather, the estimates have a clear and valid interpretation, no matter the sample size. But the price for this power is dependency upon the initial plausibilities, the prior. If the prior is a bad one, then the resulting inference will be misleading. There’s no free lunch, when it comes to learning about the world. (p. 31, emphasis in the original) 2.2.3 Evaluate. The Bayesian model learns in a way that is demonstrably optimal, provided that it accurately describes the real, large world. This is to say that your Bayesian machine guarantees perfect inference within the small world. No other way of using the available information, beginning with the same state of information, could do better. Don’t get too excited about this logical virtue, however. The calculations may malfunction, so results always have to be checked. And if there are important differences between the model and reality, then there is no logical guarantee of large world performance. And even if the two worlds did match, any particular sample of data could still be misleading. (p. 31) 2.2.3.1 Rethinking: Deflationary statistics. It may be that Bayesian inference is the best general purpose method of inference known. However, Bayesian inference is much less powerful than we’d like it to be. There is no approach to inference that provides universal guarantees. No branch of applied mathematics has unfettered access to reality, because math is not discovered, like the proton. Instead it is invented, like the shovel. (p. 32) This stance brushes up against what is sometimes called mathematical platonism, which is a position I suspect is causally held among many scientists and laypersons, alike. For more on the topic, check out Platonism in the philosophy of mathematics (Linnebo, 2018). 2.3 Components of the model We can sum up the components of the model as three things: a likelihood function: “the number of ways each conjecture could produce an observation,” one or more parameters: “the accumulated number of ways each conjecture cold produce the entire data,” and a prior: “the initial plausibility of each conjectured cause of the data” (p. 32). 2.3.1 Variables. Variables are just symbols that can take on different values. In a scientific context, variables include things we wish to infer, such as proportions and rates, as well as things we might observe, the data…. Unobserved variables are usually called parameters. (p. 32, emphasis in the original) 2.3.2 Definitions. Once we have the variables listed, we then have to define each of them. In defining each, we build a model that relates the variables to one another. Remember, the goal is to count all the ways the data could arise, given the assumptions. (p. 33) 2.3.2.1 Observed variables. So that we don’t have to literally count, we can use a mathematical function that tells us the right plausibility. In conventional statistics, a distribution function assigned to an observed variable is usually called a likelihood. (p. 33, emphasis in the original) If you let the count of water be \\(w\\) and the count of land be \\(l\\), then the binomial likelihood for the globe-tossing data may be expressed as \\[\\operatorname{Pr} (w, l| p) = \\frac{(w + l)!}{w!l!} p^w (1 - p)^l.\\] As McElreath wrote, we can read that as: \"The counts of ‘water’ W and ‘land’ L are distributed binomially, with probability \\(p\\) of ‘water’ on each toss. (p. 33, emphasis in the original). Given a probability of .5, we can use the dbinom() function to determine the likelihood of 6 out of 9 tosses coming out water. dbinom(x = 6, size = 9, prob = .5) ## [1] 0.1640625 McElreath suggested we change the values of prob. Here is a way to do so over the parameter space, \\([0, 1]\\). tibble(prob = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) + geom_line() + labs(x = &quot;probability&quot;, y = &quot;binomial likelihood&quot;) + theme(panel.grid = element_blank()) 2.3.2.1.1 Overthinking: Names and probability distributions. The “d” in dbinom stands for density. Functions named in this way almost always have corresponding partners that begin with “r” for random samples and that begin with “p” for cumulative probabilities. See for example the help ?dbinom. (p. 34, emphasis in the original) 2.3.2.2 Unobserved variables. The distributions we assign to the observed variables typically have their own variables. In the binomial above, there is $pv, the probability of sampling water. Since \\(p\\) is not observed, we usually call it a parameter. Even though we cannot observe \\(p\\), we still have to define it. (p. 34, emphasis in the original) 2.3.2.3 Overthinking: Prior as a probability distribution McElreath said that “for a uniform prior from \\(a\\) to \\(b\\), the probability of any point in the interval is \\(1 / (b - a)\\)” (p. 35). Let’s try that out. To keep things simple, we’ll hold \\(a\\) constant while varying the values for \\(b\\). tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% mutate(prob = 1 / (b - a)) ## # A tibble: 5 x 3 ## a b prob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 1 ## 2 0 1.5 0.667 ## 3 0 2 0.5 ## 4 0 3 0.333 ## 5 0 9 0.111 I like to verify things with plots. tibble(a = 0, b = c(1, 1.5, 2, 3, 9)) %&gt;% expand(nesting(a, b), parameter_space = seq(from = 0, to = 9, length.out = 500)) %&gt;% mutate(prob = dunif(parameter_space, a, b), b = str_c(&quot;b = &quot;, b)) %&gt;% ggplot(aes(x = parameter_space, y = prob)) + geom_area() + scale_x_continuous(breaks = c(0, 1:3, 9)) + scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1), labels = c(&quot;0&quot;, &quot;1/9&quot;, &quot;1/3&quot;, &quot;1/2&quot;, &quot;2/3&quot;, &quot;1&quot;)) + theme(panel.grid.minor = element_blank(), panel.grid.major.x = element_blank()) + facet_wrap(~ b, ncol = 5) As we’ll learn much later in the project, the \\(\\operatorname{Uniform}(0, 1)\\) distribution is special in that we can also express it as the beta distribution for which \\(\\alpha = 1 \\text{ and } \\beta = 1\\). E.g., tibble(parameter_space = seq(from = 0, to = 1, length.out = 50)) %&gt;% mutate(prob = dbeta(parameter_space, 1, 1)) %&gt;% ggplot(aes(x = parameter_space, y = prob)) + geom_area() + scale_y_continuous(&quot;density&quot;, limits = c(0, 2)) + ggtitle(expression(&quot;This is beta&quot;*(1*&quot;, &quot;*1))) + theme(panel.grid = element_blank()) 2.3.2.4 Rethinking: Datum or parameter? It is typical to conceive of data and parameters as completely different kinds of entities. Data are measured and known; parameters are unknown and must be estimated from data. Usefully, in the Bayesian framework the distinction between a datum and a parameter is not so fundamental. (p. 35) For more in this topic, check out McElreath’s lecture, Understanding Bayesian statistics without frequentist language. 2.3.3 A model is born. We can now describe our observed variables, \\(w\\) and \\(l\\), with parameters within the binomial likelihood, our shorthand notation for which is \\[w \\sim \\operatorname{Binomial}(n, p),\\] where \\(n = w + l\\). Our binomial likelihood contains a parameter for an unobserved variable, \\(p\\). Parameters in Bayesian models are assigned priors and we can report our prior for \\(p\\) as \\[p \\sim \\operatorname{Uniform}(0, 1),\\] which expresses the model assumption that the entire range of possible values for \\(p\\), \\([0, 1\\), are equally plausible. 2.4 Making the model go For every unique combination of data, likelihood, parameters, and prior, there is a unique posterior distribution. This distribution contains the relative plausibility of different parameter values, conditional on the data and model. The posterior distribution takes the form of the probability of the parameters, conditional on the data. (p. 36, emphasis added) 2.4.1 Bayes’ theorem. We already know about our values for \\(w\\), \\(l\\), and, by logical necessity, \\(n\\). Bayes’ theorem will allow us to determine the plausibility of various values of \\(p\\), given \\(w\\) and \\(l\\), which we can express formally as \\(\\operatorname{Pr}(p | w, l)\\). Building on some of the earlier equations on page 37, Bayes’ theorem tells us that \\[\\operatorname{Pr}(p | w, l) = \\frac{\\operatorname{Pr}(w, l | p) \\operatorname{Pr}(p )}{\\operatorname{Pr}(w, l)}.\\] And this is Bayes’ theorem. It says that the probability of any particular value of \\(p\\), considering the data, is equal to the product of the relative plausibility of the data, conditional on \\(p\\), and the prior plausibility of \\(p\\), divided by this thing \\(\\operatorname{Pr}(W, L)\\), which I’ll call the average probability of the data. (p. 37, emphasis in the original) We can express this in words as \\[\\text{Posterior} = \\frac{\\text{Probability of the data} \\times \\text{Prior}}{\\text{Average probability of the data}}.\\] The average probability of the data is often called the “evidence” or the “average likelihood” and we’ll get a sense of what that means as we go along. “The key lesson is that the posterior is proportional to the product of the prior and the probability of the data” (p. 37). Figure 2.6 will help us see what this means. Here are the preparatory steps for the data. sequence_length &lt;- 1e3 d &lt;- tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% expand(probability, row = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;)) %&gt;% arrange(row, probability) %&gt;% mutate(prior = ifelse(row == &quot;flat&quot;, 1, ifelse(row == &quot;stepped&quot;, rep(0:1, each = sequence_length / 2), exp(-abs(probability - 0.5) / .25) / ( 2 * 0.25))), likelihood = dbinom(x = 6, size = 9, prob = probability)) %&gt;% group_by(row) %&gt;% mutate(posterior = prior * likelihood / sum(prior * likelihood)) %&gt;% pivot_longer(prior:posterior) %&gt;% ungroup() %&gt;% mutate(name = factor(name, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)), row = factor(row, levels = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;))) To learn more about dplyr::arrange(), chech out R4DS, Chapter 5.3. In order to avoid unnecessary facet labels for the rows, it was easier to just make each column of the plot separately. We can then use the elegant and powerful syntax from Thomas Lin Pedersen’s (2019) patchwork package to combine them. p1 &lt;- d %&gt;% filter(row == &quot;flat&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;) p2 &lt;- d %&gt;% filter(row == &quot;stepped&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;) p3 &lt;- d %&gt;% filter(row == &quot;Laplace&quot;) %&gt;% ggplot(aes(x = probability, y = value)) + geom_line() + scale_x_continuous(NULL, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank(), strip.background = element_blank(), strip.text = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;) # combine library(patchwork) p1 / p2 / p3 I’m not sure if it’s the same McElreath used in the text, but the formula I used for the triangle-shaped prior is the Laplace distribution with a location of 0.5 and a dispersion of 0.25. Also, to learn all about dplyr::filter(), check out Baert’s Data wrangling part 3: Basic and more advanced ways to filter rows. 2.4.2 Motors. Various numerical techniques are needed to approximate the mathematics that follows from the definition of Bayes’ theorem. In this book, you’ll meet three different conditioning engines, numerical techniques for computing posterior distributions: Grid approximation Quadratic approximation Markov chain Monte Carlo (MCMC) There are many other engines, and new ones are being invented all the time. But the three you’ll get to know here are common and widely useful. (p. 39) ️ In this translation of McElreath’s text, we will get a little practice with grid approximation and the quadratic approximation. But since our aim is to practice with brms, we’ll jump rather quickly into MCMC. This will be awkward at times because it will force us to contend with technical issues in earlier problems in the text than McElreath originally did. I’ll do what I can to bridge the pedagogical gaps. 2.4.3 Grid approximation. Continuing on with our globe-tossing example, at any particular value of a parameter, \\(p&#39;\\) , it’s a simple matter to compute the posterior probability: just multiply the prior probability of \\(p&#39;\\) by the likelihood at \\(p&#39;\\). Repeating this procedure for each value in the grid generates an approximate picture of the exact posterior distribution. This procedure is called grid approximation. (pp. 39–40, emphasis in the original) We just employed grid approximation over the last figure. To get nice smooth lines, we computed the posterior over 1,000 evenly-spaced points on the probability space. Here we’ll prepare for Figure 2.7 with 20. ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20), # define grid prior = 1) %&gt;% # define prior mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% # compute likelihood at each value in grid mutate(unstd_posterior = likelihood * prior) %&gt;% # compute product of likelihood and prior mutate(posterior = unstd_posterior / sum(unstd_posterior)) # standardize the posterior, so it sums to 1 ) ## # A tibble: 20 x 5 ## p_grid prior likelihood unstd_posterior posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 0 ## 2 0.0526 1 0.00000152 0.00000152 0.000000799 ## 3 0.105 1 0.0000819 0.0000819 0.0000431 ## 4 0.158 1 0.000777 0.000777 0.000409 ## 5 0.211 1 0.00360 0.00360 0.00189 ## 6 0.263 1 0.0112 0.0112 0.00587 ## 7 0.316 1 0.0267 0.0267 0.0140 ## 8 0.368 1 0.0529 0.0529 0.0279 ## 9 0.421 1 0.0908 0.0908 0.0478 ## 10 0.474 1 0.138 0.138 0.0728 ## 11 0.526 1 0.190 0.190 0.0999 ## 12 0.579 1 0.236 0.236 0.124 ## 13 0.632 1 0.267 0.267 0.140 ## 14 0.684 1 0.271 0.271 0.143 ## 15 0.737 1 0.245 0.245 0.129 ## 16 0.789 1 0.190 0.190 0.0999 ## 17 0.842 1 0.118 0.118 0.0621 ## 18 0.895 1 0.0503 0.0503 0.0265 ## 19 0.947 1 0.00885 0.00885 0.00466 ## 20 1 1 0 0 0 Here’s the code for the right panel of Figure 2.7. p1 &lt;- d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;20 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) Now here’s the code for the left hand panel of Figure 2.7. p2 &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 5), prior = 1) %&gt;% mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% mutate(unstd_posterior = likelihood * prior) %&gt;% mutate(posterior = unstd_posterior / sum(unstd_posterior)) %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_point() + geom_line() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) Here we combine them and plot! p2 + p1 + plot_annotation(title = &quot;More grid points make for smoother approximations&quot;) In his R code 2.5 box, McElreath encouraged us to redo those plots with the two new kinds of priors. prior &lt;- ifelse( p_grid &lt; 0.5 , 0 , 1 ) prior &lt;- exp( -5*abs( p_grid - 0.5 ) ) Here’s a condensed way to make the four plots all at once. # make the data tibble(n_points = c(5, 20)) %&gt;% mutate(p_grid = map(n_points, ~seq(from = 0, to = 1, length.out = .))) %&gt;% unnest(p_grid) %&gt;% expand(nesting(n_points, p_grid), priors = c(&quot;ifelse(p_grid &lt; 0.5, 0, 1)&quot;, &quot;exp(-5 * abs(p_grid - 0.5))&quot;)) %&gt;% mutate(prior = ifelse(priors == &quot;ifelse(p_grid &lt; 0.5, 0, 1)&quot;, ifelse(p_grid &lt; 0.5, 0, 1), exp(-5 * abs(p_grid - 0.5)))) %&gt;% mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) %&gt;% mutate(n_points = str_c(&quot;# points = &quot;, n_points), priors = str_c(&quot;prior = &quot;, priors)) %&gt;% # plot! ggplot(aes(x = p_grid, y = posterior)) + geom_line() + geom_point() + labs(subtitle = &quot;5 points&quot;, x = &quot;probability of water&quot;, y = &quot;posterior probability&quot;) + theme(panel.grid = element_blank()) + facet_grid(n_points ~ priors, scales = &quot;free&quot;) 2.4.4 Quadratic approximation. Under quite general conditions, the region near the peak of the posterior distribution will be nearly Gaussian–or “normal”–in shape. This means the posterior distribution can be usefully approximated by a Gaussian distribution. A Gaussian distribution is convenient, because it can be completely described by only two numbers: the location of its center (mean) and its spread (variance). A Gaussian approximation is called “quadratic approximation” because the logarithm of a Gaussian distribution forms a parabola. And a parabola is a quadratic function. So this approximation essentially represents any log-posterior with a parabola. (p. 42, emphasis added) Though McElreath will use the quadratic approximation fir the first half of the text, we won’t use it much past this chapter. Here, though, we’ll apply the quadratic approximation to the globe tossing data with the rethinking::quap() function. library(rethinking) globe_qa &lt;- quap( alist( W ~ dbinom(W + L, p), # binomial likelihood p ~ dunif(0, 1) # uniform prior ), data = list(W = 6, L = 3) ) # display summary of quadratic approximation precis(globe_qa) ## mean sd 5.5% 94.5% ## p 0.6666666 0.1571338 0.4155365 0.9177968 In preparation for Figure 2.8, here’s the model with \\(n = 18\\) and \\(n = 36\\). globe_qa_18 &lt;- quap( alist( w ~ dbinom(9 * 2, p), p ~ dunif(0, 1) ), data = list(w = 6 * 2)) globe_qa_36 &lt;- quap( alist( w ~ dbinom(9 * 4, p), p ~ dunif(0, 1) ), data = list(w = 6 * 4)) precis(globe_qa_18) ## mean sd 5.5% 94.5% ## p 0.6666662 0.1111104 0.4890903 0.8442421 precis(globe_qa_36) ## mean sd 5.5% 94.5% ## p 0.6666646 0.07856714 0.5410991 0.79223 Now make Figure 2.8. n_grid &lt;- 100 # wrangle tibble(w = c(6, 12, 24), n = c(9, 18, 36), s = c(.16, .11, .08)) %&gt;% expand(nesting(w, n, s), p_grid = seq(from = 0, to = 1, length.out = n_grid)) %&gt;% mutate(prior = 1, m = .67) %&gt;% mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %&gt;% mutate(unstd_grid_posterior = likelihood * prior, unstd_quad_posterior = dnorm(p_grid, m, s)) %&gt;% group_by(w) %&gt;% mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior), quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior), n = str_c(&quot;n = &quot;, n)) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 9&quot;, &quot;n = 18&quot;, &quot;n = 36&quot;))) %&gt;% # plot ggplot(aes(x = p_grid)) + geom_line(aes(y = grid_posterior)) + geom_line(aes(y = quad_posterior), color = &quot;grey50&quot;) + labs(x = &quot;proportion water&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~ n, scales = &quot;free&quot;) This phenomenon, where the quadratic approximation improves with the amount of data, is very common. It’s one of the reasons that so many classical statistical procedures are nervous about small samples: Those procedures use quadratic (or other) approximations that are only known to be safe with infinite data. Often, these approximations are useful with less than infinite data, obviously. But the rate of improvement as sample size increases varies greatly depending upon the details. In some models, the quadratic approximation can remain terrible even with thousands of samples. (p. 44) 2.4.4.1 Rethinking: Maximum likelihood estimation. The quadratic approximation, either with a uniform prior or with a lot of data, is often equivalent to a maximum likelihood estimate (MLE) and its standard error. The MLE is a very common non-Bayesian parameter estimate. This correspondence between a Bayesian approximation and a common non-Bayesian estimator is both a blessing and a curse. It is a blessing, because it allows us to re-interpret a wide range of published non-Bayesian model fits in Bayesian terms. It is a curse, because maximum likelihood estimates have some curious drawbacks, and the quadratic approximation can share them. (p. 44, emphasis, in the original) Textbooks highlighting the maximum likelihood method for the generalized linear model abound. If this is new to you and you’d like to learn more, perhaps check out Linger and Roback’s (2019) Broadening your statistical horizons: Generalized linear models and multilevel models, Agresti’s (2015) Foundations of linear and generalized linear models or Dunn and Smyth’s (2018) Generalized linear models with examples in R. 2.4.5 Markov chain Monte Carlo. The most popular [alternative to grid approximation and the quadratic approximation] is Markov chain Monte Carlo (MCMC), which is a family of conditioning engines capable of handling highly complex models. It is fair to say that MCMC is largely responsible for the insurgence of Bayesian data analysis that began in the 1990s. While MCMC is older than the 1990s, affordable computer power is not, so we must also thank the engineers. Much later in the book (Chapter 9), you’ll meet simple and precise examples of MCMC model fitting, aimed at helping you understand the technique. (p. 45, emphasis in the original) The brms package uses a version of MCMC to fit Bayesian models. Since one of the main goals of this project is to highlight brms, we may as fit a model. This seems like an appropriately named subsection to do so. First we’ll have to load the package. library(brms) If you haven’t already installed brms, you can find instructions on how to do so here. Here we re-fit the last model from above, the one for which \\(w = 24\\) and \\(n = 36\\). b2.1 &lt;- brm(data = list(w = 24), family = binomial(link = &quot;identity&quot;), w | trials(36) ~ 0 + Intercept, prior(beta(1, 1), class = b, lb = 0, ub = 1), seed = 2, file = &quot;fits/b02.01&quot;) The model output from brms looks like so. print(b2.1) ## Family: binomial ## Links: mu = identity ## Formula: w | trials(36) ~ 0 + Intercept ## Data: list(w = 24) (Number of observations: 1) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.66 0.08 0.50 0.80 1.00 1483 1350 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). There’s a lot going on in that output, which we’ll start to clarify in Chapter 4. For now, focus on the ‘Intercept’ line. As we’ll also learn in Chapter 4, the intercept of a regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial, \\(\\theta\\). Also, with brms, there are many ways to summarize the results of a model. The brms::posterior_summary() function is an analogue to rethinking::precis(). We will, however, need to use round() to reduce the output to a reasonable number of decimal places. posterior_summary(b2.1) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.66 0.08 0.5 0.80 ## lp__ -4.00 0.79 -6.1 -3.46 The b_Intercept row is the probability. Don’t worry about the second line, for now. We’ll cover the details of brms model fitting in later chapters. To finish up, why not plot the results of our model and compare them with those from rethinking::quap(), above? posterior_samples(b2.1) %&gt;% mutate(n = &quot;n = 36&quot;) %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(fill = &quot;black&quot;) + scale_x_continuous(&quot;proportion water&quot;, limits = c(0, 1)) + theme(panel.grid = element_blank()) + facet_wrap(~ n) If you’re still confused. Cool. This is just a preview. We’ll start walking through fitting models with brms in Chapter 4 and we’ll learn a lot about regression with the binomial likelihood in Chapter 11. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.15.0 Rcpp_1.0.6 rethinking_2.13 rstan_2.21.2 ## [5] StanHeaders_2.21.0-7 patchwork_1.1.1 flextable_0.6.4 forcats_0.5.1 ## [9] stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 ## [13] tidyr_1.1.3 tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 uuid_0.1-4 backports_1.2.1 systemfonts_1.0.1 ## [5] plyr_1.8.6 igraph_1.2.6 splines_4.0.4 crosstalk_1.1.0.1 ## [9] TH.data_1.0-10 rstantools_2.1.1 inline_0.3.17 digest_0.6.27 ## [13] htmltools_0.5.1.1 rsconnect_0.8.16 fansi_0.4.2 magrittr_2.0.1 ## [17] modelr_0.1.8 RcppParallel_5.0.2 matrixStats_0.57.0 officer_0.3.17 ## [21] xts_0.12.1 sandwich_3.0-0 prettyunits_1.1.1 colorspace_2.0-0 ## [25] rvest_0.3.6 haven_2.3.1 xfun_0.22 callr_3.5.1 ## [29] crayon_1.4.1 jsonlite_1.7.2 lme4_1.1-25 survival_3.2-7 ## [33] zoo_1.8-8 glue_1.4.2 gtable_0.3.0 emmeans_1.5.2-1 ## [37] V8_3.4.0 pkgbuild_1.2.0 shape_1.4.5 abind_1.4-5 ## [41] scales_1.1.1 mvtnorm_1.1-1 emo_0.0.0.9000 DBI_1.1.0 ## [45] miniUI_0.1.1.1 xtable_1.8-4 stats4_4.0.4 DT_0.16 ## [49] htmlwidgets_1.5.2 httr_1.4.2 threejs_0.3.3 ellipsis_0.3.1 ## [53] pkgconfig_2.0.3 loo_2.4.1 farver_2.0.3 dbplyr_2.0.0 ## [57] utf8_1.1.4 tidyselect_1.1.0 labeling_0.4.2 rlang_0.4.10 ## [61] reshape2_1.4.4 later_1.1.0.1 munsell_0.5.0 cellranger_1.1.0 ## [65] tools_4.0.4 cli_2.3.1 generics_0.1.0 broom_0.7.5 ## [69] ggridges_0.5.2 evaluate_0.14 fastmap_1.0.1 processx_3.4.5 ## [73] knitr_1.31 fs_1.5.0 zip_2.1.1 nlme_3.1-152 ## [77] mime_0.10 projpred_2.0.2 xml2_1.3.2 compiler_4.0.4 ## [81] bayesplot_1.8.0 shinythemes_1.1.2 rstudioapi_0.13 curl_4.3 ## [85] gamm4_0.2-6 reprex_0.3.0 statmod_1.4.35 stringi_1.5.3 ## [89] highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 gdtools_0.2.2 ## [93] lattice_0.20-41 Matrix_1.3-2 nloptr_1.2.2.2 markdown_1.1 ## [97] shinyjs_2.0.0 vctrs_0.3.6 pillar_1.5.1 lifecycle_1.0.0 ## [101] bridgesampling_1.0-0 estimability_1.3 data.table_1.14.0 httpuv_1.5.4 ## [105] R6_2.5.0 bookdown_0.21 promises_1.1.1 gridExtra_2.3 ## [109] codetools_0.2-18 boot_1.3-26 colourpicker_1.1.0 MASS_7.3-53 ## [113] gtools_3.8.2 assertthat_0.2.1 withr_2.4.1 shinystan_2.5.0 ## [117] multcomp_1.4-16 mgcv_1.8-33 hms_0.5.3 grid_4.0.4 ## [121] coda_0.19-4 minqa_1.2.4 rmarkdown_2.7 shiny_1.5.0 ## [125] lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 "],["sampling-the-imaginary.html", "3 Sampling the Imaginary 3.1 Sampling from a grid-approximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction 3.4 Summary Let’s practice with brms Session info", " 3 Sampling the Imaginary If you would like to know the probability someone is a vampire given they test positive to the blood-based vampire test, you compute \\[\\text{Pr(vampire|positive)} = \\frac{\\text{Pr(positive|vampire) Pr(vampire)}}{\\text{Pr(positive)}}.\\] We’ll do so within a tibble. library(tidyverse) tibble(pr_positive_vampire = .95, pr_positive_mortal = .01, pr_vampire = .001) %&gt;% mutate(pr_positive = pr_positive_vampire * pr_vampire + pr_positive_mortal * (1 - pr_vampire)) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * pr_vampire / pr_positive) %&gt;% glimpse() ## Rows: 1 ## Columns: 5 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.01 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive &lt;dbl&gt; 0.01094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 Here’s the other way of tackling the vampire problem, this time using the frequency format. tibble(pr_vampire = 100 / 100000, pr_positive_vampire = 95 / 100, pr_positive_mortal = 99 / 99900) %&gt;% mutate(pr_positive = 95 + 999) %&gt;% mutate(pr_vampire_positive = pr_positive_vampire * 100 / pr_positive) %&gt;% glimpse() ## Rows: 1 ## Columns: 5 ## $ pr_vampire &lt;dbl&gt; 0.001 ## $ pr_positive_vampire &lt;dbl&gt; 0.95 ## $ pr_positive_mortal &lt;dbl&gt; 0.000990991 ## $ pr_positive &lt;dbl&gt; 1094 ## $ pr_vampire_positive &lt;dbl&gt; 0.08683729 The posterior distribution is a probability distribution. And like all probability distributions, we can imagine drawing samples from it. The sampled events in this case are parameter values. Most parameters have no exact empirical realization. The Bayesian formalism treats parameter distributions as relative plausibility, not as any physical random process. In any event, randomness is always a property of information, never of the real world. But inside the computer, parameters are just as empirical as the outcome of a coin flip or a die toss or an agricultural experiment. The posterior defines the expected frequency that different parameter values will appear, once we start plucking parameters out of it. (McElreath, 2020a, p. 50, emphasis in the original) 3.1 Sampling from a grid-approximate posterior Once again, here we use grid approximation to generate samples. # how many grid points would you like? n &lt;- 1000 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.00100 1 8.43e-17 8.43e-19 ## 3 0.00200 1 5.38e-15 5.38e-17 ## 4 0.00300 1 6.11e-14 6.11e-16 ## 5 0.00400 1 3.42e-13 3.42e-15 ## 6 0.00501 1 1.30e-12 1.30e-14 ## 7 0.00601 1 3.87e-12 3.88e-14 ## 8 0.00701 1 9.73e-12 9.74e-14 ## 9 0.00801 1 2.16e-11 2.16e-13 ## 10 0.00901 1 4.37e-11 4.38e-13 ## # … with 990 more rows Our d data contains all the components in McElreath’s R code 3.2 block. Do note we’ve renamed his prob_p and prob_data as prior and likelihood, respectively. Now we’ll use the dplyr::slice_sample() function to sample rows from d, saving them as sample. # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% slice_sample(n = n_samples, weight_by = posterior, replace = T) glimpse(samples) ## Rows: 10,000 ## Columns: 4 ## $ p_grid &lt;dbl&gt; 0.5645646, 0.6516517, 0.5475475, 0.5905906, 0.5955956, 0.7877878, 0.7267267, 0.… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ likelihood &lt;dbl&gt; 0.224559942, 0.271902722, 0.209666553, 0.244608692, 0.247990921, 0.191887140, 0… ## $ posterior &lt;dbl&gt; 2.247847e-03, 2.721749e-03, 2.098764e-03, 2.448535e-03, 2.482392e-03, 1.920792e… Now we can plot the left panel of Figure 3.1 with geom_point(). But before we do, we’ll need to add a variable numbering the samples. samples %&gt;% mutate(sample_number = 1:n()) %&gt;% ggplot(aes(x = sample_number, y = p_grid)) + geom_point(alpha = 1/10) + scale_y_continuous(&quot;proportion of water (p)&quot;, limits = c(0, 1)) + xlab(&quot;sample number&quot;) We’ll make the density in the right panel with geom_density(). samples %&gt;% ggplot(aes(x = p_grid)) + geom_density(fill = &quot;black&quot;) + scale_x_continuous(&quot;proportion of water (p)&quot;, limits = c(0, 1)) That was based on 1e4 samples. On page 53, McElreath said the density would converge on the idealized shape if we keep increasing the number of samples. Here’s what it looks like with 1e6. set.seed(3) d %&gt;% slice_sample(n = 1e6, weight_by = posterior, replace = T) %&gt;% ggplot(aes(x = p_grid)) + geom_density(fill = &quot;black&quot;) + scale_x_continuous(&quot;proportion of water (p)&quot;, limits = c(0, 1)) Yep, that’s more ideal. 3.2 Sampling to summarize “Once your model produces a posterior distribution, the model’s work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly how it is summarized depends upon your purpose” (p. 53). 3.2.1 Intervals of defined boundaries. To get the proportion of water less than some value of p_grid within the tidyverse, you might first filter() by that value and then take the sum() within summarise(). d %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = sum(posterior)) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.172 To learn more about dplyr::summarise() and related functions, check out Baert’s Data wrangling part 4: Summarizing and slicing your data and Section 5.6 of R4DS (Grolemund &amp; Wickham, 2017). If what you want is a frequency based on filtering by samples, then you might use n() within summarise(). samples %&gt;% filter(p_grid &lt; .5) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.163 A more explicit approach for the same computation is to follow up count() with mutate(). samples %&gt;% count(p_grid &lt; .5) %&gt;% mutate(probability = n / sum(n)) ## # A tibble: 2 x 3 ## `p_grid &lt; 0.5` n probability ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 8371 0.837 ## 2 TRUE 1629 0.163 An even trickier approach for the same is to insert the logical statement p_grid &lt; .5 within the mean() function. samples %&gt;% summarise(sum = mean(p_grid &lt; .5)) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.163 Much like McElreath discussed in the Overthinking: Counting with sum box, this works “because R internally converts a logical expression, like samples &lt; 0.5, to a vector of TRUE and FALSE results, one for each element of samples, saying whether or not each element matches the criterion” (p. 54). When we inserted that vector of TRUE and FALSE values within the mean() function, they were then internally converted to a vector of 1’s and 0’s, the mean of which was the probability. Tricky! To determine the posterior probability between 0.5 and 0.75, you can use &amp; within filter(). samples %&gt;% filter(p_grid &gt; .5 &amp; p_grid &lt; .75) %&gt;% summarise(sum = n() / n_samples) ## # A tibble: 1 x 1 ## sum ## &lt;dbl&gt; ## 1 0.606 Just multiply that value by 100 to get a percent. samples %&gt;% filter(p_grid &gt; .5 &amp; p_grid &lt; .75) %&gt;% summarise(sum = n() / n_samples, percent = 100 * n() / n_samples) ## # A tibble: 1 x 2 ## sum percent ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.606 60.6 And, of course, you can do that with our mean() trick, too. samples %&gt;% summarise(percent = 100 * mean(p_grid &gt; .5 &amp; p_grid &lt; .75)) ## # A tibble: 1 x 1 ## percent ## &lt;dbl&gt; ## 1 60.6 3.2.2 Intervals of defined mass. It is more common to see scientific journals reporting an interval of defined mass, usually known as a confidence interval. An interval of posterior probability, such as the ones we are working with, may instead be called a credible interval. We’re going to call it a compatibility interval instead, in order to avoid the unwarranted implications of “confidence” and “credibility.” What the interval indicates is a range of parameter values compatible with the model and data. The model and data themselves may not inspire confidence, in which case the interval will not either. (p. 54, emphasis in the original) As a part of this block quote, McElreath linked to endnote 54, which reads: “I learned this term from Sander Greenland and his collaborators. See Amrhein et al. (2019) and Gelman and Greenland (2019)” (p. 560). We’ll create the upper two panels for Figure 3.2 with geom_line(), geom_area(), some careful filtering, and a little patchwork syntax. # upper left panel p1 &lt;- d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_line() + geom_area(data = d %&gt;% filter(p_grid &lt; .5)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # upper right panel p2 &lt;- d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_line() + # note this next line is the only difference in code from the last plot geom_area(data = d %&gt;% filter(p_grid &lt; .75 &amp; p_grid &gt; .5)) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) library(patchwork) p1 + p2 We’ll come back for the lower two panels in a bit. Since we saved our p_grid samples within the well-named samples tibble, we’ll have to index with $ within quantile. (q_80 &lt;- quantile(samples$p_grid, prob = .8)) ## 80% ## 0.7627628 That value will come in handy for the lower left panel of Figure 3.2. For an alternative approach, we could select() the samples vector, extract it from the tibble with pull(), and then pump it into quantile(). samples %&gt;% pull(p_grid) %&gt;% quantile(prob = .8) ## 80% ## 0.7627628 We might also use quantile() within summarise(). samples %&gt;% summarise(`80th percentile` = quantile(p_grid, p = .8)) ## # A tibble: 1 x 1 ## `80th percentile` ## &lt;dbl&gt; ## 1 0.763 Here’s the summarise() approach with two probabilities. samples %&gt;% summarise(`10th percentile` = quantile(p_grid, p = .1), `90th percentile` = quantile(p_grid, p = .9)) ## # A tibble: 1 x 2 ## `10th percentile` `90th percentile` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.451 0.815 The tidyverse approach is nice in that that family of functions typically returns a data frame. But sometimes you just want your values in a numeric vector for the sake of quick indexing. In that case, base R quantile() shines. (q_10_and_90 &lt;- quantile(samples$p_grid, prob = c(.1, .9))) ## 10% 90% ## 0.4514515 0.8148148 Now we have our cutoff values saved as q_80 and q_10_and_90, we’re ready to make the bottom panels of Figure 3.2. # lower left panel p1 &lt;- d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_line() + geom_area(data = d %&gt;% filter(p_grid &lt; q_80)) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;lower 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel p2 &lt;- d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_line() + geom_area(data = d %&gt;% filter(p_grid &gt; q_10_and_90[1] &amp; p_grid &lt; q_10_and_90[2])) + annotate(geom = &quot;text&quot;, x = .25, y = .0025, label = &quot;middle 80%&quot;) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) p1 + p2 Now we follow along with McElreath’s R code 3.11 to compute a highly skewed posterior. We’ve already defined p_grid and prior within d, above. Here we’ll reuse them and update the rest of the columns. # here we update the `dbinom()` parameters n_success &lt;- 3 n_trials &lt;- 3 # update `d` d &lt;- d %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) # make the next part reproducible set.seed(3) # here&#39;s our new samples tibble ( samples &lt;- d %&gt;% slice_sample(n = n_samples, weight_by = posterior, replace = T) ) ## # A tibble: 10,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.717 1 0.368 0.00147 ## 2 0.652 1 0.277 0.00111 ## 3 0.548 1 0.164 0.000656 ## 4 1 1 1 0.00400 ## 5 0.991 1 0.973 0.00389 ## 6 0.788 1 0.489 0.00195 ## 7 0.940 1 0.830 0.00332 ## 8 0.817 1 0.545 0.00218 ## 9 0.955 1 0.871 0.00348 ## 10 0.449 1 0.0908 0.000363 ## # … with 9,990 more rows The rethinking::PI() function works like a nice shorthand for quantile(). quantile(samples$p_grid, prob = c(.25, .75)) ## 25% 75% ## 0.7087087 0.9349349 rethinking::PI(samples$p_grid, prob = .5) ## 25% 75% ## 0.7087087 0.9349349 Now’s a good time to introduce Matthew Kay’s (2020c) tidybayes package, which offers an array of convenience functions for summarizing Bayesian models of the type we’ll be working with in this project. For all the brms-related deets, see Kay’s (2020a) vignette, Extracting and visualizing tidy draws from brms models. Here we start simple. library(tidybayes) median_qi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.8428428 0.7087087 0.9349349 0.5 median qi The tidybayes package contains a family of functions that make it easy to summarize a distribution with a measure of central tendency accompanied by intervals. With median_qi(), we asked for the median and quantile-based intervals–just like we’ve been doing with quantile(). Note how the .width argument within median_qi() worked the same way the prob argument did within rethinking::PI(). With .width = .5, we indicated we wanted a quantile-based 50% interval, which was returned in the ymin and ymax columns. The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we’ll request 50%, 80%, and 99% intervals. median_qi(samples$p_grid, .width = c(.5, .8, .99)) ## y ymin ymax .width .point .interval ## 1 0.8428428 0.7087087 0.9349349 0.50 median qi ## 2 0.8428428 0.5705706 0.9749750 0.80 median qi ## 3 0.8428428 0.2562563 0.9989990 0.99 median qi The .width column in the output indexed which line presented which interval. The value in the y column remained constant across rows. That’s because that column listed the measure of central tendency, the median in this case. Now let’s use the rethinking::HPDI() function to return 50% highest posterior density intervals (HPDIs). rethinking::HPDI(samples$p_grid, prob = .5) ## |0.5 0.5| ## 0.8418418 0.9989990 The reason I introduce tidybayes now is that the functions of the brms package only support percentile-based intervals of the type we computed with quantile() and median_qi(). But tidybayes also supports HPDIs. mode_hdi(samples$p_grid, .width = .5) ## y ymin ymax .width .point .interval ## 1 0.9563768 0.8418418 0.998999 0.5 mode hdi This time we used the mode as the measure of central tendency. With this family of tidybayes functions, you specify the measure of central tendency in the prefix (i.e., mean, median, or mode) and then the type of interval you’d like (i.e., qi or hdi). If all you want are the intervals without the measure of central tendency or all that other technical information, tidybayes also offers the handy qi() and hdi() functions. qi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.7087087 0.9349349 hdi(samples$p_grid, .width = .5) ## [,1] [,2] ## [1,] 0.8418418 0.998999 These are nice in that they return simple numeric vectors, making them particularly useful to use as references within ggplot2 plots. Now we have that skill, we can use it to make Figure 3.3. # lower left panel p1 &lt;- d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + # check out our sweet `qi()` indexing geom_area(data = d %&gt;% filter(p_grid &gt; qi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; qi(samples$p_grid, .width = .5)[2]), fill = &quot;grey75&quot;) + geom_line() + labs(subtitle = &quot;50% Percentile Interval&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # lower right panel p2 &lt;- d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(data = . %&gt;% filter(p_grid &gt; hdi(samples$p_grid, .width = .5)[1] &amp; p_grid &lt; hdi(samples$p_grid, .width = .5)[2]), fill = &quot;grey75&quot;) + geom_line() + labs(subtitle = &quot;50% HPDI&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) # combine! p1 | p2 In the geom_area() line for the HPDI plot, did you notice how we replaced d with .? When using the pipe (i.e., %&gt;%), you can use the . as a placeholder for the original data object. It’s an odd and handy trick to know about. Go here to learn more. So the HPDI has some advantages over the PI. But in most cases, these two types of interval are very similar. They only look so different in this case because the posterior distribution is highly skewed. If we instead used samples from the posterior distribution for six waters in nine tosses, these intervals would be nearly identical. Try it for yourself, using different probability masses, such as prob=0.8 and prob=0.95. When the posterior is bell shaped, it hardly matters which type of interval you use. (p. 57) Let’s try it out. First we’ll update the simulation for six waters in nine tosses. # &quot;six waters in nine tosses&quot; n_success &lt;- 6 n_trials &lt;- 9 new_d &lt;- d %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(posterior)) set.seed(3) new_samples &lt;- new_d %&gt;% slice_sample(n = n_samples, weight_by = posterior, replace = T) Here are the intervals by .width and type of .interval. bind_rows(mean_hdi(new_samples$p_grid, .width = c(.8, .95)), mean_qi(new_samples$p_grid, .width = c(.8, .95))) %&gt;% select(.width, .interval, ymin:ymax) %&gt;% arrange(.width) %&gt;% mutate_if(is.double, round, digits = 2) ## .width .interval ymin ymax ## 1 0.80 hdi 0.49 0.84 ## 2 0.80 qi 0.45 0.81 ## 3 0.95 hdi 0.37 0.89 ## 4 0.95 qi 0.35 0.88 We didn’t need that last mutate_if() line. It just made it easier to compare the ymin and ymax values. Anyway, McElreath was right. This time the differences between the HPDIs and QIs were trivial. Here’s a look at the posterior. new_d %&gt;% ggplot(aes(x = p_grid)) + geom_line(aes(y = posterior)) + labs(subtitle = &quot;Six waters in nine tosses made\\nfor a more symmetrical posterior&quot;, x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) Be warned: The HPDI also has some disadvantages. HPDI is more computationally intensive than PI and suffers from greater simulation variance, which is a fancy way of saying that it is sensitive to how many samples you draw from the posterior. It is also harder to understand and many scientific audiences will not appreciate its features, while they will immediately understand a percentile interval, as ordinary non-Bayesian intervals are typically interpreted (incorrectly) as percentile intervals (pp. 57–58, emphasis in the original) For convenience, we’ll primarily stick to the PI-based intervals in this ebook. 3.2.2.1 Rethinking: What do compatibility intervals mean? At the start of this section, McElreath poked a little at frequentist confidence intervals. For an introduction to confidence intervals from the perspective of a frequentist, you might check out Cumming’s (2014) The new statistics: Why and how and the works referenced therein. Though their definition isn’t the most intuitive, I usually use confidence intervals when I wear my frequentist hat. 3.2.3 Point estimates. We’ve been calling point estimates measures of central tendency. If we arrange() our d tibble in descending order by posterior, we’ll see the corresponding p_grid value for its MAP estimate. d %&gt;% arrange(desc(posterior)) ## # A tibble: 1,000 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 0.00400 ## 2 0.999 1 0.997 0.00398 ## 3 0.998 1 0.994 0.00397 ## 4 0.997 1 0.991 0.00396 ## 5 0.996 1 0.988 0.00395 ## 6 0.995 1 0.985 0.00394 ## 7 0.994 1 0.982 0.00392 ## 8 0.993 1 0.979 0.00391 ## 9 0.992 1 0.976 0.00390 ## 10 0.991 1 0.973 0.00389 ## # … with 990 more rows To emphasize it, we can use slice() to select the top row. d %&gt;% arrange(desc(posterior)) %&gt;% slice(1) ## # A tibble: 1 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 0.00400 We can get the mode with mode_hdi() or mode_qi(). samples %&gt;% mode_hdi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.475 1 0.95 mode hdi samples %&gt;% mode_qi(p_grid) ## # A tibble: 1 x 6 ## p_grid .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.956 0.399 0.994 0.95 mode qi Those returned a lot of output in addition to the mode. If all you want is the mode itself, you can just use tidybayes::Mode(). Mode(samples$p_grid) ## [1] 0.9563768 Medians and means are typical measures of central tendency, too. samples %&gt;% summarise(mean = mean(p_grid), median = median(p_grid)) ## # A tibble: 1 x 2 ## mean median ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 0.843 We can inspect the three types of point estimate in the left panel of Figure 3.4. First we’ll bundle the three point estimates together in a tibble. ( point_estimates &lt;- bind_rows(samples %&gt;% mean_qi(p_grid), samples %&gt;% median_qi(p_grid), samples %&gt;% mode_qi(p_grid)) %&gt;% select(p_grid, .point) %&gt;% # these last two columns will help us annotate mutate(x = p_grid + c(-.03, .03, -.03), y = c(.0005, .0012, .002)) ) ## # A tibble: 3 x 4 ## p_grid .point x y ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.803 mean 0.773 0.0005 ## 2 0.843 median 0.873 0.00120 ## 3 0.956 mode 0.926 0.002 Now plot. d %&gt;% ggplot(aes(x = p_grid)) + geom_area(aes(y = posterior), fill = &quot;grey75&quot;) + geom_vline(xintercept = point_estimates$p_grid) + geom_text(data = point_estimates, aes(x = x, y = y, label = .point), angle = 90) + labs(x = &quot;proportion of water (p)&quot;, y = &quot;density&quot;) + theme(panel.grid = element_blank()) As it turns out “different loss functions imply different point estimates” (p. 59, emphasis in the original). Let \\(p\\) be the proportion of the Earth covered by water and \\(d\\) be our guess. If McElreath pays us $100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to \\(d - p\\). If we decide \\(d = .5\\), we can compute our expected loss. d %&gt;% summarise(`expected loss` = sum(posterior * abs(0.5 - p_grid))) ## # A tibble: 1 x 1 ## `expected loss` ## &lt;dbl&gt; ## 1 0.313 What McElreath did with sapply(), we’ll do with purrr::map(). If you haven’t used it, map() is part of a family of similarly-named functions (e.g., map2()) from the purrr package (Henry &amp; Wickham, 2020), which is itself part of the tidyverse. The map() family is the tidyverse alternative to the family of apply() functions from the base R framework. You can learn more about how to use the map() family here or here or here. make_loss &lt;- function(our_d) { d %&gt;% mutate(loss = posterior * abs(our_d - p_grid)) %&gt;% summarise(weighted_average_loss = sum(loss)) } ( l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest(weighted_average_loss) ) ## # A tibble: 1,000 x 2 ## decision weighted_average_loss ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.800 ## 2 0.00100 0.799 ## 3 0.00200 0.798 ## 4 0.00300 0.797 ## 5 0.00400 0.796 ## 6 0.00501 0.795 ## 7 0.00601 0.794 ## 8 0.00701 0.793 ## 9 0.00801 0.792 ## 10 0.00901 0.791 ## # … with 990 more rows Now we’re ready for the right panel of Figure 3.4. # this will help us find the x and y coordinates for the minimum value min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # the plot l %&gt;% ggplot(aes(x = decision, y = weighted_average_loss)) + geom_area(fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) We saved the exact minimum value as min_loss[1], which is 0.8408408. Within sampling error, this is the posterior median as depicted by our samples. samples %&gt;% summarise(posterior_median = median(p_grid)) ## # A tibble: 1 x 1 ## posterior_median ## &lt;dbl&gt; ## 1 0.843 The quadratic loss \\((d - p)^2\\) suggests we should use the mean instead. Let’s investigate. # amend our loss function make_loss &lt;- function(our_d) { d %&gt;% mutate(loss = posterior * (our_d - p_grid)^2) %&gt;% summarise(weighted_average_loss = sum(loss)) } # remake our `l` data l &lt;- d %&gt;% select(p_grid) %&gt;% rename(decision = p_grid) %&gt;% mutate(weighted_average_loss = purrr::map(decision, make_loss)) %&gt;% unnest(weighted_average_loss) # update to the new minimum loss coordinates min_loss &lt;- l %&gt;% filter(weighted_average_loss == min(weighted_average_loss)) %&gt;% as.numeric() # update the plot l %&gt;% ggplot(aes(x = decision, y = weighted_average_loss)) + geom_area(fill = &quot;grey75&quot;) + geom_vline(xintercept = min_loss[1], color = &quot;white&quot;, linetype = 3) + geom_hline(yintercept = min_loss[2], color = &quot;white&quot;, linetype = 3) + ylab(&quot;expected proportional loss&quot;) + theme(panel.grid = element_blank()) Based on quadratic loss \\((d - p)^2\\), the exact minimum value is 0.8008008. Within sampling error, this is the posterior mean of our samples. samples %&gt;% summarise(posterior_meaan = mean(p_grid)) ## # A tibble: 1 x 1 ## posterior_meaan ## &lt;dbl&gt; ## 1 0.803 Usually, research scientists don’t think about loss functions. And so any point estimate like the mean or MAP that they may report isn’t intended to support any particular decision, but rather to describe the shape of the posterior. You might argue that the decision to make is whether or not to accept an hypothesis. But the challenge then is to say what the relevant costs and benefits would be, in terms of the knowledge gained or lost. Usually it’s better to communicate as much as you can about the posterior distribution, as well as the data and the model itself, so that others can build upon your work. Premature decisions to accept or reject hypotheses can cost lives. (p. 61) In the endnote (62) linked to the end of that quote in the text, McElreath wrote: “See Hauer (2004) for three tales from transportation safety in which testing resulted in premature incorrect decisions and a demonstrable and continuing loss of human life” (p. 561). 3.3 Sampling to simulate prediction McElreath’s five good reasons for simulation were model design model checking, software validation, research design, and forecasting. 3.3.1 Dummy data. Dummy data for the globe tossing model arise from the binomial likelihood. If you let \\(w\\) be a count of water and \\(n\\) be the number of tosses, the binomial likelihood is \\[\\operatorname{Pr} (w|n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}.\\] Letting \\(n = 2\\), \\(p(w) = .7\\), and \\(w_\\text{observed} = 0 \\text{ through }2\\), the densities are: tibble(n = 2, `p(w)` = .7, w = 0:2) %&gt;% mutate(density = dbinom(w, size = n, prob = `p(w)`)) ## # A tibble: 3 x 4 ## n `p(w)` w density ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2 0.7 0 0.09 ## 2 2 0.7 1 0.42 ## 3 2 0.7 2 0.490 If we’re going to simulate, we should probably set our seed. Doing so makes the results reproducible. set.seed(3) rbinom(1, size = 2, prob = .7) ## [1] 2 Here are ten reproducible draws. set.seed(3) rbinom(10, size = 2, prob = .7) ## [1] 2 1 2 2 1 1 2 2 1 1 Now generate 100,000 (i.e., 1e5) reproducible dummy observations. # how many would you like? n_draws &lt;- 1e5 set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 2, prob = .7)) d %&gt;% count(draws) %&gt;% mutate(proportion = n / nrow(d)) ## # A tibble: 3 x 3 ## draws n proportion ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 9000 0.09 ## 2 1 42051 0.421 ## 3 2 48949 0.489 As McElreath mused in the text (p. 63), those simulated proportion values are very close to the analytically calculated values in our density column a few code blocks up. Here’s the simulation updated so \\(n = 9\\), which we plot in our version of Figure 3.5. set.seed(3) d &lt;- tibble(draws = rbinom(n_draws, size = 9, prob = .7)) # the histogram d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = c(0, 9)) + theme(panel.grid = element_blank()) McElreath suggested we play around with different values of size and prob. With the next block of code, we’ll simulate nine conditions. n_draws &lt;- 1e5 simulate_binom &lt;- function(n, probability) { set.seed(3) rbinom(n_draws, size = n, prob = probability) } d &lt;- crossing(n = c(3, 6, 9), probability = c(.3, .6, .9)) %&gt;% mutate(draws = map2(n, probability, simulate_binom)) %&gt;% ungroup() %&gt;% mutate(n = str_c(&quot;n = &quot;, n), probability = str_c(&quot;p = &quot;, probability)) %&gt;% unnest(draws) head(d) ## # A tibble: 6 x 3 ## n probability draws ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 n = 3 p = 0.3 0 ## 2 n = 3 p = 0.3 2 ## 3 n = 3 p = 0.3 1 ## 4 n = 3 p = 0.3 0 ## 5 n = 3 p = 0.3 1 ## 6 n = 3 p = 0.3 1 Let’s plot the simulation results. d %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;dummy water count&quot;, breaks = seq(from = 0, to = 9, by = 2)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = c(0, 9)) + theme(panel.grid = element_blank()) + facet_grid(n ~ probability) 3.3.2 Model checking. If you’re new to applied statistics, you might be surprised how often mistakes arise. 3.3.2.1 Did the software work? Let this haunt your dreams: “There is no way to really be sure that software works correctly” (p. 64). If you’d like to dive deeper into these dark waters, check out one my favorite talks from StanCon 2018, Esther Williams in the Harold Holt Memorial Swimming Pool, by the ineffable Dan Simpson. If Simpson doesn’t end up drowning you, see Gabry and Simpson’s talk at the Royal Statistical Society 2018, Visualization in Bayesian workflow, a follow-up blog called Maybe it’s time to let the old ways die; or We broke R-hat so now we have to fix it, and that blog’s associated pre-print by Vehtari, Gelman, et al. (2019), Rank-normalization, folding, and localization: An improved \\(\\widehat R\\) for assessing convergence of MCMC. 3.3.2.2 Is the model adequate? The implied predictions of the model are uncertain in two ways, and it’s important to be aware of both. First, there is observation uncertainty. For any unique value of the parameter \\(p\\), there is a unique implied pattern of observations that the model expects. These patterns of observations are the same gardens of forking data that you explored in the previous chapter. These patterns are also what you sampled in the previous section. There is uncertainty in the predicted observations, because even if you know \\(p\\) with certainty, you won’t know the next globe toss with certainty (unless \\(p = 0\\) or \\(p = 1\\)). Second, there is uncertainty about \\(p\\). The posterior distribution over \\(p\\) embodies this uncertainty. And since there is uncertainty about \\(p\\), there is uncertainty about everything that depends upon \\(p\\). The uncertainty in \\(p\\) will interact with the sampling variation, when we try to assess what the model tells us about outcomes. We’d like to propagate the parameter uncertainty–carry it forward–as we evaluate the implied predictions. All that is required is averaging over the posterior density for \\(p\\), while computing the predictions. For each possible value of the parameter \\(p\\), there is an implied distribution of outcomes. So if you were to compute the sampling distribution of outcomes at each value of \\(p\\), then you could average all of these prediction distributions together, using the posterior probabilities of each value of \\(p\\), to get a posterior predictive distribution. (pp. 64–65, emphasis in the original) All this is depicted in Figure 3.6. To get ready to make our version, let’s first refresh our original grid approximation d. # how many grid points would you like? n &lt;- 1001 n_success &lt;- 6 n_trials &lt;- 9 ( d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n), # note we&#39;re still using a flat uniform prior prior = 1) %&gt;% mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) ) ## # A tibble: 1,001 x 4 ## p_grid prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0. 0. ## 2 0.001 1 8.37e-17 8.37e-19 ## 3 0.002 1 5.34e-15 5.34e-17 ## 4 0.003 1 6.07e-14 6.07e-16 ## 5 0.004 1 3.40e-13 3.40e-15 ## 6 0.005 1 1.29e-12 1.29e-14 ## 7 0.006 1 3.85e-12 3.85e-14 ## 8 0.007 1 9.68e-12 9.68e-14 ## 9 0.008 1 2.15e-11 2.15e-13 ## 10 0.009 1 4.34e-11 4.34e-13 ## # … with 991 more rows We can make our version of the top of Figure 3.6 with a little tricky filtering. d %&gt;% ggplot(aes(x = p_grid, y = posterior)) + geom_area(color = &quot;grey67&quot;, fill = &quot;grey67&quot;) + geom_segment(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)), aes(xend = p_grid, yend = 0, size = posterior), color = &quot;grey33&quot;, show.legend = F) + geom_point(data = . %&gt;% filter(p_grid %in% c(seq(from = .1, to = .9, by = .1), 3 / 10))) + annotate(geom = &quot;text&quot;, x = .08, y = .0025, label = &quot;Posterior probability&quot;) + scale_size_continuous(range = c(0, 1)) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0:10) / 10) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Note how we weighted the widths of the vertical lines by the posterior density. We’ll need to do a bit of wrangling before we’re ready to make the plot in the middle panel of Figure 3.6. n_draws &lt;- 1e5 simulate_binom &lt;- function(probability) { set.seed(3) rbinom(n_draws, size = 9, prob = probability) } d_small &lt;- tibble(probability = seq(from = .1, to = .9, by = .1)) %&gt;% mutate(draws = purrr::map(probability, simulate_binom)) %&gt;% unnest(draws) %&gt;% mutate(label = str_c(&quot;p = &quot;, probability)) head(d_small) ## # A tibble: 6 x 3 ## probability draws label ## &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 0.1 0 p = 0.1 ## 2 0.1 2 p = 0.1 ## 3 0.1 0 p = 0.1 ## 4 0.1 0 p = 0.1 ## 5 0.1 1 p = 0.1 ## 6 0.1 1 p = 0.1 Now we’re ready to plot. d_small %&gt;% ggplot(aes(x = draws)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(NULL, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Sampling distributions&quot;) + coord_cartesian(xlim = c(0, 9)) + theme(panel.grid = element_blank()) + facet_wrap(~ label, ncol = 9) To make the plot at the bottom of Figure 3.6, we’ll redefine our samples, this time including the w variable (see the R code 3.26 block in the text). # how many samples would you like? n_samples &lt;- 1e4 # make it reproducible set.seed(3) samples &lt;- d %&gt;% slice_sample(n = n_samples, weight_by = posterior, replace = T) %&gt;% mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9)) glimpse(samples) ## Rows: 10,000 ## Columns: 5 ## $ p_grid &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0.… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, 0… ## $ posterior &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708e… ## $ w &lt;dbl&gt; 4, 7, 3, 3, 7, 6, 8, 2, 6, 4, 5, 5, 8, 6, 4, 6, 8, 2, 6, 9, 9, 7, 4, 8, 9, 8, 6… Here’s our histogram. samples %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = c(0, 9), ylim = c(0, 3000)) + theme(panel.grid = element_blank()) In Figure 3.7, McElreath considered the longest sequence of the sample values. We’ve been using rbinom() with the size parameter set to 9 for our simulations. E.g., rbinom(10, size = 9, prob = .6) ## [1] 7 5 6 8 7 5 6 3 3 4 Notice this collapsed (i.e., aggregated) over the sequences within the individual sets of 9. What we need is to simulate nine individual trials many times over. For example, this rbinom(9, size = 1, prob = .6) ## [1] 0 1 1 1 0 0 0 0 0 would be the disaggregated version of just one of the numerals returned by rbinom() when size = 9. So let’s try simulating again with un-aggregated samples. We’ll keep adding to our samples tibble. In addition to the disaggregated draws based on the \\(p\\) values listed in p_grid, we’ll also want to add a row index for each of those p_grid values–it’ll come in handy when we plot. # make it reproducible set.seed(3) samples &lt;- samples %&gt;% mutate(iter = 1:n(), draws = purrr::map(p_grid, rbinom, n = 9, size = 1)) %&gt;% unnest(draws) glimpse(samples) ## Rows: 90,000 ## Columns: 7 ## $ p_grid &lt;dbl&gt; 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.651, 0.651, 0.… ## $ prior &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ likelihood &lt;dbl&gt; 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.… ## $ posterior &lt;dbl&gt; 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0… ## $ w &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3… ## $ iter &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3… ## $ draws &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1… The main action is in the draws column. Now we have to count the longest sequences. The base R rle() function will help with that. Consider McElreath’s sequence of tosses. tosses &lt;- c(&quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;w&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;, &quot;l&quot;, &quot;w&quot;) You can plug that into rle(). rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; For our purposes, we’re interested in lengths. That tells us the length of each sequences of the same value. The 3 corresponds to our run of three ws. The max() function will help us confirm it’s the largest value. rle(tosses)$lengths %&gt;% max() ## [1] 3 Now let’s apply our method to the data and plot. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% max()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 3), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + scale_x_continuous(&quot;longest run length&quot;, breaks = seq(from = 0, to = 9, by = 3)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = c(0, 9)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Let’s look at rle() again. rle(tosses) ## Run Length Encoding ## lengths: int [1:7] 1 1 3 1 1 1 1 ## values : chr [1:7] &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; &quot;l&quot; &quot;w&quot; We can use the length of the output (i.e., 7 in this example) as the numbers of switches from, in this case, “w” and “l.” rle(tosses)$lengths %&gt;% length() ## [1] 7 With that new trick, we’re ready to make the right panel of Figure 3.7. samples %&gt;% group_by(iter) %&gt;% summarise(longest_run_length = rle(draws)$lengths %&gt;% length()) %&gt;% ggplot(aes(x = longest_run_length)) + geom_histogram(aes(fill = longest_run_length == 7), binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_fill_viridis_d(option = &quot;D&quot;, end = .9) + scale_x_continuous(&quot;number of switches&quot;, breaks = seq(from = 0, to = 9, by = 3)) + ylab(&quot;frequency&quot;) + coord_cartesian(xlim = c(0, 9)) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) 3.4 Summary Let’s practice with brms Open brms. library(brms) With brms, we’ll fit the primary model of \\(w = 6\\) and \\(n = 9\\) much like we did at the end of Chapter 2. b3.1 &lt;- brm(data = list(w = 6), family = binomial(link = &quot;identity&quot;), w | trials(9) ~ 0 + Intercept, # this is a flat prior prior(beta(1, 1), class = b, lb = 0, ub = 1), iter = 5000, warmup = 1000, seed = 3, file = &quot;fits/b03.01&quot;) We’ll learn more about the beta distribution in Chapter 12. But for now, here’s the posterior summary for b_Intercept, the probability of a “w.” posterior_summary(b3.1)[&quot;b_Intercept&quot;, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## 0.64 0.14 0.35 0.87 As we’ll fully cover in the next chapter, Estimate is the posterior mean, the two Q columns are the quantile-based 95% intervals, and Est.Error is the posterior standard deviation. Much like the way we used the samples() function to simulate probability values, above, we can do so with the brms::fitted() function. But we will have to specify scale = \"linear\" in order to return results in the probability metric. By default, brms::fitted() will return summary information. Since we want actual simulation draws, we’ll specify summary = F. f &lt;- fitted(b3.1, summary = F, scale = &quot;linear&quot;) %&gt;% as_tibble() %&gt;% set_names(&quot;p&quot;) glimpse(f) ## Rows: 16,000 ## Columns: 1 ## $ p &lt;dbl&gt; 0.6878563, 0.5386513, 0.7030050, 0.6889854, 0.4738290, 0.5879676, 0.5843004, 0.7014505, … By default, we have a generically-named vector V1 of 4,000 samples. We’ll explain the defaults in later chapters. For now, notice we can view these in a density. f %&gt;% ggplot(aes(x = p)) + geom_density(fill = &quot;grey50&quot;, color = &quot;grey50&quot;) + annotate(geom = &quot;text&quot;, x = .08, y = 2.5, label = &quot;Posterior probability&quot;) + scale_x_continuous(&quot;probability of water&quot;, breaks = c(0, .5, 1), limits = 0:1) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) Looks a lot like the posterior probability density at the top of Figure 3.6, doesn’t it? Much like we did with samples, we can use this distribution of probabilities to predict histograms of w counts. With those in hand, we can make an analogue to the histogram in the bottom panel of Figure 3.6. # the simulation set.seed(3) f &lt;- f %&gt;% mutate(w = rbinom(n(), size = n_trials, prob = p)) # the plot f %&gt;% ggplot(aes(x = w)) + geom_histogram(binwidth = 1, center = 0, color = &quot;grey92&quot;, size = 1/10) + scale_x_continuous(&quot;number of water samples&quot;, breaks = seq(from = 0, to = 9, by = 3)) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 5000)) + ggtitle(&quot;Posterior predictive distribution&quot;) + coord_cartesian(xlim = c(0, 9)) + theme(panel.grid = element_blank()) As you might imagine, we can use the output from fitted() to return disaggregated batches of 0’s and 1’s, too. And we could even use those disaggregated 0’s and 1’s to examine longest run lengths and numbers of switches as in the analyses for Figure 3.7. I’ll leave those as exercises for the interested reader. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.15.0 Rcpp_1.0.6 tidybayes_2.3.1 patchwork_1.1.1 forcats_0.5.1 stringr_1.4.0 ## [7] dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 tibble_3.1.0 ggplot2_3.3.3 ## [13] tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 igraph_1.2.6 ## [5] splines_4.0.4 svUnit_1.0.3 crosstalk_1.1.0.1 TH.data_1.0-10 ## [9] rstantools_2.1.1 inline_0.3.17 digest_0.6.27 htmltools_0.5.1.1 ## [13] rethinking_2.13 rsconnect_0.8.16 fansi_0.4.2 magrittr_2.0.1 ## [17] modelr_0.1.8 RcppParallel_5.0.2 matrixStats_0.57.0 sandwich_3.0-0 ## [21] xts_0.12.1 prettyunits_1.1.1 colorspace_2.0-0 rvest_0.3.6 ## [25] ggdist_2.4.0.9000 haven_2.3.1 xfun_0.22 callr_3.5.1 ## [29] crayon_1.4.1 jsonlite_1.7.2 lme4_1.1-25 survival_3.2-7 ## [33] zoo_1.8-8 glue_1.4.2 gtable_0.3.0 emmeans_1.5.2-1 ## [37] V8_3.4.0 distributional_0.2.2 pkgbuild_1.2.0 rstan_2.21.2 ## [41] shape_1.4.5 abind_1.4-5 scales_1.1.1 mvtnorm_1.1-1 ## [45] DBI_1.1.0 miniUI_0.1.1.1 viridisLite_0.3.0 xtable_1.8-4 ## [49] HDInterval_0.2.2 stats4_4.0.4 StanHeaders_2.21.0-7 DT_0.16 ## [53] htmlwidgets_1.5.2 httr_1.4.2 threejs_0.3.3 arrayhelpers_1.1-0 ## [57] ellipsis_0.3.1 pkgconfig_2.0.3 loo_2.4.1 farver_2.0.3 ## [61] dbplyr_2.0.0 utf8_1.1.4 tidyselect_1.1.0 labeling_0.4.2 ## [65] rlang_0.4.10 reshape2_1.4.4 later_1.1.0.1 munsell_0.5.0 ## [69] cellranger_1.1.0 tools_4.0.4 cli_2.3.1 generics_0.1.0 ## [73] broom_0.7.5 ggridges_0.5.2 evaluate_0.14 fastmap_1.0.1 ## [77] processx_3.4.5 knitr_1.31 fs_1.5.0 nlme_3.1-152 ## [81] mime_0.10 projpred_2.0.2 xml2_1.3.2 compiler_4.0.4 ## [85] bayesplot_1.8.0 shinythemes_1.1.2 rstudioapi_0.13 curl_4.3 ## [89] gamm4_0.2-6 reprex_0.3.0 statmod_1.4.35 stringi_1.5.3 ## [93] highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 lattice_0.20-41 ## [97] Matrix_1.3-2 nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 ## [101] vctrs_0.3.6 pillar_1.5.1 lifecycle_1.0.0 bridgesampling_1.0-0 ## [105] estimability_1.3 httpuv_1.5.4 R6_2.5.0 bookdown_0.21 ## [109] promises_1.1.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-26 ## [113] colourpicker_1.1.0 MASS_7.3-53 gtools_3.8.2 assertthat_0.2.1 ## [117] withr_2.4.1 shinystan_2.5.0 multcomp_1.4-16 mgcv_1.8-33 ## [121] parallel_4.0.4 hms_0.5.3 grid_4.0.4 coda_0.19-4 ## [125] minqa_1.2.4 rmarkdown_2.7 shiny_1.5.0 lubridate_1.7.9.2 ## [129] base64enc_0.1-3 dygraphs_1.1.1.6 "],["geocentric-models.html", "4 Geocentric Models 4.1 Why normal distributions are normal 4.2 A language for describing models 4.3 A Gaussian model of height 4.4 Linear prediction 4.5 Curves from lines 4.6 Summary First bonus: Smooth functions with brms::s() 4.7 Second bonus: Group predictors with matrix columns Session info", " 4 Geocentric Models Linear regression is the geocentric model of applied statistics. By “linear regression,” we will mean a family of simple statistical golems that attempt to learn about the mean and variance of some measurement, using an additive combination of other measurements. Like geocentrism, linear regression can usefully describe a very large variety of natural phenomena. Like geocentrism, linear regression is a descriptive model that corresponds to many different process models. If we read its structure too literally, we’re likely to make mistakes. But used wisely, these little linear golems continue to be useful. (McElreath, 2020a, p. 71, emphasis, in the original) 4.1 Why normal distributions are normal After laying out his soccer field coin toss shuffle premise, McElreath wrote: It’s hard to say where any individual person will end up, but you can say with great confidence what the collection of positions will be. The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution. (p. 72) 4.1.1 Normal by addition. Here’s a way to do the simulation necessary for the plot in the top panel of Figure 4.2. library(tidyverse) # we set the seed to make the results of `runif()` reproducible. set.seed(4) pos &lt;- # make data with 100 people, 16 steps each with a starting point of `step == 0` (i.e., 17 rows per person) crossing(person = 1:100, step = 0:16) %&gt;% # for all steps above `step == 0` simulate a `deviation` mutate(deviation = map_dbl(step, ~if_else(. == 0, 0, runif(1, -1, 1)))) %&gt;% # after grouping by `person`, compute the cumulative sum of the deviations, then `ungroup()` group_by(person) %&gt;% mutate(position = cumsum(deviation)) %&gt;% ungroup() That map_dbl() code within the first mutate() line might look odd. Go here to learn more about iterating with purrr::map_dbl(). We might glimpse() at the data. glimpse(pos) ## Rows: 1,700 ## Columns: 4 ## $ person &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,… ## $ step &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7… ## $ deviation &lt;dbl&gt; 0.00000000, -0.98210841, -0.41252078, -0.44525008, 0.62714843, -0.47914446, 0.44… ## $ position &lt;dbl&gt; 0.0000000, -0.9821084, -1.3946292, -1.8398793, -1.2127308, -1.6918753, -1.243063… Here’s the code to make the top panel of Figure 4.2. ggplot(data = pos, aes(x = step, y = position, group = person)) + geom_vline(xintercept = c(4, 8, 16), linetype = 2) + geom_line(aes(color = person &lt; 2, alpha = person &lt; 2)) + scale_color_manual(values = c(&quot;skyblue4&quot;, &quot;black&quot;)) + scale_alpha_manual(values = c(1/5, 1)) + scale_x_continuous(&quot;step number&quot;, breaks = c(0, 4, 8, 12, 16)) + theme(legend.position = &quot;none&quot;) Here’s the code for the bottom three plots of Figure 4.2. # Figure 4.2.a. p1 &lt;- pos %&gt;% filter(step == 4) %&gt;% ggplot(aes(x = position)) + geom_line(stat = &quot;density&quot;, color = &quot;dodgerblue1&quot;) + labs(title = &quot;4 steps&quot;) # Figure 4.2.b. p2 &lt;- pos %&gt;% filter(step == 8) %&gt;% ggplot(aes(x = position)) + geom_density(color = &quot;dodgerblue2&quot;, outline.type = &quot;full&quot;) + labs(title = &quot;8 steps&quot;) # this is an intermediary step to get an SD value sd &lt;- pos %&gt;% filter(step == 16) %&gt;% summarise(sd = sd(position)) %&gt;% pull(sd) # Figure 4.2.c. p3 &lt;- pos %&gt;% filter(step == 16) %&gt;% ggplot(aes(x = position)) + stat_function(fun = dnorm, args = list(mean = 0, sd = sd), linetype = 2) + geom_density(color = &quot;transparent&quot;, fill = &quot;dodgerblue3&quot;, alpha = 1/2) + labs(title = &quot;16 steps&quot;, y = &quot;density&quot;) library(patchwork) # combine the ggplots (p1 | p2 | p3) &amp; coord_cartesian(xlim = c(-6, 6)) While we were at it, we explored a few ways to express densities. The main action was with the geom_line(), geom_density(), and stat_function() functions, respectively. Any process that adds together random values from the same distribution converges to a normal. But it’s not easy to grasp why addition should result in a bell curve of sums. Here’s a conceptual way to think of the process. Whatever the average value of the source distribution, each sample from it can be thought of as a fluctuation from that average value. When we begin to add these fluctuations together, they also begin to cancel one another out. A large positive fluctuation will cancel a large negative one. (p. 73) 4.1.2 Normal by multiplication. Here’s McElreath’s simple random growth rate. set.seed(4) prod(1 + runif(12, 0, 0.1)) ## [1] 1.774719 In the runif() part of that code, we generated 12 random draws from the uniform distribution with bounds \\([0, 0.1]\\). Within the prod() function, we first added 1 to each of those values and then computed their product. Consider a more explicit variant of the code. set.seed(4) tibble(a = 1, b = runif(12, 0, 0.1)) %&gt;% mutate(c = a + b) %&gt;% summarise(p = prod(c)) ## # A tibble: 1 x 1 ## p ## &lt;dbl&gt; ## 1 1.77 Same result. Rather than using base R replicate() to do this many times, let’s practice with purrr::map_dbl() like before. set.seed(4) growth &lt;- tibble(growth = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.1)))) ggplot(data = growth, aes(x = growth)) + geom_density() “The smaller the effect of each locus, the better this additive approximation will be” (p. 74). Let’s compare big and small. # simulate set.seed(4) samples &lt;- tibble(big = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.5))), small = map_dbl(1:10000, ~ prod(1 + runif(12, 0, 0.01)))) # wrangle samples %&gt;% pivot_longer(everything(), values_to = &quot;samples&quot;) %&gt;% # plot ggplot(aes(x = samples)) + geom_density(fill = &quot;black&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) Yep, the small samples were more Gaussian. “The interacting growth deviations, as long as they are sufficiently small, converge to a Gaussian distribution. In this way, the range of causal forces that tend towards Gaussian distributions extends well beyond purely additive interactions” (p. 74). 4.1.3 Normal by log-multiplication. Instead of saving our tibble, we’ll just feed it directly into our plot. samples %&gt;% mutate(log_big = log(big)) %&gt;% ggplot(aes(x = log_big)) + geom_density(fill = &quot;gray33&quot;) + xlab(&quot;the log of the big&quot;) Yet another Gaussian distribution. We get the Gaussian distribution back, because adding logs is equivalent to multiplying the original numbers. So even multiplicative interactions of large deviations can produce Gaussian distributions, once we measure the outcomes on the log scale. (p. 75) 4.1.4 Using Gaussian distributions. “The justifications for using the Gaussian distribution fall into two broad categories: (1) ontological and (2) epistemological” (p. 75). I’m a fan of the justifications to follow. 4.1.4.1 Ontological justification. The Gaussian is a widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread. [However,] one consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process. (p. 75) But like Ptolemy’s circles within circles, the Gaussian can still be useful even if it sheds information. 4.1.4.2 Epistemological justification. By the epistemological justification, the Gaussian represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions. That is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions. Or rather, it is the most consistent with our golem’s assumptions. If you don’t think the distribution should be Gaussian, then that implies that you know something else that you should tell your golem about, something that would improve inference. (p. 75) We’ll dive deeper into why the Gaussian is such a natural expression of ignorance in these contexts when we cover maximum entropy in Chapter 7. 4.1.4.3 Rethinking: Heavy tails. The Gaussian distribution is common in nature and has some nice properties. But there are some risks in using it as a default data model. The extreme ends of a distribution are known as its tails. And the Gaussian distribution has some very thin tails–there is very little probability in them. Instead most of the mass in the Gaussian lies within one standard deviation of the mean. Many natural (and unnatural) processes have much heavier tails. (p. 76) You have no idea how excited I am that we’ll be covering some of these heavy-tailed alternatives! 4.1.4.4 Overthinking: Gaussian distribution. In this section McElreath gave the formula for the Gaussian probability density function. Let \\(y\\) be the criterion, \\(\\mu\\) be the mean, and \\(\\sigma\\) be the standard deviation. Then the probability density of some Gaussian value \\(y\\) is \\[p(y|\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left (- \\frac{(y - \\mu)^2}{2 \\sigma^2} \\right).\\] McElreath’s right. “This looks monstrous” (p. 76). Why not demystify that monster with a little R code? For simplicity, we’ll compute \\(p(y|\\mu, \\sigma)\\) for a series of \\(y\\)-values ranging from -1 to 1, holding \\(\\mu = 0\\) and \\(\\sigma = 0.1\\). Then we’ll plot. tibble(y = seq(from = -1, to = 1, by = .01), mu = 0, sigma = 0.1) %&gt;% # compute p(y) with a hand-made Gaussian probability density function mutate(p = (1 / sqrt(2 * pi * sigma^2)) * exp(-((y - mu)^2 / (2 * sigma^2)))) %&gt;% ggplot(aes(x = y, y = p)) + geom_line() + ylab(expression(italic(p)(italic(&quot;y|&quot;)*mu==0*&quot;,&quot;~sigma==0.1))) Notice how \\(p(y|\\mu, \\sigma)\\) peaks around 4 when \\(y = 0\\). We can also get that value with the dnorm() function, which will return the \\(p(y)\\) value for a given combination of \\(y\\), \\(\\mu\\), and \\(\\sigma\\). dnorm(0, mean = 0, sd = 0.1) ## [1] 3.989423 The answer, about 4, is no mistake. Probability density is the rate of change in cumulative probability. So where cumulative probability is increasing rapidly, density can easily exceed 1. But if we calculate the area under the density function, it will never exceed 1. Such areas are also called probability mass. You can usually ignore these density/mass details while doing computational work. But it’s good to be aware of the distinction. (p. 76, emphasis in the original) 4.2 A language for describing models Our mathy ways of summarizing models will be something like \\[\\begin{align*} \\text{criterion}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\beta \\times \\text{predictor}_i \\\\ \\beta &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1) \\\\ x_i &amp; \\sim \\operatorname{Normal}(0, 1). \\end{align*}\\] “If that doesn’t make much sense, good. That indicates that you are holding the right textbook” (p. 77). Welcome applied statistics! 🤓 4.2.1 Re-describing the globe tossing model. For the globe tossing model, the probability \\(p\\) of a count of water \\(w\\) based on \\(n\\) trials was \\[\\begin{align*} w &amp; \\sim \\operatorname{Binomial}(n, p) \\\\ p &amp; \\sim \\operatorname{Uniform}(0, 1), \\end{align*}\\] where the top line indicates we’re using the Binomial likelihood function to model \\(w\\) given unique combinations of \\(n\\) and \\(p\\). In our example, \\(w\\) and \\(n\\) were already defined in the data, so all we need to do is compute \\(p\\). Since \\(p\\) is the only parameter, it’s the only element that gets a prior, which is what that second line was. 4.2.1.1 Overthinking: From model definition to Bayes’ theorem. We can use grid approximation to work through our globe tossing model. # how many `p_grid` points would you like? n_points &lt;- 100 d &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n_points), w = 6, n = 9) %&gt;% mutate(prior = dunif(p_grid, 0, 1), likelihood = dbinom(w, n, p_grid)) %&gt;% mutate(posterior = likelihood * prior / sum(likelihood * prior)) head(d) ## # A tibble: 6 x 6 ## p_grid w n prior likelihood posterior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 6 9 1 0. 0. ## 2 0.0101 6 9 1 8.65e-11 8.74e-12 ## 3 0.0202 6 9 1 5.37e- 9 5.43e-10 ## 4 0.0303 6 9 1 5.93e- 8 5.99e- 9 ## 5 0.0404 6 9 1 3.23e- 7 3.26e- 8 ## 6 0.0505 6 9 1 1.19e- 6 1.21e- 7 In case you were curious, here’s what they look like. d %&gt;% pivot_longer(prior:posterior) %&gt;% # this line allows us to dictate the order in which the panels will appear mutate(name = factor(name, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;))) %&gt;% ggplot(aes(x = p_grid, y = value, fill = name)) + geom_area() + scale_fill_manual(values = c(&quot;blue&quot;, &quot;red&quot;, &quot;purple&quot;)) + scale_y_continuous(NULL, breaks = NULL) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ name, scales = &quot;free&quot;) The posterior is a combination of the prior and the likelihood. When the prior is flat across the parameter space, the posterior is just the likelihood re-expressed as a probability. As we go along, you’ll see we almost never use flat priors in practice. Be warned that eschewing flat priors is a recent development, however. You only have to look at the literature from a couple decades ago to see mounds and mounds of flat priors. 4.3 A Gaussian model of height There are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large \\(\\sigma\\). Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of \\(\\mu\\) and \\(\\sigma\\), and rank them by posterior plausibility. Posterior plausibility provides a measure of the logical compatibility of each possible distribution with the data and model. (p. 79) 4.3.1 The data. Let’s load the Howell (2001, 2010) data from McElreath’s (2020b) rethinking package. library(rethinking) data(Howell1) d &lt;- Howell1 Here we open our focal statistical package, Bürkner’s brms. But before we do, we’ll want to detach the rethinking package. R will not allow us to use a function from one package that shares the same name as a different function from another package if both packages are open at the same time. The rethinking and brms packages are designed for similar purposes and, unsurprisingly, overlap in some of their function names. To prevent problems, we will always make sure rethinking is detached before using brms. To learn more on the topic, see this R-bloggers post. rm(Howell1) detach(package:rethinking, unload = T) library(brms) Go ahead and investigate the data with str(), the tidyverse analogue for which is glimpse(). d %&gt;% str() ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... The brms package does not have a function that works like rethinking::precis() for providing numeric and graphical summaries of variables, as see in McElreath’s R code 4.9. We can get some of that information with summary(). d %&gt;% summary() ## height weight age male ## Min. : 53.98 Min. : 4.252 Min. : 0.00 Min. :0.0000 ## 1st Qu.:125.09 1st Qu.:22.008 1st Qu.:12.00 1st Qu.:0.0000 ## Median :148.59 Median :40.058 Median :27.00 Median :0.0000 ## Mean :138.26 Mean :35.611 Mean :29.34 Mean :0.4724 ## 3rd Qu.:157.48 3rd Qu.:47.209 3rd Qu.:43.00 3rd Qu.:1.0000 ## Max. :179.07 Max. :62.993 Max. :88.00 Max. :1.0000 We might make the histograms like this. d %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = c(&quot;height&quot;, &quot;weight&quot;, &quot;age&quot;, &quot;male&quot;))) %&gt;% ggplot(aes(x = value)) + geom_histogram(bins = 10) + facet_wrap(~ name, scales = &quot;free&quot;, ncol = 1) If you’re curious, McElreath made those tiny histograms with help from Wickham’s histospark() function. Here’s the code. sparks &lt;- c(&quot;\\u2581&quot;, &quot;\\u2582&quot;, &quot;\\u2583&quot;, &quot;\\u2585&quot;, &quot;\\u2587&quot;) histospark &lt;- function(x, width = 10) { bins &lt;- graphics::hist(x, breaks = width, plot = FALSE) factor &lt;- cut( bins$counts / max(bins$counts), breaks = seq(0, 1, length = length(sparks) + 1), labels = sparks, include.lowest = TRUE ) paste0(factor, collapse = &quot;&quot;) } Here’s how it works. histospark(d$weight) ## [1] &quot;▁▂▃▂▂▂▂▅▇▇▃▂▁&quot; One of the neat things about the histospark() function is you can insert the output right in your R Markdown prose. For example, we can use is to casually show how left skewed the our height variable is: ▁▁▁▁▁▁▁▂▁▇▇▅▁. But it’s time to get back on track. You can isolate height values with the dplyr::select() function. d %&gt;% select(height) %&gt;% glimpse() ## Rows: 544 ## Columns: 1 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 147… If you want the values in a numeric vector rather than in a data fame, try pull(d, height). We can use the dplyr::filter() function to make an adults-only data frame. d2 &lt;- d %&gt;% filter(age &gt;= 18) Our reduced d2 does indeed have \\(n = 352\\) cases. d2 %&gt;% count() ## n ## 1 352 4.3.1.1 Overthinking: Data frames and indexes. For more on indexing, check out Chapter 9 of Peng’s (2019) R programming for data science, or even the Subsetting subsection in R4DS. This probably reflects my training history, but the structure of a data frame seems natural and inherently appealing. If you’re in the other camp, do check out either of these two data wrangling talks (here and here) by the ineffable Jenny Bryan. 4.3.2 The model. Please heed McElreath’s warnings to be careful about choosing the Gaussian distribution only when the plotted outcome variable looks Gaussian to you. Gawking at the raw data, to try to decide how to model them, is usually not a good idea. The data could be a mixture of different Gaussian distributions, for example, and in that case you won’t be able to detect the underlying normality just by eyeballing the outcome distribution. Furthermore, as mentioned earlier in this chapter, the empirical distribution needn’t be actually Gaussian in order to justify using a Gaussian probability distribution. (p. 81) Anyway, the likelihood for our model is \\[\\text{heights}_i \\sim \\operatorname{Normal}(\\mu, \\sigma),\\] where the \\(i\\) subscript indexes the individual cases in the data. Our two parameters are \\(\\mu\\) and \\(\\sigma\\), which we will estimate using Bayes’ formula. Our prior for \\(\\mu\\) will be \\[\\mu \\sim \\operatorname{Normal}(178, 20)\\] and our prior for \\(\\sigma\\) will be \\[\\sigma \\sim \\operatorname{Uniform}(0, 50).\\] Here’s the shape of the prior for \\(\\mu\\), \\(\\mathcal N(178, 20)\\). p1 &lt;- tibble(x = seq(from = 100, to = 250, by = .1)) %&gt;% ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) + geom_line() + scale_x_continuous(breaks = seq(from = 100, to = 250, by = 75)) + labs(title = &quot;mu ~ dnorm(178, 20)&quot;, y = &quot;density&quot;) p1 And here’s the ggplot2 code for our prior for \\(\\sigma\\), a uniform distribution with a minimum value of 0 and a maximum value of 50. We don’t really need the \\(y\\)-axis when looking at the shapes of a density, so we’ll just remove it with scale_y_continuous(). p2 &lt;- tibble(x = seq(from = -10, to = 60, by = .1)) %&gt;% ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) + geom_line() + scale_x_continuous(breaks = c(0, 50)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;sigma ~ dunif(0, 50)&quot;) p2 We can simulate from both priors at once to get a prior probability distribution of heights. n &lt;- 1e4 set.seed(4) sim &lt;- tibble(sample_mu = rnorm(n, mean = 178, sd = 20), sample_sigma = runif(n, min = 0, max = 50)) %&gt;% mutate(height = rnorm(n, mean = sample_mu, sd = sample_sigma)) p3 &lt;- sim %&gt;% ggplot(aes(x = height)) + geom_density(fill = &quot;grey33&quot;) + scale_x_continuous(breaks = c(0, 73, 178, 283)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;height ~ dnorm(mu, sigma)&quot;) + theme(panel.grid = element_blank()) p3 If you look at the \\(x\\)-axis breaks on the plot in McElreath’s lower left panel in Figure 4.3, you’ll notice they’re intentional. To compute the mean and 3 standard deviations above and below, you might do this. sim %&gt;% summarise(ll = mean(height) - sd(height) * 3, mean = mean(height), ul = mean(height) + sd(height) * 3) %&gt;% mutate_all(round, digits = 1) ## # A tibble: 1 x 3 ## ll mean ul ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 73.9 177. 281. Our values are very close to his, but are off by just a bit due to simulation variation. Here’s the work to make the lower right panel of Figure 4.3. Watch out; we’re starting to get fancy. # simulate set.seed(4) sim &lt;- tibble(sample_mu = rnorm(n, mean = 178, sd = 100), sample_sigma = runif(n, min = 0, max = 50)) %&gt;% mutate(height = rnorm(n, mean = sample_mu, sd = sample_sigma)) # compute the values we&#39;ll use to break on our x axis breaks &lt;- c(mean(sim$height) - 3 * sd(sim$height), 0, mean(sim$height), mean(sim$height) + 3 * sd(sim$height)) %&gt;% round(digits = 0) # this is just for aesthetics text &lt;- tibble(height = 272 - 25, y = .0013, label = &quot;tallest man&quot;, angle = 90) # plot p4 &lt;- sim %&gt;% ggplot(aes(x = height)) + geom_density(fill = &quot;black&quot;, size = 0) + geom_vline(xintercept = 0, color = &quot;grey92&quot;) + geom_vline(xintercept = 272, color = &quot;grey92&quot;, linetype = 3) + geom_text(data = text, aes(y = y, label = label, angle = angle), color = &quot;grey92&quot;) + scale_x_continuous(breaks = breaks) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(&quot;height ~ dnorm(mu, sigma)\\nmu ~ dnorm(178, 100)&quot;) + theme(panel.grid = element_blank()) p4 You may have noticed how we were saving each of the four last plots as p1 through p4. Let’s combine the four to make our version of McElreath’s Figure 4.3. (p1 + xlab(&quot;mu&quot;) | p2 + xlab(&quot;sigma&quot;)) / (p3 | p4) On page 84, McElreath said his prior simulation indicated 4% of the heights would be below zero. Here’s how we might determe that percentage for our simulation. sim %&gt;% count(height &lt; 0) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 x 3 ## `height &lt; 0` n percent ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 9571 95.7 ## 2 TRUE 429 4.29 Here’s the break down compared to the tallest man on record, Robert Pershing Wadlow (1918–1940). sim %&gt;% count(height &lt; 272) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 2 x 3 ## `height &lt; 272` n percent ## &lt;lgl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 FALSE 1761 17.6 ## 2 TRUE 8239 82.4 Does this matter? In this case, we have so much data that the silly prior is harmless. But that won’t always be the case. There are plenty of inference problems for which the data alone are not sufficient, no matter how numerous. Bayes lets us proceed in these cases. But only if we use our scientific knowledge to construct sensible priors. Using scientific knowledge to build priors is not cheating. The important thing is that your prior not be based on the values in the data, but only on what you know about the data before you see it. (p. 84) 4.3.2.1 Rethinking: A farewell to epsilon. Some readers will have already met an alternative notation for a Gaussian linear model: \\[\\begin{align*} h_i &amp; = \\mu + \\epsilon_i \\\\ \\epsilon_i &amp; \\sim \\operatorname{Normal}(0, \\sigma) \\end{align*}\\] This is equivalent to the \\(h_i \\sim \\operatorname{Normal}(\\mu, \\sigma)\\) form, with the \\(\\epsilon\\) standing in for the Gaussian density. But this \\(\\epsilon\\) form is poor form. The reason is that it does not usually generalize to other types of models. This means it won’t be possible to express non-Gaussian models using tricks like \\(\\epsilon\\). Better to learn one system that does generalize. (p. 84) Agreed. 4.3.3 Grid approximation of the posterior distribution. As McElreath explained, you’ll never use this for practical data analysis. But I found this helped me better understanding what exactly we’re doing with Bayesian estimation. So let’s play along. n &lt;- 200 d_grid &lt;- # we&#39;ll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()` crossing(mu = seq(from = 140, to = 160, length.out = n), sigma = seq(from = 4, to = 9, length.out = n)) glimpse(d_grid) ## Rows: 40,000 ## Columns: 2 ## $ mu &lt;dbl&gt; 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140,… ## $ sigma &lt;dbl&gt; 4.000000, 4.025126, 4.050251, 4.075377, 4.100503, 4.125628, 4.150754, 4.175879, 4.20… d_grid contains every combination of mu and sigma across their specified values. Instead of base R sapply(), we’ll do the computations by making a custom function which we’ll then plug into purrr::map2(). grid_function &lt;- function(mu, sigma) { dnorm(d2$height, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’re ready to complete the tibble. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2(mu, sigma, grid_function)) %&gt;% unnest(log_likelihood) %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) head(d_grid) ## # A tibble: 6 x 7 ## mu sigma log_likelihood prior_mu prior_sigma product probability ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 140 4 -3813. -5.72 -3.91 -3822. 0 ## 2 140 4.03 -3778. -5.72 -3.91 -3787. 0 ## 3 140 4.05 -3743. -5.72 -3.91 -3753. 0 ## 4 140 4.08 -3709. -5.72 -3.91 -3719. 0 ## 5 140 4.10 -3676. -5.72 -3.91 -3686. 0 ## 6 140 4.13 -3644. -5.72 -3.91 -3653. 0 In the final d_grid, the probability vector contains the posterior probabilities across values of mu and sigma. We can make a contour plot with geom_contour(). d_grid %&gt;% ggplot(aes(x = mu, y = sigma, z = probability)) + geom_contour() + labs(x = expression(mu), y = expression(sigma)) + coord_cartesian(xlim = range(d_grid$mu), ylim = range(d_grid$sigma)) + theme(panel.grid = element_blank()) We’ll make our heat map with geom_raster(). d_grid %&gt;% ggplot(aes(x = mu, y = sigma, fill = probability)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(x = expression(mu), y = expression(sigma)) + theme(panel.grid = element_blank()) 4.3.4 Sampling from the posterior. We can use dplyr::sample_n() to sample rows, with replacement, from d_grid. set.seed(4) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + scale_fill_viridis_c() + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) We can use pivot_longer() and then facet_wrap() to plot the densities for both mu and sigma at once. d_grid_samples %&gt;% pivot_longer(mu:sigma) %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) We’ll use the tidybayes package to compute their posterior modes and 95% HDIs. library(tidybayes) d_grid_samples %&gt;% pivot_longer(mu:sigma) %&gt;% group_by(name) %&gt;% mode_hdi(value) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.95 mode hdi ## 2 sigma 7.82 7.14 8.30 0.95 mode hdi Let’s say you wanted their posterior medians and 50% quantile-based intervals, instead. Just switch out the last line for median_qi(value, .width = .5). 4.3.4.1 Overthinking: Sample size and the normality of \\(\\sigma\\)’s posterior. Since we’ll be fitting models with brms almost exclusively from here on out, this section is largely mute. But we’ll do it anyway for the sake of practice. I’m going to break the steps up like before rather than compress the code together. Here’s d3. set.seed(4) (d3 &lt;- sample(d2$height, size = 20)) ## [1] 147.3200 154.9400 168.9100 156.8450 165.7350 151.7650 165.7350 156.2100 144.7800 154.9400 ## [11] 151.1300 147.9550 149.8600 162.5600 161.9250 164.4650 160.9852 151.7650 163.8300 149.8600 For our first step using d3, we’ll redefine d_grid. n &lt;- 200 # note we&#39;ve redefined the ranges of `mu` and `sigma` d_grid &lt;- crossing(mu = seq(from = 150, to = 170, length.out = n), sigma = seq(from = 4, to = 20, length.out = n)) Second, we’ll redefine our custom grid_function() function to operate over the height values of d3. grid_function &lt;- function(mu, sigma) { dnorm(d3, mean = mu, sd = sigma, log = T) %&gt;% sum() } Now we’ll use the amended grid_function() to make the posterior. d_grid &lt;- d_grid %&gt;% mutate(log_likelihood = map2_dbl(mu, sigma, grid_function)) %&gt;% mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T), prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) %&gt;% mutate(product = log_likelihood + prior_mu + prior_sigma) %&gt;% mutate(probability = exp(product - max(product))) Did you catch our use of purrr::map2_dbl(), there, in place of purrr::map2()? It turns out that purrr::map() and purrr::map2() always return a list (see here and here). But we can add the _dbl suffix to those functions, which will instruct the purrr package to return a double vector (i.e., a common kind of numeric vector). The advantage of that approach is we no longer need to follow our map() or map2() lines with unnest(). To learn more about the ins and outs of the map() family, check out this section from R4DS or Jenny Bryan’s purrr tutorial. Next we’ll sample_n() and plot. set.seed(4) d_grid_samples &lt;- d_grid %&gt;% sample_n(size = 1e4, replace = T, weight = probability) d_grid_samples %&gt;% ggplot(aes(x = mu, y = sigma)) + geom_point(size = .9, alpha = 1/15) + labs(x = expression(mu[samples]), y = expression(sigma[samples])) + theme(panel.grid = element_blank()) Behold the updated densities. d_grid_samples %&gt;% pivot_longer(mu:sigma) %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey33&quot;, size = 0) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) That labeller = label_parsed bit in the facet_wrap() function is what converted our subplot strip labels into Greek. You can learn more about labeller here. Anyway, our posterior for \\(\\sigma\\) isn’t so Gaussian with that small \\(n\\). This is the point in the project where we hop off the grid-approximation train. On the one hand, I think this is a great idea. Most of y’all reading this will never use grid approximation in a real-world applied data analysis. On the other hand, there is some pedagogical utility in practicing with it. It can help you grasp what it is we’re dong when we apply Bayes’ theorem. If you’d like more practice, check out the first several chapters in John Kruschke’s (2015) textbook and the corresponding chapters in my (2020c) ebook translating it into brms and tidyverse. 4.3.5 Finding the posterior distribution with quap brm(). Here we rewrite the statistical model, this time using font color to help differentiate the likelihood from the prior(s). \\[\\begin{align*} \\color{red}{\\text{heights}_i} &amp; \\color{red}\\sim \\color{red}{\\operatorname{Normal}(\\mu, \\sigma)} &amp;&amp; \\color{red}{\\text{likelihood}} \\\\ \\color{blue}\\mu &amp; \\color{blue}\\sim \\color{blue}{\\operatorname{Normal}(178, 20)} &amp;&amp; \\color{blue}{\\text{prior}} \\\\ \\color{blue}\\sigma &amp; \\color{blue}\\sim \\color{blue}{\\operatorname{Uniform}(0, 50)} \\end{align*}\\] We won’t actually use rethinking::quap(). It’s time to jump straight to the primary brms modeling function, brm(). In the text, McElreath indexed his models with names like m4.1. I will largely follow that convention, but will replace the m with a b to stand for the brms package. Here’s how to fit the first model for this chapter. b4.1 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.01&quot;) McElreath’s uniform prior for \\(\\sigma\\) was rough on brms. It took an unusually-large number of warmup iterations before the chains sampled properly. As McElreath covered in Chapter 9, Hamiltonian Monte Carlo (HMC) tends to work better when you default to an exponential or half Cauchy for \\(\\sigma\\). Here’s how to do so with the half Cauchy.1 b4.1_hc &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 20), class = Intercept), # the magic lives here prior(cauchy(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.01_hc&quot;) This leads to an important point. After running a model fit with HMC, it’s a good idea to inspect the chains. As we’ll see, McElreath covered visual chain diagnostics in Chapter 9. Here’s a typical way to do so with brms. plot(b4.1_hc) If you want detailed diagnostics for the HMC chains, call launch_shinystan(b4.1). That’ll keep you busy for a while. But anyway, the chains look good. We can reasonably trust the results. Here’s how to get the model summary of our brm() object. print(b4.1_hc) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.61 0.41 153.81 155.42 1.00 3833 2967 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 7.74 0.29 7.20 8.35 1.00 3511 2853 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The summary() function works in a similar way. You can also get a Stan-like summary [see the RStan: the R interface to Stan vignette; Stan Development Team (2020)] with a little indexing. b4.1_hc$fit ## Inference for Stan model: dd78ed3cf429c248161e09fa2f6f34e3. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 154.61 0.01 0.41 153.81 154.34 154.61 154.89 155.42 3796 1 ## sigma 7.74 0.00 0.29 7.20 7.54 7.73 7.93 8.35 3553 1 ## lp__ -1227.52 0.02 1.00 -1230.16 -1227.91 -1227.22 -1226.79 -1226.54 1984 1 ## ## Samples were drawn using NUTS(diag_e) at Sun Mar 14 16:17:25 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Whereas rethinking defaults to 89% intervals, using print() or summary() with brms models defaults to 95% intervals. Unless otherwise specified, I will stick with 95% intervals throughout. However, if you really want those 89% intervals, an easy way is with the prob argument within brms::summary() or brms::print(). summary(b4.1_hc, prob = .89) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.61 0.41 153.96 155.27 1.00 3833 2967 ## ## Family Specific Parameters: ## Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS ## sigma 7.74 0.29 7.29 8.23 1.00 3511 2853 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Anyways, here’s the brms::brm() code for the model with the very-narrow-\\(\\mu\\)-prior corresponding to the rethinking::quap() code in McElreath’s R code 4.31. b4.2 &lt;- brm(data = d2, family = gaussian, height ~ 1, prior = c(prior(normal(178, 0.1), class = Intercept), prior(uniform(0, 50), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.02&quot;) plot(b4.2) I had to increase the warmup due to convergence issues. After doing so, everything looks to be on the up and up. The chains look great. Here’s the model summary(). summary(b4.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 ## Data: d2 (Number of observations: 352) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 177.86 0.10 177.66 178.06 1.00 3467 2724 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 24.59 0.91 22.95 26.54 1.01 483 422 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Subsetting the summary() output with $fixed provides a convenient way to compare the Intercept summaries between b4.1_hc and b4.2. rbind(summary(b4.1_hc)$fixed, summary(b4.2)$fixed) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 154.6133 0.4100253 153.8146 155.4213 1.000225 3833 2967 ## Intercept 177.8637 0.1014405 177.6609 178.0614 1.000384 3467 2724 4.3.6 Sampling from a quap() brm() fit. brms doesn’t seem to have a convenience function that works the way vcov() does for rethinking. For example: vcov(b4.1_hc) ## Intercept ## Intercept 0.1681208 This only returns the first element in the matrix it did for rethinking. That is, it appears brms::vcov() only returns the variance/covariance matrix for the single-level \\(\\beta\\) parameters. However, if you really wanted this information, you could get it after putting the HMC chains in a data frame. We do that with the posterior_samples(), which we’ll be using a lot of as we go along. post &lt;- posterior_samples(b4.1_hc) head(post) ## b_Intercept sigma lp__ ## 1 154.2645 7.992651 -1227.255 ## 2 154.6744 7.427472 -1227.075 ## 3 154.7472 7.410110 -1227.193 ## 4 154.7948 8.470115 -1229.452 ## 5 154.2154 7.940177 -1227.219 ## 6 154.7131 7.744806 -1226.553 Now select() the columns containing the draws from the desired parameters and feed them into cov(). select(post, b_Intercept:sigma) %&gt;% cov() ## b_Intercept sigma ## b_Intercept 0.168120762 -0.005250413 ## sigma -0.005250413 0.086979181 That was “(1) a vector of variances for the parameters and (2) a correlation matrix” for them (p. 90). Here are just the variances (i.e., the diagonal elements) and the correlation matrix. # variances select(post, b_Intercept:sigma) %&gt;% cov() %&gt;% diag() ## b_Intercept sigma ## 0.16812076 0.08697918 # correlation post %&gt;% select(b_Intercept, sigma) %&gt;% cor() ## b_Intercept sigma ## b_Intercept 1.00000000 -0.04341854 ## sigma -0.04341854 1.00000000 With our post &lt;- posterior_samples(b4.1_hc) code from a few lines above, we’ve already produced the brms version of what McElreath achieved with extract.samples() on page 90. However, what happened under the hood was different. Whereas rethinking used the mvnorm() function from the MASS package (Ripley, 2019; Venables &amp; Ripley, 2002), with brms we just extracted the iterations of the HMC chains and put them in a data frame. str(post) ## &#39;data.frame&#39;: 4000 obs. of 3 variables: ## $ b_Intercept: num 154 155 155 155 154 ... ## $ sigma : num 7.99 7.43 7.41 8.47 7.94 ... ## $ lp__ : num -1227 -1227 -1227 -1229 -1227 ... Notice how our data frame, post, includes a third vector named lp__. That’s the log posterior. For details, see the brms reference manual (Bürkner, 2021i), the “The Log-Posterior (function and gradient)” section of the Stan Development Team’s (2020) vignette, RStan: the R interface to Stan, or Stephen Martin’s nice explanation of the log posterior on the Stan Forums. The log posterior will largely be outside of our focus in this ebook. The summary() function doesn’t work for brms posterior data frames quite the way precis() does for posterior data frames from the rethinking package. Behold the results. summary(post[, 1:2]) ## b_Intercept sigma ## Min. :153.0 Min. :6.900 ## 1st Qu.:154.3 1st Qu.:7.541 ## Median :154.6 Median :7.733 ## Mean :154.6 Mean :7.744 ## 3rd Qu.:154.9 3rd Qu.:7.934 ## Max. :156.2 Max. :8.950 Here’s one option using the transpose of a quantile() call nested within apply(), which is a very general function you can learn more about here or here. t(apply(post[, 1:2], 2, quantile, probs = c(.5, .025, .75))) ## 50% 2.5% 75% ## b_Intercept 154.612809 153.814609 154.887202 ## sigma 7.733021 7.199217 7.934425 The base R code is compact, but somewhat opaque. Here’s how to do something similar with more explicit tidyverse code. post %&gt;% pivot_longer(-lp__) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value), `2.5%` = quantile(value, probs = .025), `97.5%` = quantile(value, probs = .975)) %&gt;% mutate_if(is.numeric, round, digits = 2) ## # A tibble: 2 x 5 ## name mean sd `2.5%` `97.5%` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept 155. 0.41 154. 155. ## 2 sigma 7.74 0.290 7.2 8.35 You can always get pretty similar information by just putting the brm() fit object into posterior_summary(). posterior_summary(b4.1_hc) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.613285 0.4100253 153.814609 155.421274 ## sigma 7.744364 0.2949223 7.199217 8.346968 ## lp__ -1227.517317 0.9973747 -1230.155469 -1226.541248 And if you’re willing to drop the posterior \\(SD\\)s, you can use tidybayes::mean_hdi(), too. post %&gt;% pivot_longer(-lp__) %&gt;% group_by(name) %&gt;% mean_qi(value) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_Intercept 155. 154. 155. 0.95 mean qi ## 2 sigma 7.74 7.20 8.35 0.95 mean qi Though none of these solutions get you those sweet little histograms, you can always make those for your HMC models by inserting the desired posterior draws into histospark(). rbind(histospark(post$b_Intercept), histospark(post$sigma)) ## [,1] ## [1,] &quot;▁▁▅▇▂▁▁&quot; ## [2,] &quot;▁▁▂▅▇▇▃▁▁▁▁&quot; Hell, you can even tack those onto the output from our verbose tidyverse code from a few blocks up. post %&gt;% pivot_longer(-lp__) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value), `2.5%` = quantile(value, probs = .025), `97.5%` = quantile(value, probs = .975)) %&gt;% mutate_if(is.numeric, round, digits = 2) %&gt;% mutate(histospark = c(histospark(post$b_Intercept), histospark(post$sigma))) ## # A tibble: 2 x 6 ## name mean sd `2.5%` `97.5%` histospark ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 b_Intercept 155. 0.41 154. 155. ▁▁▅▇▂▁▁ ## 2 sigma 7.74 0.290 7.2 8.35 ▁▁▂▅▇▇▃▁▁▁▁ 4.3.6.1 Overthinking: Start values for quap() brm(). We won’t be emphasizing start values in this ebook. But, yes, you can set start values for the HMC chains from brms, too. Within the brm() function, you do so with the inits argument. From the brm section within the brms reference manual, we read: Either \"random\" or \"0\". If inits is \"random\" (the default), Stan will randomly generate initial values for parameters. If it is \"0\", all parameters are initialized to zero. This option is sometimes useful for certain families, as it happens that default (\"random\") inits cause samples to be essentially constant. Generally, setting inits = \"0\" is worth a try, if chains do not behave well. Alternatively, inits can be a list of lists containing the initial values, or a function (or function name) generating initial values. The latter options are mainly implemented for internal testing but are available to users if necessary. If specifying initial values using a list or a function then currently the parameter names must correspond to the names used in the generated Stan code (not the names used in R). 4.3.6.2 Overthinking: Under the hood with multivariate sampling. Again, brms::posterior_samples() is not the same as rethinking::extract.samples(). Rather than use the MASS::mvnorm(), brms takes the draws from the HMC chains. McElreath covered all of this in Chapter 9 and we will too. You might also look at the brms reference manual or GitHub page for details. To get documentation in a hurry, you could also just execute ?posterior_samples. 4.4 Linear prediction Here’s our scatter plot of weight and height. ggplot(data = d2, aes(x = weight, y = height)) + geom_point(shape = 1, size = 2) + theme_bw() + theme(panel.grid = element_blank()) There’s obviously a relationship: Knowing a person’s weight helps you predict height. To make this vague observation into a more precise quantitative model that relates values of weight to plausible values of height, we need some more technology. How do we take our Gaussian model from the previous section and incorporate predictor variables? (p. 92) 4.4.1 The linear model strategy. The strategy is to make the parameter for the mean of a Gaussian distribution, \\(\\mu\\), into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the linear model. The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship. (p. 92, emphasis in the original) Like we did for our first model without a predictor, we’ll use font color to help differentiate between the likelihood and prior(s) of our new univariable model, \\[\\begin{align*} \\color{red}{\\text{height}_i} &amp; \\color{red}\\sim \\color{red}{\\operatorname{Normal}(\\mu_i, \\sigma)} &amp;&amp; \\color{red}{\\text{likelihood}} \\\\ \\color{red}{\\mu_i} &amp; \\color{red}= \\color{red}{\\alpha + \\beta (\\text{weight}_i - \\overline{\\text{weight}})} &amp;&amp; \\color{red}{\\text{\\{the linear model is just a special part of the likelihood\\}} } \\\\ \\color{blue}\\alpha &amp; \\color{blue}\\sim \\color{blue}{\\operatorname{Normal}(178, 20)} &amp;&amp; \\color{blue}{\\text{prior(s)}} \\\\ \\color{blue}\\beta &amp; \\color{blue}\\sim \\color{blue}{\\operatorname{Normal}(0, 10)} \\\\ \\color{blue}\\sigma &amp; \\color{blue}\\sim \\color{blue}{\\operatorname{Uniform}(0, 50)}. \\end{align*}\\] Do note that \\((\\text{weight}_i - \\overline{\\text{weight}})\\) part. As we’ll see, it’s often advantageous to mean center our predictors. 4.4.1.1 Probability of the data. Let’s begin with just the probability of the observed height, the first line of the model. This is nearly identical to before, except now there is a little index \\(i\\) on the \\(\\mu\\) as well as the [\\(\\text{height}\\)]. You can read [\\(\\text{height}_i\\)] as “each [\\(\\text{height}\\)]” and \\(\\mu_i\\) as “each \\(\\mu\\).” The mean \\(\\mu\\) now depends upon unique values on each row \\(i\\). So the little \\(i\\) on \\(\\mu_i\\) indicates that the mean depends upon the row. (p. 93, emphasis in the original) 4.4.1.2 Linear model. The mean \\(\\mu\\) is no longer a parameter to be estimated. Rather, as seen in the second line of the model, \\(\\mu_i\\) is constructed from other parameters, \\(\\alpha\\) and \\(\\beta\\), and the observed variable [\\(\\text{weight}\\)]. This line is not a stochastic relationship–there is no \\(\\sim\\) in it, but rather an \\(=\\) in it–because the definition of \\(\\mu_i\\) is deterministic. That is to say that, once we know \\(\\alpha\\) and \\(\\beta\\) and [\\(\\text{weight}_i\\)], we know \\(\\mu_i\\) with certainty. The value [\\(\\text{weight}_i\\)] is just the weight value on row \\(i\\). It refers to the same individual as the height value, [\\(\\text{height}_i\\)], on the same row. The parameters \\(\\alpha\\) and \\(\\beta\\) are more mysterious. Where did they come from? We made them up…. You’ll be making up all manner of parameters as your skills improve. (p. 93) 4.4.1.2.1 Rethinking: Nothing special or natural about linear models. Note that there’s nothing special about the linear model, really. You can choose a different relationship between \\(\\alpha\\) and \\(\\beta\\) and \\(\\mu\\). For example, the following is a perfectly legitimate definition for \\(\\mu_i\\): \\[\\mu_i = \\alpha \\exp(- \\beta x_i)\\] This does not define a linear regression, but it does define a regression model. The linear relationship we are using instead is conventional, but nothing requires that you use it. (p. 94) 4.4.1.3 Priors. The remaining lines in the model define distributions for the unobserved variables. These variables are commonly known as parameters, and their distributions as priors. There are three parameters: \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\). You’ve seen priors for \\(\\alpha\\) and \\(\\sigma\\) before, although \\(\\alpha\\) was called \\(\\mu\\) back then. The prior for \\(\\beta\\) deserves explanation. Why have a Gaussian prior with mean zero? (p. 94) We’ll simulate to find out. Instead of using a loop to make our data for Figure 4.5, we’ll stay within the tidyverse. set.seed(2971) # how many lines would you like? n_lines &lt;- 100 lines &lt;- tibble(n = 1:n_lines, a = rnorm(n_lines, mean = 178, sd = 20), b = rnorm(n_lines, mean = 0, sd = 10)) %&gt;% expand(nesting(n, a, b), weight = range(d2$weight)) %&gt;% mutate(height = a + b * (weight - mean(d2$weight))) head(lines) ## # A tibble: 6 x 5 ## n a b weight height ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 191. -7.06 31.1 289. ## 2 1 191. -7.06 63.0 63.5 ## 3 2 199. 0.839 31.1 187. ## 4 2 199. 0.839 63.0 214. ## 5 3 202. 3.93 31.1 147. ## 6 3 202. 3.93 63.0 272. Now we’ll plot the left panel from Figure 4.5. lines %&gt;% ggplot(aes(x = weight, y = height, group = n)) + geom_hline(yintercept = c(0, 272), linetype = 2:1, size = 1/3) + geom_line(alpha = 1/10) + coord_cartesian(ylim = c(-100, 400)) + ggtitle(&quot;b ~ dnorm(0, 10)&quot;) + theme_classic() The pattern doesn’t look like any human population at all. It essentially says that the relationship between weight and height could be absurdly positive or negative. Before we’ve even seen the data, this is a bad model. Can we do better? We can do better immediately. (pp. 95–96) One thing we know from the outset is that the correlation between human height and weight is positive. We might not be sure of the magnitude, but it’s definitely the case that, on average, taller people are heavier people. Within the univariable regression context, this implies that the regression coefficient for weight predicting height will be positive. It might be unclear how large that coefficient will be, but it will certainly be above zero. One way we might encode this information in our data is by using the log-normal distribution for our \\(\\beta\\) prior. Here’s what \\(\\operatorname{Log-Normal}(0, 1)\\) looks like. set.seed(4) tibble(b = rlnorm(1e4, mean = 0, sd = 1)) %&gt;% ggplot(aes(x = b)) + geom_density(fill = &quot;grey92&quot;) + coord_cartesian(xlim = c(0, 5)) + theme_classic() If you’re unfamiliar with the log-normal distribution, it is the distribution whose logarithm is normally distributed. For example, here’s what happens when we compare \\(\\operatorname{Normal}(0, 1)\\) with \\(\\log \\big ( \\operatorname{Log-Normal}(0, 1) \\big)\\). set.seed(4) tibble(rnorm = rnorm(1e5, mean = 0, sd = 1), `log(rlognorm)` = log(rlnorm(1e5, mean = 0, sd = 1))) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_density(fill = &quot;grey92&quot;) + coord_cartesian(xlim = c(-3, 3)) + theme_classic() + facet_wrap(~ name, nrow = 2) They are the same within simulation variance. Also, did you notice how we simulated those log-normal data with mean = 0, sd = 1? Those values are what the mean and standard deviation of the output from the rlnorm() function after they are log transformed. The formulas for the actual mean and standard deviation for the log-normal distribution itself are complicated (see here). They are \\[\\begin{align*} \\text{mean} &amp; = \\exp \\left (\\mu + \\frac{\\sigma^2}{2} \\right) &amp; \\text{and} \\\\ \\text{standard deviation} &amp; = \\sqrt{[\\exp(\\sigma ^{2})-1] \\; \\exp(2\\mu +\\sigma ^{2})}. \\end{align*}\\] If we follow those formulas through, there are the mean and standard deviation for \\(\\operatorname{Log-Normal}(0, 1)\\). mu &lt;- 0 sigma &lt;- 1 # mean exp(mu + (sigma^2) / 2) ## [1] 1.648721 # sd sqrt((exp(sigma^2) - 1) * exp(2 * mu + sigma^2)) ## [1] 2.161197 Let’s confirm with simulated draws from rlnorm(). set.seed(4) tibble(x = rlnorm(1e7, mean = 0, sd = 1)) %&gt;% summarise(mean = mean(x), sd = sd(x)) ## # A tibble: 1 x 2 ## mean sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.65 2.17 But okay, “so what [do all these complications] earn us? Do the prior predictive simulation again, now with the Log-Normal prior:” (p. 96). # make a tibble to annotate the plot text &lt;- tibble(weight = c(34, 43), height = c(0 - 25, 272 + 25), label = c(&quot;Embryo&quot;, &quot;World&#39;s tallest person (272 cm)&quot;)) # simulate set.seed(2971) tibble(n = 1:n_lines, a = rnorm(n_lines, mean = 178, sd = 20), b = rlnorm(n_lines, mean = 0, sd = 1)) %&gt;% expand(nesting(n, a, b), weight = range(d2$weight)) %&gt;% mutate(height = a + b * (weight - mean(d2$weight))) %&gt;% # plot ggplot(aes(x = weight, y = height, group = n)) + geom_hline(yintercept = c(0, 272), linetype = 2:1, size = 1/3) + geom_line(alpha = 1/10) + geom_text(data = text, aes(label = label), size = 3) + coord_cartesian(ylim = c(-100, 400)) + ggtitle(&quot;log(b) ~ dnorm(0, 1)&quot;) + theme_classic() “This is much more sensible. There is still a rare impossible relationship. But nearly all lines in the joint prior for \\(\\alpha\\) and \\(\\beta\\) are now within human reason” (p. 96) 4.4.1.3.1 Rethinking: What’s the correct prior? Good luck with that question. Hang around on academic Twitter long enough and you’ll see folks debating this. This is a mistake. There is no more a uniquely correct prior than there is a uniquely correct likelihood. Statistical models are machines for inference. Many machines will work, but some work better than others. Priors can be wrong, but only in the same sense that a kind of hammer can be wrong for building a table. (p. 96) 4.4.1.3.2 Rethinking: Prior predictive simulation and \\(p\\)-hacking. “We don’t pay any attention to \\(p\\)-values in this book” (p. 97). Off hand, I’m not sure of the exact origin of the term \\(p\\)-hacking. But the paper by Simmons, Nelson and Simonsohn (2011), False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant, is often cited as an introduction to the problem. 4.4.2 Finding the posterior distribution. Unlike with McElreath’s quap() formula syntax, I’m not aware that you can just specify something like weight – xbar in the formula argument in brm(). However, the alternative is easy: Just make a new variable in the data that is equivalent to weight – mean(weight). We’ll call it weight_c. d2 &lt;- d2 %&gt;% mutate(weight_c = weight - mean(weight)) Unlike with McElreath’s rethinking package, the brms::brm() syntax doesn’t mirror the statistical notation. But here are the analogues to the exposition at the bottom of page 97: \\(\\text{height}_i \\sim \\operatorname{Normal}(\\mu_i, \\sigma)\\): family = gaussian, \\(\\mu_i = \\alpha + \\beta \\text{weight}_i\\): height ~ 1 + weight_c, \\(\\alpha \\sim \\operatorname{Normal}(178, 20)\\): prior(normal(178, 20), class = Intercept, \\(\\beta \\sim \\operatorname{Log-Normal}(0, 1)\\): prior(lognormal(0, 1), class = b), and \\(\\sigma \\sim \\operatorname{Uniform}(0, 50)\\): prior(uniform(0, 50), class = sigma). Thus, to add a predictor you just the + operator in the model formula. b4.3 &lt;- brm(data = d2, family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b), prior(uniform(0, 50), class = sigma)), iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03&quot;) This was another example of how using a uniform prior for \\(\\sigma\\) was a major pain. As you’ll see in later chapters, you usually only need to set iter = 2000, warmup = 1000 when using even moderately better priors. But for now, we’ll continue to find ways to stick close to the text. Here are the trace plots. plot(b4.3) 4.4.2.1 Overthinking: Logs and exps, oh my. brms does not allow users to insert coefficients into functions like \\(\\exp()\\) within the conventional formula syntax. We can fit a brms model like McElreath’s m4.3b if we adopt what’s called the non-linear syntax (Bürkner, 2021e). The non-linear syntax is a lot like the syntax McElreath uses in rethinking in that it typically includes both predictor and variable names in the formula. Since this is so early in the book and we’re just working through a problem in an Overthinking tangent, I won’t go into a full-blow explanation, here. There will be many more opportunities to practice with the non-linear syntax in the chapters to come (e.g., Section 5.3.2, Section 6.2.1). For now, here’s how we might fit the model. b4.3b &lt;- brm(data = d2, family = gaussian, bf(height ~ a + exp(lb) * weight_c, a ~ 1, lb ~ 1, nl = TRUE), prior = c(prior(normal(178, 20), class = b, nlpar = a), prior(normal(0, 1), class = b, nlpar = lb), prior(uniform(0, 50), class = sigma)), iter = 31000, warmup = 30000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03b&quot;) We continue to need unusually large numbers of warmup iterations due to that pesky \\(\\operatorname{Uniform}(0, 50)\\) prior on \\(\\sigma\\). If you execute summary(b4.3b), you’ll see the intercept and \\(\\sigma\\) summaries for this model are about the same as those for b4.3, above. The difference is for the \\(\\beta\\) parameter, which we called lb in the b4.3b model. If we term that parameter from b4.3 as \\(\\beta^\\text{b4.3}\\) and the one from our new model \\(\\beta^\\text{b4.3b}\\), it turns out that \\(\\beta^\\text{b4.3} = \\exp \\left (\\beta^\\text{b4.3b} \\right )\\). fixef(b4.3)[&quot;weight_c&quot;, &quot;Estimate&quot;] ## [1] 0.9040527 fixef(b4.3b)[&quot;lb_Intercept&quot;, &quot;Estimate&quot;] %&gt;% exp() ## [1] 0.9036588 They’re the same within simulation variance. 4.4.3 Interpreting the posterior distribution. “One trouble with statistical models is that they are hard to understand” (p. 99). Welcome to the world of applied statistics, friends. 😅 4.4.3.0.1 Rethinking: What do parameters mean? A basic issue with interpreting model-based estimates is in knowing the meaning of parameters. There is no consensus about what a parameter means, however, because different people take different philosophical stances towards models, probability, and prediction. The perspective in this book is a common Bayesian perspective: Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model. (p. 99, emphasis in the original) 4.4.3.1 Tables of marginal distributions. With a little [] subsetting we can exclude the log posterior from our summary for b4.3. posterior_summary(b4.3)[1:3, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 154.6 0.27 154.08 155.14 ## b_weight_c 0.9 0.04 0.83 0.99 ## sigma 5.1 0.19 4.75 5.50 If we put our brms fit into the vcov() function, we’ll get the variance/covariance matrix of the intercept and weight_c coefficient. vcov(b4.3) %&gt;% round(3) ## Intercept weight_c ## Intercept 0.076 0.000 ## weight_c 0.000 0.002 No \\(\\sigma\\), however. To get that, we’ll have to extract the posterior draws and use the cov() function, instead. posterior_samples(b4.3) %&gt;% select(-lp__) %&gt;% cov() %&gt;% round(digits = 3) ## b_Intercept b_weight_c sigma ## b_Intercept 0.076 0.000 0.000 ## b_weight_c 0.000 0.002 0.000 ## sigma 0.000 0.000 0.036 The pairs() function will work for a brms fit much like it would one from rethinking. It will show “both the marginal posteriors and the covariance” (p. 100). pairs(b4.3) 4.4.3.2 Plotting posterior inference against the data. “It’s almost always much more useful to plot the posterior inference against the data. Not only does plotting help in interpreting the posterior, but it also provides an informal check on model assumptions” (p. 100). Here is the code for Figure 4.6. Note our use of the fixef() function. d2 %&gt;% ggplot(aes(x = weight_c, y = height)) + geom_abline(intercept = fixef(b4.3)[1], slope = fixef(b4.3)[2]) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + theme_classic() Note how the breaks on our \\(x\\)-axis look off. That’s because we fit the model with weight_c and we plotted the points in that metric, too. Since we computed weight_c by subtracting the mean of weight from the data, we can adjust the \\(x\\)-axis break point labels by simply adding that value back. labels &lt;- c(-10, 0, 10) + mean(d2$weight) %&gt;% round(digits = 0) d2 %&gt;% ggplot(aes(x = weight_c, y = height)) + geom_abline(intercept = fixef(b4.3)[1], slope = fixef(b4.3)[2]) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + scale_x_continuous(&quot;weight&quot;, breaks = c(-10, 0, 10), labels = labels) + theme_bw() + theme(panel.grid = element_blank()) 4.4.3.3 Adding uncertainty around the mean. Be default, we extract all the posterior iterations with posterior_samples(). post &lt;- posterior_samples(b4.3) post %&gt;% slice(1:5) # this serves a similar function as `head()` ## b_Intercept b_weight_c sigma lp__ ## 1 154.0395 0.8883175 5.019815 -1081.040 ## 2 155.0064 0.8813894 5.149540 -1080.008 ## 3 154.2648 0.8641695 5.200915 -1080.066 ## 4 154.4480 0.9149853 4.956529 -1079.143 ## 5 154.7072 0.9176580 5.190356 -1079.014 Here are the four models leading up to McElreath’s Figure 4.7. N &lt;- 10 b4.3_010 &lt;- brm(data = d2 %&gt;% slice(1:N), # note our tricky use of `N` and `slice()` family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b), prior(uniform(0, 50), class = sigma)), iter = 11000, warmup = 10000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03_010&quot;) N &lt;- 50 b4.3_050 &lt;- brm(data = d2 %&gt;% slice(1:N), family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b), prior(uniform(0, 50), class = sigma)), iter = 11000, warmup = 10000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03_050&quot;) N &lt;- 150 b4.3_150 &lt;- brm(data = d2 %&gt;% slice(1:N), family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b), prior(uniform(0, 50), class = sigma)), iter = 21000, warmup = 20000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03_150&quot;) N &lt;- 352 b4.3_352 &lt;- brm(data = d2 %&gt;% slice(1:N), family = gaussian, height ~ 1 + weight_c, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b), prior(uniform(0, 50), class = sigma)), iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.03_352&quot;) I’m not going to clutter up the document with all the trace plots and coefficient summaries from these four models. But here’s how to get that information. plot(b4.3_010) print(b4.3_010) plot(b4.3_050) print(b4.3_050) plot(b4.3_150) print(b4.3_150) plot(b4.3_352) print(b4.3_352) We’ll need to put the chains of each model into data frames. post010 &lt;- posterior_samples(b4.3_010) post050 &lt;- posterior_samples(b4.3_050) post150 &lt;- posterior_samples(b4.3_150) post352 &lt;- posterior_samples(b4.3_352) Here is the code for the four individual plots. p1 &lt;- ggplot(data = d2[1:10, ], aes(x = weight_c, y = height)) + geom_abline(intercept = post010[1:20, 1], slope = post010[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight_c), ylim = range(d2$height)) + labs(subtitle = &quot;N = 10&quot;) p2 &lt;- ggplot(data = d2[1:50, ], aes(x = weight_c, y = height)) + geom_abline(intercept = post050[1:20, 1], slope = post050[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight_c), ylim = range(d2$height)) + labs(subtitle = &quot;N = 50&quot;) p3 &lt;- ggplot(data = d2[1:150, ], aes(x = weight_c, y = height)) + geom_abline(intercept = post150[1:20, 1], slope = post150[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight_c), ylim = range(d2$height)) + labs(subtitle = &quot;N = 150&quot;) p4 &lt;- ggplot(data = d2[1:352, ], aes(x = weight_c, y = height)) + geom_abline(intercept = post352[1:20, 1], slope = post352[1:20, 2], size = 1/3, alpha = .3) + geom_point(shape = 1, size = 2, color = &quot;royalblue&quot;) + coord_cartesian(xlim = range(d2$weight_c), ylim = range(d2$height)) + labs(subtitle = &quot;N = 352&quot;) Note how we used the good old bracket syntax (e.g., d2[1:10 , ]) to index rows from our d2 data. With tidyverse-style syntax, we could have done slice(d2, 1:10) or d2 %&gt;% slice(1:10) instead. Now we can combine the ggplots with patchwork syntax to make the full version of Figure 4.7. (p1 + p2 + p3 + p4) &amp; scale_x_continuous(&quot;weight&quot;, breaks = c(-10, 0, 10), labels = labels) &amp; theme_classic() “Notice that the cloud of regression lines grows more compact as the sample size increases. This is a result of the model growing more confident about the location of the mean” (p. 102). 4.4.3.4 Plotting regression intervals and contours. Since we used weight_c to fit our model, we might first want to understand what exactly the mean value is for weight. mean(d2$weight) ## [1] 44.99049 Just a hair under 45. If we’re interested in \\(\\mu\\) at weight = 50, that implies we’re also interested in \\(\\mu\\) at weight_c + 5.01. mu_at_50 &lt;- post %&gt;% transmute(mu_at_50 = b_Intercept + b_weight_c + 5.01) head(mu_at_50) ## mu_at_50 ## 1 159.9379 ## 2 160.8978 ## 3 160.1390 ## 4 160.3730 ## 5 160.6349 ## 6 160.3570 And here is a version McElreath’s Figure 4.8 density plot. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50)) + geom_density(size = 0, fill = &quot;royalblue&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() We’ll use mean_hdi() to get both 89% and 95% HPDIs along with the mean. mean_hdi(mu_at_50[, 1], .width = c(.89, .95)) ## y ymin ymax .width .point .interval ## 1 160.5127 160.0827 160.9675 0.89 mean hdi ## 2 160.5127 159.9828 161.0691 0.95 mean hdi If you wanted to express those sweet 95% HPDIs on your density plot, you might use tidybayes::stat_halfeye(). Since stat_halfeye() also returns a point estimate, we’ll throw in the mode. mu_at_50 %&gt;% ggplot(aes(x = mu_at_50, y = 0)) + stat_halfeye(point_interval = mode_hdi, .width = .95, fill = &quot;royalblue&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(mu[&quot;height | weight = 50&quot;])) + theme_classic() With brms, you would use fitted() to do what McElreath accomplished with link(). mu &lt;- fitted(b4.3, summary = F) str(mu) ## num [1:4000, 1:352] 157 158 157 157 157 ... When you specify summary = F, fitted() returns a matrix of values with as many rows as there were post-warmup iterations across your HMC chains and as many columns as there were cases in your analysis. Because we had 4,000 post-warmup iterations and \\(n = 352\\), fitted() returned a matrix of 4,000 rows and 352 vectors. If you omitted the summary = F argument, the default is TRUE and fitted() will return summary information instead. Much like rethinking’s link(), brms::fitted() can accommodate custom predictor values with its newdata argument. weight_seq &lt;- tibble(weight = 25:70) %&gt;% mutate(weight_c = weight - mean(d2$weight)) mu &lt;- fitted(b4.3, summary = F, newdata = weight_seq) %&gt;% data.frame() %&gt;% # here we name the columns after the `weight` values from which they were computed set_names(25:70) %&gt;% mutate(iter = 1:n()) Anticipating ggplot2, we went ahead and converted the output to a data frame. But we might do a little more data processing with the aid of tidyr::pivot_longer(), which will convert the data from the wide format to the long format. If you are new to the distinction between wide and long data, you can learn more from the Pivot data from wide to long vignette from the tidyverse team (2020); Simon Ejdemyr’s blog post, Wide &amp; long data; or Karen Grace-Martin’s blog post, The wide and long data format for repeated measures data. mu &lt;- mu %&gt;% pivot_longer(-iter, names_to = &quot;weight&quot;, values_to = &quot;height&quot;) %&gt;% # we might reformat `weight` to numerals mutate(weight = as.numeric(weight)) head(mu) ## # A tibble: 6 x 3 ## iter weight height ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 25 136. ## 2 1 26 137. ## 3 1 27 138. ## 4 1 28 139. ## 5 1 29 140. ## 6 1 30 141. Now our data processing is done, here we reproduce McElreath’s Figure 4.9.a. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(data = mu %&gt;% filter(iter &lt; 101), color = &quot;navyblue&quot;, alpha = .05) + coord_cartesian(xlim = c(30, 65)) + theme(panel.grid = element_blank()) With fitted(), it’s quite easy to plot a regression line and its intervals. Just omit the summary = T argument. mu_summary &lt;- fitted(b4.3, newdata = weight_seq) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) head(mu_summary) ## Estimate Est.Error Q2.5 Q97.5 weight weight_c ## 1 136.5262 0.8833715 134.7405 138.1822 25 -19.99049 ## 2 137.4303 0.8437461 135.7240 139.0135 26 -18.99049 ## 3 138.3343 0.8043407 136.7118 139.8447 27 -17.99049 ## 4 139.2384 0.7651894 137.7003 140.6785 28 -16.99049 ## 5 140.1424 0.7263331 138.6846 141.5160 29 -15.99049 ## 6 141.0465 0.6878220 139.6690 142.3502 30 -14.99049 Here it is, our analogue to Figure 4.9.b. d2 %&gt;% ggplot(aes(x = weight, y = height)) + geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) if you wanted to use intervals other than the default 95% ones, you’d enter a probs argument like this: fitted(b4.3, newdata = weight.seq, probs = c(.25, .75)). The resulting third and fourth vectors from the fitted() object would be named Q25 and Q75 instead of the default Q2.5 and Q97.5. The Q prefix stands for quantile. 4.4.3.4.1 Rethinking: Overconfident intervals. The compatibility interval for the regression line in Figure 4.9 clings tightly to the MAP line. Thus there is very little uncertainty about the average height as a function of average weight. But you have to keep in mind that these inferences are always conditional on the model. Even a very bad model can have very tight compatibility intervals. It may help if you think of the regression line in Figure 4.9 as saying: Conditional on the assumption that height and weight are related by a straight line, then this is the most plausible line, and these are its plausible bounds. (p. 107, emphasis in the original) 4.4.3.4.2 Overthinking: How link fitted() works. Similar to rethinking::link(), brms::fitted() uses the formula from your model to compute the model expectations for a given set of predictor values. I use it a lot in this project. If you follow along, you’ll get a good handle on it. But to dive deeper, you can go here for the documentation. Though we won’t be using it in this project, brms users might want to know that fitted() is also an alias for the posterior_epred() function, about which you might learn more here. Users can always learn more about them and other functions in the brms reference manual. 4.4.3.5 Prediction intervals. Even though our statistical model (omitting priors for the sake of simplicity) is \\[\\text{height}_i \\sim \\operatorname{Normal}(\\mu_i = \\alpha + \\beta x_, \\sigma),\\] we’ve only been plotting the \\(\\mu\\) part. In order to bring in the variability expressed by \\(\\sigma\\), we’ll have to switch to the predict() function. Much as brms::fitted() was our analogue to rethinking::link(), brms::predict() is our analogue to rethinking::sim(). We can reuse our weight_seq data from before. But in case you forgot, here’s that code again. weight_seq &lt;- tibble(weight = 25:70) %&gt;% mutate(weight_c = weight - mean(d2$weight)) The predict() code looks a lot like what we used for fitted(). pred_height &lt;- predict(b4.3, newdata = weight_seq) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) pred_height %&gt;% slice(1:6) ## Estimate Est.Error Q2.5 Q97.5 weight weight_c ## 1 136.5890 5.204946 126.6381 146.9790 25 -19.99049 ## 2 137.2489 5.166086 126.9521 147.3029 26 -18.99049 ## 3 138.3838 5.246919 128.1976 148.8397 27 -17.99049 ## 4 139.2259 5.143560 129.2588 149.1741 28 -16.99049 ## 5 139.9700 5.170863 129.7009 149.9936 29 -15.99049 ## 6 141.0105 5.101771 130.6494 151.2377 30 -14.99049 This time the summary information in our data frame is for, as McElreath put it, “simulated heights, not distributions of plausible average height, \\(\\mu\\)” (p. 108). Another way of saying that is that these simulations are the joint consequence of both \\(\\mu\\) and \\(\\sigma\\), unlike the results of fitted(), which only reflect \\(\\mu\\). Figure 4.10 shows how you might visualize them. d2 %&gt;% ggplot(aes(x = weight)) + geom_ribbon(data = pred_height, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = mu_summary, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) Notice that the outline for the wide shaded interval is a little rough. This is the simulation variance in the tails of the sampled Gaussian values. If it really bothers you, increase the number of samples you take from the posterior distribution. (p. 109) With our brms model fitting approach, that would mean we’d have to refit b4.3 after specifying a larger number of post-warmup iterations with alterations to the iter and warmup parameters. 4.4.3.5.1 Overthinking: Rolling your own sim predict(). Here we follow McElreath’s example and do our model-based predictions by hand. Instead of relying on base R apply() and sapply(), here the main action is in expand(), the second mutate() line and the group_by() + summarise() combination. # `predict()` by hand post %&gt;% expand(nesting(b_Intercept, b_weight_c, sigma), weight = 25:70) %&gt;% mutate(weight_c = weight - mean(d2$weight)) %&gt;% mutate(sim_height = rnorm(n(), mean = b_Intercept + b_weight_c * weight_c, sd = sigma)) %&gt;% group_by(weight) %&gt;% summarise(mean = mean(sim_height), ll = quantile(sim_height, prob = .025), ul = quantile(sim_height, prob = .975)) %&gt;% # plot ggplot(aes(x = weight)) + geom_smooth(aes(y = mean, ymin = ll, ymax = ul), stat = &quot;identity&quot;, fill = &quot;grey83&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(data = d2, aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + coord_cartesian(xlim = range(d2$weight), ylim = range(d2$height)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) We specifically left out the fitted() intervals to make it more apparent what we were simulating. You might also note that we could have easily replaced that three-line summarise() code with a single line of tidybayes::mean_qi(sim_height), or whatever combination of central tendency and interval type you wanted (e.g., mode_hdi(sim_height, .width = .89)). 4.5 Curves from lines “The models so far all assume that a straight line describes the relationship. But there’s nothing special about straight lines, aside from their simplicity.” (p. 110). 4.5.1 Polynomial regression. Remember d? d %&gt;% glimpse() ## Rows: 544 ## Columns: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 147… ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, 34.… ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20.0,… ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,… McElreath suggested we plot height against weight using the full sample. d %&gt;% ggplot(aes(x = weight, y = height)) + geom_point(color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 2/3) + annotate(geom = &quot;text&quot;, x = 42, y = 115, label = &quot;This relation is\\nvisibly curved.&quot;, family = &quot;Times&quot;) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) Those variables two appear to follow an orderly relation, but whatever it is, it’s clearly not a simple straight line. The quadratic model is probably the most commonly used polynomial regression model. It follows the generic form \\[\\mu = \\alpha + \\beta_1 x_i + \\color{navy}{\\beta_2 x_i^2}.\\] McElreath warned: “Fitting these models to data is easy. Interpreting them can be hard” (p. 111). Standardizing will help brm() fit the model. We might standardize our weight variable like so. d &lt;- d %&gt;% mutate(weight_s = (weight - mean(weight)) / sd(weight)) %&gt;% mutate(weight_s2 = weight_s^2) While we were at it, we just went ahead and computed the weight_s2 variable. We can express our statistical model as \\[\\begin{align*} \\text{height}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{weight_s}_i + \\color{navy}{\\beta_2 \\text{weight_s}^2_i} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(178, 20) \\\\ \\beta_1 &amp; \\sim \\operatorname{Log-Normal}(0, 1) \\\\ \\color{navy}{\\beta_2} &amp; \\color{navy}\\sim \\color{navy}{\\operatorname{Normal}(0, 1)} \\\\ \\sigma &amp; \\sim \\operatorname{Uniform}(0, 50). \\end{align*}\\] Here’s how we might fit the quadratic model with brms. b4.5 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + weight_s2, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b, coef = &quot;weight_s&quot;), prior(normal(0, 1), class = b, coef = &quot;weight_s2&quot;), prior(uniform(0, 50), class = sigma)), iter = 30000, warmup = 29000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.05&quot;) Note our use of the coef argument within our prior statements. Since \\(\\beta_1\\) and \\(\\beta_2\\) are both parameters of class = b within the brms set-up, we need to use the coef argument in cases when we want their priors to differ. plot(b4.5) print(b4.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + weight_s + weight_s2 ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 30000; warmup = 29000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 146.03 0.38 145.29 146.78 1.01 1369 2019 ## weight_s 21.75 0.29 21.17 22.33 1.00 1773 2031 ## weight_s2 -7.77 0.28 -8.34 -7.25 1.01 901 935 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.81 0.18 5.48 6.18 1.00 2472 2150 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our quadratic plot requires new fitted()- and predict()-oriented wrangling. weight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) %&gt;% mutate(weight_s2 = weight_s^2) fitd_quad &lt;- fitted(b4.5, newdata = weight_seq) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) pred_quad &lt;- predict(b4.5, newdata = weight_seq) %&gt;% data.frame() %&gt;% bind_cols(weight_seq) Behold the code for our version of Figure 4.11.b. p2 &lt;- ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = pred_quad, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = fitd_quad, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/2) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + labs(subtitle = &quot;quadratic&quot;, y = &quot;height&quot;) + coord_cartesian(xlim = range(d$weight_s), ylim = range(d$height)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) p2 From a formula perspective, the cubic model is a simple extension of the quadratic: \\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3.\\] Before we fit the model, we need to wrangle the data again. d &lt;- d %&gt;% mutate(weight_s3 = weight_s^3) Now fit the model like so. b4.6 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s + weight_s2 + weight_s3, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b, coef = &quot;weight_s&quot;), prior(normal(0, 1), class = b, coef = &quot;weight_s2&quot;), prior(normal(0, 1), class = b, coef = &quot;weight_s3&quot;), prior(uniform(0, 50), class = sigma)), iter = 40000, warmup = 39000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.06&quot;) And now we’ll fit the good old linear model. b4.7 &lt;- brm(data = d, family = gaussian, height ~ 1 + weight_s, prior = c(prior(normal(178, 20), class = Intercept), prior(lognormal(0, 1), class = b, coef = &quot;weight_s&quot;), prior(uniform(0, 50), class = sigma)), iter = 40000, warmup = 39000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.07&quot;) Here’s the fitted(), predict(), and ggplot2 code for Figure 4.11.c, the cubic model. weight_seq &lt;- weight_seq %&gt;% mutate(weight_s3 = weight_s^3) fitd_cub &lt;- fitted(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) pred_cub &lt;- predict(b4.6, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p3 &lt;- ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = pred_cub, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = fitd_cub, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + labs(subtitle = &quot;cubic&quot;, y = &quot;height&quot;) + coord_cartesian(xlim = range(d$weight_s), ylim = range(d$height)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) p3 And here’s the fitted(), predict(), and ggplot2 code for Figure 4.11.a, the linear model. fitd_line &lt;- fitted(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) pred_line &lt;- predict(b4.7, newdata = weight_seq) %&gt;% as_tibble() %&gt;% bind_cols(weight_seq) p1 &lt;- ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = pred_line, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_smooth(data = fitd_line, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey70&quot;, color = &quot;black&quot;, alpha = 1, size = 1/4) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + labs(subtitle = &quot;linear&quot;, y = &quot;height&quot;) + coord_cartesian(xlim = range(d$weight_s), ylim = range(d$height)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) p1 Did you notice how we labeled each of the past three plots as p1, p2, and p3? Here we use those names to plot them all together with patchwork syntax. p1 | p2 | p3 As fun as this all has been, it’s not clear that any of these models make a lot of sense. They are good geocentric descriptions of the sample, yes. But there are two problems. First, a better fit to the sample might not actually be a better model. That’s the subject of Chapter 7. Second, the model contains no biological information. We aren’t learning any causal relationship between height and weight. We’ll deal with this second problem much later, in Chapter 16. (p. 113) 4.5.1.0.1 Overthinking: Converting back to natural scale. You can apply McElreath’s conversion trick within the ggplot2 environment, too. Here it is with the cubic model. at &lt;- c(-2, -1, 0, 1, 2) ggplot(data = d, aes(x = weight_s)) + geom_ribbon(data = pred_cub, aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey83&quot;) + geom_point(aes(y = height), color = &quot;navyblue&quot;, shape = 1, size = 1.5, alpha = 1/3) + coord_cartesian(xlim = range(d$weight_s)) + theme(text = element_text(family = &quot;Times&quot;), panel.grid = element_blank()) + # here it is! scale_x_continuous(&quot;standardized weight converted back&quot;, breaks = at, labels = round(at*sd(d$weight) + mean(d$weight), 1)) 4.5.2 Splines. Load the cherry_blossoms data (Aono, 2012; Aono &amp; Kazui, 2008; Aono &amp; Saito, 2010). library(rethinking) data(cherry_blossoms) d &lt;- cherry_blossoms rm(cherry_blossoms) detach(package:rethinking, unload = T) Minus the mini histograms, here is our ground-up tidyverse way to summarize our new d data the way McElreath did with his precis(). d %&gt;% gather() %&gt;% group_by(key) %&gt;% summarise(mean = mean(value, na.rm = T), sd = sd(value, na.rm = T), ll = quantile(value, prob = .055, na.rm = T), ul = quantile(value, prob = .945, na.rm = T)) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 5 x 5 ## key mean sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 doy 105. 6.41 94.4 115 ## 2 temp 6.14 0.66 5.15 7.29 ## 3 temp_lower 5.1 0.85 3.79 6.37 ## 4 temp_upper 7.19 0.99 5.9 8.9 ## 5 year 1408 351. 868. 1948. McElreath encouraged us to plot doy against year. d %&gt;% ggplot(aes(x = year, y = doy)) + # color from here: https://www.colorhexa.com/ffb7c5 geom_point(color = &quot;#ffb7c5&quot;, alpha = 1/2) + theme_bw() + theme(panel.grid = element_blank(), # color from here: https://www.colordic.org/w/, inspired by https://chichacha.netlify.com/2018/11/29/plotting-traditional-colours-of-japan/ panel.background = element_rect(fill = &quot;#4f455c&quot;)) It looks like there are some wiggly trends, but it’s hard to tell with a scatter plot. Our goal is to approximate the blossom trend with a wiggly function. With B-splines, just like with polynomial regression, we do this by generating new predictor variables and using those in the linear model, \\(\\mu_i\\). Unlike polynomial regression, B-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a basis function. The linear model ends up looking very familiar: \\[\\mu_i = \\alpha + w_1 B_{i, 1} + w_2 B_{i, 2} + w_3 B_{i, 3} + \\dots\\] where \\(B_{i,n}\\) is the \\(n\\)-th basis function’s value on row \\(i\\), and the \\(w\\) parameters are corresponding weights for each. The parameters act like slopes, adjusting the influence of each basis function on the mean \\(\\mu_i\\). So really this is just another linear regression, but with some fancy, synthetic predictor variables. (p. 115, emphasis in the original) It turns out there are cases with missing data for the doy variable. d %&gt;% count(is.na(doy)) %&gt;% mutate(percent = 100 * n / sum(n)) ## is.na(doy) n percent ## 1 FALSE 827 68.06584 ## 2 TRUE 388 31.93416 Let’s follow McElreath and make a subset of the data that excludes cases with missing data in doy. Within the tidyverse, we might do so with the tidyr::drop_na() function. d2 &lt;- d %&gt;% drop_na(doy) On page 117 in the text, McElreath indirectly explained how to make Figure 4.12 by walking through the workflow for making Figure 4.13. Here we mimic that ordering. First, we choose the knots. Remember, the knots are just values of year that serve as pivots for our spline. Where should the knots go? There are different ways to answer this question. You can, in principle, put the knots wherever you like. Their locations are part of the model, and you are responsible for them. Let’s do what we did in the simple example above, place the knots at different evenlyspaced quantiles of the predictor variable. This gives you more knots where there are more observations. We used only 5 knots in the first example. Now let’s go for 15: num_knots &lt;- 15 knot_list &lt;- quantile(d2$year, probs = seq(from = 0, to = 1, length.out = num_knots)) Our knot_list contains 15 year values. knot_list ## 0% 7.142857% 14.28571% 21.42857% 28.57143% 35.71429% 42.85714% 50% 57.14286% 64.28571% ## 812 1036 1174 1269 1377 1454 1518 1583 1650 1714 ## 71.42857% 78.57143% 85.71429% 92.85714% 100% ## 1774 1833 1893 1956 2015 Here’s what it looks like if we use those knot_list values to chop up our year/doy scatter plot, from above. d %&gt;% ggplot(aes(x = year, y = doy)) + geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2) + geom_point(color = &quot;#ffb7c5&quot;, alpha = 1/2) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) The next choice is polynomial degree. This determines how basis functions combine, which determines how the parameters interact to produce the spline. For degree 1, as in Figure 4.12, two basis functions combine at each point. For degree 2, three functions combine at each point. For degree 3, four combine. R already has a nice function that will build basis functions for any list of knots and degree. This code will construct the necessary basis functions for a degree 3 (cubic) spline: (p. 117) library(splines) B &lt;- bs(d2$year, knots = knot_list[-c(1, num_knots)], degree = 3, intercept = TRUE) Look closely at McElreath’s tricky knot_list[-c(1, num_knots)] code. Whereas knot_list contains 15 ordered year values, McElreath shaved off the first and last year values with knot_list[-c(1, num_knots)], leaving 13. This is because, by default, the bs() function places knots at the boundaries. Since the first and 15th values in knot_list were boundary values for year, we removed them to avoid redundancies. We can confirm this with the code, below. B %&gt;% str() ## &#39;bs&#39; num [1:827, 1:17] 1 0.96 0.767 0.563 0.545 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:17] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## - attr(*, &quot;degree&quot;)= int 3 ## - attr(*, &quot;knots&quot;)= Named num [1:13] 1036 1174 1269 1377 1454 ... ## ..- attr(*, &quot;names&quot;)= chr [1:13] &quot;7.142857%&quot; &quot;14.28571%&quot; &quot;21.42857%&quot; &quot;28.57143%&quot; ... ## - attr(*, &quot;Boundary.knots&quot;)= int [1:2] 812 2015 ## - attr(*, &quot;intercept&quot;)= logi TRUE Look at the second to last line, - attr(*, \"Boundary.knots\")= int [1:2] 812 2015. Those default \"Boundary.knots\" are the same as knot_list[c(1, num_knots)]. Let’s confirm. knot_list[c(1, num_knots)] ## 0% 100% ## 812 2015 By the degree = 3 argument, we indicated we wanted a cubic spline. McElreath used degree = 1 for Figure 4.12. For reasons I’m not prepared to get into, here, splines don’t always include intercept parameters. Indeed, the bs() default is intercept = FALSE. McElreath’s code indicated he wanted to fit a B-spline that included an intercept. Thus: intercept = TRUE. Here’s how we might make our version of the top panel of Figure 4.13. # wrangle a bit b &lt;- B %&gt;% data.frame() %&gt;% set_names(str_c(0, 1:9), 10:17) %&gt;% bind_cols(select(d2, year)) %&gt;% pivot_longer(-year, names_to = &quot;bias_function&quot;, values_to = &quot;bias&quot;) # plot b %&gt;% ggplot(aes(x = year, y = bias, group = bias_function)) + geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2) + geom_line(color = &quot;#ffb7c5&quot;, alpha = 1/2, size = 1.5) + ylab(&quot;bias value&quot;) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) To elucidate what’s going on in that plot, we might break it up with facet_wrap(). b %&gt;% mutate(bias_function = str_c(&quot;bias function &quot;, bias_function)) %&gt;% ggplot(aes(x = year, y = bias)) + geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2) + geom_line(color = &quot;#ffb7c5&quot;, size = 1.5) + ylab(&quot;bias value&quot;) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank(), strip.background = element_rect(fill = scales::alpha(&quot;#ffb7c5&quot;, .25), color = &quot;transparent&quot;), strip.text = element_text(size = 8, margin = margin(0.1, 0, 0.1, 0, &quot;cm&quot;))) + facet_wrap(~ bias_function, ncol = 1) Now to get the parameter weights for each basis function, we need to actually define the model and make it run. The model is just a linear regression. The synthetic basis functions do all the work. We’ll use each column of the matrix B as a variable. We’ll also have an intercept to capture the average blossom day. This will make it easier to define priors on the basis weights, because then we can just conceive of each as a deviation from the intercept. (p. 117) That last line is another indication for why we set intercept = TRUE. Our model will follow the form \\[\\begin{align*} \\text{day_in_year}_i &amp; \\sim \\operatorname{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\color{#4f455c}{\\sum_{k=1}^K w_k B_{k, i}} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(100, 10) \\\\ \\color{#4f455c}{w_j} &amp; \\color{#4f455c}\\sim \\color{#4f455c}{\\operatorname{Normal}(0, 10)} \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where \\(\\alpha\\) is the intercept, \\(B_{k, i}\\) is the value of the \\(k^\\text{th}\\) bias function on the \\(i^\\text{th}\\) row of the data, and \\(w_k\\) is the estimated regression weight for the corresponding \\(k^\\text{th}\\) bias function. Throughout this chapter, I’ve griped a bit about using the uniform prior for \\(\\sigma\\). Now that McElreath has introduced the exponential distribution as an alternative, those gripes are coming to an end. The exponential distribution is controlled by a single parameter, \\(\\lambda\\), which is also called the rate. As it turns out, the mean of the exponential distribution is the inverse of the rate, \\(1 / \\lambda\\). Here we use the dexp() function to get a sense of what that prior looks like. tibble(x = seq(from = 0, to = 10, by = 0.1)) %&gt;% mutate(d = dexp(x, rate = 1)) %&gt;% ggplot(aes(x = x, y = d)) + geom_area(fill = &quot;#ffb7c5&quot;) + scale_y_continuous(NULL, breaks = NULL) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) “We’ll use exponential priors for the rest of the book, in place of uniform priors. It is much more common to have a sense of the average deviation than of the maximum” (p. 119). 🎉 Acknowledgment: The workflow to follow is heavily influenced by the helpful contributions from Stephen Wild. My first pass through the material in this section was a mess. Wild’s insights knocked it out of the park and I couldn’t be more grateful. 🍻 Before fitting this model in brms, well take a minor detour on the data structure. In his R code 4.76, McElreath defined his data in a list, list( D=d2$doy , B=B ). Our approach will be a little different. Here, we’ll add the B matrix to our d2 data frame and name the results as d3. d3 &lt;- d2 %&gt;% mutate(B = B) # take a look at the structure of `d3 d3 %&gt;% glimpse() ## Rows: 827 ## Columns: 6 ## $ year &lt;int&gt; 812, 815, 831, 851, 853, 864, 866, 869, 889, 891, 892, 894, 895, 896, 902, 908,… ## $ doy &lt;int&gt; 92, 105, 96, 108, 104, 100, 106, 95, 104, 109, 108, 106, 104, 104, 102, 98, 95,… ## $ temp &lt;dbl&gt; NA, NA, NA, 7.38, NA, 6.42, 6.44, NA, 6.83, 6.98, 7.11, 6.98, 7.08, 7.20, 7.50,… ## $ temp_upper &lt;dbl&gt; NA, NA, NA, 12.10, NA, 8.69, 8.11, NA, 8.48, 8.96, 9.11, 8.40, 8.57, 8.69, 8.95… ## $ temp_lower &lt;dbl&gt; NA, NA, NA, 2.66, NA, 4.14, 4.77, NA, 5.19, 5.00, 5.11, 5.55, 5.58, 5.72, 6.06,… ## $ B &lt;dbl[,17]&gt; &lt;matrix[33 x 17]&gt; In our d3 data, columns year through temp_lower are all standard data columns. The B column is a matrix column, which contains the same number of rows as the others, but also smuggled in 17 columns within that column. Each of those 17 columns corresponds to one of our synthetic \\(B_k\\) variables. The advantage of such a data structure is we can simply define our formula argument as doy ~ 1 + B, where B is a stand-in for B.1 + B.2 + ... + B.17. Here’s how to fit the model. b4.8 &lt;- brm(data = d3, family = gaussian, doy ~ 1 + B, prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.08&quot;) Here’s the model summary. print(b4.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: doy ~ 1 + B ## Data: d3 (Number of observations: 827) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 103.59 2.49 98.58 108.45 1.01 761 1071 ## B1 -3.19 3.88 -10.80 4.21 1.00 1591 2435 ## B2 -1.11 3.92 -8.82 6.62 1.00 1463 1859 ## B3 -1.27 3.68 -8.58 6.07 1.00 1332 1833 ## B4 4.56 2.96 -1.13 10.54 1.00 1046 1648 ## B5 -1.08 2.99 -6.89 4.78 1.00 954 1399 ## B6 4.03 3.02 -1.86 10.07 1.00 1056 1515 ## B7 -5.55 2.91 -11.23 0.30 1.00 981 1672 ## B8 7.57 2.92 1.92 13.29 1.00 1005 1583 ## B9 -1.22 2.97 -6.90 4.58 1.01 1001 1458 ## B10 2.79 3.03 -3.17 9.02 1.00 1019 1527 ## B11 4.42 3.00 -1.49 10.27 1.00 1056 1846 ## B12 -0.37 2.99 -5.98 5.70 1.00 1013 1551 ## B13 5.28 2.99 -0.52 11.07 1.00 1024 1768 ## B14 0.46 3.09 -5.44 6.74 1.00 1036 1712 ## B15 -1.05 3.37 -7.77 5.59 1.00 1215 2147 ## B16 -7.25 3.44 -14.01 -0.49 1.00 1287 1936 ## B17 -7.88 3.26 -14.24 -1.29 1.00 1246 1824 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.94 0.14 5.67 6.23 1.00 4173 2921 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Look at that. Each of the 17 columns in our B matrix was assigned its own parameter. If you fit this model using McElreath’s rethinking code, you’ll see the results are very similar. Anyway, McElreath’s comments are in line with the general consensus on spline modes: the parameter estimates are very difficult to interpret directly. It’s often easier to just plot the results. First we’ll use posterior_samples(). post &lt;- posterior_samples(b4.8) glimpse(post) ## Rows: 4,000 ## Columns: 20 ## $ b_Intercept &lt;dbl&gt; 103.74086, 104.25796, 103.13392, 104.66690, 105.20821, 105.41704, 105.65525, 1… ## $ b_B1 &lt;dbl&gt; -6.4826048, -5.4990824, -2.1646605, 0.4607746, -9.0040921, -7.3966727, -11.508… ## $ b_B2 &lt;dbl&gt; 3.74875270, -0.53550232, -0.42138745, -8.06386888, 0.66505588, 0.72880418, 2.9… ## $ b_B3 &lt;dbl&gt; -4.22042929, -2.26081740, -0.95183959, -1.78965585, -8.02914674, -9.16372510, … ## $ b_B4 &lt;dbl&gt; 5.8833689, 4.7218957, 3.5397449, 3.2324508, 5.1640256, 5.9370012, 2.3368581, 3… ## $ b_B5 &lt;dbl&gt; -4.6338971, -2.0490845, -1.7740736, -1.4582729, -4.3913120, -3.1619283, -4.307… ## $ b_B6 &lt;dbl&gt; 6.00593380, 2.37093832, 6.60788828, 0.95493974, 2.78863329, 2.14199241, 1.6839… ## $ b_B7 &lt;dbl&gt; -6.9436490, -3.8421888, -7.9974119, -6.7766133, -5.6201809, -6.7678482, -8.336… ## $ b_B8 &lt;dbl&gt; 7.316514, 5.119854, 8.393274, 5.799375, 6.329872, 5.731784, 5.708979, 6.760733… ## $ b_B9 &lt;dbl&gt; -1.9490036, -0.2646558, -1.4861210, -1.3906578, -3.7449285, -3.6035334, -4.008… ## $ b_B10 &lt;dbl&gt; 3.9720418, 2.9088482, 4.1508398, 1.3121326, 2.3465406, 3.1685000, 1.6009567, 0… ## $ b_B11 &lt;dbl&gt; 3.6933735, 3.2552081, 2.3290076, 3.5780345, 1.4853387, 1.4431854, 1.8636113, 5… ## $ b_B12 &lt;dbl&gt; -0.2419564, -2.2181982, -0.6924766, -1.5380999, -1.3345390, -1.7848732, -1.199… ## $ b_B13 &lt;dbl&gt; 5.416313, 3.645593, 6.208850, 4.199717, 4.373958, 3.727896, 1.780963, 4.056961… ## $ b_B14 &lt;dbl&gt; 1.5023931, 2.6503143, 1.1680142, 1.9840733, -3.6880151, -4.3624263, -1.3456437… ## $ b_B15 &lt;dbl&gt; -1.51102386, -4.81636227, 1.75411291, -3.42256204, -1.68102337, -2.74340145, -… ## $ b_B16 &lt;dbl&gt; -8.4839941, -7.0335783, -8.7948696, -4.4579658, -10.4273168, -10.3023468, -13.… ## $ b_B17 &lt;dbl&gt; -9.947067, -10.641582, -8.199229, -8.768240, -7.976792, -8.981154, -7.529841, … ## $ sigma &lt;dbl&gt; 6.077514, 5.949900, 6.119600, 5.906621, 5.937513, 5.884939, 5.901013, 5.992085… ## $ lp__ &lt;dbl&gt; -2711.191, -2710.897, -2714.579, -2714.185, -2711.390, -2713.697, -2712.118, -… With a little wrangling, we can use summary information from post to make our version of the middle panel of Figure 4.13. post %&gt;% select(b_B1:b_B17) %&gt;% set_names(c(str_c(0, 1:9), 10:17)) %&gt;% pivot_longer(everything(), names_to = &quot;bias_function&quot;) %&gt;% group_by(bias_function) %&gt;% summarise(weight = mean(value)) %&gt;% full_join(b, by = &quot;bias_function&quot;) %&gt;% # plot ggplot(aes(x = year, y = bias * weight, group = bias_function)) + geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2) + geom_line(color = &quot;#ffb7c5&quot;, alpha = 1/2, size = 1.5) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) In case you missed it, the main action in the ggplot2 code was y = bias * weight, where we defined the \\(y\\)-axis as the product of bias and weight. This is fulfillment of the \\(w_k B_{k, i}\\) parts of the model. Now here’s how we might use brms::fitted() to make the lower plot of Figure 4.13. f &lt;- fitted(b4.8) f %&gt;% data.frame() %&gt;% bind_cols(d2) %&gt;% ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2) + geom_hline(yintercept = fixef(b4.8)[1, 1], color = &quot;white&quot;, linetype = 2) + geom_point(color = &quot;#ffb7c5&quot;, alpha = 1/2) + geom_ribbon(fill = &quot;white&quot;, alpha = 2/3) + labs(x = &quot;year&quot;, y = &quot;day in year&quot;) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) If it wasn’t clear, the dashed horizontal line intersecting a little above 100 on the \\(y\\)-axis is the poster mean for the intercept. Now let’s use our skills to remake the simpler model expressed in Figure 4.12. This model, recall, is based on 5 knots. # redo the `B` splines num_knots &lt;- 5 knot_list &lt;- quantile(d2$year, probs = seq(from = 0, to = 1, length.out = num_knots)) B &lt;- bs(d2$year, knots = knot_list[-c(1, num_knots)], # this makes the splines liner rater than cubic degree = 1, intercept = TRUE) # define a new `d4` data d4 &lt;- d2 %&gt;% mutate(B = B) b4.9 &lt;- brm(data = d4, family = gaussian, formula = doy ~ 1 + B, prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.09&quot;) Review the new model summary. print(b4.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: doy ~ 1 + B ## Data: d4 (Number of observations: 827) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 103.46 4.57 94.27 112.27 1.01 673 1077 ## B1 -0.32 4.65 -9.48 9.16 1.01 694 1124 ## B2 1.49 4.60 -7.52 10.84 1.01 692 1048 ## B3 1.57 4.60 -7.40 10.90 1.01 681 1117 ## B4 3.99 4.59 -5.00 13.02 1.01 689 1140 ## B5 -5.56 4.63 -14.64 3.76 1.01 685 1175 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 6.10 0.15 5.82 6.41 1.00 2537 1993 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we do all the work in bulk to make and save the three subplots for Figure 4.12. ## top # wrangle a bit b &lt;- invoke(data.frame, d4) %&gt;% pivot_longer(starts_with(&quot;B&quot;), names_to = &quot;bias_function&quot;, values_to = &quot;bias&quot;) # plot p1 &lt;- b %&gt;% ggplot(aes(x = year, y = bias, group = bias_function)) + geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2) + geom_line(color = &quot;#ffb7c5&quot;, alpha = 1/2, size = 1.5) + scale_x_continuous(NULL, breaks = NULL) + ylab(&quot;bias value&quot;) ## middle # wrangle p2 &lt;- posterior_samples(b4.9) %&gt;% select(b_B1:b_B5) %&gt;% set_names(str_c(&quot;B.&quot;, 1:5)) %&gt;% pivot_longer(everything(), names_to = &quot;bias_function&quot;) %&gt;% group_by(bias_function) %&gt;% summarise(weight = mean(value)) %&gt;% full_join(b, by = &quot;bias_function&quot;) %&gt;% # plot ggplot(aes(x = year, y = bias * weight, group = bias_function)) + geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2) + geom_line(color = &quot;#ffb7c5&quot;, alpha = 1/2, size = 1.5) + scale_x_continuous(NULL, breaks = NULL) ## bottom # wrangle f &lt;- fitted(b4.9) p3 &lt;- f %&gt;% data.frame() %&gt;% bind_cols(d2) %&gt;% # plot ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + geom_vline(xintercept = knot_list, color = &quot;white&quot;, alpha = 1/2) + geom_hline(yintercept = fixef(b4.9)[1, 1], color = &quot;white&quot;, linetype = 2) + geom_point(color = &quot;#ffb7c5&quot;, alpha = 1/2) + geom_ribbon(fill = &quot;white&quot;, alpha = 2/3) + labs(x = &quot;year&quot;, y = &quot;day in year&quot;) Now combine the subplots with patchwork syntax and behold their glory. (p1 / p2 / p3) &amp; theme_bw() &amp; theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) 4.5.3 Smooth functions for a rough world. The splines in the previous section are just the beginning. A entire class of models, generalized additive models (GAMs), focuses on predicting an outcome variable using smooth functions of some predictor variables. The topic is deep enough to deserve its own book. (p. 120, emphasis in the original) McElreath ended that block quote with a reference to his endnote #78. On page 562, we read: “A very popular and comprehensive text is Wood (2017a).” 4.6 Summary First bonus: Smooth functions with brms::s() It’s convenient for us how McElreath ended that last section with a reference to Simon Wood’s work because brms allows for a variety of non-linear models by borrowing functions from Woods’s mgcv package (Wood, 2003, 2004, 2011, 2017b, 2019; Wood et al., 2016). The two smooth functions brms imports from mgcv are s() and t2(). We’ll be exploring s(). We might use the brms::get_prior() function to get a sense of how to set up the priors when using s(). get_prior(data = d2, family = gaussian, doy ~ 1 + s(year)) ## prior class coef group resp dpar nlpar bound source ## (flat) b default ## (flat) b syear_1 (vectorized) ## student_t(3, 105, 5.9) Intercept default ## student_t(3, 0, 5.9) sds default ## student_t(3, 0, 5.9) sds s(year) (vectorized) ## student_t(3, 0, 5.9) sigma default We have an overall intercept (class = Intercept), a single \\(\\beta\\) parameter for year (class = b), a \\(\\sigma\\) parameter (class = sigma), and an unfamiliar parameter of class = sds. I’m not going to go into that last parameter in any detail, here. We’ll need to work our way up through Chapter 13 and the multilevel model to get a full picture of what it means. The important thing to note here is that the priors for our s()-based alternative to the B-spline models, above, are going to look a little different. Here’s how we might fit an alternative to b4.8 b4.10 &lt;- brm(data = d2, family = gaussian, doy ~ 1 + s(year), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 5.9), class = sds), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, control = list(adapt_delta = .99), file = &quot;fits/b04.10&quot;) Check out the model summary. print(b4.10) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: doy ~ 1 + s(year) ## Data: d2 (Number of observations: 827) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Smooth Terms: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sds(syear_1) 22.36 7.56 11.87 40.48 1.00 1115 1863 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 104.54 0.22 104.13 104.98 1.00 4095 2264 ## syear_1 -9.87 8.94 -27.42 7.43 1.00 3123 2818 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 6.04 0.15 5.75 6.36 1.00 4305 2900 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our intercept and \\(\\sigma\\) summaries are similar to those we got from b4.8. The rest looks different. Here’s what happens when we use brms::fitted() to plot the implications of the model. fitted(b4.10) %&gt;% data.frame() %&gt;% bind_cols(select(d2, year, doy)) %&gt;% ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = fixef(b4.10)[1, 1], color = &quot;white&quot;, linetype = 2) + geom_point(color = &quot;#ffb7c5&quot;, alpha = 1/2) + geom_ribbon(fill = &quot;white&quot;, alpha = 2/3) + labs(subtitle = &quot;b4.7 using s(year)&quot;, y = &quot;day in year&quot;) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) That smooth doesn’t look quite the same. Hopefully this isn’t terribly surprising. We used a function from a different package and ended up with a different underlying statistical model. In fact, we didn’t even use a B-spline. The default for s() is to use what’s called a thin plate regression spline. If we’d like to fit a B-spline, we have to set bs = \"bs\". Here’s an example. b4.11 &lt;- brm(data = d2, family = gaussian, doy ~ 1 + s(year, bs = &quot;bs&quot;, k = 19), prior = c(prior(normal(100, 10), class = Intercept), prior(normal(0, 10), class = b), prior(student_t(3, 0, 5.9), class = sds), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, control = list(adapt_delta = .99), file = &quot;fits/b04.11&quot;) print(b4.11) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: doy ~ 1 + s(year, bs = &quot;bs&quot;, k = 19) ## Data: d2 (Number of observations: 827) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Smooth Terms: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sds(syear_1) 1.31 0.59 0.54 2.80 1.01 622 988 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 104.54 0.20 104.14 104.95 1.00 5150 2533 ## syear_1 -0.09 0.33 -0.75 0.57 1.00 1873 2216 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 5.98 0.15 5.70 6.28 1.00 4065 3050 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now here’s the depiction of our s()-based B-spline model. fitted(b4.11) %&gt;% data.frame() %&gt;% bind_cols(select(d2, year, doy)) %&gt;% ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = fixef(b4.11)[1, 1], color = &quot;white&quot;, linetype = 2) + geom_point(color = &quot;#ffb7c5&quot;, alpha = 1/2) + geom_ribbon(fill = &quot;white&quot;, alpha = 2/3) + labs(subtitle = &#39;b4.7_bs using s(year, bs = &quot;bs&quot;)&#39;, y = &quot;day in year&quot;) + theme_bw() + theme(panel.background = element_rect(fill = &quot;#4f455c&quot;), panel.grid = element_blank()) There are still other important differences between the underlying statistical model for b4.11 and the earlier b4.8 that I’m just not going to go into, here. For more on the B-splines and smooths, more generally, check out the blog post by the great Gavin Simpson, Extrapolating with B splines and GAMs. For a high-level introduction to the models you can fit with mgcv, check out the nice talk by Noam Ross, Nonlinear models in R: The wonderful world of mgcv, or the equally-nice presentation by Simpson, Introduction to generalized additive models with R and mgcv. Ross offers a free online course covering mgcv, called GAMS in R, and he maintains a GitHub repo cataloguing other GAM-related resources, called Resources for learning about and using GAMs in R. For specific examples of fitting various GAMS with brms, check out Simpson’s blog post, Fitting GAMs with brms: part 1. Finally, Tristan Mahr has a nice blog post called Random effects and penalized splines are the same thing, where he outlined the connections between penalized smooths, such as you might fit with mgcv, with the multilevel model, which we’ll learn all about starting in Chapter 13, which helps explain what’s going on with the s() function in our last two models, b4.10 and b4.11. 4.7 Second bonus: Group predictors with matrix columns When we fit b4.8, our direct brms analogue to McElreath’s m4.72, we used a compact syntax to pass a matrix column of predictors into the formula. If memory serves, this is one of the only places in the text where we see this. It would be easy for the casual reader to think this was only appropriate for something like a spline model. But that’s not the case. One could use the matrix-column trick as a general approach. In this bonus section, we’ll explore how. In Section 11.2.6 of their (2020) text, Gelman, Hill, and Vehtari worked through an example of a multiple regression model, \\[ \\begin{align*} y_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\theta z_i + \\sum_{k = 1}^K b_k x_{k, i} \\\\ &amp; \\text{&lt;priors&gt;}, \\end{align*} \\] where \\(y_i\\) was some continuous variable collected across participants, \\(i\\). The \\(\\alpha\\) term was the intercept and the \\(\\theta\\) term was the regression slope for a binary variable \\(z\\)–we’ll practice with binary predictors in Section 5.3. More to our interest, the last portion of the equation is a compact way to convey there are \\(K\\) additional predictors and their associated regression coefficients, which we might more explicitly express as \\(\\beta_1 x_{1, i} + \\cdots + \\beta_k x_{k, i}\\), where \\(K \\geq 1\\). In this particular example, \\(K = 10\\), meaning there were ten \\(x_{k, i}\\) predictors, making this an example of a model with 11 total predictor variables. Riffing off of Gelman and colleagues, here’s how you might simulate data of this kind. # how many cases would you like? n &lt;- 100 # how many continuous x predictor variables would you like? k &lt;- 10 # simulate a dichotomous dummy variable for z # simulate an n by k array for X set.seed(4) d &lt;- tibble(z = sample(0:1, size = n, replace = T), X = array(runif(n * k, min = 0, max = 1), dim = c(n, k))) # set the data-generating parameter values a &lt;- 1 theta &lt;- 5 b &lt;- 1:k sigma &lt;- 2 # simulate the criterion d &lt;- d %&gt;% mutate(y = as.vector(a + X %*% b + theta * z + rnorm(n, mean = 0, sd = sigma))) # check the data structure d %&gt;% glimpse() ## Rows: 100 ## Columns: 3 ## $ z &lt;int&gt; 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1… ## $ X &lt;dbl[,10]&gt; &lt;matrix[33 x 10]&gt; ## $ y &lt;dbl&gt; 19.51231, 26.75217, 28.92495, 20.63144, 29.18219, 43.67198, 36.98107, 36.68241, 28.… Although our d tibble has only three columns, the X column is a matrix column into which we’ve smuggled ten columns more. Here’s how we might access them more directly. d %&gt;% pull(X) %&gt;% glimpse() ## num [1:100, 1:10] 0.253 0.63 0.266 0.532 0.468 ... See? There’s an \\(100 \\times 10\\) data matrix in there. Tricky. Here’s how to fit the full model with brms where we use the compact matrix-column syntax in the formula argument. b4.12 &lt;- brm(data = d, family = gaussian, y ~ 1 + z + X, prior = c(prior(normal(0, 2), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4, file = &quot;fits/b04.12&quot;) Check the parameter summary. print(b4.12) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 + z + X ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.95 1.21 -1.42 3.24 1.00 7096 3172 ## z 4.74 0.42 3.92 5.57 1.00 7132 2651 ## X1 0.56 0.74 -0.88 2.04 1.00 7515 3646 ## X2 0.88 0.68 -0.47 2.24 1.00 7180 3427 ## X3 3.40 0.72 1.99 4.82 1.00 7592 3281 ## X4 2.82 0.73 1.38 4.26 1.00 7795 3134 ## X5 5.75 0.71 4.35 7.15 1.00 6154 3273 ## X6 6.41 0.74 4.98 7.85 1.00 6828 3383 ## X7 8.49 0.73 7.06 9.92 1.00 7494 3063 ## X8 8.40 0.71 7.02 9.78 1.00 7389 3078 ## X9 8.83 0.80 7.28 10.39 1.00 6807 3026 ## X10 9.33 0.70 7.94 10.69 1.00 6668 2701 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.99 0.16 1.71 2.32 1.00 5356 3113 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). brms automatically numbered our \\(K = 10\\) X variables as X1 through X10. As far as applications go, I’m not sure where I’d use this way of storing and modeling data in real life. But maybe some of y’all work in domains where this is just the right way to approach your data needs. If so, good luck and happy modeling. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] splines parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.3.1 brms_2.15.0 Rcpp_1.0.6 rstan_2.21.2 ## [5] StanHeaders_2.21.0-7 patchwork_1.1.1 forcats_0.5.1 stringr_1.4.0 ## [9] dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 ## [13] tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 igraph_1.2.6 ## [5] svUnit_1.0.3 crosstalk_1.1.0.1 TH.data_1.0-10 rstantools_2.1.1 ## [9] inline_0.3.17 digest_0.6.27 htmltools_0.5.1.1 rsconnect_0.8.16 ## [13] fansi_0.4.2 magrittr_2.0.1 modelr_0.1.8 RcppParallel_5.0.2 ## [17] matrixStats_0.57.0 xts_0.12.1 sandwich_3.0-0 prettyunits_1.1.1 ## [21] colorspace_2.0-0 rvest_0.3.6 ggdist_2.4.0.9000 haven_2.3.1 ## [25] xfun_0.22 callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 ## [29] lme4_1.1-25 survival_3.2-7 zoo_1.8-8 glue_1.4.2 ## [33] gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 distributional_0.2.2 ## [37] pkgbuild_1.2.0 shape_1.4.5 abind_1.4-5 scales_1.1.1 ## [41] mvtnorm_1.1-1 emo_0.0.0.9000 DBI_1.1.0 miniUI_0.1.1.1 ## [45] isoband_0.2.3 viridisLite_0.3.0 xtable_1.8-4 HDInterval_0.2.2 ## [49] stats4_4.0.4 DT_0.16 htmlwidgets_1.5.2 httr_1.4.2 ## [53] threejs_0.3.3 arrayhelpers_1.1-0 ellipsis_0.3.1 pkgconfig_2.0.3 ## [57] loo_2.4.1 farver_2.0.3 dbplyr_2.0.0 utf8_1.1.4 ## [61] tidyselect_1.1.0 labeling_0.4.2 rlang_0.4.10 reshape2_1.4.4 ## [65] later_1.1.0.1 munsell_0.5.0 cellranger_1.1.0 tools_4.0.4 ## [69] cli_2.3.1 generics_0.1.0 broom_0.7.5 ggridges_0.5.2 ## [73] evaluate_0.14 fastmap_1.0.1 processx_3.4.5 knitr_1.31 ## [77] fs_1.5.0 nlme_3.1-152 mime_0.10 projpred_2.0.2 ## [81] xml2_1.3.2 compiler_4.0.4 bayesplot_1.8.0 shinythemes_1.1.2 ## [85] rstudioapi_0.13 curl_4.3 gamm4_0.2-6 reprex_0.3.0 ## [89] statmod_1.4.35 stringi_1.5.3 highr_0.8 ps_1.6.0 ## [93] Brobdingnag_1.2-6 lattice_0.20-41 Matrix_1.3-2 nloptr_1.2.2.2 ## [97] markdown_1.1 shinyjs_2.0.0 vctrs_0.3.6 pillar_1.5.1 ## [101] lifecycle_1.0.0 bridgesampling_1.0-0 estimability_1.3 httpuv_1.5.4 ## [105] R6_2.5.0 bookdown_0.21 promises_1.1.1 gridExtra_2.3 ## [109] codetools_0.2-18 boot_1.3-26 colourpicker_1.1.0 MASS_7.3-53 ## [113] gtools_3.8.2 assertthat_0.2.1 withr_2.4.1 shinystan_2.5.0 ## [117] multcomp_1.4-16 mgcv_1.8-33 hms_0.5.3 grid_4.0.4 ## [121] coda_0.19-4 minqa_1.2.4 rmarkdown_2.7 shiny_1.5.0 ## [125] lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 In the first edition of his text, McElreath (2015) started out with the uniform prior for \\(\\sigma\\) and transitioned to the half Cauchy when he introduced HMC in Chapter 8. In this edition, he largely replaced the half Cauchy with the exponential distribution. Both are fine. If you’d like to learn more about the half Cauchy, you might check out Section 8.4.3.1 of my (2020a) translation of his first edition. We’ll learn about the exponential distribution just a little later in this chapter.↩︎ I know. I’m sorry our model numbering diverged from the numbering in the text. The problem popped up in Section 4.5.1. where McElreath fit a linear model with the weight_s predictor, but didn’t show the code in the text and, as a consequence, didn’t assign that model a number. Since we explicitly fit that linear model, we assigned it the next number in the sequence. So it goes. But yes, I am correct in comparing our b4.8 to McElreath’s m4.7.↩︎ "],["the-many-variables-the-spurious-waffles.html", "5 The Many Variables &amp; The Spurious Waffles 5.1 Spurious associations 5.2 Masked relationship 5.3 Categorical variables 5.4 Summary Bonus: We can model categorical variables in more ways than one Session info", " 5 The Many Variables &amp; The Spurious Waffles Correlation in general is not surprising. In large data sets, every pair of variables has a statistically discernible non-zero correlation. But since most correlations do not indicate causal relationships, we need tools for distinguishing mere association from evidence of causation. This is why so much effort is devoted to multiple regression, using more than one predictor variable to simultaneously model an outcome. (McElreath, 2020a, p. 123, emphasis in the original) In his endnote #80 (p. 562), McElreath wrote: “See Meehl (1990), in particular the ‘crud factor’ described on page 204.” For a fun look at some dubious correlations, check out the examples at https://www.tylervigen.com/spurious-correlations. But back to the text, McElreath’s listed reasons for multivariable regression include: statistical control for confounds multiple/complex causation interactions We’ll approach the first two in this chapter. Interactions are reserved for Chapter 7. 5.0.0.1 Rethinking: Causal inference. “Despite its central importance, there is no unified approach to causal inference yet in the sciences” (p. 124). To dip into the topic, you might check out the recent blog post by Finn Lattimore and David Rohde, Causal inference with Bayes rule. 5.1 Spurious associations Load the Waffle House data. library(tidyverse) data(WaffleDivorce, package = &quot;rethinking&quot;) d &lt;- WaffleDivorce Did you notice how we used the package argument within the data() function, there? That allowed us to load the WaffleDivorce without actually loading the rethinking package. Since we generally don’t want to have both rethinking and brms loaded up at the same time, using the package function will save us a line of code. Now standardize the focal variables with the rethinking::standardize() function. d &lt;- d %&gt;% mutate(d = rethinking::standardize(Divorce), m = rethinking::standardize(Marriage), a = rethinking::standardize(MedianAgeMarriage)) Because we avoided directly loading the rethinking package, we did not have immediate access to McElreath’s handy standardize() function. If you want to use a function from a package without loading that package, you can use the double colon operator ::. You can learn more about the double colon operator here. Now load brms. rm(WaffleDivorce) library(brms) I’m not going to show the output, but you might go ahead and investigate the data with the typical functions. E.g., head(d) glimpse(d) Now we have our data, we can reproduce Figure 5.1. One convenient way to get the handful of sate labels into the plot was with the geom_text_repel() function from the ggrepel package (Slowikowski, 2020). But first, we spent the last few chapters warming up with ggplot2. Going forward, each chapter will have its own plot theme. In this chapter, we’ll characterize the plots with theme_bw() + theme(panel.grid = element_rect()) and coloring based off of \"firebrick\". library(ggrepel) d %&gt;% ggplot(aes(x = WaffleHouses/Population, y = Divorce)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, size = 1/2, color = &quot;firebrick4&quot;, fill = &quot;firebrick&quot;, alpha = 1/5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 1/2) + geom_text_repel(data = d %&gt;% filter(Loc %in% c(&quot;ME&quot;, &quot;OK&quot;, &quot;AR&quot;, &quot;AL&quot;, &quot;GA&quot;, &quot;SC&quot;, &quot;NJ&quot;)), aes(label = Loc), size = 3, seed = 1042) + # this makes it reproducible scale_x_continuous(&quot;Waffle Houses per million&quot;, limits = c(0, 55)) + ylab(&quot;Divorce rate&quot;) + coord_cartesian(xlim = c(0, 50), ylim = c(5, 15)) + theme_bw() + theme(panel.grid = element_blank()) Since these are geographically-based data, we might plot our three major variables in a map format. The urbnmapr package (Urban Institute, 2020) provides latitude and longitude data for the 50 states and the geom_sf() function for plotting them. We’ll use the left_join() function to combine those data with our primary data d. library(urbnmapr) left_join( # get the map data get_urbn_map(map = &quot;states&quot;, sf = TRUE), # add the primary data d %&gt;% mutate(state_name = Location %&gt;% as.character()) %&gt;% select(d:a, state_name), by = &quot;state_name&quot; ) %&gt;% # convert to the long format for faceting pivot_longer(d:a) %&gt;% # plot! ggplot() + geom_sf(aes(fill = value, geometry = geometry), size = 0) + scale_fill_gradient(low = &quot;#f8eaea&quot;, high = &quot;firebrick4&quot;) + theme_void() + theme(legend.position = &quot;none&quot;, strip.text = element_text(margin = margin(0, 0, .5, 0))) + facet_wrap(~ name) One of the advantages of this visualization method is it just became clear that Nevada is missing from the WaffleDivorce data. Execute d %&gt;% distinct(Location) to see for yourself and click here to find out why it’s missing. Those missing data should motivate the skills we’ll cover in Chapter 15. But let’s get back on track. Here’s the standard deviation for MedianAgeMarriage in its current metric. sd(d$MedianAgeMarriage) ## [1] 1.24363 Our first statistical model follows the form \\[\\begin{align*} \\text{divorce_std}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{median_age_at_marriage_std}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where the _std suffix indicates the variables are standardized (i.e., zero centered, with a standard deviation of one). Let’s fit the first univariable model. b5.1 &lt;- brm(data = d, family = gaussian, d ~ 1 + a, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, sample_prior = T, file = &quot;fits/b05.01&quot;) Did you notice the sample_prior = T line? This told brms to take draws from both the posterior distribution (as usual) and from the prior predictive distribution. If you look at McElreath’s R code 5.4, you’ll see he plotted 50 draws from the prior predictive distribution of his m5.1. For our brms workflow, our first step is the extract our prior draws with prior_samples(). prior &lt;- prior_samples(b5.1) prior %&gt;% glimpse() ## Rows: 4,000 ## Columns: 3 ## $ Intercept &lt;dbl&gt; 0.262235852, 0.435592131, -0.294742095, -0.007232040, 0.058308258, 0.050358902, … ## $ b &lt;dbl&gt; 0.01116464, -0.18996254, 0.43071360, -0.60435682, -0.31401810, 0.22439532, -0.06… ## $ sigma &lt;dbl&gt; 2.20380842, 0.57847121, 1.00816092, 2.94427170, 0.30736807, 0.94694471, 0.660626… We ended up with 4,000 draws from the prior predictive distribution, much like posterior_samples() would return 4,000 draws from the posterior. Next we’ll use slice_sample() to take a random sample from our prior object. After just a little more wrangling, we’ll be in good shape to plot our version of Figure 5.3. set.seed(5) prior %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column(&quot;draw&quot;) %&gt;% expand(nesting(draw, Intercept, b), a = c(-2, 2)) %&gt;% mutate(d = Intercept + b * a) %&gt;% ggplot(aes(x = a, y = d)) + geom_line(aes(group = draw), color = &quot;firebrick&quot;, alpha = .4) + labs(x = &quot;Median age marriage (std)&quot;, y = &quot;Divorce rate (std)&quot;) + coord_cartesian(ylim = c(-2, 2)) + theme_bw() + theme(panel.grid = element_blank()) To get the posterior predictions from our brms model, we’ll use fitted() in place of link(). # determine the range of `a` values we&#39;d like to feed into `fitted()` nd &lt;- tibble(a = seq(from = -3, to = 3.2, length.out = 30)) # now use `fitted()` to get the model-implied trajectories fitted(b5.1, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # plot ggplot(aes(x = a)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + geom_point(data = d, aes(y = d), size = 2, color = &quot;firebrick4&quot;) + labs(x = &quot;Median age marriage (std)&quot;, y = &quot;Divorce rate (std)&quot;) + coord_cartesian(xlim = range(d$a), ylim = range(d$d)) + theme_bw() + theme(panel.grid = element_blank()) That’ll serve as our version of the right panel of Figure 5.2. To paraphrase McElreath, “if you inspect the [print()] output, you’ll see that posterior for \\([\\beta_\\text{a}]\\) is reliably negative” (p. 127). Let’s see. print(b5.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d ~ 1 + a ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.10 -0.19 0.19 1.00 3996 2866 ## a -0.57 0.11 -0.79 -0.34 1.00 3262 2655 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.82 0.09 0.68 1.01 1.00 3512 2309 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). On the standardized scale, -0.57 95% CI [-0.79, -0.34] is pretty negative, indeed. We’re ready to fit our second univariable model. b5.2 &lt;- brm(data = d, family = gaussian, d ~ 1 + m, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.02&quot;) The summary suggests \\(\\beta_\\text{m}\\) is of a smaller magnitude. print(b5.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d ~ 1 + m ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.11 -0.22 0.22 1.00 4140 3154 ## m 0.35 0.13 0.09 0.61 1.00 3307 2899 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.95 0.10 0.79 1.17 1.00 4091 2703 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we’ll wangle and plot our version of the left panel in Figure 5.2. nd &lt;- tibble(m = seq(from = -2.5, to = 3.5, length.out = 30)) fitted(b5.2, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = m)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + geom_point(data = d, aes(y = d), size = 2, color = &quot;firebrick4&quot;) + labs(x = &quot;Marriage rate (std)&quot;, y = &quot;Divorce rate (std)&quot;) + coord_cartesian(xlim = range(d$m), ylim = range(d$d)) + theme_bw() + theme(panel.grid = element_blank()) But merely comparing parameter means between different bivariate regressions is no way to decide which predictor is better. Both of these predictors could provide independent value, or they could be redundant, or one could eliminate the value of the other. To make sense of this, we’re going to have to think causally. And then, only after we’ve done some thinking, a bigger regression model that includes both age at marriage and marriage rate will help us. (pp. 127–128) 5.1.1 Think before you regress. It is helpful to introduce a particular type of causal graph known as a DAG, short for directed acyclic graph. Graph means it is nodes and connections. Directed means the connections have arrows that indicate directions of causal influence. And acyclic means that causes do not eventually flow back on themselves. A DAG is a way of describing qualitative causal relationships among variables. It isn’t as detailed as a full model description, but it contains information that a purely statistical model does not. Unlike a statistical model, a DAG will tell you the consequences of intervening to change a variable. But only if the DAG is correct. There is no inference without assumption. (p. 128, emphasis in the original) If you’re interested in making directed acyclic graphs (DAG) in R, the dagitty (Textor &amp; der Zander, 2016) and ggdag (Barrett, 2021b) packages are handy. Our approach will focus on ggdag. # library(dagitty) library(ggdag) If all you want is a quick and dirty DAG for our three variables, you might execute something like this. set.seed(5) dagify(M ~ A, D ~ A + M) %&gt;% ggdag(node_size = 8) We can pretty it up a little, too. dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;D&quot;), x = c(1, 3, 2), y = c(2, 2, 1)) p1 &lt;- dagify(M ~ A, D ~ A + M, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 10) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) p1 We could have left out the coords argument and let the dagify() function set the layout of the nodes on its own. But since we were picky and wanted to ape McElreath, we first specified our coordinates in a tibble and then included that tibble in the coords argument. For more on the topic, check out the Barrett’s (2021a) vignette, An introduction to ggdag. Buy anyway, our DAG represents a heuristic causal model. Like other models, it is an analytical assumption. The symbols \\(A\\), \\(M\\), and \\(D\\) are our observed variables. The arrows show directions of influence. What this DAG says is: \\(A\\) directly influences \\(D\\) \\(M\\) directly influences \\(D\\) \\(A\\) directly influences \\(M\\) These statements can then have further implications. In this case, age of marriage influences divorce in two ways. First it has a direct effect, \\(A \\rightarrow D\\). Perhaps a direct effect would arise because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it has an indirect effect by influencing the marriage rate, which then influences divorce, \\(A \\rightarrow M \\rightarrow D\\). If people get married earlier, then the marriage rate may rise, because there are more young people. (p. 128) Considering alternative models, “It could be that the association between \\(M\\) and \\(D\\) arises entirely from \\(A\\)’s influence on both \\(M\\) and \\(D\\). Like this:” (p. 129) p2 &lt;- dagify(M ~ A, D ~ A, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 10) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) p2 This DAG is also consistent with the posterior distributions of models [b5.1] and [b5.2]. Why? Because both \\(M\\) and \\(D\\) “listen” to \\(A\\). They have information from \\(A\\). So when you inspect the association between \\(D\\) and \\(M\\), you pick up that common information that they both got from listening to \\(A\\). You’ll see a more formal way to deduce this, in the next chapter. So which is it? Is there a direct effect of marriage rate, or rather is age at marriage just driving both, creating a spurious correlation between marriage rate and divorce rate? To find out, we need to consider carefully what each DAG implies. That’s what’s next. (p. 129) 5.1.1.1 Rethinking: What’s a cause? Questions of causation can become bogged down in philosophical debates. These debates are worth having. But they don’t usually intersect with statistical concerns. Knowing a cause in statistics means being able to correctly predict the consequences of an intervention. There are contexts in which even this is complicated. (p. 129) 5.1.2 Testable implications. So far, we have entertained two DAGs. Here we use patchwork to combine them into one plot. library(patchwork) p1 | p2 McElreath encouraged us to examine the correlations among these three variables with cor(). d %&gt;% select(d:a) %&gt;% cor() ## d m a ## d 1.0000000 0.3737314 -0.5972392 ## m 0.3737314 1.0000000 -0.7210960 ## a -0.5972392 -0.7210960 1.0000000 If you just want the lower triangle, you can use the lowerCor() function from the psych package (Revelle, 2020). library(psych) d %&gt;% select(d:a) %&gt;% lowerCor(digits = 3) ## d m a ## d 1.000 ## m 0.374 1.000 ## a -0.597 -0.721 1.000 Our second DAG, above, suggests “that \\(D\\) is independent of \\(M\\), conditional on \\(A\\)” (p. 130). We can use the dagitty::impliedConditionalIndependencies() function to express that conditional independence in formal notation. library(dagitty) dagitty(&#39;dag{ D &lt;- A -&gt; M }&#39;) %&gt;% impliedConditionalIndependencies() ## D _||_ M | A The lack of conditional dependencies in the first DAG may be expressed this way. dagitty(&#39;dag{D &lt;- A -&gt; M -&gt; D}&#39;) %&gt;% impliedConditionalIndependencies() Okay, that was a bit of a tease. “There are no conditional independencies, so there is no output to display” (p. 131). To close out this section, once you fit a multiple regression to predict divorce using both marriage rate and age at marriage, the model addresses the questions: After I already know marriage rate, what additional value is there in also knowing age at marriage? After I already know age at marriage, what additional value is there in also knowing marriage rate? The parameter estimates corresponding to each predictor are the (often opaque) answers to these questions. The questions above are descriptive, and the answers are also descriptive. It is only the derivation of the testable implications above that gives these descriptive results a causal meaning. But that meaning is still dependent upon believing the DAG. (p. 131) 5.1.3 Multiple regression notation. We can write the statistical formula for our first multivariable model as \\[\\begin{align*} \\text{Divorce_std}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{Marriage_std}_i + \\beta_2 \\text{MedianAgeMarriage_std}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] 5.1.4 Approximating the posterior. Much like we used the + operator to add single predictors to the intercept, we just use more + operators in the formula argument to add more predictors. Also notice we’re using the same prior prior(normal(0, 1), class = b) for both predictors. Within the brms framework, they are both of class = b. But if we wanted their priors to differ, we’d make two prior() statements and differentiate them with the coef argument. You’ll see examples of that later on. b5.3 &lt;- brm(data = d, family = gaussian, d ~ 1 + m + a, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.03&quot;) Behold the summary. print(b5.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: d ~ 1 + m + a ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.10 -0.19 0.20 1.00 4036 2610 ## m -0.06 0.16 -0.37 0.25 1.00 2532 2536 ## a -0.61 0.16 -0.91 -0.30 1.00 2522 2486 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.83 0.09 0.68 1.02 1.00 3601 2653 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The brms package doesn’t have a convenience function like rethinking::coeftab(). However, we can make something similar with a little deft wrangling and ggplot2 code. # first, extract and rename the necessary posterior parameters bind_cols( posterior_samples(b5.1) %&gt;% transmute(`b5.1_beta[A]` = b_a), posterior_samples(b5.2) %&gt;% transmute(`b5.2_beta[M]` = b_m), posterior_samples(b5.3) %&gt;% transmute(`b5.3_beta[M]` = b_m, `b5.3_beta[A]` = b_a) ) %&gt;% # convert them to the long format, group, and get the posterior summaries pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% # since the `key` variable is really two variables in one, here we split them up separate(col = name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;_&quot;) %&gt;% # plot! ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = &quot;firebrick&quot;, alpha = 1/5) + geom_pointrange(color = &quot;firebrick&quot;) + labs(x = &quot;posterior&quot;, y = NULL) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;)) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) Don’t worry, coefficient plots won’t always be this complicated. We’ll walk out simpler ones toward the end of the chapter. The substantive interpretation of all those coefficients is: “Once we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State” (p. 134, emphasis in the original). This coheres well with one of our impliedConditionalIndependencies() statements, from above. dagitty(&#39;dag{ D &lt;- A -&gt; M }&#39;) %&gt;% impliedConditionalIndependencies() ## D _||_ M | A 5.1.4.1 Overthinking: Simulating the divorce example. Okay, let’s simulate our divorce data in a tidyverse sort of way. # how many states would you like? n &lt;- 50 set.seed(5) sim_d &lt;- tibble(age = rnorm(n, mean = 0, sd = 1)) %&gt;% # sim A mutate(mar = rnorm(n, mean = -age, sd = 1), # sim A -&gt; M div = rnorm(n, mean = age, sd = 1)) # sim A -&gt; D head(sim_d) ## # A tibble: 6 x 3 ## age mar div ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.841 2.30 -2.84 ## 2 1.38 -1.20 2.52 ## 3 -1.26 2.28 -0.580 ## 4 0.0701 -0.662 0.279 ## 5 1.71 -1.82 1.65 ## 6 -0.603 -0.322 0.291 We simulated those data based on this formulation. dagitty(&#39;dag{divorce &lt;- age -&gt; marriage}&#39;) %&gt;% impliedConditionalIndependencies() ## dvrc _||_ mrrg | age Here are the quick pairs() plots. pairs(sim_d, col = &quot;firebrick4&quot;) If we use the update() function, we can refit the last models in haste. b5.1_sim &lt;- update(b5.1, newdata = sim_d, formula = div ~ 1 + age, seed = 5, file = &quot;fits/b05.01_sim&quot;) b5.2_sim &lt;- update(b5.2, newdata = sim_d, formula = div ~ 1 + mar, seed = 5, file = &quot;fits/b05.02_sim&quot;) b5.3_sim &lt;- update(b5.3, newdata = sim_d, formula = div ~ 1 + mar + age, seed = 5, file = &quot;fits/b05.03_sim&quot;) The steps for our homemade coefplot() plot are basically the same. Just switch out some of the names. bind_cols( posterior_samples(b5.1_sim) %&gt;% transmute(`b5.1_beta[A]` = b_age), posterior_samples(b5.2_sim) %&gt;% transmute(`b5.2_beta[M]` = b_mar), posterior_samples(b5.3_sim) %&gt;% transmute(`b5.3_beta[M]` = b_mar, `b5.3_beta[A]` = b_age) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% # since the `key` variable is really two variables in one, here we split them up separate(name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;_&quot;) %&gt;% # plot! ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) + geom_vline(xintercept = 0, color = &quot;firebrick&quot;, alpha = 1/5) + geom_pointrange(color = &quot;firebrick&quot;) + labs(x = &quot;posterior&quot;, y = NULL) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) Well, okay. This is the same basic pattern, but with the signs switched and with a little simulation variability thrown in. But you get the picture. 5.1.5 Plotting multivariate posteriors. “Let’s pause for a moment, before moving on. There are a lot of moving parts here: three variables, some strange DAGs, and three models. If you feel at all confused, it is only because you are paying attention” (p. 133). Preach, brother. Down a little further, McElreath gave us this deflationary delight: “There is a huge literature detailing a variety of plotting techniques that all attempt to help one understand multiple linear regression. None of these techniques is suitable for all jobs, and most do not generalize beyond linear regression” (pp. 134–135). Now you’re inspired, let’s learn three: predictor residual plots posterior prediction plots counterfactual plots 5.1.5.1 Predictor residual plots. To get ready to make our residual plots, we’ll predict one predictor, m, with another one, a. b5.4 &lt;- brm(data = d, family = gaussian, m ~ 1 + a, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.04&quot;) print(b5.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: m ~ 1 + a ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.09 -0.18 0.18 1.00 3699 3022 ## a -0.69 0.10 -0.89 -0.50 1.00 3628 2808 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.71 0.07 0.59 0.87 1.00 3316 2683 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). With fitted(), we compute the expected values for each state (with the exception of Nevada). Since the a values for each state are in the date we used to fit the model, we’ll omit the newdata argument. f &lt;- fitted(b5.4) %&gt;% data.frame() %&gt;% bind_cols(d) glimpse(f) ## Rows: 50 ## Columns: 20 ## $ Estimate &lt;dbl&gt; 0.41910637, 0.47479957, 0.14064036, 0.97603837, -0.41629164, 0.19633356,… ## $ Est.Error &lt;dbl&gt; 0.10921357, 0.11410274, 0.09208704, 0.16994304, 0.10790150, 0.09434686, … ## $ Q2.5 &lt;dbl&gt; 0.20609178, 0.25056245, -0.04056262, 0.64629944, -0.62775040, 0.01235227… ## $ Q97.5 &lt;dbl&gt; 0.63362431, 0.69624957, 0.32271400, 1.30996611, -0.20523617, 0.38152568,… ## $ Location &lt;fct&gt; Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, D… ## $ Loc &lt;fct&gt; AL, AK, AZ, AR, CA, CO, CT, DE, DC, FL, GA, HI, ID, IL, IN, IA, KS, KY, … ## $ Population &lt;dbl&gt; 4.78, 0.71, 6.33, 2.92, 37.25, 5.03, 3.57, 0.90, 0.60, 18.80, 9.69, 1.36… ## $ MedianAgeMarriage &lt;dbl&gt; 25.3, 25.2, 25.8, 24.3, 26.8, 25.7, 27.6, 26.6, 29.7, 26.4, 25.9, 26.9, … ## $ Marriage &lt;dbl&gt; 20.2, 26.0, 20.3, 26.4, 19.1, 23.5, 17.1, 23.1, 17.7, 17.0, 22.1, 24.9, … ## $ Marriage.SE &lt;dbl&gt; 1.27, 2.93, 0.98, 1.70, 0.39, 1.24, 1.06, 2.89, 2.53, 0.58, 0.81, 2.54, … ## $ Divorce &lt;dbl&gt; 12.7, 12.5, 10.8, 13.5, 8.0, 11.6, 6.7, 8.9, 6.3, 8.5, 11.5, 8.3, 7.7, 8… ## $ Divorce.SE &lt;dbl&gt; 0.79, 2.05, 0.74, 1.22, 0.24, 0.94, 0.77, 1.39, 1.89, 0.32, 0.58, 1.27, … ## $ WaffleHouses &lt;int&gt; 128, 0, 18, 41, 0, 11, 0, 3, 0, 133, 381, 0, 0, 2, 17, 0, 6, 64, 66, 0, … ## $ South &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, … ## $ Slaves1860 &lt;int&gt; 435080, 0, 0, 111115, 0, 0, 0, 1798, 0, 61745, 462198, 0, 0, 0, 0, 0, 2,… ## $ Population1860 &lt;int&gt; 964201, 0, 0, 435450, 379994, 34277, 460147, 112216, 75080, 140424, 1057… ## $ PropSlaves1860 &lt;dbl&gt; 4.5e-01, 0.0e+00, 0.0e+00, 2.6e-01, 0.0e+00, 0.0e+00, 0.0e+00, 1.6e-02, … ## $ d &lt;dbl&gt; 1.6542053, 1.5443643, 0.6107159, 2.0935693, -0.9270579, 1.0500799, -1.64… ## $ m &lt;dbl&gt; 0.02264406, 1.54980162, 0.04897436, 1.65512283, -0.26698927, 0.89154405,… ## $ a &lt;dbl&gt; -0.6062895, -0.6866993, -0.2042408, -1.4103870, 0.5998567, -0.2846505, 1… After a little data processing, we can make the upper left panel of Figure 5.4. p1 &lt;- f %&gt;% ggplot(aes(x = a, y = m)) + geom_point(size = 2, shape = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(xend = a, yend = Estimate), size = 1/4) + geom_line(aes(y = Estimate), color = &quot;firebrick4&quot;) + geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 14) + labs(x = &quot;Age at marriage (std)&quot;, y = &quot;Marriage rate (std)&quot;) + coord_cartesian(ylim = range(d$m)) + theme_bw() + theme(panel.grid = element_blank()) p1 We get the residuals with the well-named residuals() function. Much like with brms::fitted(), brms::residuals() returns a four-vector matrix with the number of rows equal to the number of observations in the original data (by default, anyway). The vectors have the familiar names: Estimate, Est.Error, Q2.5, and Q97.5. See the brms reference manual (Bürkner, 2021i) for details. With our residuals in hand, we just need a little more data processing to make lower left panel of Figure 5.4. r &lt;- residuals(b5.4) %&gt;% # to use this in ggplot2, we need to make it a tibble or data frame data.frame() %&gt;% bind_cols(d) p3 &lt;- r %&gt;% ggplot(aes(x = Estimate, y = d)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;WY&quot;, &quot;ND&quot;, &quot;ME&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 5) + scale_x_continuous(limits = c(-2, 2)) + coord_cartesian(xlim = range(r$Estimate)) + labs(x = &quot;Marriage rate residuals&quot;, y = &quot;Divorce rate (std)&quot;) + theme_bw() + theme(panel.grid = element_blank()) p3 To get the MedianAgeMarriage_s residuals, we have to fit the corresponding model where m predicts a. b5.4b &lt;- brm(data = d, family = gaussian, a ~ 1 + m, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.04b&quot;) With b5.4b in hand, we’re ready to make the upper right panel of Figure 5.4. p2 &lt;- fitted(b5.4b) %&gt;% data.frame() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = m, y = a)) + geom_point(size = 2, shape = 1, color = &quot;firebrick4&quot;) + geom_segment(aes(xend = m, yend = Estimate), size = 1/4) + geom_line(aes(y = Estimate), color = &quot;firebrick4&quot;) + geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;DC&quot;, &quot;HI&quot;, &quot;ID&quot;)), aes(label = Loc), size = 3, seed = 5) + labs(x = &quot;Marriage rate (std)&quot;, y = &quot;Age at marriage (std)&quot;) + coord_cartesian(ylim = range(d$a)) + theme_bw() + theme(panel.grid = element_blank()) p2 And now we’ll get the new batch of residuals, do a little data processing, and make a plot corresponding to the final panel of Figure 5.4. r &lt;- residuals(b5.4b) %&gt;% data.frame() %&gt;% bind_cols(d) p4 &lt;- r %&gt;% ggplot(aes(x = Estimate, y = d)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + geom_point(size = 2, color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 5) + scale_x_continuous(limits = c(-2, 3)) + coord_cartesian(xlim = range(r$Estimate), ylim = range(d$d)) + labs(x = &quot;Age at marriage residuals&quot;, y = &quot;Divorce rate (std)&quot;) + theme_bw() + theme(panel.grid = element_blank()) p4 Here we close out the section by combining our four subplots into one glorious whole with a little patchwork syntax. p1 + p2 + p3 + p4 + plot_annotation(title = &quot;Understanding multiple regression through residuals&quot;) 5.1.5.1.1 Rethinking: Residuals are parameters, not data. There is a tradition, especially in parts of biology, of using residuals from one model as data in another model. For example, a biologist might regress brain size on body size and then use the brain size residuals as data in another model. This procedure is always a mistake. Residuals are not known. They are parameters, variables with unobserved values. Treating them as known values throws away uncertainty. (p. 137) Let’s hammer this point home. Recall how brms::residuals() returns four columns: Estimate, Est.Error, Q2.5, and Q97.5. r %&gt;% glimpse() ## Rows: 50 ## Columns: 20 ## $ Estimate &lt;dbl&gt; -0.58902924, 0.38776286, -0.16875288, -0.26301441, 0.41661319, 0.3341211… ## $ Est.Error &lt;dbl&gt; 0.08768933, 0.17676818, 0.08778774, 0.18591313, 0.09165187, 0.12435474, … ## $ Q2.5 &lt;dbl&gt; -0.75889766, 0.03461877, -0.33885619, -0.63501083, 0.23443209, 0.0823210… ## $ Q97.5 &lt;dbl&gt; -0.4185013025, 0.7310140636, 0.0006765707, 0.0975187886, 0.5958362565, 0… ## $ Location &lt;fct&gt; Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, D… ## $ Loc &lt;fct&gt; AL, AK, AZ, AR, CA, CO, CT, DE, DC, FL, GA, HI, ID, IL, IN, IA, KS, KY, … ## $ Population &lt;dbl&gt; 4.78, 0.71, 6.33, 2.92, 37.25, 5.03, 3.57, 0.90, 0.60, 18.80, 9.69, 1.36… ## $ MedianAgeMarriage &lt;dbl&gt; 25.3, 25.2, 25.8, 24.3, 26.8, 25.7, 27.6, 26.6, 29.7, 26.4, 25.9, 26.9, … ## $ Marriage &lt;dbl&gt; 20.2, 26.0, 20.3, 26.4, 19.1, 23.5, 17.1, 23.1, 17.7, 17.0, 22.1, 24.9, … ## $ Marriage.SE &lt;dbl&gt; 1.27, 2.93, 0.98, 1.70, 0.39, 1.24, 1.06, 2.89, 2.53, 0.58, 0.81, 2.54, … ## $ Divorce &lt;dbl&gt; 12.7, 12.5, 10.8, 13.5, 8.0, 11.6, 6.7, 8.9, 6.3, 8.5, 11.5, 8.3, 7.7, 8… ## $ Divorce.SE &lt;dbl&gt; 0.79, 2.05, 0.74, 1.22, 0.24, 0.94, 0.77, 1.39, 1.89, 0.32, 0.58, 1.27, … ## $ WaffleHouses &lt;int&gt; 128, 0, 18, 41, 0, 11, 0, 3, 0, 133, 381, 0, 0, 2, 17, 0, 6, 64, 66, 0, … ## $ South &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, … ## $ Slaves1860 &lt;int&gt; 435080, 0, 0, 111115, 0, 0, 0, 1798, 0, 61745, 462198, 0, 0, 0, 0, 0, 2,… ## $ Population1860 &lt;int&gt; 964201, 0, 0, 435450, 379994, 34277, 460147, 112216, 75080, 140424, 1057… ## $ PropSlaves1860 &lt;dbl&gt; 4.5e-01, 0.0e+00, 0.0e+00, 2.6e-01, 0.0e+00, 0.0e+00, 0.0e+00, 1.6e-02, … ## $ d &lt;dbl&gt; 1.6542053, 1.5443643, 0.6107159, 2.0935693, -0.9270579, 1.0500799, -1.64… ## $ m &lt;dbl&gt; 0.02264406, 1.54980162, 0.04897436, 1.65512283, -0.26698927, 0.89154405,… ## $ a &lt;dbl&gt; -0.6062895, -0.6866993, -0.2042408, -1.4103870, 0.5998567, -0.2846505, 1… In the residual plots from the lower two panels of Figure 5.4, we focused on the means of the residuals (i.e., Estimate). However, we can express the uncertainty in the residuals by including error bars for the 95% intervals. Here’s what that might look like with a slight reworking of the lower right panel of Figure 5.4. r %&gt;% ggplot(aes(x = Estimate, y = d)) + stat_smooth(method = &quot;lm&quot;, fullrange = T, color = &quot;firebrick4&quot;, fill = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_vline(xintercept = 0, linetype = 2, color = &quot;grey50&quot;) + # the only change is here geom_pointrange(aes(xmin = Q2.5, xmax = Q97.5), color = &quot;firebrick4&quot;, alpha = 2/3) + geom_text_repel(data = . %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;HI&quot;, &quot;DC&quot;)), aes(label = Loc), size = 3, seed = 5) + scale_x_continuous(limits = c(-2, 3)) + coord_cartesian(xlim = range(r$Estimate), ylim = range(d$d)) + labs(x = &quot;Age at marriage residuals&quot;, y = &quot;Divorce rate (std)&quot;) + theme_bw() + theme(panel.grid = element_blank()) Look at that. If you were to fit a follow-up model based on only the point estimates (posterior means) of those residuals, you’d be ignoring a lot of uncertainty. 5.1.5.2 Posterior prediction plots. “It’s important to check the model’s implied predictions against the observed data” (p. 137). For more on the topic, check out Gabry and colleagues’ (2019) Visualization in Bayesian workflow or Simpson’s related blog post, Touch me, I want to feel your data. The code below will make our version of Figure 5.5. fitted(b5.3) %&gt;% data.frame() %&gt;% # un-standardize the model predictions mutate_all(~. * sd(d$Divorce) + mean(d$Divorce)) %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = Divorce, y = Estimate)) + geom_abline(linetype = 2, color = &quot;grey50&quot;, size = .5) + geom_point(size = 1.5, color = &quot;firebrick4&quot;, alpha = 3/4) + geom_linerange(aes(ymin = Q2.5, ymax = Q97.5), size = 1/4, color = &quot;firebrick4&quot;) + geom_text(data = . %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;UT&quot;, &quot;RI&quot;, &quot;ME&quot;)), aes(label = Loc), hjust = 1, nudge_x = - 0.25) + labs(x = &quot;Observed divorce&quot;, y = &quot;Predicted divorce&quot;) + theme_bw() + theme(panel.grid = element_blank()) It’s easy to see from this arrangement of the simulations that the model under-predicts for States with very high divorce rates while it over-predicts for States with very low divorce rates. That’s normal. This is what regression does–it is skeptical of extreme values, so it expects regression towards the mean. But beyond this general regression to the mean, some States are very frustrating to the model, lying very far from the diagonal. (p. 139) 5.1.5.2.1 Rethinking: Stats, huh, yeah what is it good for? Often people want statistical modeling to do things that statistical modeling cannot do. For example, we’d like to know whether an effect is “real” or rather spurious. Unfortunately, modeling merely quantifies uncertainty in the precise way that the model understands the problem. Usually answers to large world questions about truth and causation depend upon information not included in the model. For example, any observed correlation between an outcome and predictor could be eliminated or reversed once another predictor is added to the model. But if we cannot think of the right variable, we might never notice. Therefore all statistical models are vulnerable to and demand critique, regardless of the precision of their estimates and apparent accuracy of their predictions. (p. 139) 5.1.5.2.2 Overthinking: Simulating spurious association. n &lt;- 100 # number of cases set.seed(5) # setting the seed makes the results reproducible d_spur &lt;- tibble(x_real = rnorm(n), # x_real as Gaussian with mean 0 and SD 1 (i.e., the defaults) x_spur = rnorm(n, x_real), # x_spur as Gaussian with mean = x_real y = rnorm(n, x_real)) # y as Gaussian with mean = x_real Here are the quick pairs() plots. pairs(d_spur, col = &quot;firebrick4&quot;) We may as well fit and evaluate a model. b5.0_spur &lt;- brm(data = d_spur, family = gaussian, y ~ 1 + x_real + x_spur, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.00_spur&quot;) fixef(b5.0_spur) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.01 0.09 -0.18 0.17 ## x_real 0.93 0.14 0.65 1.20 ## x_spur 0.08 0.09 -0.09 0.26 If we let “r” stand for x_rel and “s” stand for x_spur, here’s how we might depict that our simulation in a DAG. dag_coords &lt;- tibble(name = c(&quot;r&quot;, &quot;s&quot;, &quot;y&quot;), x = c(1, 3, 2), y = c(2, 2, 1)) dagify(s ~ r, y ~ r, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 10) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) 5.1.5.3 Counterfactual plots. A second sort of inferential plot displays the causal implications of the model. I call these plots counterfactual, because they can be produced for any values of the predictor variables you like, even unobserved combinations like very high median age of marriage and very high marriage rate. There are no States with this combination, but in a counterfactual plot, you can ask the model for a prediction for such a State. (p. 140, emphasis in the original) Take another look at one of the DAGs from back in Section 5.1.2. dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;D&quot;), x = c(1, 3, 2), y = c(2, 2, 1)) dagify(M ~ A, D ~ A + M, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 10) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) The full statistical model implied in this DAG requires we have two criterion variables, \\(D\\) and \\(M\\). To simultaneously model the effects of \\(A\\) on \\(M\\) and \\(D\\) AND the effects of \\(A\\) on \\(M\\) with brms, we’ll need to invoke the multivariate syntax. There are several ways to do this with brms, which Bürkner outlines in his (2021d) vignette, Estimating multivariate models with brms. At this point, it’s important to recognize we have two regression models. As a first step, we might specify each model separately in a bf() function and save them as objects. d_model &lt;- bf(d ~ 1 + a + m) m_model &lt;- bf(m ~ 1 + a) Next we will combine our bf() objects with the + operator within the brm() function. For a model like this, we also specify set_rescor(FALSE) to prevent brms from adding a residual correlation between d and m. Also, notice how each prior statement includes a resp argument. This clarifies which sub-model the prior refers to. b5.3_A &lt;- brm(data = d, family = gaussian, d_model + m_model + set_rescor(FALSE), prior = c(prior(normal(0, 0.2), class = Intercept, resp = d), prior(normal(0, 0.5), class = b, resp = d), prior(exponential(1), class = sigma, resp = d), prior(normal(0, 0.2), class = Intercept, resp = m), prior(normal(0, 0.5), class = b, resp = m), prior(exponential(1), class = sigma, resp = m)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.03_A&quot;) Look at the summary. print(b5.3_A) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: d ~ 1 + a + m ## m ~ 1 + a ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## d_Intercept -0.00 0.10 -0.20 0.19 1.00 6100 3001 ## m_Intercept 0.00 0.09 -0.17 0.17 1.00 5988 3295 ## d_a -0.60 0.16 -0.91 -0.30 1.00 3350 3309 ## d_m -0.06 0.15 -0.36 0.24 1.00 3575 3316 ## m_a -0.69 0.10 -0.88 -0.50 1.00 5653 3081 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_d 0.82 0.09 0.68 1.01 1.00 6052 2843 ## sigma_m 0.71 0.07 0.58 0.87 1.00 6475 3141 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Note our parameters now all have either a d_ or an m_ prefix to help clarify which sub-model they were for. The m_a row shows how strongly and negatively associated a is to m. Here’s how we might use predict() to make our version of the counterfactual plot in the left panel of Figure 5.6. nd &lt;- tibble(a = seq(from = -2, to = 2, length.out = 30), m = 0) p1 &lt;- predict(b5.3_A, resp = &quot;d&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = a, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + labs(subtitle = &quot;Total counterfactual effect of A on D&quot;, x = &quot;manipulated A&quot;, y = &quot;counterfactual D&quot;) + coord_cartesian(ylim = c(-2, 2)) + theme_bw() + theme(panel.grid = element_blank()) p1 Because the plot is based on a multivariate model, we used the resp argument within predict() to tell brms which of our two criterion variables (d or m) we were interested in. Unlike McElreath’s R code 5.20, we included predictor values for both a and m. This is because brms requires we provide values for all predictors in a model when using predict(). Even though we set all the m values to 0 for the counterfactual, it was necessary to tell predict() that’s exactly what we wanted. Let’s do that all again, this time making the counterfactual for d. While we’re at it, we’ll combine this subplot with the last one to make the full version of Figure 5.6. nd &lt;- tibble(a = seq(from = -2, to = 2, length.out = 30)) p2 &lt;- predict(b5.3_A, resp = &quot;m&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = a, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + labs(subtitle = &quot;Counterfactual effect of A on M&quot;, x = &quot;manipulated A&quot;, y = &quot;counterfactual M&quot;) + coord_cartesian(ylim = c(-2, 2)) + theme_bw() + theme(panel.grid = element_blank()) p1 + p2 + plot_annotation(title = &quot;Counterfactual plots for the multivariate divorce model&quot;) With our brms + tidyverse paradigm, we might compute “the expected causal effect of increasing median age at marriage from 20 to 30” (p. 142) like this. # new data frame, standardized to mean 26.1 and std dev 1.24 nd &lt;- tibble(a = (c(20, 30) - 26.1) / 1.24, m = 0) predict(b5.3_A, resp = &quot;d&quot;, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% set_names(&quot;a20&quot;, &quot;a30&quot;) %&gt;% mutate(difference = a30 - a20) %&gt;% summarise(mean = mean(difference)) ## mean ## 1 -4.860737 The trick with simulating counterfactuals is to realize that when we manipulate some variable \\(X\\), we break the causal influence of other variables on \\(X\\). This is the same as saying we modify the DAG so that no arrows enter \\(X\\). Suppose for example that we now simulate the effect of manipulating \\(M.\\) (p. 143) Here’s how to plot that DAG. dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;D&quot;), x = c(1, 3, 2), y = c(2, 2, 1)) dagify(D ~ A + M, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 10) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) Here’s the new counterfactual plot focusing on \\(M \\rightarrow D\\), holding \\(A = 0\\), Figure 5.7. nd &lt;- tibble(m = seq(from = -2, to = 2, length.out = 30), a = 0) predict(b5.3_A, resp = &quot;d&quot;, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = m, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_smooth(stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + labs(subtitle = &quot;Total counterfactual effect of M on D&quot;, x = &quot;manipulated M&quot;, y = &quot;counterfactual D&quot;) + coord_cartesian(ylim = c(-2, 2)) + theme_bw() + theme(panel.grid = element_blank()) 5.1.5.3.1 Overthinking: Simulating counterfactuals. Just like McElreath showed how to compute the counterfactuals without his sim() function, we can make ours without brms::predict(). First we’ll start out extracting the posterior draws. post &lt;- posterior_samples(b5.3_A) %&gt;% mutate(iter = 1:n()) Here we use expand() elongate the output from above by a factor of thirty, each time corresponding to one of the levels of a = seq(from = -2, to = 2, length.out = 30). In the two mutate() lines that follow, we plug the model formulas into the rnorm() function to take random draws from posterior predictive distribution. The rest is just wrangling and summarizing. post &lt;- post %&gt;% expand(nesting(iter, b_m_Intercept, b_m_a, sigma_m, b_d_Intercept, b_d_a, b_d_m, sigma_d), a = seq(from = -2, to = 2, length.out = 30)) %&gt;% mutate(m_sim = rnorm(n(), mean = b_m_Intercept + b_m_a * a, sd = sigma_m)) %&gt;% mutate(d_sim = rnorm(n(), mean = b_d_Intercept + b_d_a * a + b_d_m * m_sim, sd = sigma_d)) %&gt;% pivot_longer(ends_with(&quot;sim&quot;)) %&gt;% group_by(a, name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) # what did we do? head(post) ## # A tibble: 6 x 5 ## # Groups: a [3] ## a name mean ll ul ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -2 d_sim 1.13 -0.580 2.87 ## 2 -2 m_sim 1.37 -0.0851 2.84 ## 3 -1.86 d_sim 1.03 -0.684 2.72 ## 4 -1.86 m_sim 1.29 -0.170 2.75 ## 5 -1.72 d_sim 0.973 -0.732 2.71 ## 6 -1.72 m_sim 1.19 -0.216 2.63 Now we plot. post %&gt;% mutate(dv = if_else(name == &quot;d_sim&quot;, &quot;predictions for D&quot;, &quot;predictions for M&quot;)) %&gt;% ggplot(aes(x = a, y = mean, ymin = ll, ymax = ul)) + geom_smooth(stat = &quot;identity&quot;, fill = &quot;firebrick&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/4) + labs(title = &quot;Hand-made counterfactual plots for the multivariate divorce model&quot;, x = &quot;manipulated A&quot;, y = &quot;counterfactual&quot;) + coord_cartesian(ylim = c(-2, 2)) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) + facet_wrap(~ dv) 5.2 Masked relationship A second reason to use more than one predictor variable is to measure the direct influences of multiple factors on an outcome, when none of those influences is apparent from bivariate relationships. This kind of problem tends to arise when there are two predictor variables that are correlated with one another. However, one of these is positively correlated with the outcome and the other is negatively correlated with it. (p. 144) Let’s load the Hinde &amp; Milligan (2011) milk data. data(milk, package = &quot;rethinking&quot;) d &lt;- milk rm(milk) glimpse(d) ## Rows: 29 ## Columns: 8 ## $ clade &lt;fct&gt; Strepsirrhine, Strepsirrhine, Strepsirrhine, Strepsirrhine, Strepsirrhine, … ## $ species &lt;fct&gt; Eulemur fulvus, E macaco, E mongoz, E rubriventer, Lemur catta, Alouatta se… ## $ kcal.per.g &lt;dbl&gt; 0.49, 0.51, 0.46, 0.48, 0.60, 0.47, 0.56, 0.89, 0.91, 0.92, 0.80, 0.46, 0.7… ## $ perc.fat &lt;dbl&gt; 16.60, 19.27, 14.11, 14.91, 27.28, 21.22, 29.66, 53.41, 46.08, 50.58, 41.35… ## $ perc.protein &lt;dbl&gt; 15.42, 16.91, 16.85, 13.18, 19.50, 23.58, 23.46, 15.80, 23.34, 22.33, 20.85… ## $ perc.lactose &lt;dbl&gt; 67.98, 63.82, 69.04, 71.91, 53.22, 55.20, 46.88, 30.79, 30.58, 27.09, 37.80… ## $ mass &lt;dbl&gt; 1.95, 2.09, 2.51, 1.62, 2.19, 5.25, 5.37, 2.51, 0.71, 0.68, 0.12, 0.47, 0.3… ## $ neocortex.perc &lt;dbl&gt; 55.16, NA, NA, NA, NA, 64.54, 64.54, 67.64, NA, 68.85, 58.85, 61.69, 60.32,… You might inspect the primary variables in the data with the pairs() function. d %&gt;% select(kcal.per.g, mass, neocortex.perc) %&gt;% pairs(col = &quot;firebrick4&quot;) By just looking at that mess, do you think you could describe the associations of mass and neocortex.perc with the criterion, kcal.per.g? I couldn’t. It’s a good thing we have math. Let’s standardize our variables by hand. d &lt;- d %&gt;% mutate(kcal.per.g_s = (kcal.per.g - mean(kcal.per.g)) / sd(kcal.per.g), log_mass_s = (log(mass) - mean(log(mass))) / sd(log(mass)), neocortex.perc_s = (neocortex.perc - mean(neocortex.perc, na.rm = T)) / sd(neocortex.perc, na.rm = T)) McElreath has us starting off our first milk model with more permissive priors than we’ve used in the past. Although we should note that from a historical perspective, these priors are pretty informative. Times keep changing. b5.5_draft &lt;- brm(data = d, family = gaussian, kcal.per.g_s ~ 1 + neocortex.perc_s, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, sample_prior = T, file = &quot;fits/b05.05_draft&quot;) Similar to the rethinking example in the text, brms warned that “Rows containing NAs were excluded from the model.” This isn’t necessarily a problem; the model fit just fine. But we should be ashamed of ourselves and look eagerly forward to Chapter 15 where we’ll learn how to do better. To compliment how McElreath removed cases with missing values on our variables of interest with base R complete.cases(), here we’ll do so with tidyr::drop_na() and a little help with ends_with(). dcc &lt;- d %&gt;% drop_na(ends_with(&quot;_s&quot;)) # how many rows did we drop? nrow(d) - nrow(dcc) ## [1] 12 We’ll use update() to refit the model with the altered data. b5.5_draft &lt;- update(b5.5_draft, newdata = dcc, seed = 5) “Before considering the posterior predictions, let’s consider those priors. As in many simple linear regression problems, these priors are harmless. But are they reasonable?” (p. 146). Let’s find out with our version of Figure 5.8.a. set.seed(5) prior_samples(b5.5_draft) %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), neocortex.perc_s = c(-2, 2)) %&gt;% mutate(kcal.per.g_s = Intercept + b * neocortex.perc_s) %&gt;% ggplot(aes(x = neocortex.perc_s, y = kcal.per.g_s)) + geom_line(aes(group = rowname), color = &quot;firebrick&quot;, alpha = .4) + coord_cartesian(ylim = c(-2, 2)) + labs(x = &quot;neocortex percent (std)&quot;, y = &quot;kilocal per g (std)&quot;, subtitle = &quot;Intercept ~ dnorm(0, 1)\\nb ~ dnorm(0, 1)&quot;) + theme_bw() + theme(panel.grid = element_blank()) That’s a mess. How’d the posterior turn out? print(b5.5_draft) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g_s ~ 1 + neocortex.perc_s ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.09 0.26 -0.44 0.61 1.00 3341 2624 ## neocortex.perc_s 0.16 0.27 -0.38 0.70 1.00 3647 2763 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.13 0.21 0.81 1.63 1.00 2827 2553 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Let’s tighten up our priors and fit b5.5. b5.5 &lt;- brm(data = dcc, family = gaussian, kcal.per.g_s ~ 1 + neocortex.perc_s, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, sample_prior = T, file = &quot;fits/b05.05&quot;) Now make our version of Figure 5.8.b. set.seed(5) prior_samples(b5.5) %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), neocortex.perc_s = c(-2, 2)) %&gt;% mutate(kcal.per.g_s = Intercept + b * neocortex.perc_s) %&gt;% ggplot(aes(x = neocortex.perc_s, y = kcal.per.g_s, group = rowname)) + geom_line(color = &quot;firebrick&quot;, alpha = .4) + coord_cartesian(ylim = c(-2, 2)) + labs(subtitle = &quot;Intercept ~ dnorm(0, 0.2)\\nb ~ dnorm(0, 0.5)&quot;, x = &quot;neocortex percent (std)&quot;, y = &quot;kilocal per g (std)&quot;) + theme_bw() + theme(panel.grid = element_blank()) Look at the posterior summary. print(b5.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g_s ~ 1 + neocortex.perc_s ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.04 0.16 -0.27 0.35 1.00 3606 2670 ## neocortex.perc_s 0.13 0.24 -0.35 0.59 1.00 3510 2316 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.10 0.20 0.79 1.58 1.00 3002 2584 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The results are very similar to those returned earlier from print(b5.5_draft). It’s not in the text, but let’s compare the parameter estimates between the two models with another version of our homemade coeftab() plot. # wrangle bind_rows( posterior_samples(b5.5_draft) %&gt;% select(b_Intercept:sigma), posterior_samples(b5.5) %&gt;% select(b_Intercept:sigma) ) %&gt;% mutate(fit = rep(c(&quot;b5.5_draft&quot;, &quot;b5.5&quot;), each = n() / 2)) %&gt;% pivot_longer(-fit) %&gt;% group_by(name, fit) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate(fit = factor(fit, levels = c(&quot;b5.5_draft&quot;, &quot;b5.5&quot;))) %&gt;% # plot ggplot(aes(x = mean, y = fit, xmin = ll, xmax = ul)) + geom_pointrange(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0, color = &quot;firebrick&quot;, alpha = 1/5) + labs(x = &quot;posterior&quot;, y = NULL) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank(), strip.background = element_blank()) + facet_wrap(~ name, ncol = 1) The results were quite similar, but the estimates from b5.5 are more precise. Let’s get back on track with the text and make the top left panel of Figure 5.9. Just for kicks, we’ll superimpose 50% intervals atop 95% intervals for the next few plots. Here’s Figure 5.9, top left. nd &lt;- tibble(neocortex.perc_s = seq(from = -2.5, to = 2, length.out = 30)) fitted(b5.5, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc_s, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(x = neocortex.perc_s, y = kcal.per.g_s), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$neocortex.perc_s), ylim = range(dcc$kcal.per.g_s)) + labs(x = &quot;neocortex percent (std)&quot;, y = &quot;kilocal per g (std)&quot;) + theme_bw() + theme(panel.grid = element_blank()) Do note the probs argument in the fitted() code, above. Now we use log_mass_s as the new sole predictor. b5.6 &lt;- brm(data = dcc, family = gaussian, kcal.per.g_s ~ 1 + log_mass_s, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, sample_prior = T, file = &quot;fits/b05.06&quot;) print(b5.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g_s ~ 1 + log_mass_s ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.04 0.15 -0.26 0.34 1.00 4160 2389 ## log_mass_s -0.27 0.21 -0.68 0.17 1.00 3708 2578 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.05 0.19 0.75 1.47 1.00 3106 2655 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Make Figure 5.9, top right. nd &lt;- tibble(log_mass_s = seq(from = -2.5, to = 2.5, length.out = 30)) fitted(b5.6, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass_s, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + geom_point(data = dcc, aes(y = kcal.per.g_s), size = 2, color = &quot;firebrick4&quot;) + coord_cartesian(xlim = range(dcc$log_mass_s), ylim = range(dcc$kcal.per.g_s)) + labs(x = &quot;log body mass (std)&quot;, y = &quot;kilocal per g (std)&quot;) + theme_bw() + theme(panel.grid = element_blank()) Finally, we’re ready to fit with both predictors included in a multivariable model. The statistical formula is \\[\\begin{align*} \\text{kcal.per.g_s}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{neocortex.perc_s}_i + \\beta_2 \\text{log_mass_s} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Fit the model. b5.7 &lt;- brm(data = dcc, family = gaussian, kcal.per.g_s ~ 1 + neocortex.perc_s + log_mass_s, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.07&quot;) print(b5.7) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g_s ~ 1 + neocortex.perc_s + log_mass_s ## Data: dcc (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.07 0.15 -0.23 0.35 1.00 3492 2495 ## neocortex.perc_s 0.60 0.28 -0.00 1.12 1.00 2331 1968 ## log_mass_s -0.64 0.25 -1.11 -0.11 1.00 2412 2618 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.87 0.18 0.59 1.28 1.00 2337 2059 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Once again, let’s roll out our homemade coefplot() plot code. bind_cols( posterior_samples(b5.5) %&gt;% transmute(`b5.5_beta[N]` = b_neocortex.perc_s), posterior_samples(b5.6) %&gt;% transmute(`b5.6_beta[M]` = b_log_mass_s), posterior_samples(b5.7) %&gt;% transmute(`b5.7_beta[N]` = b_neocortex.perc_s, `b5.7_beta[M]` = b_log_mass_s) ) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% separate(name, into = c(&quot;fit&quot;, &quot;parameter&quot;), sep = &quot;_&quot;) %&gt;% # complete(fit, parameter) %&gt;% ggplot(aes(x = mean, y = fit, xmin = ll, xmax = ul)) + geom_pointrange(color = &quot;firebrick&quot;) + geom_hline(yintercept = 0, color = &quot;firebrick&quot;, alpha = 1/5) + ylab(NULL) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;)) + facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) On page 151, McElreath suggested we look at a pairs plot to get a sense of the zero-order correlations. We did that once with the raw data. Here it is, again, but with the transformed variables. dcc %&gt;% select(ends_with(&quot;_s&quot;)) %&gt;% pairs(col = &quot;firebrick4&quot;) Have you noticed how un-tidyverse-like those pairs() plots are? I have. Within the tidyverse, you can make custom pairs plots with the GGally package (Schloerke et al., 2020), which will also compute the point estimates for the bivariate correlations. Here’s a default-style plot. library(GGally) dcc %&gt;% select(ends_with(&quot;_s&quot;)) %&gt;% ggpairs() But you can customize these, too. E.g., # define custom functions my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(fill = &quot;firebrick4&quot;, size = 0) } my_lower &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_smooth(method = &quot;lm&quot;, color = &quot;firebrick4&quot;, size = 1, se = F) + geom_point(color = &quot;firebrick&quot;, alpha = .8, size = 1/3) } # plot dcc %&gt;% select(ends_with(&quot;_s&quot;)) %&gt;% ggpairs(upper = list(continuous = wrap(&quot;cor&quot;, family = &quot;sans&quot;, color = &quot;black&quot;)), # plug those custom functions into `ggpairs()` diag = list(continuous = my_diag), lower = list(continuous = my_lower)) + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank(), strip.background = element_rect(fill = &quot;white&quot;, color = &quot;white&quot;)) What the regression model does is ask if species that have high neocortex percent for their body mass have higher milk energy. Likewise, the model asks if species with high body mass for their neocortex percent have higher milk energy. Bigger species, like apes, have milk with less energy. But species with more neocortex tend to have richer milk. The fact that these two variables, body size and neocortex, are correlated across species makes it hard to see these relationships, unless we account for both. Some DAGs will help. (p. 148, emphasis in the original) Here are three. I’m not aware we can facet dagify() objects. But we can take cues from Chapter 4 to link our three DAGs like McElreath did his. first, we’ll recognize the ggplot2 code will be nearly identical for each DAG. So we can just wrap the ggplot2 code into a compact function, like so. gg_dag &lt;- function(d) { d %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 10) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank()) } Now we’ll make the three individual DAGs, saving each. # left DAG dag_coords &lt;- tibble(name = c(&quot;M&quot;, &quot;N&quot;, &quot;K&quot;), x = c(1, 3, 2), y = c(2, 2, 1)) p1 &lt;- dagify(N ~ M, K ~ M + N, coords = dag_coords) %&gt;% gg_dag() # middle DAG p2 &lt;- dagify(M ~ N, K ~ M + N, coords = dag_coords) %&gt;% gg_dag() # right DAG dag_coords &lt;- tibble(name = c(&quot;M&quot;, &quot;N&quot;, &quot;K&quot;, &quot;U&quot;), x = c(1, 3, 2, 2), y = c(2, 2, 1, 2)) p3 &lt;- dagify(M ~ U, N ~ U, K ~ M + N, coords = dag_coords) %&gt;% gg_dag() + geom_point(x = 2, y = 2, shape = 1, size = 10, stroke = 1.25, color = &quot;firebrick4&quot;) Now we combine our gg_dag() plots together with patchwork syntax. p1 + p2 + p3 Which of these graphs is right? We can’t tell from the data alone, because these graphs imply the same set of conditional independencies. In this case, there are no conditional independencies–each DAG above implies that all pairs of variables are associated, regardless of what we condition on. A set of DAGs with the same conditional independencies is known as a Markov equivalence set. (p. 151, emphasis in the original). Let’s make the counterfactual plots at the bottom of Figure 5.9. Here’s the one on the left. nd &lt;- tibble(neocortex.perc_s = seq(from = -2.5, to = 2, length.out = 30), log_mass_s = 0) p1 &lt;- fitted(b5.7, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = neocortex.perc_s, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + coord_cartesian(xlim = range(dcc$neocortex.perc_s), ylim = range(dcc$kcal.per.g_s)) + labs(subtitle = &quot;Counterfactual holding M = 0&quot;, x = &quot;neocortex percent (std)&quot;, y = &quot;kilocal per g (std)&quot;) Now make Figure 5.9, bottom right, and combine the two. nd &lt;- tibble(log_mass_s = seq(from = -2.5, to = 2.5, length.out = 30), neocortex.perc_s = 0) p2 &lt;- fitted(b5.7, newdata = nd, probs = c(.025, .975, .25, .75)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = log_mass_s, y = Estimate)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;firebrick&quot;, alpha = 1/5) + geom_smooth(aes(ymin = Q25, ymax = Q75), stat = &quot;identity&quot;, fill = &quot;firebrick4&quot;, color = &quot;firebrick4&quot;, alpha = 1/5, size = 1/2) + coord_cartesian(xlim = range(dcc$log_mass_s), ylim = range(dcc$kcal.per.g_s)) + labs(subtitle = &quot;Counterfactual holding N = 0&quot;, x = &quot;log body mass (std)&quot;, y = &quot;kilocal per g (std)&quot;) # combine p1 + p2 + plot_annotation(title = &quot;Figure 5.9 [bottom row]. Milk energy and neocortex among primates.&quot;) &amp; theme_bw() &amp; theme(panel.grid = element_blank()) 5.2.0.1 Overthinking: Simulating a masking relationship. As a refresher, here’s our focal DAG. dag_coords &lt;- tibble(name = c(&quot;M&quot;, &quot;N&quot;, &quot;K&quot;), x = c(1, 3, 2), y = c(2, 2, 1)) dagify(N ~ M, K ~ M + N, coords = dag_coords) %&gt;% gg_dag() Now simulate data consistent with that DAG. # how many cases would you like? n &lt;- 100 set.seed(5) d_sim &lt;- tibble(m = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(n = rnorm(n, mean = m, sd = 1)) %&gt;% mutate(k = rnorm(n, mean = n - m, sd = 1)) Use ggpairs() to get a sense of what we just simulated. d_sim %&gt;% ggpairs(upper = list(continuous = wrap(&quot;cor&quot;, family = &quot;sans&quot;, color = &quot;firebrick4&quot;)), diag = list(continuous = my_diag), lower = list(continuous = my_lower)) + theme_bw() + theme(axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank(), strip.background = element_rect(fill = &quot;white&quot;, color = &quot;white&quot;)) Here we fit the simulation models with a little help from the update() function. b5.7_sim &lt;- update(b5.7, newdata = d_sim, formula = k ~ 1 + n + m, seed = 5, file = &quot;fits/b05.07_sim&quot;) b5.5_sim &lt;- update(b5.7_sim, formula = k ~ 1 + n, seed = 5, file = &quot;fits/b05.05_sim&quot;) b5.6_sim &lt;- update(b5.7_sim, formula = k ~ 1 + m, seed = 5, file = &quot;fits/b05.06_sim&quot;) Compare the coefficients. fixef(b5.5_sim) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.02 0.10 -0.22 0.17 ## n 0.58 0.08 0.43 0.74 fixef(b5.6_sim) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.01 0.12 -0.23 0.24 ## m 0.18 0.15 -0.12 0.48 fixef(b5.7_sim) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept -0.01 0.09 -0.17 0.17 ## n 0.98 0.09 0.80 1.16 ## m -0.88 0.14 -1.16 -0.61 Due to space considerations, I’m not going to show the code corresponding to the other two DAGs from the R code 5.43 block. Rather, I’ll leave that as an exercise for the interested reader. Let’s do the preliminary work to making our DAGs. dag5.7 &lt;- dagitty(&quot;dag{ M -&gt; K &lt;- N M -&gt; N }&quot; ) coordinates(dag5.7) &lt;- list(x = c(M = 0, K = 1, N = 2), y = c(M = 0.5, K = 1, N = 0.5)) If you just want a quick default plot, ggdag::ggdag_equivalent_dags() is the way to go. ggdag_equivalent_dags(dag5.7) However, if you’d like to customize your DAGs, start with the ggdag::node_equivalent_dags() function and build from there. dag5.7 %&gt;% node_equivalent_dags() %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;firebrick&quot;, alpha = 1/4, size = 10) + geom_dag_text(color = &quot;firebrick&quot;) + geom_dag_edges(edge_color = &quot;firebrick&quot;) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) + facet_wrap(~ dag) These all demonstrate Markov equivalence. I should note that I got help from the great Malcolm Barrett on how to make this plot with ggdag. 5.3 Categorical variables Many readers will already know that variables like this, routinely called factors, can easily be included in linear models. But what is not widely understood is how these variables are represented in a model… Knowing how the machine (golem) works both helps you interpret the posterior distribution and gives you additional power in building the model. (p. 153, emphasis in the original) 5.3.1 Binary categories. Reload the Howell1 data. data(Howell1, package = &quot;rethinking&quot;) d &lt;- Howell1 rm(Howell1) If you forgot what these data were like, take a glimpse(). d %&gt;% glimpse() ## Rows: 544 ## Columns: 4 ## $ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 147… ## $ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, 34.… ## $ age &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20.0,… ## $ male &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,… The male variable is our new predictor, an example of a indicator variable. Indicator variables—sometimes also called “dummy” variables–are devices for encoding unordered categories into quantitative models. There is no sense here in which “male” is one more than “female.” The purpose of the male variable is to indicate when a person in the sample is “male.” So it takes the value 1 whenever the person is male, but it takes the value 0 when the person belongs to any other category. It doesn’t matter which category is indicated by the 1. The model won’t care. But correctly interpreting the model demands that you remember, so it’s a good idea to name the variable after the category assigned the 1 value. (p. 154, emphasis in the original) The statistical model including a male dummy might follow the form \\[\\begin{align*} \\text{height}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{male}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(178, 20) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where \\(\\beta_1\\) is the expected (i.e., average) difference between males and females for height. Note we’re deviating from the text a little and entertaining an \\(\\operatorname{Exponential}(1)\\) prior on \\(\\sigma\\) rather than the uniform prior McElreath reverted to. As we saw in the last chapter, brms can accommodate uniform priors on \\(\\sigma\\), but it often causes problems for HMC and, IMO, is more trouble that it’s worth. Anyway, here we simulate from our priors and summarise() the results. set.seed(5) prior &lt;- tibble(mu_female = rnorm(1e4, mean = 178, sd = 20)) %&gt;% mutate(mu_male = mu_female + rnorm(1e4, mean = 0, sd = 10)) prior %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value), ll = quantile(value, prob = .025), ul = quantile(value, prob = .975)) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 5 ## name mean sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mu_female 178. 20.2 138. 219. ## 2 mu_male 178. 22.5 133. 222. We might visualize the two prior predictive distributions as overlapping densities. prior %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, fill = name, color = name)) + geom_density(size = 2/3, alpha = 2/3) + scale_fill_manual(NULL, values = c(&quot;firebrick4&quot;, &quot;black&quot;)) + scale_color_manual(NULL, values = c(&quot;firebrick4&quot;, &quot;black&quot;)) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;prior predictive distribution for our dummy groups&quot;) + theme_bw() + theme(panel.grid = element_blank(), legend.position = c(.82, .83)) Yep, this parameterization makes \\(\\alpha + \\beta_1\\) more uncertain than \\(\\alpha\\). A nice alternative is to make an index variable. We’ll call it sex, for which 1 = female and 2 = male. “No order is implied. These are just labels” (p. 155). d &lt;- d %&gt;% mutate(sex = ifelse(male == 1, 2, 1)) head(d) ## height weight age male sex ## 1 151.765 47.82561 63 1 2 ## 2 139.700 36.48581 63 0 1 ## 3 136.525 31.86484 65 0 1 ## 4 156.845 53.04191 41 1 2 ## 5 145.415 41.27687 51 0 1 ## 6 163.830 62.99259 35 1 2 We can update our statistical model to include sex with the formula \\[\\begin{align*} \\text{height}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_{\\text{sex}[i]} \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal}(178, 20) &amp; \\text{for } j = 1 \\; \\&amp; \\; 2 \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where now we have rows indexed by \\(i\\) and two levels of sex indexed by \\(j\\). Again, for our version of this model, we will continue using the simple \\(\\lambda = 1\\) exponential prior on \\(\\sigma\\), rather than the uniform. The exponential is just much easier on Stan than the uniform. But if you prefer to go uniform, have at it. One more thing before we fit our model: Notice McElreath’s a[sex] notation in his R code 5.48. I’m not aware that brms will accommodate this notation. The fix is easy. Just save sex as a factor. d &lt;- d %&gt;% mutate(sex = factor(sex)) We’re ready to fit the model. b5.8 &lt;- brm(data = d, family = gaussian, height ~ 0 + sex, prior = c(prior(normal(178, 20), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.08&quot;) Behold the summary. print(b5.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 0 + sex ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sex1 134.85 1.58 131.82 137.92 1.00 4036 3015 ## sex2 142.61 1.71 139.29 145.91 1.00 3244 2615 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 26.79 0.77 25.33 28.38 1.00 3510 2929 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Note that for us, there was no depth=2 argument to get all the model output. When you fit a model with brms that excludes the typical intercept parameter–when you use the 0 + ... syntax–, you’ll get a separate intercept for each of your factor variables. The brm() function noticed there were two levels for our sex factor, and therefore gave use two intercepts: sex1 and sex2. Here’s how you might compute the difference score. library(tidybayes) posterior_samples(b5.8) %&gt;% mutate(diff_fm = b_sex1 - b_sex2) %&gt;% gather(key, value, -`lp__`) %&gt;% group_by(key) %&gt;% mean_qi(value, .width = .89) ## # A tibble: 4 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_sex1 135. 132. 137. 0.89 mean qi ## 2 b_sex2 143. 140. 145. 0.89 mean qi ## 3 diff_fm -7.76 -11.5 -3.89 0.89 mean qi ## 4 sigma 26.8 25.6 28.1 0.89 mean qi Note how we used tidybayes::mean_qi() to summarize our difference variable, diff_fm. Anyway, “this kind of calculation is called a contrast. No matter how many categories you have, you can use samples from the posterior to compute the contrast between any two” (p. 156, emphasis in the original). 5.3.2 Many categories. Binary categories are easy, whether you use an indicator variable or instead an index variable. But when there are more than two categories, the indicator variable approach explodes. You’ll need a new indicator variable for each new category. If you have \\(k\\) unique categories, you need \\(k - 1\\) indicator variables. Automated tools like R’s lm do in fact go this route, constructing \\(k - 1\\) indicator variables for you and returning \\(k - 1\\) parameters (in addition to the intercept). But we’ll instead stick with the index variable approach. It does not change at all when you add more categories. You do get more parameters, of course, just as many as in the indicator variable approach. But the model specification looks just like it does in the binary case. (p. 156) We’ll practice with milk. data(milk, package = &quot;rethinking&quot;) d &lt;- milk rm(milk) With the tidyverse, we can peek at clade with distinct() in the place of base R unique(). d %&gt;% distinct(clade) ## clade ## 1 Strepsirrhine ## 2 New World Monkey ## 3 Old World Monkey ## 4 Ape Rather than make the clade_id index variable, like McElreath did in the text, we’ll just use the clade factor. It will actually work easier within the brms framework. We will, however, standardize the kcal.per.g variable, again. d &lt;- d %&gt;% mutate(kcal.per.g_s = (kcal.per.g - mean(kcal.per.g)) / sd(kcal.per.g)) Our statistical model follows the form \\[\\begin{align*} \\text{kcal.per.g_s}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_{\\text{clade}[i]} \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal}(0, 0.5), &amp; \\text{for } j = 1, \\dots, 4 \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Now fit that model. b5.9 &lt;- brm(data = d, family = gaussian, kcal.per.g_s ~ 0 + clade, prior = c(prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.09&quot;) print(b5.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g_s ~ 0 + clade ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## cladeApe -0.46 0.25 -0.93 0.03 1.00 4699 2968 ## cladeNewWorldMonkey 0.35 0.24 -0.14 0.79 1.00 4923 2929 ## cladeOldWorldMonkey 0.63 0.27 0.07 1.15 1.00 4212 2878 ## cladeStrepsirrhine -0.55 0.29 -1.11 0.02 1.00 4735 2564 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.80 0.12 0.60 1.08 1.00 3960 2649 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Up to this point, all of our coefficient plots have been of a rather complicated type. We tried to mimic McElreath’s coeftab() plots without the aid of the rethinking convenience function. But now the coefficient plot from McElreath’s R code 5.42 is of a much simpler type. We can finally take it easy and use some of the convenience functions available to us within our framework. The mcmc_plot() function is an easy way to get a default coefficient plot. You just put the brms fit object into the function. mcmc_plot(b5.9, pars = &quot;^b_&quot;) There are numerous ways to make a coefficient plot. Another is with the mcmc_intervals() function from the bayesplot package (Gabry et al., 2019; Gabry &amp; Mahr, 2021). A nice feature of the bayesplot package is its convenient way to alter the color scheme with the color_scheme_set() function. Here, for example, we’ll make the theme red. But note how the mcmc_intervals() function requires you to work with the posterior_samples() instead of the brmsfit object. library(bayesplot) color_scheme_set(&quot;red&quot;) post &lt;- posterior_samples(b5.9) post %&gt;% select(starts_with(&quot;b_&quot;)) %&gt;% mcmc_intervals(prob = .5, point_est = &quot;median&quot;) + labs(title = &quot;My fancy bayesplot-based coefficient plot&quot;) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) Because bayesplot returns a ggplot2 object, the plot was adjustable with familiar ggplot2 syntax. For more ideas, check out Gabry’s (2021) vignette, Plotting MCMC draws using the bayesplot package. The tidybayes::stat_pointinterval() function offers a third way, this time with a more ground-up ggplot2 workflow. library(tidybayes) post %&gt;% select(starts_with(&quot;b&quot;)) %&gt;% set_names(distinct(d, clade) %&gt;% arrange(clade) %&gt;% pull()) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = &quot;firebrick4&quot;, alpha = 1/10) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 1, color = &quot;firebrick4&quot;) + labs(title = &quot;My tidybayes-based coefficient plot&quot;, x = &quot;expected kcal (std)&quot;, y = NULL) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) Okay, let’s simulate some “made up categories” (p. 157). We’ll use names rather than numeric indices. houses &lt;- c(&quot;Gryffindor&quot;, &quot;Hufflepuff&quot;, &quot;Ravenclaw&quot;, &quot;Slytherin&quot;) set.seed(63) d &lt;- d %&gt;% mutate(house = sample(rep(houses, each = 8), size = n())) Here we attempt to fit the model with brms using the naïve approach. ️ b5.10 &lt;- brm(data = d, family = gaussian, kcal.per.g_s ~ 0 + clade + house, prior = c(prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.10&quot;) Yep, the parameter summary suggests Slytherin stood out. print(b5.10) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g_s ~ 0 + clade + house ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## cladeApe -0.44 0.26 -0.96 0.09 1.00 3977 3335 ## cladeNewWorldMonkey 0.33 0.26 -0.16 0.84 1.00 4054 3259 ## cladeOldWorldMonkey 0.50 0.29 -0.09 1.06 1.00 4328 2808 ## cladeStrepsirrhine -0.51 0.30 -1.08 0.09 1.00 4519 2982 ## houseHufflepuff -0.16 0.29 -0.71 0.39 1.00 4670 3556 ## houseRavenclaw -0.12 0.27 -0.65 0.41 1.00 3989 3025 ## houseSlytherin 0.49 0.30 -0.09 1.08 1.00 3764 2918 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.77 0.11 0.59 1.02 1.00 3984 2942 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Even though that last model fit well and is a fine and valid way to examine the relevant variables, it’s a bit misleading. If you look closely at the output, you’ll see there are rows for all four of the clade levels, but only rows for three of the four levels of house. This is easy to miss because McElreath didn’t show the precis() output from his m5.10, but if you run and inspect his code for yourself you’ll see it returns all rows for all four of the house variable. What gives? brms syntax is flexible in that it provided multiple ways to fit and post-process models. The formula syntax we use most of the type with brms is called the design formula syntax. In the first edition of his text (2015, pp. 159–161), McElreath contrasted design formula syntax, which is widely used in base R lm(), the lme4 package, and brms, with the more explicit syntax he uses in his rethinking package. I bring this all up because the rethinking package has no problem handling multiple index variables. Just use McElreath’s slick bracket code like he did with his m5.10 (i.e., mu &lt;- a[clade_id] + h[house]). In such a model, there is no default intercept. That is, McElreath’s m5.10 followed the statistical formula of \\[\\begin{align*} \\text{kcal.per.g_s}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_{\\color{#8B1A1A}{\\text{clade}[i]}} + \\alpha_{\\color{#8B1A1A}{\\text{house}[i]}} \\\\ \\alpha_{\\color{#8B1A1A}{\\text{clade}, j}} &amp; \\sim \\operatorname{Normal}(0, 0.5), &amp;&amp; \\color{#8B1A1A}{\\text{for } j = 1, \\dots, 4} \\\\ \\alpha_{\\color{#8B1A1A}{\\text{house}, k}} &amp; \\sim \\operatorname{Normal}(0, 0.5), &amp;&amp; \\color{#8B1A1A}{\\text{for } k = 1, \\dots, 4} \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where there is an \\(\\alpha_\\text{clade}\\) “intercept” for each of the four levels of clade and an \\(\\alpha_\\text{house}\\) “intercept” for each of the four levels of house. But there is no overall intercept, \\(\\alpha\\), that stands for the expected value when all the predictors are set to 0. When we use the typical formula syntax with brms, we can suppress the overall intercept when for a single index variable with the &lt;criterion&gt; ~ 0 + &lt;index variable&gt; syntax. That’s exactly what we did with our b5.9 model. The catch is this approach only works with one index variable within brms. Even though we suppressed the default intercept with our b5.10 formula, kcal.per.g_s ~ 0 + clade + house, we ended up loosing the first category of the second variable, house. Fortunately, we have a fix, which I learned about from one of Bürkner’s answers to Paul Dong’s thread in the Stan Forums, Indexing approach to predictors. The solution is the use the non-linear syntax. If you recall, our first encounter with the brms non-linear syntax was with model b4.3b in Section 4.4.2.1 where we used it to model the exponent of a parameter. Here we’ll use it to model our two index variables while excluding an overall or reference-category intercept. I’m still going to wait until Section 6.2.1 before walking out the non-linear syntax in detail. But if you’re burning with curiosity, check out Bürkner’s (2021e) vignette, Estimating non-linear models with brms. For now, we focus on how to faithfully replicate McElreath’s m510 with brms::brm(). In the code below, we defined our two model parameters, a and h, in the first bf() line, connected each to a index variable in the next two lines, and closed out the bf() statement with nl = TRUE to alert brm() we were using the non-linear syntax. b5.11 &lt;- brm(data = d, family = gaussian, bf(kcal.per.g_s ~ 0 + a + h, a ~ 0 + clade, h ~ 0 + house, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = a), prior(normal(0, 0.5), nlpar = h), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.11&quot;) Here’s the summary of the correct model. print(b5.11) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g_s ~ 0 + a + h ## a ~ 0 + clade ## h ~ 0 + house ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_cladeApe -0.39 0.28 -0.93 0.17 1.00 2770 2860 ## a_cladeNewWorldMonkey 0.36 0.29 -0.22 0.91 1.00 3073 3273 ## a_cladeOldWorldMonkey 0.53 0.32 -0.10 1.15 1.00 3827 2986 ## a_cladeStrepsirrhine -0.46 0.32 -1.07 0.17 1.00 3413 3272 ## h_houseGryffindor -0.10 0.28 -0.65 0.45 1.00 2677 3061 ## h_houseHufflepuff -0.19 0.30 -0.77 0.40 1.00 3111 3259 ## h_houseRavenclaw -0.15 0.29 -0.74 0.41 1.00 3114 3163 ## h_houseSlytherin 0.45 0.32 -0.16 1.07 1.00 3898 3290 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.78 0.12 0.59 1.06 1.00 3643 2636 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Finally, here’s how we might visualize the results in a faceted coefficient plot. posterior_samples(b5.11) %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% mutate(name = str_remove(name, &quot;b_&quot;) %&gt;% str_remove(., &quot;clade&quot;) %&gt;% str_remove(., &quot;house&quot;) %&gt;% str_replace(., &quot;World&quot;, &quot; World &quot;)) %&gt;% separate(name, into = c(&quot;predictor&quot;, &quot;level&quot;), sep = &quot;_&quot;) %&gt;% mutate(predictor = if_else(predictor == &quot;a&quot;, &quot;predictor: clade&quot;, &quot;predictor: house&quot;)) %&gt;% ggplot(aes(x = value, y = reorder(level, value))) + # note how we used `reorder()` to arrange the coefficients geom_vline(xintercept = 0, color = &quot;firebrick4&quot;, alpha = 1/10) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 1, color = &quot;firebrick4&quot;) + labs(x = &quot;expected kcal (std)&quot;, y = NULL) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank(), strip.background = element_blank()) + facet_wrap(~ predictor, scales = &quot;free_y&quot;) Just for fun, we used 89% intervals. 5.3.2.1 Rethinking: Differences and statistical significance. A common error in interpretation of parameter estimates is to suppose that because one parameter is sufficiently far from zero–is “significant”–and another parameter is not–is “not significant”–that the difference between the parameters is also significant. This is not necessarily so. This isn’t just an issue for non-Bayesian analysis: If you want to know the distribution of a difference, then you must compute that difference, a contrast. (p. 158, emphasis in the original) This reminds me of a paper by Gelman and Stern (2006), The difference between “significant” and “not significant” is not itself statistically significant. 5.4 Summary Bonus: We can model categorical variables in more ways than one One of the noteworthy changes in the second edition of Statistical rethinking is McElreath’s consistent use of the index variable approach. It’s a fine approach and I’m glad he emphasized it. But as McElreath alluded to in the early part of Section 5.3.1, we have other modeling options at our disposal. I’d like to briefly walk through three more. 5.4.1 You can use a dummy. The first alternative is the one McElreath mentioned directly at the top of page 154. If you have \\(K\\) categories in a nominal variable, you can depict the categories in the model with \\(K - 1\\) dummy variables. To start simple, consider the case of the \\(K = 2\\) variable sex we used in b5.8. b5.8$data %&gt;% head() ## height sex ## 1 151.765 2 ## 2 139.700 1 ## 3 136.525 1 ## 4 156.845 2 ## 5 145.415 1 ## 6 163.830 2 We can convert our index variable sex into two dummy variables, which we might call male and female. d &lt;- b5.8$data %&gt;% mutate(male = if_else(sex == &quot;1&quot;, 1, 0), female = if_else(sex == &quot;2&quot;, 1, 0)) head(d) ## height sex male female ## 1 151.765 2 0 1 ## 2 139.700 1 1 0 ## 3 136.525 1 1 0 ## 4 156.845 2 0 1 ## 5 145.415 1 1 0 ## 6 163.830 2 0 1 As with the index variable sex, no order is implied by the 0’s and 1’s in our male and female dummies. But remember, we only need \\(K - 1\\) dummies, not both. If we wanted to fit the model using the female dummy variable, we might express the statistical model as \\[\\begin{align*} \\text{height}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{female}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(178, 20) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where \\(\\alpha\\) would be the intercept (i.e., the mean) for males and \\(\\beta_1\\) would be the expected change in the mean for females, relative to males. This parameterization makes males the reference category. Here’s how to fit the model with brms. b5.8b &lt;- brm(data = d, family = gaussian, height ~ 1 + female, prior = c(prior(normal(178, 20), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.08b&quot;) print(b5.8b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + female ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 134.95 1.57 131.95 138.10 1.00 3739 2802 ## female 7.27 2.18 2.88 11.52 1.00 3785 3111 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 26.77 0.77 25.31 28.28 1.00 3689 2990 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’m not going to show it, here, but this approach generalizes to instances where you have 3 or more categories. In the case of the monkey model b5.9, we had a \\(K = 4\\) variable clade, which could have been entered in the model with three dummy variables instead. Whichever of the four levels of clade that is not entered into the model as a dummy, that’s the level that would become the reference category. 5.4.2 Consider contrast coding. Contrast coding offers something of a blend between the index and dummy approach. Consider out new contrast variable, sex_c. d &lt;- d %&gt;% mutate(sex_c = if_else(sex == &quot;1&quot;, -0.5, 0.5)) head(d) ## height sex male female sex_c ## 1 151.765 2 0 1 0.5 ## 2 139.700 1 1 0 -0.5 ## 3 136.525 1 1 0 -0.5 ## 4 156.845 2 0 1 0.5 ## 5 145.415 1 1 0 -0.5 ## 6 163.830 2 0 1 0.5 sex_c == -0.5 for males and sex_c == 0.5 for females. We might express our revised statistical model as \\[\\begin{align*} \\text{height}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{sex_c}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(178, 20) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where \\(\\alpha\\) is now the average of the mean heights of males and females and \\(\\beta_1\\) is the difference when subtracting the mean height of males from the mean height of females. Fit the model. b5.8c &lt;- brm(data = d, family = gaussian, height ~ 1 + sex_c, prior = c(prior(normal(178, 20), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.08c&quot;) Inspect the summary. print(b5.8c) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + sex_c ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 138.59 1.15 136.38 140.85 1.00 3734 2878 ## sex_c 7.27 2.18 2.88 11.52 1.00 3785 3111 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 26.77 0.77 25.31 28.28 1.00 3689 2990 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you take a look, you’ll discover that posterior summary for \\(\\beta_1\\) is basically the same, here, as it was for b5.8b. The intercept \\(\\alpha\\) is a little more complicated. To build a sense of what it means, first consider the simple mean for height, ignoring sex, computed directly from the data. d %&gt;% summarise(mean_height = mean(height)) ## mean_height ## 1 138.2636 Now here is the average of the mean of height when first computed separately by levels of sex. d %&gt;% group_by(sex) %&gt;% summarise(group_mean = mean(height)) %&gt;% summarise(average_of_the_group_means_in_height = mean(group_mean)) ## # A tibble: 1 x 1 ## average_of_the_group_means_in_height ## &lt;dbl&gt; ## 1 138. Our posterior for \\(\\alpha\\), above, is designed to capture the average_of_the_group_means_in_height, not mean_height. In cases where the sample sizes in the two groups were equal, these two would be same. Since we have different numbers of males and females in our data, the two values differ a bit. d %&gt;% count(sex) %&gt;% mutate(percent = 100 * n / sum(n)) ## sex n percent ## 1 1 287 52.75735 ## 2 2 257 47.24265 Anyway, here’s how we might wrangle the posterior samples to make our marginal posteriors for males, females, and their difference. posterior_samples(b5.8c) %&gt;% mutate(male = b_Intercept - b_sex_c * 0.5, female = b_Intercept + b_sex_c * 0.5, `female - male` = b_sex_c) %&gt;% pivot_longer(male:`female - male`) %&gt;% ggplot(aes(x = value, y = 0)) + stat_halfeye(.width = .95, fill = &quot;firebrick4&quot;, normalize = &quot;panels&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;height&quot;) + theme_bw() + theme(panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free&quot;) This approach can also be generalized to contexts where you have three or more groups. I’m not going to show you how to do so, here. It’s a deep rabbit hole and I’m just not interested in shoveling that hard, today. For those who want more, I’ll provide some references at the end. 5.4.3 There’s always the multilevel ANOVA approach. It’s a little absurd I’m covering this, here, because (a) I’m not a fan of the ANOVA framework and (b) McElreath didn’t properly introduce the multilevel framework until Chapter 13. But we’ll throw this in, anyway. For those of you who are new to the multilevel model, make a mental note of this section and then back away slowly and come back after you’ve worked through the later chapters in this text. For those of y’all who have some experience with the multilevel model, perhaps this will offer an application you hadn’t considered. For this approach, let’s consider our \\(K = 4\\) variable clade from back in b5.9. We used clade as an index variable to model four means for standardized kilocalories per gram of milk kcal.per.g_s. d &lt;- b5.9$data head(d) ## kcal.per.g_s clade ## 1 -0.9400408 Strepsirrhine ## 2 -0.8161263 Strepsirrhine ## 3 -1.1259125 Strepsirrhine ## 4 -1.0019980 Strepsirrhine ## 5 -0.2585112 Strepsirrhine ## 6 -1.0639553 New World Monkey Our multilevel ANOVA approach will give us an overall intercept (mean) for kcal.per.g_s as well as four clade-specific deviations from that overall intercept. One way to express this might be \\[\\begin{align*} \\text{kcal.per.g_s} &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\color{#8B1A1A}{u_{j[i]}} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\color{#8B1A1A}{u_j} &amp; \\color{#8B1A1A}\\sim \\color{#8B1A1A}{\\operatorname{Normal}(0, \\sigma_\\text{clade})}, &amp; \\color{#8B1A1A}{\\text{for } j = 1, \\dots, 4} \\\\ \\color{#8B1A1A}{\\sigma_\\text{clade}} &amp; \\color{#8B1A1A}\\sim \\color{#8B1A1A}{\\operatorname{Exponential}(1)}, \\end{align*}\\] where \\(\\alpha\\) is the overall intercept (mean) and the four clade-specific deviations from that mean are captured by the four levels of \\(u_j\\), which are themselves modeled as normally distributed with a mean of zero (because they are deviations, after all) and a standard deviation \\(\\sigma_\\text{clade}\\). Here’s how to fit such a model with brms. b5.9b &lt;- brm(data = d, family = gaussian, kcal.per.g_s ~ 1 + (1 | clade), prior = c(prior(normal(0, 0.5), class = Intercept), prior(exponential(1), class = sigma), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.09b&quot;) Check the results. print(b5.9b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: kcal.per.g_s ~ 1 + (1 | clade) ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~clade (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.82 0.40 0.26 1.82 1.00 1093 1251 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.01 0.32 -0.64 0.65 1.00 1524 1782 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.80 0.12 0.61 1.09 1.00 1970 2134 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The ‘Group-Level Effects:’ section in that output won’t make much sense until we cover the material in Chapter 13. Here’s how one might wrangle the posterior samples to make a coefficient plot similar to the one we made for the original b5.9. posterior_samples(b5.9b) %&gt;% mutate(Ape = b_Intercept + `r_clade[Ape,Intercept]`, `New World Monkey` = b_Intercept + `r_clade[New.World.Monkey,Intercept]`, `Old World Monkey` = b_Intercept + `r_clade[Old.World.Monkey,Intercept]`, Strepsirrhine = b_Intercept + `r_clade[Strepsirrhine,Intercept]`) %&gt;% pivot_longer(Ape:Strepsirrhine) %&gt;% ggplot(aes(x = value, y = reorder(name, value))) + geom_vline(xintercept = 0, color = &quot;firebrick4&quot;, alpha = 1/10) + stat_pointinterval(point_interval = mode_hdi, .width = .89, size = 1, color = &quot;firebrick4&quot;) + labs(title = &quot;My tidybayes-based coefficient plot for b5.9b&quot;, x = &quot;expected kcal (std)&quot;, y = NULL) + theme_bw() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), panel.grid = element_blank()) As we will learn, an advantage of this approach is we get improved estimates due to multilevel partial pooling. Technically, you can apply this approach to cases with as few as \\(K = 2\\) categories. In practice, you’ll probably want to use this only when \\(K \\geq 3\\). 5.4.4 Would you like to learn more? To learn more about the dummy coding approach, McElreath covered it in Chapter 5 of his first (2015) edition, Hayes covered it in Chapter 2 of his (2017) text, and it’s been covered in many other texts, such as the modern classic by Cohen et al. (2013). I have ebook translations for the first of those two (Kurz, 2020a, 2019). If you like learning by video, the great Erin Buchanan has a video or two (e.g., here) on her Statistics of DOOM chanel. The specific type of contrast coding we used is part of a more general approach. You can learn more about that in Cohen et al (2013, Chapter 8) or from this vignette by the good folks at the UCLA Institute for Digital Research and Education (“R Library Contrast Coding Systems for Categorical Variables,” n.d.). In addition to waiting for Chapter 13 in this ebook, you can learn more about the multilevel ANOVA approach in Chapters 19 and 24 of Kruschke’s (2015), in the corresponding chapters in my (2020c) ebook translation of the same, and in some of Gelman’s peer-reviewed work (Gelman, 2005, 2006). Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bayesplot_1.8.0 tidybayes_2.3.1 GGally_2.1.1 dagitty_0.3-1 ## [5] psych_2.0.12 patchwork_1.1.1 ggdag_0.2.3 urbnmapr_0.0.0.9002 ## [9] ggrepel_0.9.1 brms_2.15.0 Rcpp_1.0.6 forcats_0.5.1 ## [13] stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 ## [17] tidyr_1.1.3 tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 igraph_1.2.6 ## [5] svUnit_1.0.3 splines_4.0.4 crosstalk_1.1.0.1 TH.data_1.0-10 ## [9] rstantools_2.1.1 inline_0.3.17 digest_0.6.27 htmltools_0.5.1.1 ## [13] viridis_0.5.1 rethinking_2.13 rsconnect_0.8.16 fansi_0.4.2 ## [17] magrittr_2.0.1 graphlayouts_0.7.1 modelr_0.1.8 RcppParallel_5.0.2 ## [21] matrixStats_0.57.0 xts_0.12.1 sandwich_3.0-0 prettyunits_1.1.1 ## [25] colorspace_2.0-0 rvest_0.3.6 ggdist_2.4.0.9000 haven_2.3.1 ## [29] xfun_0.22 callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 ## [33] lme4_1.1-25 survival_3.2-7 zoo_1.8-8 glue_1.4.2 ## [37] polyclip_1.10-0 gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 ## [41] distributional_0.2.2 pkgbuild_1.2.0 rstan_2.21.2 shape_1.4.5 ## [45] abind_1.4-5 scales_1.1.1 mvtnorm_1.1-1 emo_0.0.0.9000 ## [49] DBI_1.1.0 miniUI_0.1.1.1 viridisLite_0.3.0 xtable_1.8-4 ## [53] HDInterval_0.2.2 tmvnsim_1.0-2 units_0.6-7 stats4_4.0.4 ## [57] StanHeaders_2.21.0-7 DT_0.16 htmlwidgets_1.5.2 httr_1.4.2 ## [61] threejs_0.3.3 arrayhelpers_1.1-0 RColorBrewer_1.1-2 ellipsis_0.3.1 ## [65] reshape_0.8.8 pkgconfig_2.0.3 loo_2.4.1 farver_2.0.3 ## [69] dbplyr_2.0.0 utf8_1.1.4 tidyselect_1.1.0 labeling_0.4.2 ## [73] rlang_0.4.10 reshape2_1.4.4 later_1.1.0.1 munsell_0.5.0 ## [77] cellranger_1.1.0 tools_4.0.4 cli_2.3.1 generics_0.1.0 ## [81] broom_0.7.5 ggridges_0.5.2 evaluate_0.14 fastmap_1.0.1 ## [85] processx_3.4.5 knitr_1.31 fs_1.5.0 tidygraph_1.2.0 ## [89] ggraph_2.0.4 nlme_3.1-152 mime_0.10 projpred_2.0.2 ## [93] xml2_1.3.2 compiler_4.0.4 shinythemes_1.1.2 rstudioapi_0.13 ## [97] curl_4.3 gamm4_0.2-6 e1071_1.7-4 reprex_0.3.0 ## [101] tweenr_1.0.1 statmod_1.4.35 stringi_1.5.3 highr_0.8 ## [105] ps_1.6.0 Brobdingnag_1.2-6 lattice_0.20-41 Matrix_1.3-2 ## [109] classInt_0.4-3 nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 ## [113] vctrs_0.3.6 pillar_1.5.1 lifecycle_1.0.0 bridgesampling_1.0-0 ## [117] estimability_1.3 httpuv_1.5.4 R6_2.5.0 bookdown_0.21 ## [121] promises_1.1.1 KernSmooth_2.23-18 gridExtra_2.3 codetools_0.2-18 ## [125] boot_1.3-26 colourpicker_1.1.0 MASS_7.3-53 gtools_3.8.2 ## [129] assertthat_0.2.1 withr_2.4.1 mnormt_2.0.2 shinystan_2.5.0 ## [133] multcomp_1.4-16 mgcv_1.8-33 parallel_4.0.4 hms_0.5.3 ## [137] grid_4.0.4 coda_0.19-4 class_7.3-18 minqa_1.2.4 ## [141] rmarkdown_2.7 ggforce_0.3.2 sf_0.9-6 shiny_1.5.0 ## [145] lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 "],["the-haunted-dag-the-causal-terror.html", "6 The Haunted DAG &amp; The Causal Terror 6.1 Multicollinearity 6.2 Post-treatment bias 6.3 Collider bias 6.4 Confronting confounding 6.5 Summary [and a little more practice] Session info", " 6 The Haunted DAG &amp; The Causal Terror Read this opening and cry: It seems like the most newsworthy scientific studies are the least trustworthy. The more likely it is to kill you, if true, the less likely it is to be true. The more boring the topic, the more rigorous the results. How could this widely believed negative correlation exist? There doesn’t seem to be any reason for studies of topics that people care about to produce less reliable results. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey? Actually all that is necessary for such a negative correlation to arise is that peer reviewers care about both newsworthiness and trustworthiness. Whether it is grant review or journal review, if editors and reviewers care about both, then the act of selection itself is enough to make the most newsworthy studies the least trustworthy…. Strong selection induces a negative correlation among the criteria used in selection. Why? If the only way to cross the threshold is to score high, it is more common to score high on one item than on both. Therefore among funded proposals, the most newsworthy studies can actually have less than average trustworthiness (less than 0 in the figure). Similarly the most trustworthy studies can be less newsworthy than average. Berkson’s paradox. But it is easier to remember if we call it the selection-distortion effect. Once you appreciate this effect, you’ll see it everywhere…. The selection-distortion effect can happen inside of a multiple regression, because the act of adding a predictor induces statistical selection within the model, a phenomenon that goes by the unhelpful name collider bias. This can mislead us into believing, for example, that there is a negative association between newsworthiness and trustworthiness in general, when in fact it is just a consequence of conditioning on some variable. This is both a deeply confusing fact and one that is important to understand in order to regress responsibly. This chapter and the next are both about terrible things that can happen when we simply add variables to a regression, without a clear idea of a causal model. (McElreath, 2020a, pp. 161–162, emphasis in the original) 6.0.0.1 Overthinking: Simulated science distortion. First let’s run the simulation. library(tidyverse) set.seed(1914) n &lt;- 200 # number of grant proposals p &lt;- 0.1 # proportion to select d &lt;- # uncorrelated newsworthiness and trustworthiness tibble(newsworthiness = rnorm(n, mean = 0, sd = 1), trustworthiness = rnorm(n, mean = 0, sd = 1)) %&gt;% # total_score mutate(total_score = newsworthiness + trustworthiness) %&gt;% # select top 10% of combined scores mutate(selected = ifelse(total_score &gt;= quantile(total_score, 1 - p), TRUE, FALSE)) head(d) ## # A tibble: 6 x 4 ## newsworthiness trustworthiness total_score selected ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 -0.379 -1.20 -1.58 FALSE ## 2 0.130 0.504 0.634 FALSE ## 3 0.334 0.532 0.866 FALSE ## 4 -1.89 -0.594 -2.48 FALSE ## 5 2.05 0.0672 2.12 TRUE ## 6 2.54 1.02 3.56 TRUE Here’s the correlation among those cases for which selected == TRUE. d %&gt;% filter(selected == TRUE) %&gt;% select(newsworthiness, trustworthiness) %&gt;% cor() ## newsworthiness trustworthiness ## newsworthiness 1.0000000 -0.7680083 ## trustworthiness -0.7680083 1.0000000 For the plots in this chapter, we’ll take some aesthetic cues from Aki Vehtari’s great GitHub repo, Bayesian Data Analysis R Demos. theme_set(theme_minimal()) Okay, let’s make Figure 6.1. # we&#39;ll need this for the annotation text &lt;- tibble(newsworthiness = c(2, 1), trustworthiness = c(2.25, -2.5), selected = c(TRUE, FALSE), label = c(&quot;selected&quot;, &quot;rejected&quot;)) d %&gt;% ggplot(aes(x = newsworthiness, y = trustworthiness, color = selected)) + geom_point(aes(shape = selected), alpha = 3/4) + geom_text(data = text, aes(label = label)) + geom_smooth(data = . %&gt;% filter(selected == TRUE), method = &quot;lm&quot;, fullrange = T, color = &quot;lightblue&quot;, se = F, size = 1/2) + scale_color_manual(values = c(&quot;black&quot;, &quot;lightblue&quot;)) + scale_shape_manual(values = c(1, 19)) + scale_x_continuous(limits = c(-3, 3.9), expand = c(0, 0)) + coord_cartesian(ylim = range(d$trustworthiness)) + theme(legend.position = &quot;none&quot;) Why might “the most newsworthy studies might be the least trustworthy?” The selection-distortion effect. 6.1 Multicollinearity Multicollinearity means a very strong association between two or more predictor variables. The raw correlation isn’t what matters. Rather what matters is the association, conditional on the other variables in the model. The consequence of multicollinearity is that the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome, even if all of the variables are in reality strongly associated with the outcome. This frustrating phenomenon arises from the details of how multiple regression works. In fact, there is nothing wrong with multicollinearity. The model will work fine for prediction. You will just be frustrated trying to understand it. The hope is that once you understand multicollinearity, you will better understand regression models in general. (p. 163, emphasis added) For more on this topic, check out Jan VanHove’s interesting blog post, Collinearity isn’t a disease that needs curing. 6.1.1 Multicollinear legs. Let’s simulate some leg data. n &lt;- 100 set.seed(909) d &lt;- tibble(height = rnorm(n, mean = 10, sd = 2), leg_prop = runif(n, min = 0.4, max = 0.5)) %&gt;% mutate(leg_left = leg_prop * height + rnorm(n, mean = 0, sd = 0.02), leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02)) As you might expect in real-life data, the leg_left and leg_right columns are highly correlated. d %&gt;% select(leg_left:leg_right) %&gt;% cor() %&gt;% round(digits = 4) ## leg_left leg_right ## leg_left 1.0000 0.9997 ## leg_right 0.9997 1.0000 Have you ever even seen a \\(\\rho = .9997\\) correlation, before? Here it is in a plot. d %&gt;% ggplot(aes(x = leg_left, y = leg_right)) + geom_point(alpha = 1/2, color = &quot;forestgreen&quot;) Load brms. library(brms) Here’s our attempt to predict height with both legs. b6.1 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left + leg_right, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.01&quot;) Let’s inspect the damage. print(b6.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left + leg_right ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.97 0.30 0.37 1.55 1.00 3783 2879 ## leg_left 0.20 2.59 -4.98 5.14 1.01 1692 1804 ## leg_right 1.79 2.60 -3.17 6.95 1.01 1694 1786 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.63 0.05 0.55 0.73 1.00 2347 1805 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That ‘Est.Error’ column isn’t looking too good. But it’s easy to miss that, which is why McElreath suggested “a graphical view of the [output] is more useful because it displays the posterior means and [intervals] in a way that allows us with a glance to see that something has gone wrong here” (p. 164). Here’s our coefficient plot using brms::mcmc_plot() with a little help from bayesplot::color_scheme_set(). library(bayesplot) color_scheme_set(&quot;orange&quot;) mcmc_plot(b6.1, type = &quot;intervals&quot;, prob = .5, prob_outer = .95, point_est = &quot;mean&quot;) + labs(title = &quot;The coefficient plot for the two-leg model&quot;, subtitle = &quot;Holy smokes; look at the widths of those betas!&quot;) + theme(axis.text.y = element_text(hjust = 0), panel.grid.minor = element_blank(), strip.text = element_text(hjust = 0)) Now you can use the brms::mcmc_plot() function without explicitly loading the bayesplot package. But loading bayesplot allows you to set the color scheme with color_scheme_set(). In the middle of page 164, McElreath suggested we might try this again with different seeds. This might be a good place to introduce iteration. Our first step will be to make a custom function that will simulate new data of the same form as above and then immediately fit a model based on b6.1 to those new data. To speed up the process, we’ll use the update() function to avoid recompiling the model. Our custom function, sim_and_fit(), will take two arguments. Theseedargument will allow us to keep the results reproducible by setting a seed for the data simulation. Then` argument will allow us, should we wish, to change the sample size. sim_and_fit &lt;- function(seed, n = 100) { # set up the parameters n &lt;- n set.seed(seed) # simulate the new data d &lt;- tibble(height = rnorm(n, mean = 10, sd = 2), leg_prop = runif(n, min = 0.4, max = 0.5)) %&gt;% mutate(leg_left = leg_prop * height + rnorm(n, mean = 0, sd = 0.02), leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02)) # update b6.1 to the new data fit &lt;- update(b6.1, newdata = d, seed = 6) } Now use sim_and_fit() to make our simulations which correspond to seed values 1:4. By nesting the seed values and the sim_and_fit() function within purrr::map(), the results will be saved within our tibble, sim. sim &lt;- tibble(seed = 1:4) %&gt;% mutate(post = map(seed, ~sim_and_fit(.) %&gt;% posterior_samples())) Take a look at what we did. head(sim) ## # A tibble: 4 x 2 ## seed post ## &lt;int&gt; &lt;list&gt; ## 1 1 &lt;df[,5] [4,000 × 5]&gt; ## 2 2 &lt;df[,5] [4,000 × 5]&gt; ## 3 3 &lt;df[,5] [4,000 × 5]&gt; ## 4 4 &lt;df[,5] [4,000 × 5]&gt; We have a tibble with four rows and two columns. Hopefully it’s unsurprising the first column shows our four seed values. The second column, post, might look odd. If you look back up at the code we used to make sim, you’ll notice we fed the results from sim_and_fit() into the brms::posterior_samples() function. Each model object contained 4,000 poster draws. There are four parameters in the model and, by brms default, the posterior_samples() function also returns the lp__, which we generally ignore in this project. Anyway, that’s why you see each of the four rows of post containing &lt;data.frame [4,000 × 5]&gt;. Each cell contains an entire \\(4,000 × 5\\) data frame–the results from posterior_samples(). When you have data frames nested within data frames, this is called a nested data structure. To unpack the contents of those four nested data frames, you simply call nest(d, post). In the next code block, we’ll do that within the context of a larger work low designed to plot the results of each in a faceted coefficient plot. For data of this kind of structure, the tidybayes::stat_pointinterval() function will be particularly useful. library(tidybayes) sim %&gt;% unnest(post) %&gt;% pivot_longer(b_Intercept:sigma) %&gt;% mutate(seed = str_c(&quot;seed &quot;, seed)) %&gt;% ggplot(aes(x = value, y = name)) + stat_pointinterval(.width = .95, color = &quot;forestgreen&quot;) + labs(x = &quot;posterior&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), panel.border = element_rect(color = &quot;black&quot;, fill = &quot;transparent&quot;), panel.grid.minor = element_blank(), strip.text = element_text(hjust = 0)) + facet_wrap(~ seed, ncol = 1) Though the results varied across iterations, the overall pattern was massive uncertainty in the two \\(\\beta\\) parameters. You may be wondering, Did the posterior approximation work correctly? It did work correctly, and the posterior distribution here is the right answer to the question we asked. The problem is the question. Recall that a multiple linear regression answers the question: What is the value of knowing each predictor, after already knowing all of the other predictors? So in this case, the question becomes: What is the value of knowing each leg’s length, after already knowing the other leg’s length? The answer to this weird question is equally weird, but perfectly logical. (p. 164, emphasis in the original) To further answer this weird question, we might start making the panels from Figure 6.2. Starting with the left panel, this is perhaps the simplest way to plot the bivariate posterior of our two predictor coefficients. pairs(b6.1, pars = parnames(b6.1)[2:3]) If you’d like a nicer and more focused attempt, you might have to revert to the posterior_samples() function and a little ggplot2 code. post &lt;- posterior_samples(b6.1) post %&gt;% ggplot(aes(x = b_leg_left, y = b_leg_right)) + geom_point(color = &quot;forestgreen&quot;, alpha = 1/10, size = 1/2) While we’re at it, you can make a similar plot with the mcmc_scatter() function (see Gabry, 2021, Plotting MCMC draws using the bayesplot package). color_scheme_set(&quot;green&quot;) post %&gt;% mcmc_scatter(pars = c(&quot;b_leg_left&quot;, &quot;b_leg_right&quot;), size = 1/2, alpha = 1/10) But wow, those coefficients look about as highly correlated as the predictors, just with the reversed sign. post %&gt;% select(b_leg_left:b_leg_right) %&gt;% cor() ## b_leg_left b_leg_right ## b_leg_left 1.0000000 -0.9996948 ## b_leg_right -0.9996948 1.0000000 On page 165, McElreath clarified that from the perspective of brms, this model may as well be \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + (\\beta_1 + \\beta_2) x_i. \\end{align*}\\] Accordingly, here’s the posterior of the sum of the two regression coefficients, Figure 6.2.b. We’ll use tidybayes::stat_halfeye() to both plot the density and mark off the posterior median and percentile-based 95% probability intervals at its base. post %&gt;% ggplot(aes(x = b_leg_left + b_leg_right, y = 0)) + stat_halfeye(point_interval = median_qi, fill = &quot;steelblue&quot;, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Sum the multicollinear coefficients&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;) Now we fit the revised model after ditching one of the leg lengths. b6.2 &lt;- brm(data = d, family = gaussian, height ~ 1 + leg_left, prior = c(prior(normal(10, 100), class = Intercept), prior(normal(2, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.02&quot;) print(b6.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: height ~ 1 + leg_left ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.01 0.29 0.43 1.58 1.00 3998 2753 ## leg_left 1.99 0.06 1.87 2.11 1.00 4009 3011 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.63 0.04 0.55 0.73 1.00 3402 2899 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). That posterior \\(SD\\) for leg_left looks much better. Compare this density to the one in Figure 6.2.b. posterior_samples(b6.2) %&gt;% ggplot(aes(x = b_leg_left, y = 0)) + stat_halfeye(point_interval = median_qi, fill = &quot;steelblue&quot;, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Just one coefficient needed&quot;, subtitle = &quot;Marked by the median and 95% PIs&quot;, x = &quot;only b_leg_left, this time&quot;) The basic lesson is only this: When two predictor variables are very strongly correlated (conditional on other variables in the model), including both in a model may lead to confusion. The posterior distribution isn’t wrong, in such cases. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. And if you are just interested in prediction, you’ll find that this leg model makes fine predictions. It just doesn’t make any claims about which leg is more important. (p. 166) 6.1.2 Multicollinear milk. Multicollinearity arises in real data, too. Load the milk data. data(milk, package = &quot;rethinking&quot;) d &lt;- milk rm(milk) Now use the handy rethinking::standardize() function to standardize our focal variables. d &lt;- d %&gt;% mutate(k = rethinking::standardize(kcal.per.g), f = rethinking::standardize(perc.fat), l = rethinking::standardize(perc.lactose)) We’ll follow the text and fit the two univariable models, first. Note our use of the update() function. # k regressed on f b6.3 &lt;- brm(data = d, family = gaussian, k ~ 1 + f, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.03&quot;) # k regressed on l b6.4 &lt;- update(b6.3, newdata = d, formula = k ~ 1 + l, seed = 6, file = &quot;fits/b06.04&quot;) posterior_summary(b6.3) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.00 0.08 -0.17 0.17 ## b_f 0.86 0.09 0.67 1.03 ## sigma 0.49 0.07 0.38 0.64 ## lp__ -22.08 1.23 -25.20 -20.67 posterior_summary(b6.4) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.00 0.07 -0.14 0.15 ## b_l -0.90 0.08 -1.05 -0.74 ## sigma 0.41 0.06 0.32 0.54 ## lp__ -17.37 1.24 -20.53 -15.92 Now “watch what happens when we place both predictor variables in the same regression model” (p. 167). b6.5 &lt;- update(b6.4, newdata = d, formula = k ~ 1 + f + l, seed = 6, file = &quot;fits/b06.05&quot;) Now the \\(\\beta\\)’s are smaller and less certain. posterior_summary(b6.5) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.00 0.07 -0.14 0.13 ## b_f 0.25 0.19 -0.13 0.63 ## b_l -0.67 0.19 -1.04 -0.30 ## sigma 0.41 0.06 0.31 0.55 ## lp__ -17.23 1.44 -21.00 -15.45 Here’s a quick paris() plot. d %&gt;% select(kcal.per.g, perc.fat, perc.lactose) %&gt;% pairs(col = &quot;forestgreen&quot;) Now here’s the custom GGally version. library(GGally) # define a couple custom functions my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(fill = &quot;steelblue&quot;, color = &quot;black&quot;) } my_lower &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_smooth(method = &quot;lm&quot;, color = &quot;orange&quot;, se = F) + geom_point(alpha = .8, size = 1/4, color = &quot;blue&quot;) } # plug those custom functions into `ggpairs()` ggpairs(data = d, columns = c(3:4, 6), upper = list(continuous = wrap(&quot;cor&quot;, family = &quot;sans&quot;, color = &quot;black&quot;)), diag = list(continuous = my_diag), lower = list(continuous = my_lower)) Our two predictor “variables are negatively correlated, and so strongly so that they are nearly redundant. Either helps in predicting kcal.per.g, but neither helps much once you already know the other” (p. 169, emphasis in the original). You can really see that on the lower two scatter plots. You’ll note the ggpairs() plot also returned the Pearson’s correlation coefficients with their \\(p\\)-value stars (\\(^\\star\\)). Later on in the ebook, we’ll practice setting custom correlation functions which avoid displaying that dirty \\(p\\)-value stuff. Anyway, making a DAG might help us make sense of this. library(ggdag) dag_coords &lt;- tibble(name = c(&quot;L&quot;, &quot;D&quot;, &quot;F&quot;, &quot;K&quot;), x = c(1, 2, 3, 2), y = c(2, 2, 2, 1)) dagify(L ~ D, F ~ D, K ~ L + F, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;D&quot;), alpha = 1/2, size = 6.5, show.legend = F) + geom_point(x = 2, y = 2, size = 6.5, shape = 1, stroke = 1, color = &quot;orange&quot;) + geom_dag_text(color = &quot;black&quot;) + geom_dag_edges() + scale_color_manual(values = c(&quot;steelblue&quot;, &quot;orange&quot;)) + scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) + scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1)) The central tradeoff decides how dense, \\(D\\), the milk needs to be. We haven’t observed this variable, so it’s shown circled. Then fat, \\(F\\), and lactose, \\(L\\), are determined. Finally, the composition of \\(F\\) and \\(L\\) determines the kilocalories, \\(K\\). If we could measure \\(D\\), or had an evolutionary and economic model to predict it based upon other aspects of a species, that would be better than stumbling through regressions. (p. 167) 6.1.2.1 Rethinking: Identification guaranteed; comprehension up to you. If you come from a frequentist background, this may jar you, but technically speaking, identifiability is not a concern for Bayesian models. The reason is that as long as the posterior distribution is proper–which just means that it integrates to 1–then all of the parameters are identified. But this technical fact doesn’t also mean that you can make sense of the posterior distribution. So it’s probably better to speak of weakly identified parameters in a Bayesian context. (pp. 169–170, emphasis in the original) 6.1.2.2 Overthinking: Simulating collinearity. First we’ll get the data and define the functions. You’ll note I’ve defined my sim_coll() a little differently from sim.coll() in the text. I’ve omitted rep.sim.coll() as an independent function altogether, but computed similar summary information with the summarise() code at the bottom of the block. # define a custom function sim_coll &lt;- function(seed, rho) { # simulate the data set.seed(seed) d &lt;- d %&gt;% mutate(x = rnorm(n(), mean = perc.fat * rho, sd = sqrt((1 - rho^2) * var(perc.fat)))) # fit an OLS model m &lt;- lm(kcal.per.g ~ perc.fat + x, data = d) # extract the parameter SD sqrt(diag(vcov(m)))[2] } # how many simulations per `rho`-value would you like? n_seed &lt;- 100 # how many `rho`-values from 0 to .99 would you like to evaluate the process over? n_rho &lt;- 30 d &lt;- crossing(seed = 1:n_seed, rho = seq(from = 0, to = .99, length.out = n_rho)) %&gt;% mutate(parameter_sd = purrr::map2_dbl(seed, rho, sim_coll)) %&gt;% group_by(rho) %&gt;% # we&#39;ll `summarise()` our output by the mean and 95% intervals summarise(mean = mean(parameter_sd), ll = quantile(parameter_sd, prob = .025), ul = quantile(parameter_sd, prob = .975)) We’ve added 95% interval bands to our version of Figure 5.10. d %&gt;% ggplot(aes(x = rho, y = mean)) + geom_smooth(aes(ymin = ll, ymax = ul), stat = &quot;identity&quot;, fill = &quot;orange&quot;, color = &quot;orange&quot;, alpha = 1/3, size = 2/3) + labs(x = expression(rho), y = &quot;parameter SD&quot;) + coord_cartesian(ylim = c(0, .0072)) 6.2 Post-treatment bias It helped me understand the next example by mapping out the sequence of events McElreath described in the second paragraph: seed and sprout plants measure heights apply different antifungal soil treatments (i.e., the experimental manipulation) measure (a) the heights and (b) the presence of fungus Based on the design, let’s simulate our data. # how many plants would you like? n &lt;- 100 set.seed(71) d &lt;- tibble(h0 = rnorm(n, mean = 10, sd = 2), treatment = rep(0:1, each = n / 2), fungus = rbinom(n, size = 1, prob = .5 - treatment * 0.4), h1 = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1)) We’ll use head() to peek at the data. d %&gt;% head() ## # A tibble: 6 x 4 ## h0 treatment fungus h1 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9.14 0 0 14.3 ## 2 9.11 0 0 15.6 ## 3 9.04 0 0 14.4 ## 4 10.8 0 0 15.8 ## 5 9.16 0 1 11.5 ## 6 7.63 0 0 11.1 And here’s a quick summary with tidybayes::mean_qi(). d %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mean_qi(.width = .89) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 4 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 fungus 0.23 0 1 0.89 mean qi ## 2 h0 9.96 6.57 13.1 0.89 mean qi ## 3 h1 14.4 10.6 17.9 0.89 mean qi ## 4 treatment 0.5 0 1 0.89 mean qi If you want to make those cute mini histograms, go back and check out our custom histospark() code from Chapter 4. 6.2.1 A prior is born. Let’s take a look at the \\(p \\sim \\operatorname{Log-Normal}(0, 0.25)\\) prior distribution. set.seed(6) # simulate sim_p &lt;- tibble(sim_p = rlnorm(1e4, meanlog = 0, sdlog = 0.25)) # wrangle sim_p %&gt;% mutate(`exp(sim_p)` = exp(sim_p)) %&gt;% gather() %&gt;% # plot ggplot(aes(x = value)) + geom_density(fill = &quot;steelblue&quot;) + scale_x_continuous(breaks = c(0, .5, 1, 1.5, 2, 3, 5)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 6)) + theme(panel.grid.minor.x = element_blank()) + facet_wrap(~ key, scale = &quot;free_y&quot;, ncol = 1) Summarize. sim_p %&gt;% mutate(`exp(sim_p)` = exp(sim_p)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mean_qi(.width = .89) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 exp(sim_p) 2.92 1.96 4.49 0.89 mean qi ## 2 sim_p 1.03 0.67 1.5 0.89 mean qi “This prior expects anything from 40% shrinkage up to 50% growth” (p. 172). So then, our initial statistical model will follow the form \\[\\begin{align*} h_{1i} &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = h_{0i} \\times p \\\\ p &amp; \\sim \\operatorname{Log-Normal}(0, 0.25) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Let’s fit that model. b6.6 &lt;- brm(data = d, family = gaussian, h1 ~ 0 + h0, prior = c(prior(lognormal(0, 0.25), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.06&quot;) Behold the summary. print(b6.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ 0 + h0 ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## h0 1.43 0.02 1.39 1.46 1.00 3136 2484 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.83 0.13 1.60 2.10 1.00 3161 2412 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). So then, the expectation is an increase of about 43 percent relative to \\(h_0\\). But this isn’t the best model. We’re leaving important predictors on the table. Our updated model follows the form \\[\\begin{align*} h_{1i} &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = h_{0,i} \\times p \\\\ p &amp; = \\alpha + \\beta_1 \\text{treatment}_i + \\beta_2 \\text{fungus}_i \\\\ \\alpha &amp; \\sim \\operatorname{Log-Normal}(0, 0.25) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] That is, the “proportion of growth \\(p\\) is now a function of the predictor variables” (p. 172). Although we will fit the equivalent of McElreath’s model in brms, I’m not aware that we can translate it directly into conventional brms syntax. But take a look at the critical two lines from above: \\[\\begin{align*} \\mu_i &amp; = h_{0,i} \\times p \\\\ p &amp; = \\alpha + \\beta_1 \\text{treatment}_i + \\beta_2 \\text{fungus}_i. \\\\ \\end{align*}\\] With just a little algebra, we can re-express that as \\[\\mu_i = h_{0i} \\times (\\alpha + \\beta_1 \\text{treatment}_i + \\beta_2 \\text{fungus}_i).\\] With brms, we fit models like that using the non-linear syntax (Bürkner, 2021e), which we briefly introduced in Section 4.4.2.1 and Section 5.3.2. Yes friends, it’s now time we discuss the non-linear brms syntax in detail. Bur first, here’s what it looks like for b6.7. b6.7 &lt;- brm(data = d, family = gaussian, bf(h1 ~ h0 * (a + t * treatment + f * fungus), a + t + f ~ 1, nl = TRUE), prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0), prior(normal(0, 0.5), nlpar = t), prior(normal(0, 0.5), nlpar = f), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.07&quot;) To explain what’s going on with our formulasyntax, it’s probably best to quote Bürkner’s (2021e) vignette at length: When looking at the above code, the first thing that becomes obvious is that we changed the formula syntax to display the non-linear formula including predictors (i.e., [h0, treatment, and fungus]) and parameters (i.e., [a, t, and f]) wrapped in a call to [the bf() function]. This stands in contrast to classical R formulas, where only predictors are given and parameters are implicit. The argument [a + t + f ~ 1] serves two purposes. First, it provides information, which variables in formula are parameters, and second, it specifies the linear predictor terms for each parameter. In fact, we should think of non-linear parameters as placeholders for linear predictor terms rather than as parameters themselves (see also the following examples). In the present case, we have no further variables to predict [a, t, and f] and thus we just fit intercepts that represent our estimates of [\\(\\alpha\\), \\(t\\), and \\(f\\)]. The formula [a + t + f ~ 1] is a short form of [a ~ 1, t ~ 1, f ~ 1] that can be used if multiple non-linear parameters share the same formula. Setting nl = TRUE tells brms that the formula should be treated as non-linear. In contrast to generalized linear models, priors on population-level parameters (i.e., ‘fixed effects’) are often mandatory to identify a non-linear model. Thus, brms requires the user to explicitly specify these priors. In the present example, we used a [lognormal(0, 0.2) prior on (the population-level intercept of) a, while we used a normal(0, 0.5) prior on both (population-level intercepts of) t and f]. Setting priors is a non-trivial task in all kinds of models, especially in non-linear models, so you should always invest some time to think of appropriate priors. Quite often, you may be forced to change your priors after fitting a non-linear model for the first time, when you observe different MCMC chains converging to different posterior regions. This is a clear sign of an identification problem and one solution is to set stronger (i.e., more narrow) priors. (emphasis in the original) Let’s see what we’ve done. print(b6.7) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ h0 * (a + t * treatment + f * fungus) ## a ~ 1 ## t ~ 1 ## f ~ 1 ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_Intercept 1.48 0.03 1.43 1.53 1.00 1565 1952 ## t_Intercept 0.00 0.03 -0.06 0.06 1.00 1669 1940 ## f_Intercept -0.27 0.04 -0.34 -0.19 1.00 1923 2305 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.45 0.10 1.26 1.67 1.00 3162 2588 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). All in all, it looks like we did a good job matching up McElreath’s results. The posterior doesn’t, however, match up well with the way we generated the data… 6.2.2 Blocked by consequence. To measure the treatment effect properly, we should omit fungus from the model. This leaves us with the equation \\[\\begin{align*} h_{1i} &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = h_{0i} \\times (\\alpha + \\beta_1 \\text{treatment}_i) \\\\ \\alpha &amp; \\sim \\operatorname{Log-Normal}(0, 0.25) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Fit this model with the non-linear syntax, too. b6.8 &lt;- brm(data = d, family = gaussian, bf(h1 ~ h0 * (a + t * treatment), a + t ~ 1, nl = TRUE), prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0), prior(normal(0, 0.5), nlpar = t), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.08&quot;) Did we do better? print(b6.8) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: h1 ~ h0 * (a + t * treatment) ## a ~ 1 ## t ~ 1 ## Data: d (Number of observations: 100) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_Intercept 1.38 0.03 1.33 1.43 1.00 2197 2079 ## t_Intercept 0.09 0.04 0.02 0.15 1.00 2273 2062 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.79 0.13 1.56 2.04 1.00 2466 2548 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Yes, now we have a positive treatment effect. 6.2.3 Fungus and \\(d\\)-separation. Let’s make a DAG. # define our coordinates dag_coords &lt;- tibble(name = c(&quot;H0&quot;, &quot;T&quot;, &quot;F&quot;, &quot;H1&quot;), x = c(1, 5, 4, 3), y = c(2, 2, 1.5, 1)) # save our DAG dag &lt;- dagify(F ~ T, H1 ~ H0 + F, coords = dag_coords) # plot dag %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;steelblue&quot;, alpha = 1/2, size = 6.5) + geom_dag_text(color = &quot;black&quot;) + geom_dag_edges() + theme_dag() We’ll be making a lot of simple DAGs following this format over this chapter. To streamline out plotting code, let’s make a custom plotting function. I’ll call it gg_simple_dag(). gg_simple_dag &lt;- function(d) { d %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(color = &quot;steelblue&quot;, alpha = 1/2, size = 6.5) + geom_dag_text(color = &quot;black&quot;) + geom_dag_edges() + theme_dag() } # try it out! dag %&gt;% gg_simple_dag() Anyway, the DAG clarifies that learning the treatment tells us nothing about the outcome, once we know the fungus status. An even more DAG way to say this is that conditioning on \\(F\\) induces d-separation. The “d” stands for directional. D-separation means that some variables on a directed graph are independent of others. There is no path connecting them. In this case, \\(H_1\\) is d-separated from \\(T\\), but only when we condition on \\(F\\). Conditioning on \\(F\\) effectively blocks the directed path \\(T \\rightarrow F \\rightarrow H_1\\), making \\(T\\) and \\(H_1\\) independent (d-separated). (p. 174, emphasis in the original) Note that our ggdag object, dag, will also work with the dagitty::dseparated() function. library(dagitty) dag %&gt;% dseparated(&quot;T&quot;, &quot;H1&quot;) ## [1] FALSE dag %&gt;% dseparated(&quot;T&quot;, &quot;H1&quot;, &quot;F&quot;) ## [1] TRUE The descriptively-named dagitty::mpliedConditionalIndependencies() function will work, too. impliedConditionalIndependencies(dag) ## F _||_ H0 ## H0 _||_ T ## H1 _||_ T | F Now consider a DAG of a different kind of causal structure. # define our coordinates dag_coords &lt;- tibble(name = c(&quot;H0&quot;, &quot;H1&quot;, &quot;M&quot;, &quot;F&quot;, &quot;T&quot;), x = c(1, 2, 2.5, 3, 4), y = c(2, 2, 1, 2, 2)) # save our DAG dag &lt;- dagify(F ~ M + T, H1 ~ H0 + M, coords = dag_coords) # plot dag %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;M&quot;), alpha = 1/2, size = 6.5, show.legend = F) + geom_point(x = 2.5, y = 1, size = 6.5, shape = 1, stroke = 1, color = &quot;orange&quot;) + geom_dag_text(color = &quot;black&quot;) + geom_dag_edges() + scale_color_manual(values = c(&quot;steelblue&quot;, &quot;orange&quot;)) + theme_dag() Our custom gg_simple_dag() was a little too brittle to accommodate DAGs that mark of unobserved variables. Since we’ll be making a few more DAGs of this kind, we’ll make one more custom plotting function. We’ll call this one gg_fancy_dag(). gg_fancy_dag &lt;- function(d, x = 1, y = 1, circle = &quot;U&quot;) { d %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == circle), alpha = 1/2, size = 6.5, show.legend = F) + geom_point(x = x, y = y, size = 6.5, shape = 1, stroke = 1, color = &quot;orange&quot;) + geom_dag_text(color = &quot;black&quot;) + geom_dag_edges() + scale_color_manual(values = c(&quot;steelblue&quot;, &quot;orange&quot;)) + theme_dag() } # check it out dag %&gt;% gg_fancy_dag(x = 2.5, y = 1, circle = &quot;M&quot;) Based on McElreath’s R code 6.20, here we simulate some data based on the new DAG. set.seed(71) n &lt;- 1000 d2 &lt;- tibble(h0 = rnorm(n, mean = 10, sd = 2), treatment = rep(0:1, each = n / 2), m = rbinom(n, size = 1, prob = .5), fungus = rbinom(n, size = 1, prob = .5 - treatment * 0.4 + 0.4 * m), h1 = h0 + rnorm(n, mean = 5 + 3 * m, sd = 1)) head(d2) ## # A tibble: 6 x 5 ## h0 treatment m fungus h1 ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 9.14 0 0 0 14.8 ## 2 9.11 0 0 0 15.3 ## 3 9.04 0 1 1 16.4 ## 4 10.8 0 1 1 19.1 ## 5 9.16 0 1 1 17.2 ## 6 7.63 0 0 0 13.4 Use update() to refit b6.7 and b6.8 to the new data. b6.7b &lt;- update(b6.7, newdata = d2, seed = 6, file = &quot;fits/b06.07b&quot;) b6.8b &lt;- update(b6.8, newdata = d2, seed = 6, file = &quot;fits/b06.08b&quot;) Check the results. posterior_summary(b6.7b) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_a_Intercept 1.52 0.01 1.49 1.55 ## b_t_Intercept 0.05 0.01 0.02 0.08 ## b_f_Intercept 0.14 0.01 0.12 0.17 ## sigma 2.11 0.05 2.02 2.20 ## lp__ -2168.53 1.39 -2172.03 -2166.77 posterior_summary(b6.8b) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_a_Intercept 1.62 0.01 1.60 1.64 ## b_t_Intercept -0.01 0.01 -0.04 0.02 ## sigma 2.21 0.05 2.12 2.31 ## lp__ -2216.20 1.21 -2219.39 -2214.81 “Including fungus again confounds inference about the treatment, this time by making it seem like it helped the plants, even though it had no effect” (p. 175). 6.2.3.1 Rethinking: Model selection doesn’t help. In the next chapter, you’ll learn about model selection using information criteria. Like other model comparison and selection schemes, these criteria help in contrasting and choosing model structure. But such approaches are no help in the example presented just above, since the model that includes fungus both fits the sample better and would make better out-of-sample predictions. Model [b6.7] misleads because it asks the wrong question, not because it would make poor predictions. As argued in Chapter 1, prediction and causal inference are just not the same task. No statistical procedure can substitute for scientific knowledge and attention to it. We need multiple models because they help us understand causal paths, not just so we can choose one or another for prediction. (p. 175) Brutal. 6.3 Collider bias Make the collider bias DAG of the trustworthiness/newsworthiness example. dag_coords &lt;- tibble(name = c(&quot;T&quot;, &quot;S&quot;, &quot;N&quot;), x = 1:3, y = 1) dagify(S ~ T + N, coords = dag_coords) %&gt;% gg_simple_dag() The fact that two arrows enter S means it is a collider. The core concept is easy to understand: When you condition on a collider, it creates statistical–but not necessarily causal–associations among its causes. In this case, once you learn that a proposal has been selected (\\(S\\)), then learning its trustworthiness (\\(T\\)) also provides information about its newsworthiness (\\(N\\)). Why? Because if, for example, a selected proposal has low trustworthiness, then it must have high newsworthiness. Otherwise it wouldn’t have been funded. The same works in reverse: If a proposal has low newsworthiness, we’d infer that it must have higher than average trustworthiness. Otherwise it would not have been selected for funding. (p. 176. emphasis in the original) 6.3.1 Collider of false sorrow. All it takes is a single mutate() line in the dagify() function to amend our previous DAG. dagify(M ~ H + A, coords = dag_coords %&gt;% mutate(name = c(&quot;H&quot;, &quot;M&quot;, &quot;A&quot;))) %&gt;% gg_simple_dag() In this made-up example, happiness (\\(H\\)) and age (\\(A\\)) both cause marriage (\\(M\\)). Marriage is therefore a collider. Even though there is no causal association between happiness and age, if we condition on marriage–which means here, if we include it as a predictor in a regression–then it will induce a statistical association between age and happiness. And this can mislead us to think that happiness changes with age, when in fact it is constant. To convince you of this, let’s do another simulation. (pp. 176–177) Here’s the code for McElreath’s rethinking::sim_happiness() function. rethinking::sim_happiness ## function (seed = 1977, N_years = 1000, max_age = 65, N_births = 20, ## aom = 18) ## { ## set.seed(seed) ## H &lt;- M &lt;- A &lt;- c() ## for (t in 1:N_years) { ## A &lt;- A + 1 ## A &lt;- c(A, rep(1, N_births)) ## H &lt;- c(H, seq(from = -2, to = 2, length.out = N_births)) ## M &lt;- c(M, rep(0, N_births)) ## for (i in 1:length(A)) { ## if (A[i] &gt;= aom &amp; M[i] == 0) { ## M[i] &lt;- rbern(1, inv_logit(H[i] - 4)) ## } ## } ## deaths &lt;- which(A &gt; max_age) ## if (length(deaths) &gt; 0) { ## A &lt;- A[-deaths] ## H &lt;- H[-deaths] ## M &lt;- M[-deaths] ## } ## } ## d &lt;- data.frame(age = A, married = M, happiness = H) ## return(d) ## } ## &lt;bytecode: 0x11817d7b8&gt; ## &lt;environment: namespace:rethinking&gt; Quite frankly, I can’t make sense of it. So we’ll just have to move forward and use the convenience function rather than practicing a tidyverse alternative. If you have a handle on what’s going on and have a tidyverse alternative, please share your code. d &lt;- rethinking::sim_happiness(seed = 1977, N_years = 1000) head(d) ## age married happiness ## 1 65 0 -2.0000000 ## 2 65 0 -1.7894737 ## 3 65 1 -1.5789474 ## 4 65 0 -1.3684211 ## 5 65 0 -1.1578947 ## 6 65 0 -0.9473684 Summarize the variables. d %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mean_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 3 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 age 33 2 64 0.95 mean qi ## 2 happiness 0 -2 2 0.95 mean qi ## 3 married 0.3 0 1 0.95 mean qi Here’s our version of Figure 6.5. d %&gt;% mutate(married = factor(married, labels = c(&quot;unmarried&quot;, &quot;married&quot;))) %&gt;% ggplot(aes(x = age, y = happiness, color = married)) + geom_point(size = 1.75) + scale_color_manual(NULL, values = c(&quot;grey85&quot;, &quot;forestgreen&quot;)) + scale_x_continuous(expand = c(.015, .015)) + theme(panel.grid = element_blank()) Here’s the likelihood for the simple Gaussian multivariable model predicting happiness: \\[\\begin{align*} \\text{happiness}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_{\\text{married} [i]} + \\beta_1 \\text{age}_i ,\\\\ \\end{align*}\\] where \\(\\text{married}[i]\\) is the marriage status of individual \\(i\\). Here we make d2, the subset of d containing only those 18 and up. We then make a new age variable, a, which is scaled such that \\(18 = 0\\), \\(65 = 1\\), and so on. d2 &lt;- d %&gt;% filter(age &gt; 17) %&gt;% mutate(a = (age - 18) / (65 - 18)) head(d2) ## age married happiness a ## 1 65 0 -2.0000000 1 ## 2 65 0 -1.7894737 1 ## 3 65 1 -1.5789474 1 ## 4 65 0 -1.3684211 1 ## 5 65 0 -1.1578947 1 ## 6 65 0 -0.9473684 1 With respect to priors, happiness is on an arbitrary scale, in these data, from \\(-2\\) to \\(+2\\). So our imaginary strongest relationship, taking happiness from maximum to minimum, has a slope with rise over run of \\((2 - (-2))/1 = 4\\). Remember that 95% of the mass of a normal distribution is contained within 2 standard deviations. So if we set the standard deviation of the prior to half of 4, we are saying that we expect 95% of plausible slopes to be less than maximally strong. That isn’t a very strong prior, but again, it at least helps bound inference to realistic ranges. Now for the intercepts. Each \\(\\alpha\\) is the value of \\(\\mu_i\\) when \\(A_i = 0\\). In this case, that means at age 18. So we need to allow \\(\\alpha\\) to cover the full range of happiness scores. \\(\\operatorname{Normal}(0, 1)\\) will put 95% of the mass in the \\(-2\\) to \\(+2\\) interval. (p. 178) Here we’ll take one last step before fitting our model with brms. Saving mid as a factor will make it easier to interpret the model results. d2 &lt;- d2 %&gt;% mutate(mid = factor(married + 1, labels = c(&quot;single&quot;, &quot;married&quot;))) head(d2) ## age married happiness a mid ## 1 65 0 -2.0000000 1 single ## 2 65 0 -1.7894737 1 single ## 3 65 1 -1.5789474 1 married ## 4 65 0 -1.3684211 1 single ## 5 65 0 -1.1578947 1 single ## 6 65 0 -0.9473684 1 single Fit the model. b6.9 &lt;- brm(data = d2, family = gaussian, happiness ~ 0 + mid + a, prior = c(prior(normal(0, 1), class = b, coef = midmarried), prior(normal(0, 1), class = b, coef = midsingle), prior(normal(0, 2), class = b, coef = a), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.09&quot;) print(b6.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: happiness ~ 0 + mid + a ## Data: d2 (Number of observations: 960) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## midsingle -0.23 0.06 -0.36 -0.11 1.00 1649 2365 ## midmarried 1.26 0.09 1.09 1.43 1.00 1665 2341 ## a -0.75 0.11 -0.97 -0.53 1.00 1501 1925 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.99 0.02 0.95 1.04 1.00 2380 2263 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now drop marriage status, mid. b6.10 &lt;- brm(data = d2, family = gaussian, happiness ~ 0 + Intercept + a, prior = c(prior(normal(0, 1), class = b, coef = Intercept), prior(normal(0, 2), class = b, coef = a), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.10&quot;) print(b6.10) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: happiness ~ 0 + Intercept + a ## Data: d2 (Number of observations: 960) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.08 -0.15 0.15 1.00 1514 1752 ## a 0.00 0.13 -0.25 0.25 1.00 1626 2112 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.22 0.03 1.17 1.27 1.00 2363 1956 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Wow. So when we take out mid, the coefficient for a drops to zero. The pattern above is exactly what we should expect when we condition on a collider. The collider is marriage status. It is a common consequence of age and happiness. As a result, when we condition on it, we induce a spurious association between the two causes. So it looks like, to model [b6.9], that age is negatively associated with happiness. But this is just a statistical association, not a causal association. Once we know whether someone is married or not, then their age does provide information about how happy they are. (p. 179) A little further down, McElreath gets rough: It’s easy to plead with this example. Shouldn’t marriage also influence happiness? What if happiness does change with age? But this misses the point. If you don’t have a causal model, you can’t make inferences from a multiple regression. And the regression itself does not provide the evidence you need to justify a causal model. Instead, you need some science. (pp. 179–180, emphasis added) 6.3.2 The haunted DAG. It gets worse. “Unmeasured causes can still induce collider bias. So I’m sorry to say that we also have to consider the possibility that our DAG may be haunted” (p. 180). Here’s the unhaunted DAG. dag_coords &lt;- tibble(name = c(&quot;G&quot;, &quot;P&quot;, &quot;C&quot;), x = c(1, 2, 2), y = c(2, 2, 1)) dagify(P ~ G, C ~ P + G, coords = dag_coords) %&gt;% gg_simple_dag() Now we add the haunting variable, U. dag_coords &lt;- tibble(name = c(&quot;G&quot;, &quot;P&quot;, &quot;C&quot;, &quot;U&quot;), x = c(1, 2, 2, 2.5), y = c(2, 2, 1, 1.5)) dagify(P ~ G + U, C ~ P + G + U, coords = dag_coords) %&gt;% gg_fancy_dag(x = 2.5, y = 1.5, circle = &quot;U&quot;) This is a mess. Let’s simulate some data. # how many grandparent-parent-child triads would you like? n &lt;- 200 b_gp &lt;- 1 # direct effect of G on P b_gc &lt;- 0 # direct effect of G on C b_pc &lt;- 1 # direct effect of P on C b_u &lt;- 2 # direct effect of U on P and C # simulate triads set.seed(1) d &lt;- tibble(u = 2 * rbinom(n, size = 1, prob = .5) - 1, g = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(p = rnorm(n, mean = b_gp * g + b_u * u, sd = 1)) %&gt;% mutate(c = rnorm(n, mean = b_pc * p + b_gc * g + b_u * u, sd = 1)) head(d) ## # A tibble: 6 x 4 ## u g p c ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1 -0.620 -1.73 -3.65 ## 2 -1 0.0421 -3.01 -5.30 ## 3 1 -0.911 3.06 3.88 ## 4 1 0.158 1.77 3.79 ## 5 -1 -0.655 -1.00 -2.01 ## 6 1 1.77 5.28 8.87 Fit the model without u. b6.11 &lt;- brm(data = d, family = gaussian, c ~ 0 + Intercept + p + g, prior = c(prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.11&quot;) print(b6.11) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: c ~ 0 + Intercept + p + g ## Data: d (Number of observations: 200) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.12 0.10 -0.31 0.07 1.00 3832 3110 ## p 1.79 0.04 1.70 1.87 1.00 3508 2925 ## g -0.84 0.11 -1.05 -0.63 1.00 3431 3045 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.43 0.07 1.30 1.58 1.00 4158 3115 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s our version of Figure 6.5. d %&gt;% mutate(centile = ifelse(p &gt;= quantile(p, prob = .45) &amp; p &lt;= quantile(p, prob = .60), &quot;a&quot;, &quot;b&quot;), u = factor(u)) %&gt;% ggplot(aes(x = g, y = c)) + geom_point(aes(shape = centile, color = u), size = 2.5, stroke = 1/4) + stat_smooth(data = . %&gt;% filter(centile == &quot;a&quot;), method = &quot;lm&quot;, se = F, size = 1/2, color = &quot;black&quot;, fullrange = T) + scale_shape_manual(values = c(19, 1)) + scale_color_manual(values = c(&quot;black&quot;, &quot;lightblue&quot;)) + theme(legend.position = &quot;none&quot;) Now fit the model including u. b6.12 &lt;- update(b6.11, newdata = d, formula = c ~ 0 + Intercept + p + g + u, seed = 6, file = &quot;fits/b06.12&quot;) Check the results. print(b6.12) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: c ~ Intercept + p + g + u - 1 ## Data: d (Number of observations: 200) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.12 0.07 -0.27 0.02 1.00 2589 2138 ## p 1.01 0.07 0.88 1.15 1.00 1415 1980 ## g -0.04 0.10 -0.24 0.16 1.00 1814 2178 ## u 1.99 0.15 1.69 2.29 1.00 1395 1949 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.04 0.05 0.94 1.15 1.00 3001 2545 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now the posterior for \\(\\beta_\\text g\\) is hovering around 0, where it belongs. b_gc ## [1] 0 6.4 Confronting confounding “Let’s define confounding as any context in which the association between an outcome \\(Y\\) and a predictor of interest \\(X\\) is not the same as it would be, if we had experimentally determined the values of \\(X\\)” (p. 183, emphasis in the original). Here’s the \\(E\\)-\\(U\\)-\\(W\\) DAG to help us sort this out. dag_coords &lt;- tibble(name = c(&quot;E&quot;, &quot;U&quot;, &quot;W&quot;), x = c(1, 2, 3), y = c(1, 2, 1)) dagify(E ~ U, W ~ E + U, coords = dag_coords) %&gt;% gg_simple_dag() Here’s the alternative if we were to randomly assign \\(E\\). dagify(W ~ E + U, coords = dag_coords) %&gt;% gg_simple_dag() “Manipulation removes the confounding, because it blocks the other path between \\(E\\) and \\(W\\)” (p. 184). 6.4.1 Shutting the backdoor. Blocking confounding paths between some predictor \\(X\\) and some outcome \\(Y\\) is known as shutting the backdoor. We don’t want any spurious association sneaking in through a non-causal path that enters the back of the predictor \\(X\\). In the example above, the path \\(E \\leftarrow U \\rightarrow W\\) is a backdoor path, because it enters $$E with an arrow and also connects \\(E\\) to \\(W\\). This path is non-causal–intervening on \\(E\\) will not cause a change in \\(W\\) through this path–but it still produces an association between \\(E\\) and \\(W\\). Now for some good news. Given a causal DAG, it is always possible to say which, if any, variables one must control for in order to shut all the backdoor paths. It is also possible to say which variables one must not control for, in order to avoid making new confounds. And–some more good news–there are only four types of variable relations that combine to form all possible paths. (p. 184, emphasis in the original) Here are the representations for our four types of variable relations: the fork, pipe, collider, and descendant. d1 &lt;- dagify(X ~ Z, Y ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(1, 3, 2), y = c(2, 2, 1))) d2 &lt;- dagify(Z ~ X, Y ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(1, 3, 2), y = c(2, 1, 1.5))) d3 &lt;- dagify(Z ~ X + Y, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;), x = c(1, 3, 2), y = c(1, 1, 2))) d4 &lt;- dagify(Z ~ X + Y, D ~ Z, coords = tibble(name = c(&quot;X&quot;, &quot;Y&quot;, &quot;Z&quot;, &quot;D&quot;), x = c(1, 3, 2, 2), y = c(1, 1, 2, 1.05))) p1 &lt;- gg_simple_dag(d1) + labs(subtitle = &quot;The Fork&quot;) p2 &lt;- gg_simple_dag(d2) + labs(subtitle = &quot;The Pipe&quot;) p3 &lt;- gg_simple_dag(d3) + labs(subtitle = &quot;The Collider&quot;) p4 &lt;- gg_simple_dag(d4) + labs(subtitle = &quot;The Descendant&quot;) Outside of the DAGs produced by the node_equivalent_dags(), the ggdag package does not simultaneously plot multiple DAGs the way one might with ggplot::facet_wrap(). For details, see GitHub issues #41 and #42. However, a simple way around that is to make each plot separately, save them, and then combine the individual plots with patchwork. Here it is, our DAGgy Figure 6.6. library(patchwork) (p1 | p2 | p3 | p4) &amp; theme(plot.subtitle = element_text(hjust = 0.5)) &amp; plot_annotation(title = &quot;The four elemental confounds&quot;) No matter how complicated a causal DAG appears, it is always built out of these four types of relations. And since you know how to open and close each, you (or your computer) can figure out which variables you need to include or not include. Here’s the recipe: 1 List all of the paths connecting \\(X\\) (the potential cause of interest) and \\(Y\\) (the outcome). 2 Classify each path by whether it is open or closed. A path is open unless it contains a collider. 3 Classify each path by whether it is a backdoor path. A backdoor path has an arrow entering \\(X\\). 4 If there are any open backdoor paths, decide which variable(s) to condition on to close it (if possible). (p. 185) 6.4.2 Two roads. “The DAG below contains an exposure of interest \\(X\\), an outcome of interest \\(Y\\), an unobserved variable \\(U\\), and three observed covariates (\\(A\\), \\(B\\), and \\(C\\))” (p. 186). dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;U&quot;, &quot;X&quot;, &quot;Y&quot;), x = c(2, 2, 3, 1, 1, 3), y = c(4, 2, 3, 3, 1, 1)) dagify(B ~ C + U, C ~ A, U ~ A, X ~ U, Y ~ C + X, coords = dag_coords) %&gt;% gg_fancy_dag(x = 1, y = 3, circle = &quot;U&quot;) In this DAG, there are two indirect (backdoor) paths from \\(X\\) to \\(Y\\), which are \\(X \\leftarrow U \\leftarrow A \\rightarrow C \\rightarrow Y\\), which is open; and \\(X \\leftarrow U \\rightarrow B \\leftarrow C \\rightarrow Y\\), which is closed. Conditioning on either \\(C\\) or \\(A\\) will close the open backdoor. dag_6.1 &lt;- dagitty( &quot;dag { U [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C }&quot; ) adjustmentSets(dag_6.1, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) ## { C } ## { A } 6.4.3 Backdoor waffles. dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;D&quot;, &quot;M&quot;, &quot;S&quot;, &quot;W&quot;), x = c(1, 3, 2, 1, 3), y = c(1, 1, 2, 3, 3)) dagify(A ~ S, D ~ A + M + W, M ~ A + S, W ~ S, coords = dag_coords) %&gt;% gg_simple_dag() In this graph, \\(S\\) is whether or not a State is in the southern United States, \\(A\\) is median age at marriage, \\(M\\) is marriage rate, \\(W\\) is number of Waffle Houses, and \\(D\\) is divorce rate. This graph assumes that southern States have lower ages of marriage (\\(S \\rightarrow A\\)), higher rates of marriage both directly (\\(S \\rightarrow M\\)) and mediated through age of marriage (\\(S \\rightarrow A \\rightarrow M\\)), as well as more waffles (\\(S \\rightarrow W\\)). Age of marriage and marriage rate both influence divorce. There are three open backdoor paths between \\(W\\) and \\(D\\). (p. 187) dag_6.2 &lt;- dagitty( &quot;dag { A -&gt; D A -&gt; M -&gt; D A &lt;- S -&gt; M S -&gt; W -&gt; D }&quot; ) adjustmentSets(dag_6.2, exposure = &quot;W&quot;, outcome = &quot;D&quot;) ## { A, M } ## { S } The first line of output indicates we’d have to condition on \\(A\\) and \\(M\\) simultaneously. As an alternative, we could just condition on \\(S\\). Here are the conditional independencies implied in that DAG. impliedConditionalIndependencies(dag_6.2) ## A _||_ W | S ## D _||_ S | A, M, W ## M _||_ W | S 6.5 Summary [and a little more practice] Multiple regression is no oracle, but only a golem. It is logical, but the relationships it describes are conditional associations, not causal influences. Therefore additional information, from outside the model, is needed to make sense of it. This chapter presented introductory examples of some common frustrations: multicollinearity, post-treatment bias, and collider bias. Solutions to these frustrations can be organized under a coherent framework in which hypothetical causal relations among variables are analyzed to cope with confounding. (p. 189) In the last section, we used a DAG to explore how including/excluding different covariates might influence our estimate of the causal relationship between the number of Waffle Houses and the divorce rate (\\(W \\rightarrow D\\)). To finish that example out, we might explore some of the possible models informed by the DAG. First we load the Waffle House data. data(WaffleDivorce, package = &quot;rethinking&quot;) d &lt;- WaffleDivorce # standardize the continuous focal variables. d &lt;- d %&gt;% mutate(a = rethinking::standardize(MedianAgeMarriage), d = rethinking::standardize(Divorce), m = rethinking::standardize(Marriage), s = factor(South, levels = 0:1, labels = c(&quot;North&quot;, &quot;South&quot;)), w = rethinking::standardize(WaffleHouses)) # tidy up rm(WaffleDivorce) The only focal variable we did not standardize was South, which is binary. Technically, you can standardize binary variables and not break the regression model. However, my preference it so leave them in their 0/1 metric. To do otherwise would complicate how you’d interpret the result of any model including them. Here is our ggpairs() plot of all five focal variables. Remember, the central issue is the causal relation between w and d. ggpairs(data = d, columns = c(14:16, 18, 17), upper = list(continuous = wrap(&quot;cor&quot;, family = &quot;sans&quot;, color = &quot;black&quot;, size = 3)), diag = list(continuous = my_diag), lower = list(continuous = my_lower), mapping = aes(color = s)) + scale_fill_manual(values = c(&quot;forestgreen&quot;, &quot;lightblue&quot;)) Let’s fit a series of models. The priors are based on those we used the last time we saw the WaffleHouses data (Chapter 5). b6.13 &lt;- brm(data = d, family = gaussian, d ~ 1 + w, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.13&quot;) b6.14 &lt;- brm(data = d, family = gaussian, d ~ 1 + w + s, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.14&quot;) b6.15 &lt;- brm(data = d, family = gaussian, d ~ 1 + w + a + m, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.15&quot;) b6.16 &lt;- brm(data = d, family = gaussian, d ~ 1 + w + a, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.16&quot;) b6.17 &lt;- brm(data = d, family = gaussian, d ~ 1 + w + m, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 6, file = &quot;fits/b06.17&quot;) If we extract the posterior draws from each model, the column corresponding to \\(W \\rightarrow D\\) will be b_w. Here we extract those columns, wrangle, and compare the posteriors in a coefficient plot. formula &lt;- c(&quot;d ~ 1 + w&quot;, &quot;d ~ 1 + w + s&quot;, &quot;d ~ 1 + w + a + m&quot;, &quot;d ~ 1 + w + a&quot;, &quot;d ~ 1 + w + m&quot;) tibble(fit = str_c(&quot;b6.1&quot;, 3:7)) %&gt;% mutate(y = str_c(fit, &quot; (&quot;, formula, &quot;)&quot;), post = purrr::map(fit, ~get(.) %&gt;% posterior_samples() %&gt;% select(b_w))) %&gt;% unnest(post) %&gt;% ggplot(aes(x = b_w, y = y, color = fit %in% c(&quot;b6.14&quot;, &quot;b6.15&quot;))) + stat_pointinterval(.width = .95) + scale_color_manual(values = c(&quot;grey50&quot;, &quot;forestgreen&quot;)) + labs(x = expression(beta[w]), y = NULL) + coord_cartesian(xlim = c(-0.4, 0.6)) + theme(axis.text.y = element_text(hjust = 0), legend.position = &quot;none&quot;) The two \\(\\beta_w\\) posteriors corresponding to those endorsed by our DAG are in green. Frustratingly, the results from a real-world data analysis aren’t as conclusive as we may have naïvely supposed. Why? Well, McElreath left us with some words of wisdom a few pages back: This DAG is obviously not satisfactory–it assumes there are no unobserved confounds, which is very unlikely for this sort of data. But we can still learn something by analyzing it. While the data cannot tell us whether a graph is correct, it can sometimes suggest how a graph is wrong. (p. 187) DAGs are nice tool for informing our data analytic strategy. But just as our multivariable predictor models are no fail-safes, our DAGs aren’t either. Models, theories, data-collection procedures–they all work together (or not) to determine the quality of our posteriors. To the extent any of those components are off, watch out. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] patchwork_1.1.1 dagitty_0.3-1 ggdag_0.2.3 GGally_2.1.1 tidybayes_2.3.1 bayesplot_1.8.0 ## [7] brms_2.15.0 Rcpp_1.0.6 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 ## [13] readr_1.4.0 tidyr_1.1.3 tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 igraph_1.2.6 ## [5] svUnit_1.0.3 splines_4.0.4 crosstalk_1.1.0.1 TH.data_1.0-10 ## [9] rstantools_2.1.1 inline_0.3.17 digest_0.6.27 htmltools_0.5.1.1 ## [13] viridis_0.5.1 rethinking_2.13 rsconnect_0.8.16 fansi_0.4.2 ## [17] magrittr_2.0.1 graphlayouts_0.7.1 modelr_0.1.8 RcppParallel_5.0.2 ## [21] matrixStats_0.57.0 xts_0.12.1 sandwich_3.0-0 prettyunits_1.1.1 ## [25] colorspace_2.0-0 ggrepel_0.9.1 rvest_0.3.6 ggdist_2.4.0.9000 ## [29] haven_2.3.1 xfun_0.22 callr_3.5.1 crayon_1.4.1 ## [33] jsonlite_1.7.2 lme4_1.1-25 survival_3.2-7 zoo_1.8-8 ## [37] glue_1.4.2 polyclip_1.10-0 gtable_0.3.0 emmeans_1.5.2-1 ## [41] V8_3.4.0 distributional_0.2.2 pkgbuild_1.2.0 rstan_2.21.2 ## [45] shape_1.4.5 abind_1.4-5 scales_1.1.1 mvtnorm_1.1-1 ## [49] DBI_1.1.0 miniUI_0.1.1.1 viridisLite_0.3.0 xtable_1.8-4 ## [53] stats4_4.0.4 StanHeaders_2.21.0-7 DT_0.16 htmlwidgets_1.5.2 ## [57] httr_1.4.2 threejs_0.3.3 RColorBrewer_1.1-2 arrayhelpers_1.1-0 ## [61] ellipsis_0.3.1 reshape_0.8.8 pkgconfig_2.0.3 loo_2.4.1 ## [65] farver_2.0.3 dbplyr_2.0.0 utf8_1.1.4 tidyselect_1.1.0 ## [69] labeling_0.4.2 rlang_0.4.10 reshape2_1.4.4 later_1.1.0.1 ## [73] munsell_0.5.0 cellranger_1.1.0 tools_4.0.4 cli_2.3.1 ## [77] generics_0.1.0 broom_0.7.5 ggridges_0.5.2 evaluate_0.14 ## [81] fastmap_1.0.1 processx_3.4.5 knitr_1.31 fs_1.5.0 ## [85] tidygraph_1.2.0 ggraph_2.0.4 nlme_3.1-152 mime_0.10 ## [89] projpred_2.0.2 xml2_1.3.2 compiler_4.0.4 shinythemes_1.1.2 ## [93] rstudioapi_0.13 gamm4_0.2-6 curl_4.3 reprex_0.3.0 ## [97] tweenr_1.0.1 statmod_1.4.35 stringi_1.5.3 highr_0.8 ## [101] ps_1.6.0 Brobdingnag_1.2-6 lattice_0.20-41 Matrix_1.3-2 ## [105] nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 vctrs_0.3.6 ## [109] pillar_1.5.1 lifecycle_1.0.0 bridgesampling_1.0-0 estimability_1.3 ## [113] httpuv_1.5.4 R6_2.5.0 bookdown_0.21 promises_1.1.1 ## [117] gridExtra_2.3 codetools_0.2-18 boot_1.3-26 colourpicker_1.1.0 ## [121] MASS_7.3-53 gtools_3.8.2 assertthat_0.2.1 withr_2.4.1 ## [125] shinystan_2.5.0 multcomp_1.4-16 mgcv_1.8-33 parallel_4.0.4 ## [129] hms_0.5.3 grid_4.0.4 coda_0.19-4 minqa_1.2.4 ## [133] rmarkdown_2.7 ggforce_0.3.2 shiny_1.5.0 lubridate_1.7.9.2 ## [137] base64enc_0.1-3 dygraphs_1.1.1.6 "],["ulysses-compass.html", "7 Ulysses’ Compass 7.1 The problem with parameters 7.2 Entropy and accuracy 7.3 Golem taming: regularization 7.4 Predicting predictive accuracy 7.5 Information criteria 7.6 Model comparison 7.7 Summary Bonus: \\(R^2\\) talk Session info", " 7 Ulysses’ Compass In this chapter we contend with two contrasting kinds of statistical error: overfitting, “which leads to poor prediction by learning too much from the data” underfitting, “which leads to poor prediction by learning too little from the data” (McElreath, 2020a, p. 192, emphasis added) Our job is to carefully navigate among these monsters. There are two common families of approaches. The first approach is to use a regularizing prior to tell the model not to get too excited by the data. This is the same device that non-Bayesian methods refer to as “penalized likelihood.” The second approach is to use some scoring device, like information criteria or cross-validation, to model the prediction task and estimate predictive accuracy. Both families of approaches are routinely used in the natural and social sciences. Furthermore, they can be–maybe should be–used in combination. So it’s worth understanding both, as you’re going to need both at some point. (p. 192, emphasis in the original) There’s a lot going on in this chapter. More more practice with these ideas, check out Yarkoni &amp; Westfall (2017), Choosing prediction over explanation in psychology: Lessons from machine learning. 7.0.0.1 Rethinking stargazing. The most common form of model selection among practicing scientists is to search for a model in which every coefficient is statistically significant. Statisticians sometimes call this stargazing, as it is embodied by scanning for asterisks (\\(^{\\star \\star}\\)) trailing after estimates…. Whatever you think about null hypothesis significance testing in general, using it to select among structurally different models is a mistake–\\(p\\)-values are not designed to help you navigate between underfitting and overfitting. (p. 193, emphasis in the original). McElreath spent little time discussing \\(p\\)-values and null hypothesis testing in the text. If you’d like to learn more from a Bayesian perspective, you might check out the first several chapters (particularly 10–13) in Kruschke’s (2015) text and my (2020c) ebook translating it to brms and the tidyverse. The great Frank Harrell has complied A Litany of Problems With p-values. Also, don’t miss the statement on \\(p\\)-values released by the American Statistical Association (Wasserstein &amp; Lazar, 2016). 7.1 The problem with parameters The \\(R^2\\) is a popular way to measure how well you can retrodict the data. It traditionally follows the form \\[R^2 = \\frac{\\text{var(outcome)} - \\text{var(residuals)}}{\\text{var(outcome)}} = 1 - \\frac{\\text{var(residuals)}}{\\text{var(outcome)}}.\\] By \\(\\operatorname{var}()\\), of course, we meant variance (i.e., what you get from the var() function in R). McElreath is not a fan of the \\(R^2\\). But it’s important in my field, so instead of a summary at the end of the chapter, we will cover the Bayesian version of \\(R^2\\) and how to use it in brms. 7.1.1 More parameters (almost) always improve fit. We’ll start off by making the data with brain size and body size for seven species. library(tidyverse) ( d &lt;- tibble(species = c(&quot;afarensis&quot;, &quot;africanus&quot;, &quot;habilis&quot;, &quot;boisei&quot;, &quot;rudolfensis&quot;, &quot;ergaster&quot;, &quot;sapiens&quot;), brain = c(438, 452, 612, 521, 752, 871, 1350), mass = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)) ) ## # A tibble: 7 x 3 ## species brain mass ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 afarensis 438 37 ## 2 africanus 452 35.5 ## 3 habilis 612 34.5 ## 4 boisei 521 41.5 ## 5 rudolfensis 752 55.5 ## 6 ergaster 871 61 ## 7 sapiens 1350 53.5 Let’s get ready for Figure 7.2. The plots in this chapter will be characterized by theme_classic() + theme(text = element_text(family = \"Courier\")). Our color palette will come from the rcartocolor package (Nowosad, 2019), which provides color schemes designed by ‘CARTO’. library(rcartocolor) The specific palette we’ll be using is “BurgYl.” In addition to palettes, the rcartocolor package offers a few convenience functions which make it easier to use their palettes. The carto_pal() function will return the HEX numbers associated with a given palette’s colors and the display_carto_pal() function will display the actual colors. carto_pal(7, &quot;BurgYl&quot;) ## [1] &quot;#fbe6c5&quot; &quot;#f5ba98&quot; &quot;#ee8a82&quot; &quot;#dc7176&quot; &quot;#c8586c&quot; &quot;#9c3f5d&quot; &quot;#70284a&quot; display_carto_pal(7, &quot;BurgYl&quot;) We’ll be using a diluted version of the third color for the panel background (i.e., theme(panel.background = element_rect(fill = alpha(carto_pal(7, \"BurgYl\")[3], 1/4)))) and the darker purples for other plot elements. Here’s the plot. library(ggrepel) theme_set( theme_classic() + theme(text = element_text(family = &quot;Courier&quot;), panel.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[3], 1/4))) ) d %&gt;% ggplot(aes(x = mass, y = brain, label = species)) + geom_point(color = carto_pal(7, &quot;BurgYl&quot;)[5]) + geom_text_repel(size = 3, color = carto_pal(7, &quot;BurgYl&quot;)[7], family = &quot;Courier&quot;, seed = 438) + labs(subtitle = &quot;Average brain volume by body\\nmass for six hominin species&quot;, x = &quot;body mass (kg)&quot;, y = &quot;brain volume (cc)&quot;) + xlim(30, 65) Before fitting our models, we want to standardize body mass–give it mean zero and standard deviation one–and rescale the outcome, brain volume, so that the largest observed value is 1. Why not standardize brain volume as well? Because we want to preserve zero as a reference point: No brain at all. You can’t have negative brain. I don’t think. (p. 195) d &lt;- d %&gt;% mutate(mass_std = (mass - mean(mass)) / sd(mass), brain_std = brain / max(brain)) Our first statistical model will follow the form \\[\\begin{align*} \\text{brain_std}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta \\text{mass_std}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0.5, 1) \\\\ \\beta &amp; \\sim \\operatorname{Normal}(0, 10) \\\\ \\sigma &amp; \\sim \\operatorname{Log-Normal}(0, 1). \\end{align*}\\] This simply says that the average brain volume \\(b_i\\) of species \\(i\\) is a linear function of its body mass \\(m_i\\). Now consider what the priors imply. The prior for \\(\\alpha\\) is just centered on the mean brain volume (rescaled) in the data. So it says that the average species with an average body mass has a brain volume with an 89% credible interval from about −1 to 2. That is ridiculously wide and includes impossible (negative) values. The prior for \\(\\beta\\) is very flat and centered on zero. It allows for absurdly large positive and negative relationships. These priors allow for absurd inferences, especially as the model gets more complex. And that’s part of the lesson. (p. 196) Fire up brms. library(brms) A careful study of McElreath’s R code 7.3 will show he is modeling log_sigma, rather than \\(\\sigma\\). There are ways to do this with brms (see Bürkner, 2021b), but I’m going to keep things simple, here. Our approach will be to follow the above equation more literally and just slap the \\(\\operatorname{Log-Normal}(0, 1)\\) prior directly onto \\(\\sigma\\). b7.1 &lt;- brm(data = d, family = gaussian, brain_std ~ 1 + mass_std, prior = c(prior(normal(0.5, 1), class = Intercept), prior(normal(0, 10), class = b), prior(lognormal(0, 1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.01&quot;) Check the model summary. print(b7.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: brain_std ~ 1 + mass_std ## Data: d (Number of observations: 7) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.53 0.10 0.31 0.74 1.00 2487 1978 ## mass_std 0.17 0.11 -0.06 0.41 1.00 2619 2123 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.26 0.11 0.13 0.55 1.00 1543 2174 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As we’ll learn later on, brms already has a convenience function for computing the Bayesian \\(R^2\\). At this point in the chapter, we’ll follow along and make a brms-centric version of McElreath’s R2_is_bad(). But because part of our version of R2_is_bad() will contain the brms::predict() function, I’m going to add a seed argument to make the results more reproducible. R2_is_bad &lt;- function(brm_fit, seed = 7, ...) { set.seed(seed) p &lt;- predict(brm_fit, summary = F, ...) r &lt;- apply(p, 2, mean) - d$brain_std 1 - rethinking::var2(r) / rethinking::var2(d$brain_std) } Here’s the estimate for our \\(R^2\\). R2_is_bad(b7.1) ## [1] 0.4910024 Do note that, in principle, the Bayesian approach mandates that we do this for each sample from the posterior. But \\(R^2\\) is traditionally computed only at the mean prediction. So we’ll do that as well here. Later in the chapter you’ll learn a properly Bayesian score that uses the entire posterior distribution. (p. 197) Now fit the quadratic through the fifth-order polynomial models using update(). # quadratic b7.2 &lt;- update(b7.1, newdata = d, formula = brain_std ~ 1 + mass_std + I(mass_std^2), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.02&quot;) # cubic b7.3 &lt;- update(b7.1, newdata = d, formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, control = list(adapt_delta = .9), file = &quot;fits/b07.03&quot;) # fourth-order b7.4 &lt;- update(b7.1, newdata = d, formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, control = list(adapt_delta = .995), file = &quot;fits/b07.04&quot;) # fifth-order b7.5 &lt;- update(b7.1, newdata = d, formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4) + I(mass_std^5), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, control = list(adapt_delta = .99999), file = &quot;fits/b07.05&quot;) You may have noticed we fiddled with the adapt_delta setting for some of the models. When you try to fit complex models with few data points and wide priors, that can cause difficulties for Stan. I’m not going to get into the details on what adapt_delta does, right now. But it’ll make appearances in later chapters and we’ll more formally introduce it in Section 13.4.2. Now returning to the text, That last model, m7.6, has one trick in it. The standard deviation is replaced with a constant value 0.001. The model will not work otherwise, for a very important reason that will become clear as we plot these monsters. (p. 198) By “last model, m7.6,” McElreath was referring to the sixth-order polynomial, fit on page 199. McElreath’s rethinking package is set up so the syntax is simple to replacing \\(\\sigma\\) with a constant value. We can do this with brms, too, but it’ll take more effort. If we want to fix \\(\\sigma\\) to a constant, we’ll need to define a custom likelihood. Bürkner explained how to do so in his (2021a) vignette, Define custom response distributions with brms. I’m not going to explain this in great detail, here. In brief, first we use the custom_family() function to define the name and parameters of a custom_normal() likelihood that will set \\(\\sigma\\) to a constant value, 0.001. Second, we’ll define some functions for Stan which are not defined in Stan itself and save them as stan_funs. Third, we make a stanvar() statement which will allow us to pass our stan_funs to brm(). custom_normal &lt;- custom_family( &quot;custom_normal&quot;, dpars = &quot;mu&quot;, links = &quot;identity&quot;, type = &quot;real&quot; ) stan_funs &lt;- &quot;real custom_normal_lpdf(real y, real mu) { return normal_lpdf(y | mu, 0.001); } real custom_normal_rng(real mu) { return normal_rng(mu, 0.001); } &quot; stanvars &lt;- stanvar(scode = stan_funs, block = &quot;functions&quot;) Now we can fit the model by setting family = custom_normal. Note that since we’ve set \\(\\sigma = 0.001\\), we don’t need to include a prior for \\(\\sigma\\). Also notice our stanvars = stanvars line. b7.6 &lt;- brm(data = d, family = custom_normal, brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4) + I(mass_std^5) + I(mass_std^6), prior = c(prior(normal(0.5, 1), class = Intercept), prior(normal(0, 10), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, stanvars = stanvars, file = &quot;fits/b07.06&quot;) Before we can do our variant of Figure 7.3, we’ll need to define a few more custom functions to work with b7.6. The posterior_epred_custom_normal() function is required for brms::fitted() to work with our family = custom_normal brmsfit object. Same thing for posterior_predict_custom_normal() and brms::predict(). Though we won’t need it until Section 7.2.5, we’ll also define log_lik_custom_normal so we can use the log_lik() function for any models fit with family = custom_normal. Before all that, we need to throw in a line with the expose_functions() function. If you want to understand why, read up in Bürkner’s (2021a) vignette. For now, just go with it. expose_functions(b7.6, vectorize = TRUE) posterior_epred_custom_normal &lt;- function(prep) { mu &lt;- prep$dpars$mu mu } posterior_predict_custom_normal &lt;- function(i, prep, ...) { mu &lt;- prep$dpars$mu mu custom_normal_rng(mu) } log_lik_custom_normal &lt;- function(i, prep) { mu &lt;- prep$dpars$mu y &lt;- prep$data$Y[i] custom_normal_lpdf(y, mu) } Okay, here’s how we might plot the result for the first model, b7.1. library(tidybayes) nd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 100)) fitted(b7.1, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = mass_std, y = Estimate)) + geom_lineribbon(aes(ymin = Q5.5, ymax = Q94.5), color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 1/2, fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[6], 1/3)) + geom_point(data = d, aes(y = brain_std), color = carto_pal(7, &quot;BurgYl&quot;)[7]) + labs(subtitle = bquote(italic(R)^2==.(round(R2_is_bad(b7.1), digits = 2))), x = &quot;body mass (standardized)&quot;, y = &quot;brain volume (standardized)&quot;) + coord_cartesian(xlim = range(d$mass_std)) To slightly repurpose a quote from McElreath: We’ll want to do this for the next several models, so let’s write a function to make it repeatable. If you find yourself writing code more than once, it is usually saner to write a function and call the function more than once instead. (p. 197) Our make_figure7.3() function will wrap the simulation, data wrangling, and plotting code all in one. It takes two arguments, the first of which defines which fit object we’d like to plot. If you look closely at Figure 7.3 in the text, you’ll notice that the range of the \\(y\\)-axis changes in the last three plots. Our second argument, ylim, will allow us to vary those parameters across subplots. make_figure7.3 &lt;- function(brms_fit, ylim = range(d$brain_std)) { # compute the R2 r2 &lt;- R2_is_bad(brms_fit) # define the new data nd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 200)) # simulate and wrangle fitted(brms_fit, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # plot! ggplot(aes(x = mass_std)) + geom_lineribbon(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5), color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 1/2, fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[6], 1/3)) + geom_point(data = d, aes(y = brain_std), color = carto_pal(7, &quot;BurgYl&quot;)[7]) + labs(subtitle = bquote(italic(R)^2==.(round(r2, digits = 2))), x = &quot;body mass (std)&quot;, y = &quot;brain volume (std)&quot;) + coord_cartesian(xlim = c(-1.2, 1.5), ylim = ylim) } Here we make and save the six subplots in bulk. p1 &lt;- make_figure7.3(b7.1) p2 &lt;- make_figure7.3(b7.2) p3 &lt;- make_figure7.3(b7.3) p4 &lt;- make_figure7.3(b7.4, ylim = c(.25, 1.1)) p5 &lt;- make_figure7.3(b7.5, ylim = c(.1, 1.4)) p6 &lt;- make_figure7.3(b7.6, ylim = c(-0.25, 1.5)) + geom_hline(yintercept = 0, color = carto_pal(7, &quot;BurgYl&quot;)[2], linetype = 2) Now use patchwork syntax to bundle them all together. library(patchwork) ((p1 | p2) / (p3 | p4) / (p5 | p6)) + plot_annotation(title = &quot;Figure7.3. Polynomial linear models of increasing\\ndegree for the hominin data.&quot;) It’s not clear, to me, why our brms-based 89% intervals are so much broader than those in the text. But if you do fit the size models with rethinking::quap() and compare the summaries, you’ll see our brms-based parameters are systemically less certain than those fit with quap(). In you have an answer or perhaps even an alternative workflow to solve the issue, share on GitHub. If you really what the axes scaled in the original metrics of the variables rather than their standardized form, you can use the re-scaling techniques from back in Section 4.5.1.0.1. “This is a general phenomenon: If you adopt a model family with enough parameters, you can fit the data exactly. But such a model will make rather absurd predictions for yet-to-be-observed cases” (pp. 199–201). 7.1.1.1 Rethinking: Model fitting as compression. Another perspective on the absurd model just above is to consider that model fitting can be considered a form of data compression. Parameters summarize relationships among the data. These summaries compress the data into a simpler form, although with loss of information (“lossy” compression) about the sample. The parameters can then be used to generate new data, effectively decompressing the data. (p. 201, emphasis in the original) 7.1.2 Too few parameters hurts, too. The overfit polynomial models fit the data extremely well, but they suffer for this within-sample accuracy by making nonsensical out-of-sample predictions. In contrast, underfitting produces models that are inaccurate both within and out of sample. They learn too little, failing to recover regular features of the sample. (p. 201, emphasis in the original) To explore the distinctions between overfitting and underfitting, we’ll need to refit b7.1 and b7.4 several times after serially dropping one of the rows in the data. You can filter() by row_number() to drop rows in a tidyverse kind of way. For example, we can drop the second row of d like this. d %&gt;% mutate(row = 1:n()) %&gt;% filter(row_number() != 2) ## # A tibble: 6 x 6 ## species brain mass mass_std brain_std row ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 afarensis 438 37 -0.779 0.324 1 ## 2 habilis 612 34.5 -1.01 0.453 3 ## 3 boisei 521 41.5 -0.367 0.386 4 ## 4 rudolfensis 752 55.5 0.917 0.557 5 ## 5 ergaster 871 61 1.42 0.645 6 ## 6 sapiens 1350 53.5 0.734 1 7 In his Overthinking: Dropping rows box (p. 202), McElreath encouraged us to take a look at the brain_loo_plot() function to get a sense of how he made his Figure 7.4. Here it is. library(rethinking) brain_loo_plot ## function (fit, atx = c(35, 47, 60), aty = c(450, 900, 1300), ## xlim, ylim, npts = 100) ## { ## post &lt;- extract.samples(fit) ## n &lt;- dim(post$b)[2] ## if (is.null(n)) ## n &lt;- 1 ## if (missing(xlim)) ## xlim &lt;- range(d$mass_std) ## else xlim &lt;- (xlim - mean(d$mass))/sd(d$mass) ## if (missing(ylim)) ## ylim &lt;- range(d$brain_std) ## else ylim &lt;- ylim/max(d$brain) ## plot(d$brain_std ~ d$mass_std, xaxt = &quot;n&quot;, yaxt = &quot;n&quot;, xlab = &quot;body mass (kg)&quot;, ## ylab = &quot;brain volume (cc)&quot;, col = rangi2, pch = 16, xlim = xlim, ## ylim = ylim) ## axis_unscale(1, atx, d$mass) ## axis_unscale(2, at = aty, factor = max(d$brain)) ## d &lt;- as.data.frame(fit@data) ## for (i in 1:nrow(d)) { ## di &lt;- d[-i, ] ## m_temp &lt;- quap(fit@formula, data = di, start = list(b = rep(0, ## n))) ## xseq &lt;- seq(from = xlim[1] - 0.2, to = xlim[2] + 0.2, ## length.out = npts) ## l &lt;- link(m_temp, data = list(mass_std = xseq), refresh = 0) ## mu &lt;- apply(l, 2, mean) ## lines(xseq, mu, lwd = 2, col = col.alpha(&quot;black&quot;, 0.3)) ## } ## model_name &lt;- deparse(match.call()[[2]]) ## mtext(model_name, adj = 0) ## } ## &lt;bytecode: 0x12c8b2a58&gt; ## &lt;environment: namespace:rethinking&gt; Though we’ll be taking a slightly different route than the one outlined in McElreath’s brain_loo_plot() function, we can glean some great insights. For example, we’ll be refitting our brms models with update(). b7.1.1 &lt;- update(b7.1, newdata = filter(d, row_number() != 1), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.01.1&quot;) print(b7.1.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: brain_std ~ 1 + mass_std ## Data: filter(d, row_number() != 1) (Number of observations: 6) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.54 0.14 0.25 0.83 1.00 2007 1625 ## mass_std 0.16 0.15 -0.15 0.49 1.00 2416 1595 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.31 0.16 0.14 0.74 1.00 1169 1657 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). You can see by the newdata statement that b7.1.1 is fit on the d data after dropping the first row. Here’s how we might amend out plotting strategy from before to visualize the posterior mean for the model-implied trajectory. fitted(b7.1.1, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = mass_std)) + geom_line(aes(y = Estimate), color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 1/2, alpha = 1/2) + geom_point(data = d, aes(y = brain_std), color = carto_pal(7, &quot;BurgYl&quot;)[7]) + labs(subtitle = &quot;b7.1.1&quot;, x = &quot;body mass (std)&quot;, y = &quot;brain volume (std)&quot;) + coord_cartesian(xlim = range(d$mass_std), ylim = range(d$brain_std)) Now we’ll make a brms-oriented version of McElreath’s brain_loo_plot() function. Our version brain_loo_lines() will refit the model and extract the lines information in one step. We’ll leave the plotting for a second step. brain_loo_lines &lt;- function(brms_fit, row, ...) { # refit the model new_fit &lt;- update(brms_fit, newdata = filter(d, row_number() != row), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, refresh = 0, ...) # pull the lines values fitted(new_fit, newdata = nd) %&gt;% data.frame() %&gt;% select(Estimate) %&gt;% bind_cols(nd) } Here’s how brain_loo_lines() works. brain_loo_lines(b7.1, row = 1) %&gt;% glimpse() ## Rows: 100 ## Columns: 2 ## $ Estimate &lt;dbl&gt; 0.2221350, 0.2286178, 0.2351006, 0.2415834, 0.2480662, 0.2545490, 0.2610318, 0.26… ## $ mass_std &lt;dbl&gt; -2.0000000, -1.9595960, -1.9191919, -1.8787879, -1.8383838, -1.7979798, -1.757575… Working within the tidyverse paradigm, we’ll make a tibble with the predefined row values. We will then use purrr::map() to plug those row values into brain_loo_lines(), which will return the desired posterior mean values for each corresponding value of mass_std. Here we do that for both b7.1 and b7.4. b7.1_fits &lt;- tibble(row = 1:7) %&gt;% mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = b7.1, row = .))) %&gt;% unnest(post) b7.4_fits &lt;- tibble(row = 1:7) %&gt;% mutate(post = purrr::map(row, ~brain_loo_lines(brms_fit = b7.4, row = ., control = list(adapt_delta = .999)))) %&gt;% unnest(post) Now for each, pump those values into ggplot(), customize the settings, and combine the two ggplots to make the full Figure 7.4. # left p1 &lt;- b7.1_fits %&gt;% ggplot(aes(x = mass_std)) + geom_line(aes(y = Estimate, group = row), color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 1/2, alpha = 1/2) + geom_point(data = d, aes(y = brain_std), color = carto_pal(7, &quot;BurgYl&quot;)[7]) + labs(subtitle = &quot;b7.1&quot;, x = &quot;body mass (std)&quot;, y = &quot;brain volume (std)&quot;) + coord_cartesian(xlim = range(d$mass_std), ylim = range(d$brain_std)) # right p2 &lt;- b7.4_fits %&gt;% ggplot(aes(x = mass_std, y = Estimate)) + geom_line(aes(group = row), color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 1/2, alpha = 1/2) + geom_point(data = d, aes(y = brain_std), color = carto_pal(7, &quot;BurgYl&quot;)[7]) + labs(subtitle = &quot;b7.4&quot;, x = &quot;body mass (std)&quot;, y = &quot;brain volume (std)&quot;) + coord_cartesian(xlim = range(d$mass_std), ylim = c(-0.1, 1.4)) # combine p1 + p2 “Notice that the straight lines hardly vary, while the curves fly about wildly. This is a general contrast between underfit and overfit models: sensitivity to the exact composition of the sample used to fit the model” (p. 201). 7.1.2.1 Rethinking: Bias and variance. The underfitting/overfitting dichotomy is often described as the bias-variance trade-off. While not exactly the same distinction, the bias-variance trade-off addresses the same problem. “Bias” is related to underfitting, while “variance” is related to overfitting. These terms are confusing, because they are used in many different ways in different contexts, even within statistics. The term “bias” also sounds like a bad thing, even though increasing bias often leads to better predictions. (p. 201, emphasis in the original) Take a look at Yarkoni &amp; Westfall (2017) for more on the bias-variance trade-off. As McElreath indicated in his endnote #104 (p. 563), Hastie, Tibshirani and Friedman (2009) broadly cover these ideas in their freely-downloadable text, The elements of statistical learning. 7.2 Entropy and accuracy So how do we navigate between the hydra of overfitting and the vortex of underfitting? Whether you end up using regularization or information criteria or both, the first thing you must do is pick a criterion of model performance. What do you want the model to do well at? We’ll call this criterion the target, and in this section you’ll see how information theory provides a common and useful target. (p. 202, emphasis in the original) 7.2.1 Firing the weatherperson. If you let rain = 1 and sun = 0, here’s a way to make a plot of the first table of page 203, the weatherperson’s predictions. weatherperson &lt;- tibble(day = 1:10, prediction = rep(c(1, 0.6), times = c(3, 7)), observed = rep(c(1, 0), times = c(3, 7))) weatherperson %&gt;% pivot_longer(-day) %&gt;% ggplot(aes(x = day, y = name, fill = value)) + geom_tile(color = &quot;white&quot;) + geom_text(aes(label = value, color = value == 0)) + scale_x_continuous(breaks = 1:10, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + scale_fill_viridis_c(direction = -1) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) Here’s how the newcomer fared: newcomer &lt;- tibble(day = 1:10, prediction = 0, observed = rep(c(1, 0), times = c(3, 7))) newcomer %&gt;% pivot_longer(-day) %&gt;% ggplot(aes(x = day, y = name, fill = value)) + geom_tile(color = &quot;white&quot;) + geom_text(aes(label = value, color = value == 0)) + scale_x_continuous(breaks = 1:10, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + scale_fill_viridis_c(direction = -1) + scale_color_manual(values = c(&quot;white&quot;, &quot;black&quot;)) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) If we do the math entailed in the tibbles, we’ll see why the newcomer could boast “I’m the best person for the job” (p. 203). weatherperson %&gt;% bind_rows(newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n()/2), hit = ifelse(prediction == observed, 1, 1 - prediction - observed)) %&gt;% group_by(person) %&gt;% summarise(hit_rate = mean(hit)) ## # A tibble: 2 x 2 ## person hit_rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 newcomer 0.7 ## 2 weatherperson 0.58 7.2.1.1 Costs and benefits. Our new points variable doesn’t fit into the nice color-based geom_tile() plots from above, but we can still do the math. bind_rows(weatherperson, newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n()/2), points = ifelse(observed == 1 &amp; prediction != 1, -5, ifelse(observed == 1 &amp; prediction == 1, -1, -1 * prediction))) %&gt;% group_by(person) %&gt;% summarise(happiness = sum(points)) ## # A tibble: 2 x 2 ## person happiness ## &lt;chr&gt; &lt;dbl&gt; ## 1 newcomer -15 ## 2 weatherperson -7.2 7.2.1.2 Measuring accuracy. Consider computing the probability of predicting the exact sequence of days. This means computing the probability of a correct prediction for each day. Then multiply all of these probabilities together to get the joint probability of correctly predicting the observed sequence. This is the same thing as the joint likelihood, which you’ve been using up to this point to fit models with Bayes’ theorem. This is the definition of accuracy that is maximized by the correct model. In this light, the newcomer looks even worse. (p. 204) bind_rows(weatherperson, newcomer) %&gt;% mutate(person = rep(c(&quot;weatherperson&quot;, &quot;newcomer&quot;), each = n() / 2), hit = ifelse(prediction == observed, 1, 1 - prediction - observed)) %&gt;% count(person, hit) %&gt;% mutate(power = hit ^ n, term = rep(letters[1:2], times = 2)) %&gt;% select(person, term, power) %&gt;% pivot_wider(names_from = term, values_from = power) %&gt;% mutate(probability_correct_sequence = a * b) ## # A tibble: 2 x 4 ## person a b probability_correct_sequence ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 newcomer 0 1 0 ## 2 weatherperson 0.00164 1 0.00164 7.2.2 Information and uncertainty. Within the context of information theory (Shannon, 1948; also Cover &amp; Thomas, 2006), information is “the reduction in uncertainty when we learn an outcome” (p. 205). Information entropy is a way of measuring that uncertainty in a way that is (a) continuous, (b) increases as the number of possible events increases, and (c) is additive. The formula for information entropy is: \\[H(p) = - \\text E \\log (p_i) = - \\sum_{i = 1}^n p_i \\log (p_i).\\] McElreath put it in words as: “The uncertainty contained in a probability distribution is the average log-probability of an event.” (p. 206). We’ll compute the information entropy for weather at the first unnamed location, which we’ll call McElreath's house, and Abu Dhabi at once. tibble(place = c(&quot;McElreath&#39;s house&quot;, &quot;Abu Dhabi&quot;), p_rain = c(.3, .01)) %&gt;% mutate(p_shine = 1 - p_rain) %&gt;% group_by(place) %&gt;% mutate(h_p = (p_rain * log(p_rain) + p_shine * log(p_shine)) %&gt;% mean() * -1) ## # A tibble: 2 x 4 ## # Groups: place [2] ## place p_rain p_shine h_p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 McElreath&#39;s house 0.3 0.7 0.611 ## 2 Abu Dhabi 0.01 0.99 0.0560 Did you catch how we used the equation \\(H(p) = - \\sum_{i = 1}^n p_i \\log (p_i)\\) in our mutate() code, there? Our computation indicated the uncertainty is less in Abu Dhabi because it rarely rains, there. If you have sun, rain and snow, the entropy for weather is: p &lt;- c(.7, .15, .15) -sum(p * log(p)) ## [1] 0.8188085 “These entropy values by themselves don’t mean much to us, though. Instead we can use them to build a measure of accuracy. That comes next” (p. 206). 7.2.3 From entropy to accuracy. How can we use information entropy to say how far a model is from the target? The key lies in divergence: Divergence: The additional uncertainty induced by using probabilities from one distribution to describe another distribution. This is often known as Kullback-Leibler divergence or simply KL divergence. (p. 207, emphasis in the original, see Kullback &amp; Leibler, 1951) The formula for the KL divergence is \\[D_\\text{KL} (p, q) = \\sum_i p_i \\big ( \\log (p_i) - \\log (q_i) \\big ) = \\sum_i p_i \\log \\left ( \\frac{p_i}{q_i} \\right ),\\] which is what McElreath described in plainer language as “the average difference in log probability between the target (\\(p\\)) and model (\\(q\\))” (p. 207). In McElreath’s initial example \\(p_1 = .3\\), \\(p_2 = .7\\), \\(q_1 = .25\\), and \\(q_2 = .75\\). With those values, we can compute \\(D_\\text{KL} (p, q)\\) within a tibble like so: tibble(p_1 = .3, p_2 = .7, q_1 = .25, q_2 = .75) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 1 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.25 0.75 0.00640 Our systems in this section are binary (e.g., \\(q = \\{ q_i, q_2 \\}\\)). Thus if you know \\(q_1 = .3\\) you know of a necessity \\(q_2 = 1 - q_1\\). Therefore we can code the tibble for the next example, for when \\(p = q\\), like this. tibble(p_1 = .3) %&gt;% mutate(p_2 = 1 - p_1, q_1 = p_1) %&gt;% mutate(q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 1 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.3 0.7 0 Building off of that, you can make the data required for Figure 7.6 like this. t &lt;- tibble(p_1 = .3, p_2 = .7, q_1 = seq(from = .01, to = .99, by = .01)) %&gt;% mutate(q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) head(t) ## # A tibble: 6 x 5 ## p_1 p_2 q_1 q_2 d_kl ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.3 0.7 0.01 0.99 0.778 ## 2 0.3 0.7 0.02 0.98 0.577 ## 3 0.3 0.7 0.03 0.97 0.462 ## 4 0.3 0.7 0.04 0.96 0.383 ## 5 0.3 0.7 0.05 0.95 0.324 ## 6 0.3 0.7 0.06 0.94 0.276 Now we have the data, plotting Figure 7.6 is a just geom_line() with stylistic flourishes. t %&gt;% ggplot(aes(x = q_1, y = d_kl)) + geom_vline(xintercept = .3, color = carto_pal(7, &quot;BurgYl&quot;)[5], linetype = 2) + geom_line(color = carto_pal(7, &quot;BurgYl&quot;)[7], size = 1.5) + annotate(geom = &quot;text&quot;, x = .4, y = 1.5, label = &quot;q = p&quot;, color = carto_pal(7, &quot;BurgYl&quot;)[5], family = &quot;Courier&quot;, size = 3.5) + labs(x = &quot;q[1]&quot;, y = &quot;Divergence of q from p&quot;) What divergence can do for us now is help us contrast different approximations to \\(p\\). As an approximating function \\(q\\) becomes more accurate, \\(D_\\text{KL} (p, q)\\) will shrink. So if we have a pair of candidate distributions, then the candidate that minimizes the divergence will be closest to the target. Since predictive models specify probabilities of events (observations), we can use divergence to compare the accuracy of models. (p. 208) 7.2.3.1 Rethinking: Divergence depends upon direction. Here we see \\(H(p, q) \\neq H(q, p)\\). That is, direction matters. tibble(direction = c(&quot;Earth to Mars&quot;, &quot;Mars to Earth&quot;), p_1 = c(.01, .7), q_1 = c(.7, .01)) %&gt;% mutate(p_2 = 1 - p_1, q_2 = 1 - q_1) %&gt;% mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2))) ## # A tibble: 2 x 6 ## direction p_1 q_1 p_2 q_2 d_kl ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Earth to Mars 0.01 0.7 0.99 0.3 1.14 ## 2 Mars to Earth 0.7 0.01 0.3 0.99 2.62 The \\(D_\\text{KL}\\) was double when applying Martian estimates to Terran estimates. An important practical consequence of this asymmetry, in a model fitting context, is that if we use a distribution with high entropy to approximate an unknown true distribution of events, we will reduce the distance to the truth and therefore the error. This fact will help us build generalized linear models, later on in Chapter 10. (p. 209) 7.2.4 Estimating divergence. The point of all the preceding material about information theory and divergence is to establish both: How to measure the distance of a model from our target. Information theory gives us the distance measure we need, the KL divergence. How to estimate the divergence. Having identified the right measure of distance, we now need a way to estimate it in real statistical modeling tasks. (p. 209) Now we’ll start working on item #2. Within the context of science, say we’ve labeled the true model for our topic of interest as \\(p\\). We don’t actually know what \\(p\\) is–we wouldn’t need the scientific method if we did. But say what we do have are two candidate models \\(q\\) and \\(r\\). We would at least like to know which is closer to \\(p\\). It turns out we don’t even need to know the absolute value of \\(p\\) to achieve this. Just the relative values of \\(q\\) and \\(r\\) will suffice. We express model \\(q\\)’s average log-probability as \\(\\text E \\log (q_i)\\). Extrapolating, the difference \\(\\text E \\log (q_i) - \\text E \\log (r_i)\\) gives us a sense about the divergence of both \\(q\\) and \\(r\\) from the target \\(p\\). That is, “we can compare the average log-probability from each model to get an estimate of the relative distance of each model from the target” (p. 210). Deviance and related statistics can help us towards this end. We define deviance as \\[D(q) = -2 \\sum_i \\log (q_i),\\] where \\(i\\) indexes each case and \\(q_i\\) is the likelihood for each case. Here’s the deviance from the OLS version of model m7.1. lm(data = d, brain_std ~ mass_std) %&gt;% logLik() * -2 ## &#39;log Lik.&#39; -5.985049 (df=3) In our \\(D(q)\\) formula, did you notice how we ended up multiplying \\(\\sum_i \\log (p_i)\\) by \\(-2\\)? Frequentists and Bayesians alike make use of information theory, KL divergence, and deviance. It turns out that the differences between two \\(D(q)\\) values follows a \\(\\chi^2\\) distribution (Wilks, 1938), which frequentists like to reference for the purpose of null-hypothesis significance testing. Many Bayesians, however, are not into all that significance-testing stuff and they aren’t as inclined to multiply \\(\\sum_i \\log (p_i)\\) by \\(-2\\) for the simple purpose of scaling the associated difference distribution to follow the \\(\\chi^2\\). If we leave that part out of the equation, we end up with \\[S(q) = \\sum_i \\log (q_i),\\] which we can think of as a log-probability score which is “the gold standard way to compare the predictive accuracy of different models. It is an estimate of \\(\\text E \\log (q_i)\\), just without the final step of dividing by the number of observations” (p. 210). When Bayesians compute \\(S(q)\\), they do so over the entire posterior distribution. “Doing this calculation correctly requires a little subtlety. The rethinking package has a function called lppd–log-pointwise-predictive-density–to do this calculation for quap models” (p. 210, emphasis in the original for the second time, but not the first). However, I’m now aware of a similar function within brms. If you’re willing to roll up your sleeves, a little, you can do it by hand. Here’s an example with b7.1. log_lik(b7.1) %&gt;% data.frame() %&gt;% set_names(pull(d, species)) %&gt;% pivot_longer(everything(), names_to = &quot;species&quot;, values_to = &quot;logprob&quot;) %&gt;% mutate(prob = exp(logprob)) %&gt;% group_by(species) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) ## # A tibble: 7 x 2 ## species log_probability_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 afarensis 0.384 ## 2 africanus 0.411 ## 3 boisei 0.400 ## 4 ergaster 0.231 ## 5 habilis 0.336 ## 6 rudolfensis 0.274 ## 7 sapiens -0.607 “If you sum these values, you’ll have the total log-probability score for the model and data” (p. 210). Here we sum those \\(\\log (q_i)\\) values up to compute \\(S(q)\\). log_lik(b7.1) %&gt;% data.frame() %&gt;% set_names(pull(d, species)) %&gt;% pivot_longer(everything(), names_to = &quot;species&quot;, values_to = &quot;logprob&quot;) %&gt;% mutate(prob = exp(logprob)) %&gt;% group_by(species) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) %&gt;% summarise(total_log_probability_score = sum(log_probability_score)) ## # A tibble: 1 x 1 ## total_log_probability_score ## &lt;dbl&gt; ## 1 1.43 7.2.4.1 Overthinking: Computing the lppd. The Bayesian version of the log-probability score, what we’ve been calling the lppd, has to account for the data and the posterior distribution. It follows the form \\[\\text{lppd}(y, \\Theta) = \\sum_i \\log \\frac{1}{S} \\sum_s p (y_i | \\Theta_s),\\] where \\(S\\) is the number of samples and \\(\\Theta_s\\) is the \\(s\\)-th set of sampled parameter values in the posterior distribution. While in principle this is easy–you just need to compute the probability (density) of each observation \\(i\\) for each sample \\(s\\), take the average, and then the logarithm–in practice it is not so easy. The reason is that doing arithmetic in a computer often requires some tricks to retain precision. (p. 210) Our approach to McElreath’s R code 7.14 code will look very different. Going step by step, first we use the brms::log_lik() function. log_prob &lt;- log_lik(b7.1) log_prob %&gt;% glimpse() ## num [1:4000, 1:7] 0.769 0.396 0.894 0.558 -0.235 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL The log_lik() function returned a matrix. Each occasion in the original data, \\(y_i\\), got a column and each HMC chain iteration gets a row. Given we used the brms default of 4,000 HMC iterations, which corresponds to \\(S = 4{,}000\\) in the formula. Note the each of these \\(7 \\times 4{,}000\\) values is a log-probability, not a probability itself. Thus, if we want to start summing these \\(s\\) iterations within cases, we’ll need to exponentiate them into probabilities. prob &lt;- log_prob %&gt;% # make it a data frame data.frame() %&gt;% # add case names, for convenience set_names(pull(d, species)) %&gt;% # add an s iteration index, for convenience mutate(s = 1:n()) %&gt;% # make it long pivot_longer(-s, names_to = &quot;species&quot;, values_to = &quot;logprob&quot;) %&gt;% # compute the probability scores mutate(prob = exp(logprob)) prob ## # A tibble: 28,000 x 4 ## s species logprob prob ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 afarensis 0.769 2.16 ## 2 1 africanus 0.837 2.31 ## 3 1 habilis 0.647 1.91 ## 4 1 boisei 0.690 1.99 ## 5 1 rudolfensis 0.140 1.15 ## 6 1 ergaster -0.0126 0.987 ## 7 1 sapiens -0.497 0.608 ## 8 2 afarensis 0.396 1.49 ## 9 2 africanus 0.479 1.61 ## 10 2 habilis 0.768 2.16 ## # … with 27,990 more rows Now for each case, we take the average of each of the probability sores, and then take the log of that. prob &lt;- prob %&gt;% group_by(species) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) prob ## # A tibble: 7 x 2 ## species log_probability_score ## &lt;chr&gt; &lt;dbl&gt; ## 1 afarensis 0.384 ## 2 africanus 0.411 ## 3 boisei 0.400 ## 4 ergaster 0.231 ## 5 habilis 0.336 ## 6 rudolfensis 0.274 ## 7 sapiens -0.607 For our last step, we sum those values up. prob %&gt;% summarise(total_log_probability_score = sum(log_probability_score)) ## # A tibble: 1 x 1 ## total_log_probability_score ## &lt;dbl&gt; ## 1 1.43 That, my friends, is the log-pointwise-predictive-density, \\(\\text{lppd}(y, \\Theta)\\). 7.2.5 Scoring the right data. Since we don’t have a lppd() function for brms, we’ll have to turn our workflow from the last two sections into a custom function. We’ll call it my_lppd(). my_lppd &lt;- function(brms_fit) { log_lik(brms_fit) %&gt;% data.frame() %&gt;% pivot_longer(everything(), values_to = &quot;logprob&quot;) %&gt;% mutate(prob = exp(logprob)) %&gt;% group_by(name) %&gt;% summarise(log_probability_score = mean(prob) %&gt;% log()) %&gt;% summarise(total_log_probability_score = sum(log_probability_score)) } Here’s a tidyverse-style approach for computing the lppd for each of our six brms models. tibble(name = str_c(&quot;b7.&quot;, 1:6)) %&gt;% mutate(brms_fit = purrr::map(name, get)) %&gt;% mutate(lppd = purrr::map(brms_fit, ~my_lppd(.))) %&gt;% unnest(lppd) ## # A tibble: 6 x 3 ## name brms_fit total_log_probability_score ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 b7.1 &lt;brmsfit&gt; 1.43 ## 2 b7.2 &lt;brmsfit&gt; 0.686 ## 3 b7.3 &lt;brmsfit&gt; 0.397 ## 4 b7.4 &lt;brmsfit&gt; 0.168 ## 5 b7.5 &lt;brmsfit&gt; 2.65 ## 6 b7.6 &lt;brmsfit&gt; 25.9 When we usually have data and use it to fit a statistical model, the data comprise a training sample. Parameters are estimated from it, and then we can imagine using those estimates to predict outcomes in a new sample, called the test sample. R is going to do all of this for you. But here’s the full procedure, in outline: Suppose there’s a training sample of size \\(N\\). Compute the posterior distribution of a model for the training sample, and compute the score on the training sample. Call this score \\(D_\\text{train}\\). Suppose another sample of size \\(N\\) from the same process. This is the test sample. Compute the score on the test sample, using the posterior trained on the training sample. Call this new score \\(D_\\text{test}\\). The above is a thought experiment. It allows us to explore the distinction between accuracy measured in and out of sample, using a simple prediction scenario. (p. 211, emphasis in the original) We’ll see how to carry out such a thought experiment in the next section. 7.2.5.1 Overthinking: Simulated training and testing. McElreath plotted the results of such a thought experiment in implemented in R with the aid of his sim_train_test() function. If you’re interested in how the function pulls this off, execute the code below. sim_train_test For the sake of brevity, I am going to show the results of a simulation based on 1,000 simulations rather than McElreath’s 10,000. # I&#39;ve reduced this number by one order of magnitude to reduce computation time n_sim &lt;- 1e3 n_cores &lt;- 8 kseq &lt;- 1:5 # define the simulation function my_sim &lt;- function(k) { print(k); r &lt;- mcreplicate(n_sim, sim_train_test(N = n, k = k), mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) } # here&#39;s our dev object based on `N &lt;- 20` n &lt;- 20 dev_20 &lt;- sapply(kseq, my_sim) # here&#39;s our dev object based on N &lt;- 100 n &lt;- 100 dev_100 &lt;- sapply(kseq, my_sim) If you didn’t quite catch it, the simulation yields dev_20 and dev_100. We’ll want to convert them to tibbles, bind them together, and wrangle extensively before we’re ready to plot. dev_tibble &lt;- rbind(dev_20, dev_100) %&gt;% data.frame() %&gt;% mutate(statistic = rep(c(&quot;mean&quot;, &quot;sd&quot;), each = 2) %&gt;% rep(., times = 2), sample = rep(c(&quot;in&quot;, &quot;out&quot;), times = 2) %&gt;% rep(., times = 2), n = rep(c(&quot;n = 20&quot;, &quot;n = 100&quot;), each = 4)) %&gt;% pivot_longer(-(statistic:n)) %&gt;% pivot_wider(names_from = statistic, values_from = value) %&gt;% mutate(n = factor(n, levels = c(&quot;n = 20&quot;, &quot;n = 100&quot;)), n_par = str_extract(name, &quot;\\\\d+&quot;) %&gt;% as.double()) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par - .075, n_par + .075)) head(dev_tibble) ## # A tibble: 6 x 6 ## sample n name mean sd n_par ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 in n = 20 X1 55.8 5.77 0.925 ## 2 in n = 20 X2 54.7 5.50 1.92 ## 3 in n = 20 X3 51.6 4.24 2.92 ## 4 in n = 20 X4 51.2 3.87 3.92 ## 5 in n = 20 X5 51.1 3.54 4.92 ## 6 out n = 20 X1 57.5 6.79 1.08 Now we’re ready to make Figure 7.6. # for the annotation text &lt;- dev_tibble %&gt;% filter(n_par &gt; 1.5, n_par &lt; 2.5) %&gt;% mutate(n_par = ifelse(sample == &quot;in&quot;, n_par - 0.2, n_par + 0.29)) # plot! dev_tibble %&gt;% ggplot(aes(x = n_par, y = mean, ymin = mean - sd, ymax = mean + sd, group = sample, color = sample, fill = sample)) + geom_pointrange(shape = 21) + geom_text(data = text, aes(label = sample)) + scale_fill_manual(values = carto_pal(7, &quot;BurgYl&quot;)[c(5, 7)]) + scale_color_manual(values = carto_pal(7, &quot;BurgYl&quot;)[c(7, 5)]) + labs(title = &quot;Figure 7.6. Deviance in and out of sample.&quot;, x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme(legend.position = &quot;none&quot;, strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;transparent&quot;)) + facet_wrap(~ n, scale = &quot;free_y&quot;) Even with a substantially smaller \\(N\\), our simulation results matched up well with those in the text. Deviance is an assessment of predictive accuracy, not of truth. The true model, in terms of which predictors are included, is not guaranteed to produce the best predictions. Likewise a false model, in terms of which predictors are included, is not guaranteed to produce poor predictions. The point of this thought experiment is to demonstrate how deviance behaves, in theory. While deviance on training data always improves with additional predictor variables, deviance on future data may or may not, depending upon both the true data-generating process and how much data is available to precisely estimate the parameters. These facts form the basis for understanding both regularizing priors and information criteria. (p. 213) 7.3 Golem taming: regularization The root of overfitting is a model’s tendency to get overexcited by the training sample. When the priors are flat or nearly flat, the machine interprets this to mean that every parameter value is equally plausible. As a result, the model returns a posterior that encodes as much of the training sample–as represented by the likelihood function–as possible. One way to prevent a model from getting too excited by the training sample is to use a skeptical prior. By “skeptical,” I mean a prior that slows the rate of learning from the sample. The most common skeptical prior is a regularizing prior. Such a prior, when tuned properly, reduces overfitting while still allowing the model to learn the regular features of a sample. (p. 214, emphasis in the original) In case you were curious, here’s how you might make a version Figure 7.7 with ggplot2. tibble(x = seq(from = - 3.5, to = 3.5, by = 0.01)) %&gt;% mutate(a = dnorm(x, mean = 0, sd = 0.2), b = dnorm(x, mean = 0, sd = 0.5), c = dnorm(x, mean = 0, sd = 1.0)) %&gt;% pivot_longer(-x) %&gt;% ggplot(aes(x = x, y = value, fill = name, color = name, linetype = name)) + geom_area(alpha = 1/2, size = 1/2, position = &quot;identity&quot;) + scale_fill_manual(values = carto_pal(7, &quot;BurgYl&quot;)[7:5]) + scale_color_manual(values = carto_pal(7, &quot;BurgYl&quot;)[7:5]) + scale_linetype_manual(values = 1:3) + scale_x_continuous(&quot;parameter value&quot;, breaks = -3:3) + scale_y_continuous(NULL, breaks = NULL) + theme(legend.position = &quot;none&quot;) In our version of the plot, darker purple = more regularizing. To prepare for Figure 7.8, we need to simulate. This time we’ll wrap the basic simulation code we used before into a function we’ll call make_sim(). Our make_sim() function has two parameters, n and b_sigma, both of which come from McElreath’s simulation code. So you’ll note that instead of hard coding the values for n and b_sigma within the simulation, we’re leaving them adjustable (i.e., sim_train_test(N = n, k = k, b_sigma = b_sigma)). Also notice that instead of saving the simulation results as objects, like before, we’re just converting them to a data frame with the data.frame() function at the bottom. Our goal is to use make_sim() within a purrr::map2() statement. The result will be a nested data frame into which we’ve saved the results of 6 simulations based off of two sample sizes (i.e., n = c(20, 100)) and three values of \\(\\sigma\\) for our Gaussian \\(\\beta\\) prior (i.e., b_sigma = c(1, .5, .2)). library(rethinking) # I&#39;ve reduced this number by one order of magnitude to reduce computation time n_sim &lt;- 1e3 n_cores &lt;- 8 make_sim &lt;- function(n, b_sigma) { sapply(kseq, function(k) { print(k); r &lt;- mcreplicate(n_sim, sim_train_test(N = n, k = k, b_sigma = b_sigma), # this is an augmented line of code mc.cores = n_cores); c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) } ) %&gt;% # this is a new line of code data.frame() } s &lt;- crossing(n = c(20, 100), b_sigma = c(1, 0.5, 0.2)) %&gt;% mutate(sim = map2(n, b_sigma, make_sim)) %&gt;% unnest(sim) We’ll follow the same principles for wrangling these data as we did those from the previous simulation, dev_tibble. After wrangling, we’ll feed the data directly into the code for our version of Figure 7.8. # wrangle the simulation data s %&gt;% mutate(statistic = rep(c(&quot;mean&quot;, &quot;sd&quot;), each = 2) %&gt;% rep(., times = 3 * 2), sample = rep(c(&quot;in&quot;, &quot;out&quot;), times = 2) %&gt;% rep(., times = 3 * 2)) %&gt;% pivot_longer(-c(n:b_sigma, statistic:sample)) %&gt;% pivot_wider(names_from = statistic, values_from = value) %&gt;% mutate(n = str_c(&quot;n = &quot;, n) %&gt;% factor(., levels = c(&quot;n = 20&quot;, &quot;n = 100&quot;)), n_par = str_extract(name, &quot;\\\\d+&quot;) %&gt;% as.double()) %&gt;% # plot ggplot(aes(x = n_par, y = mean, group = interaction(sample, b_sigma))) + geom_line(aes(color = sample, size = b_sigma %&gt;% as.character())) + # this function contains the data from the previous simulation geom_point(data = dev_tibble, aes(group = sample, fill = sample), color = &quot;black&quot;, shape = 21, size = 2.5, stroke = .1) + scale_size_manual(values = c(1, 0.5, 0.2)) + scale_fill_manual(values = carto_pal(7, &quot;BurgYl&quot;)[c(7, 5)]) + scale_color_manual(values = carto_pal(7, &quot;BurgYl&quot;)[c(7, 5)]) + labs(x = &quot;number of parameters&quot;, y = &quot;deviance&quot;) + theme(legend.position = &quot;none&quot;, strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;transparent&quot;)) + facet_wrap(~ n, scale = &quot;free_y&quot;) Our results don’t perfectly align with those in the text. I suspect his is because we used 1e3 iterations, rather than the 1e4 of the text. If you’d like to wait all night long for the simulation to yield more stable results, be my guest. Regularizing priors are great, because they reduce overfitting. But if they are too skeptical, they prevent the model from learning from the data. When you encounter multilevel models in Chapter 13, you’ll see that their central device is to learn the strength of the prior from the data itself. So you can think of multilevel models as adaptive regularization, where the model itself tries to learn how skeptical it should be. (p. 216) I found this connection difficult to grasp for a long time. Practice now and hopefully it’ll sink in for you faster than it did me. 7.3.0.1 Rethinking: Ridge regression. Within the brms framework, you can do something like this with the horseshoe prior via the horseshoe() function. You can learn all about it from the horseshoe section of the brms reference manual (Bürkner, 2021i). Here’s an extract from the section: The horseshoe prior is a special shrinkage prior initially proposed by Carvalho et al. (2009). It is symmetric around zero with fat tails and an infinitely large spike at zero. This makes it ideal for sparse models that have many regression coefficients, although only a minority of them is non-zero. The horseshoe prior can be applied on all population-level effects at once (excluding the intercept) by using set_prior(\"horseshoe(1)\"). (p. 97) To dive even deeper into the horseshoe prior, check out Michael Betancourt’s (2018) tutorial, Bayes sparse regression. Gelman, Hill, and Vehtari cover the horseshoe prior with rstanarm in Section 12.7 of their (2020) text, Regression and other stories. I also have an example of the horseshoe prior (fit18.5) in Section 18.3 of my (2020c) ebook translation of Kruschke’s (2015) text. 7.4 Predicting predictive accuracy All of the preceding suggests one way to navigate overfitting and underfitting: Evaluate our models out-of-sample. But we do not have the out-of-sample, by definition, so how can we evaluate our models on it? There are two families of strategies: cross-validation and information criteria. These strategies try to guess how well models will perform, on average, in predicting new data. (p. 217, emphasis in the original) 7.4.1 Cross-validation. A popular strategy for estimating predictive accuracy is to actually test the model’s predictive accuracy on another sample. This is known as cross-validation, leaving out a small chunk of observations from our sample and evaluating the model on the observations that were left out. Of course we don’t want to leave out data. So what is usually done is to divide the sample in a number of chunks, called “folds.” The model is asked to predict each fold, after training on all the others. We then average over the score for each fold to get an estimate of out-of-sample accuracy. The minimum number of folds is 2. At the other extreme, you could make each point observation a fold and fit as many models as you have individual observations. (p. 217, emphasis in the original) Folds are typically equivalent in size and we often denote the total number of folds by \\(k\\), which means that the number of cases will get smaller as \\(k\\) increases. In the extreme \\(k = N\\). Leave-one-out cross-validation (LOO-CV) is the name for this popular type of cross-validation which uses the largest number of folds possible by including a single case in each fold (de Rooij &amp; Weeda, 2020; see Zhang &amp; Yang, 2015). This will be our approach. A practical difficulty with LOO-CV is it’s costly in terms of the time and memory required to refit the model \\(k = N\\) times. Happily, we have an approximation to pure LOO-CV. Vehtari, Gelman, and Gabry (2017) proposed Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO-CV) as an efficient way to approximate true LOO-CV. 7.5 Information criteria The second approach is the use of information criteria to compute an expected score out of sample. Information criteria construct a theoretical estimate of the relative out-of-sample KL divergence. (p. 219, emphasis in the original) The frequentist Akaike information criterion (AIC, Akaike, 1998) is the oldest and most restrictive. Among Bayesians, the deviance information criterion (DIC, D. J. Spiegelhalter et al., 2002) has been widely used for some time, now. For a great talk on the DIC, check out the authoritative David Spiegelhalter’s Retrospective read paper: Bayesian measure of model complexity and fit. However, the DIC is limited in that it presumes the posterior is multivariate Gaussian, which is not always the case. In this book, our focus will be on the widely applicable information criterion (WAIC, Watanabe, 2010), which does not impose assumptions on the shape of the posterior distribution. The WAIC both provides an estimate of out-of-sample deviance and converges with LOO-CV as \\(N \\rightarrow \\infty\\). The WAIC follows the formula \\[\\text{WAIC}(y, \\Theta) = -2 \\big (\\text{lppd} - \\underbrace{\\sum_i \\operatorname{var}_\\theta \\log p(y_i | \\theta)}_\\text{penalty term} \\big),\\] where \\(y\\) is the data, \\(\\Theta\\) is the posterior distribution, and \\(\\text{lppd}\\) is the log-posterior-predictive-density from before. The penalty term is also referred to at the effective number of parameters, \\(p_\\text{WAIC}\\). There are a few ways to compute the WAIC with brms, including with the waic() function. 7.5.0.1 Overthinking: WAIC calculation. Here is how to fit the pre-WAIC model with brms. data(cars) b7.m &lt;- brm(data = cars, family = gaussian, dist ~ 1 + speed, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 7, file = &quot;fits/b07.0m&quot;) Behold the posterior summary. print(b7.m) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: dist ~ 1 + speed ## Data: cars (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -17.54 6.14 -29.45 -5.13 1.00 3878 2503 ## speed 3.93 0.38 3.16 4.67 1.00 3872 2895 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 13.82 1.22 11.66 16.40 1.00 3662 2650 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now use the brms::log_lik() function to return the log-likelihood for each observation \\(i\\) at each posterior draw \\(s\\), where \\(S = 4{,}000\\). n_cases &lt;- nrow(cars) ll &lt;- log_lik(b7.m) %&gt;% data.frame() %&gt;% set_names(c(str_c(0, 1:9), 10:n_cases)) dim(ll) We have a \\(4{,}000 \\times 50\\) (i.e., \\(S \\times N\\)) data frame with posterior draws in rows and cases in columns. Computing the \\(\\text{lppd}\\), the “Bayesian deviance,” takes a bit of leg work. Recall the formula for \\(\\text{lppd}\\), \\[\\text{lppd}(y, \\Theta) = \\sum_i \\log \\frac{1}{S} \\sum_s p (y_i | \\Theta_s),\\] where \\(p (y_i | \\Theta_s)\\) is the likelihood of case \\(i\\) on posterior draw \\(s\\). Since log_lik() returns the pointwise log-likelihood, our first step is to exponentiate those values. For each case \\(i\\) (i.e., \\(\\sum_i\\)), we then take the average likelihood value [i.e., \\(\\frac{1}{S} \\sum_s p (y_i | \\Theta_s)\\)] and transform the result by taking its log [i.e., \\(\\log \\left (\\frac{1}{S} \\sum_s p (y_i | \\Theta_s) \\right )\\)]. Here we’ll save the pointwise solution as log_mu_l. log_mu_l &lt;- ll %&gt;% pivot_longer(everything(), names_to = &quot;i&quot;, values_to = &quot;loglikelihood&quot;) %&gt;% mutate(likelihood = exp(loglikelihood)) %&gt;% group_by(i) %&gt;% summarise(log_mean_likelihood = mean(likelihood) %&gt;% log()) ( lppd &lt;- log_mu_l %&gt;% summarise(lppd = sum(log_mean_likelihood)) %&gt;% pull(lppd) ) ## [1] -206.6265 It’s a little easier to compute the effective number of parameters, \\(p_\\text{WAIC}\\). First, let’s use a shorthand notation and define \\(V(y_i)\\) as the variance in log-likelihood for the \\(i^\\text{th}\\) case across all \\(S\\) samples. We define \\(p_\\text{WAIC}\\) as their sum \\[p_\\text{WAIC} = \\sum_{i=1}^N V (y_i).\\] We’ll save the pointwise results [i.e., \\(V (y_i)\\)] as v_i and their sum [i.e., \\(\\sum_{i=1}^N V (y_i)\\)] as pwaic. v_i &lt;- ll %&gt;% pivot_longer(everything(), names_to = &quot;i&quot;, values_to = &quot;loglikelihood&quot;) %&gt;% group_by(i) %&gt;% summarise(var_loglikelihood = var(loglikelihood)) pwaic &lt;- v_i %&gt;% summarise(pwaic = sum(var_loglikelihood)) %&gt;% pull() pwaic ## [1] 4.111924 Now we can finally plug our hand-made lppd and pwaic values into the formula \\(-2 (\\text{lppd} - p_\\text{WAIC})\\) to compute the WAIC. Compare it to the value returned by the brms waic() function. -2 * (lppd - pwaic) ## [1] 421.4769 waic(b7.m) ## ## Computed from 4000 by 50 log-likelihood matrix ## ## Estimate SE ## elpd_waic -210.7 8.2 ## p_waic 4.1 1.6 ## waic 421.5 16.4 ## ## 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. Before we move on, did you notice the elpd_waic row in the tibble returned by thewaic() function? That value is the lppd minus the pwaic, but without multiplying the result by -2. E.g., (lppd - pwaic) ## [1] -210.7384 Finally, here’s how we compute the WAIC standard error. tibble(lppd = pull(log_mu_l, log_mean_likelihood), p_waic = pull(v_i, var_loglikelihood)) %&gt;% mutate(waic_vec = -2 * (lppd - p_waic)) %&gt;% summarise(waic_se = sqrt(n_cases * var(waic_vec))) ## # A tibble: 1 x 1 ## waic_se ## &lt;dbl&gt; ## 1 16.4 If you’d like the pointwise values from brms::waic(), just index. waic(b7.m)$pointwise %&gt;% head() ## elpd_waic p_waic waic ## [1,] -3.649870 0.02182091 7.299741 ## [2,] -4.023347 0.09214764 8.046694 ## [3,] -3.684137 0.02150916 7.368275 ## [4,] -3.995993 0.05840990 7.991986 ## [5,] -3.588907 0.01056900 7.177814 ## [6,] -3.741526 0.02125830 7.483051 7.5.1 Comparing CV, PSIS, and WAIC. Here we update our make_sim() to accommodate the simulation for Figure 7.9. make_sim &lt;- function(n, k, b_sigma) { r &lt;- mcreplicate(n_sim, sim_train_test(N = n, k = k, b_sigma = b_sigma, WAIC = T, LOOCV = T, LOOIC = T), mc.cores = n_cores) t &lt;- tibble( deviance_os = mean(unlist(r[2, ])), deviance_w = mean(unlist(r[3, ])), deviance_p = mean(unlist(r[11, ])), deviance_c = mean(unlist(r[19, ])), error_w = mean(unlist(r[7, ])), error_p = mean(unlist(r[15, ])), error_c = mean(unlist(r[20, ])) ) return(t) } Computing all three WAIC, PSIS-LOO-CV, and actual LOO-CV for many models across multiple combinations of k and b_sigma takes a long time–many hours. If you plan on running this code to replicate the figure on your own, take it for a spin first with n_sim set to something small like 10 to get a sense of the speed of your machine and the nature of the output. Alternatively, consider running the simulation for one combination of k and b_sigma at a time. n_sim &lt;- 1e3 n_cores &lt;- 8 s &lt;- crossing(n = c(20, 100), k = 1:5, b_sigma = c(0.5, 100)) %&gt;% mutate(sim = pmap(list(n, k, b_sigma), make_sim)) %&gt;% unnest(sim) Now we have our results saved as s, we’re ready to make our version of Figure 7.9. I’m going to deviate from McElreath’s version a bit and express the two average deviance plots on the left column into two columns. To my eyes, the reduced clutter makes it easier to track what I’m looking at. s %&gt;% pivot_longer(deviance_w:deviance_c) %&gt;% mutate(criteria = ifelse(name == &quot;deviance_w&quot;, &quot;WAIC&quot;, ifelse(name == &quot;deviance_p&quot;, &quot;PSIS&quot;, &quot;CV&quot;))) %&gt;% mutate(n = factor(str_c(&quot;N = &quot;, n), levels = str_c(&quot;N = &quot;, c(20, 100))), b_sigma = factor(str_c(&quot;sigma = &quot;, b_sigma), levels = (str_c(&quot;sigma = &quot;, c(0.5, 100))))) %&gt;% ggplot(aes(x = k)) + geom_point(aes(y = deviance_os, shape = b_sigma), show.legend = F) + geom_line(aes(y = value, color = criteria)) + scale_shape_manual(values = c(19, 1)) + scale_color_manual(values = carto_pal(7, &quot;BurgYl&quot;)[c(3, 5, 7)]) + labs(x = &quot;number of parameters (k)&quot;, y = &quot;average deviance&quot;) + theme(strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;transparent&quot;)) + facet_grid(n ~ b_sigma, scales = &quot;free_y&quot;) Although our specific values vary, the overall patterns in our simulations are well aligned with those in the text. The CV, PSIS, and WAIC all did a reasonable job approximating out-of-sample deviance. Now we’ll follow the same sensibilities to make our version of the right column of McElreath’s Figure 7.9. s %&gt;% pivot_longer(error_w:error_c) %&gt;% mutate(criteria = ifelse(name == &quot;error_w&quot;, &quot;WAIC&quot;, ifelse(name == &quot;error_p&quot;, &quot;PSIS&quot;, &quot;CV&quot;))) %&gt;% mutate(n = factor(str_c(&quot;N = &quot;, n), levels = str_c(&quot;N = &quot;, c(20, 100))), b_sigma = factor(str_c(&quot;sigma = &quot;, b_sigma), levels = (str_c(&quot;sigma = &quot;, c(0.5, 100))))) %&gt;% ggplot(aes(x = k)) + geom_line(aes(y = value, color = criteria)) + scale_shape_manual(values = c(19, 1)) + scale_color_manual(values = carto_pal(7, &quot;BurgYl&quot;)[c(3, 5, 7)]) + labs(x = &quot;number of parameters (k)&quot;, y = &quot;average error (test deviance)&quot;) + theme(strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;transparent&quot;)) + facet_grid(n ~ b_sigma, scales = &quot;free_y&quot;) As in the text, the mean error was very similar across the three criteria. When \\(N = 100\\), they were nearly identical. When \\(N = 20\\), the WAIC was slightly better than the PSIS, which was slightly better than the CV. As we will see, the PSIS-LOO-CV has the advantage of a built-in diagnostic. 7.6 Model comparison In the sections to follow, we’ll practice the model comparison approach, as opposed to the widely-used model selection approach. 7.6.1 Model mis-selection. We must keep in mind the lessons of the previous chapters: Inferring cause and making predictions are different tasks. Cross-validation and WAIC aim to find models that make good predictions. They don’t solve any causal inference problem. If you select a model based only on expected predictive accuracy, you could easily be confounded. The reason is that backdoor paths do give us valid information about statistical associations in the data. So they can improve prediction, as long as we don’t intervene in the system and the future is like the past. But recall that our working definition of knowing a cause is that we can predict the consequences of an intervention. So a good PSIS or WAIC score does not in general indicate a good causal model. (p. 226) If you have been following along and fitting the model on your own and saving them as external .rds files the way I have, you can use the readRDS() to retrieve them. b6.6 &lt;- readRDS(&quot;fits/b06.06.rds&quot;) b6.7 &lt;- readRDS(&quot;fits/b06.07.rds&quot;) b6.8 &lt;- readRDS(&quot;fits/b06.08.rds&quot;) With our brms paradigm, we also use the waic() function. Both the rethinking and brms packages get their functionality for the waic() and related functions from the loo package (Vehtari et al., 2017; Vehtari, Gabry, et al., 2019; Yao et al., 2018). Since the brms::brm() function fits the models with HMC, we don’t need to set a seed before calling waic() the way McElreath did with his rethinking::quap() model. We’re already drawn from the posterior. waic(b6.7) ## ## Computed from 4000 by 100 log-likelihood matrix ## ## Estimate SE ## elpd_waic -180.7 6.7 ## p_waic 3.5 0.5 ## waic 361.5 13.4 The WAIC estimate and its standard error are on the bottom row. The \\(p_\\text{WAIC}\\)–what McElreath’s output called the penalty–and its SE are stacked atop that. And look there on the top row. Remember how we pointed out, above, that we get the WAIC by multiplying (lppd - pwaic) by -2? Well, if you just do the subtraction without multiplying the result by -2, you get the elpd_waic. File that away. It’ll become important in a bit. In McElreath’s output, that was called the lppd. Following the version 2.8.0 update, part of the suggested workflow for using information criteria with brms (i.e., execute ?loo.brmsfit) is to add the estimates to the brm() fit object itself. You do that with the add_criterion() function. Here’s how we’d do so with b6.7. b6.7 &lt;- add_criterion(b6.7, criterion = &quot;waic&quot;) With that in place, here’s how you’d extract the WAIC information from the fit object. b6.7$criteria$waic ## ## Computed from 4000 by 100 log-likelihood matrix ## ## Estimate SE ## elpd_waic -180.7 6.7 ## p_waic 3.5 0.5 ## waic 361.5 13.4 Why would I go through all that trouble?, you might ask. Well, two reasons. First, now your WAIC information is saved with all the rest of your fit output, which can be convenient. But second, it sets you up to use the loo_compare() function to compare models by their information criteria. To get a sense of that workflow, here we use add_criterion() for the next three models. Then we’ll use loo_compare(). # compute and save the WAIC information for the next three models b6.6 &lt;- add_criterion(b6.6, criterion = &quot;waic&quot;) b6.8 &lt;- add_criterion(b6.8, criterion = &quot;waic&quot;) # compare the WAIC estimates w &lt;- loo_compare(b6.6, b6.7, b6.8, criterion = &quot;waic&quot;) print(w) ## elpd_diff se_diff ## b6.7 0.0 0.0 ## b6.8 -20.5 4.9 ## b6.6 -22.1 5.8 print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b6.7 0.0 0.0 -180.7 6.7 3.5 0.5 361.5 13.4 ## b6.8 -20.5 4.9 -201.2 5.4 2.5 0.3 402.5 10.8 ## b6.6 -22.1 5.8 -202.9 5.7 1.5 0.2 405.7 11.3 You don’t have to save those results as an object like we just did with w. But that’ll serve some pedagogical purposes in just a bit. With respect to the output, notice the elpd_diff column and the adjacent se_diff column. Those are our WAIC differences in the elpd metric. The models have been rank ordered from the highest (i.e., b6.7) to the highest (i.e., b6.6). The scores listed are the differences of b6.7 minus the comparison model. Since b6.7 is the comparison model in the top row, the values are naturally 0 (i.e., \\(x - x = 0\\)). But now here’s another critical thing to understand: Since the brms version 2.8.0 update, WAIC and LOO differences are no longer reported in the \\(-2 \\times x\\) metric. Remember how multiplying (lppd - pwaic) by -2 is a historic artifact associated with the frequentist \\(\\chi^2\\) test? We’ll, the makers of the loo package aren’t fans and they no longer support the conversion. So here’s the deal. The substantive interpretations of the differences presented in an elpd_diff metric will be the same as if presented in a WAIC metric. But if we want to compare our elpd_diff results to those in the text, we will have to multiply them by -2. And also, if we want the associated standard error in the same metric, we’ll need to multiply the se_diff column by 2. You wouldn’t multiply by -2 because that would return a negative standard error, which would be silly. Here’s a quick way to do those conversions. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) ## waic_diff se ## b6.7 0.00000 0.000000 ## b6.8 41.03941 9.832777 ## b6.6 44.28358 11.564703 Now those match up reasonably well with the values in McElreath’s dWAIC and dSE columns. One more thing. On page 227, and on many other pages to follow in the text, McElreath used the rethinking::compare() function to return a rich table of information about the WAIC information for several models. If we’re tricky, we can do something similar with loo_compare. To learn how, let’s peer further into the structure of our w object. str(w) ## &#39;compare.loo&#39; num [1:3, 1:8] 0 -20.52 -22.14 0 4.92 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:3] &quot;b6.7&quot; &quot;b6.8&quot; &quot;b6.6&quot; ## ..$ : chr [1:8] &quot;elpd_diff&quot; &quot;se_diff&quot; &quot;elpd_waic&quot; &quot;se_elpd_waic&quot; ... When we used print(w), a few code blocks earlier, it only returned two columns. It appears we actually have eight. We can see the full output with the simplify = F argument. print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b6.7 0.0 0.0 -180.7 6.7 3.5 0.5 361.5 13.4 ## b6.8 -20.5 4.9 -201.2 5.4 2.5 0.3 402.5 10.8 ## b6.6 -22.1 5.8 -202.9 5.7 1.5 0.2 405.7 11.3 The results are quite analogous to those from rethinking::compare(). Again, the difference estimates are in the metric of the \\(\\text{elpd}\\). But the interpretation is the same and we can convert them to the traditional information criteria metric with simple multiplication. As we’ll see later, this basic workflow also applies to the PSIS-LOO. Okay, we’ve deviated a bit from the text. Let’s reign things back in and note that right after McElreath’s R code 7.26, he wrote: “PSIS will give you almost identical values. You can add func=PSIS to the compare call to check” (p. 227). Our brms::loo_compare() function has a similar argument, but it’s called criterion. We set it to criterion = \"waic\" to compare the models by the WAIC. What McElreath is calling func=PSIS, we’d call criterion = \"loo\". Either way, we’re asking the software the compare the models using leave-one-out cross-validation with Pareto-smoothed importance sampling. b6.6 &lt;- add_criterion(b6.6, criterion = &quot;loo&quot;) b6.7 &lt;- add_criterion(b6.7, criterion = &quot;loo&quot;) b6.8 &lt;- add_criterion(b6.8, criterion = &quot;loo&quot;) # compare the WAIC estimates loo_compare(b6.6, b6.7, b6.8, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b6.7 0.0 0.0 -180.7 6.7 3.5 0.5 361.5 13.4 ## b6.8 -20.5 4.9 -201.3 5.4 2.5 0.3 402.5 10.8 ## b6.6 -22.1 5.8 -202.9 5.7 1.5 0.2 405.7 11.3 Yep, the LOO values are very similar to those from the WAIC. Anyway, at the bottom of page 227, McElreath showed how to compute the standard error of the WAIC difference for models m6.7 and m6.8. Here’s the procedure for us. n &lt;- length(b6.7$criteria$waic$pointwise[, &quot;waic&quot;]) tibble(waic_b6.7 = b6.7$criteria$waic$pointwise[, &quot;waic&quot;], waic_b6.8 = b6.8$criteria$waic$pointwise[, &quot;waic&quot;]) %&gt;% mutate(diff = waic_b6.7 - waic_b6.8) %&gt;% summarise(diff_se = sqrt(n * var(diff))) ## # A tibble: 1 x 1 ## diff_se ## &lt;dbl&gt; ## 1 9.83 Since we used different estimation methods (HMC versus the quadratic approximation) via different software (brms::brm() versus rethinking::quap()), we shouldn’t be surprised our values are a little different from those in the text. But we’re in the ballpark, for sure. For us, this value was in the second row of the second column in our w object. But remember we have to multiply that value by 2 to convert it from the \\(\\text{elpd}\\) metric to that of the WAIC. w[2, 2] * 2 ## [1] 9.832777 Presuming the difference is Gaussian distributed, here’s our 99% interval. (w[2, 1] * -2) + c(-1, 1) * (w[2, 2] * 2) * 2.6 ## [1] 15.47419 66.60463 With our brms paradigm, we won’t get a comparison plot by inserting loo_compare(b6.6, b6.7, b6.8, criterion = \"waic\") within plot(). But with a little [] subsetting and light wrangling, we can convert the contents of our w object to a format suitable for plotting the WAIC estimates with ggplot2. w[, 7:8] %&gt;% data.frame() %&gt;% rownames_to_column(&quot;model_name&quot;) %&gt;% mutate(model_name = fct_reorder(model_name, waic, .desc = T)) %&gt;% ggplot(aes(x = waic, y = model_name, xmin = waic - se_waic, xmax = waic + se_waic)) + geom_pointrange(color = carto_pal(7, &quot;BurgYl&quot;)[7], fill = carto_pal(7, &quot;BurgYl&quot;)[5], shape = 21) + labs(title = &quot;My custom WAIC plot&quot;, x = NULL, y = NULL) + theme(axis.ticks.y = element_blank()) We don’t get the deviance points with this method, but that’s okay. Our primary focus is on the WAIC and its standard errors. Here’s how to hand-compute the standard error for the difference between b6.6 and b6.8. tibble(waic_b6.6 = waic(b6.6)$pointwise[, &quot;waic&quot;], waic_b6.8 = waic(b6.8)$pointwise[, &quot;waic&quot;]) %&gt;% mutate(diff = waic_b6.6 - waic_b6.8) %&gt;% summarise(diff_se = sqrt(n * var(diff))) ## # A tibble: 1 x 1 ## diff_se ## &lt;dbl&gt; ## 1 4.74 Unlike rethinking::compare(), the loo_compare() function will not allow us to so easily pull this value by indexing with @dSE. loo_compare(b6.6, b6.7, b6.8, criterion = &quot;waic&quot;) %&gt;% str() ## &#39;compare.loo&#39; num [1:3, 1:8] 0 -20.52 -22.14 0 4.92 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:3] &quot;b6.7&quot; &quot;b6.8&quot; &quot;b6.6&quot; ## ..$ : chr [1:8] &quot;elpd_diff&quot; &quot;se_diff&quot; &quot;elpd_waic&quot; &quot;se_elpd_waic&quot; ... However, if you really want that value, make a simple WAIC comparison between b6.6 and b6.8. loo_compare(b6.6, b6.8, criterion = &quot;waic&quot;) ## elpd_diff se_diff ## b6.8 0.0 0.0 ## b6.6 -1.6 2.4 The value we’re looking for is in the second row of the se_diff column. But remember this is in the \\(\\text{elpd}\\) metric. Here’s the conversion. loo_compare(b6.6, b6.8, criterion = &quot;waic&quot;)[2, 2] * 2 ## [1] 4.743149 Unlike rethinking::compare(), our brms::loo_compare() (even when we print(simplify = T)) does not contain a weight column. Don’t worry; we can still compute them. We’ll just have to do so with a different function. Before we do, here’s the equation they’re based on: \\[w_i = \\frac{\\exp(-0.5 \\Delta_i)}{\\sum_j \\exp(-0.5 \\Delta_j)},\\] where \\(w_i\\) is the weight for the \\(i^\\text{th}\\) model and \\(\\Delta_i\\) is the difference between that model and the WAIC for the best one in the comparison set. If you want to get those WAIC weights, you can use the brms::model_weights() function like so. model_weights(b6.6, b6.7, b6.8, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b6.6 b6.7 b6.8 ## 0 1 0 In his endnote #130, McElreath discussed how these WAIC weights can be used for model averaging. From the endnote, we read: The first edition had a section on model averaging, but the topic has been dropped in this edition to save space. The approach is really focused on prediction, not inference, and so it doesn’t fit the flow of the second edition. But it is an important approach. (p. 565) A variety of approaches to model averaging are available with brms and I covered them in my (2020a) ebook translation of the first edition of McElreath’s text. 7.6.1.1 Rethinking: WAIC metaphors. Think of models as race horses. In any particular race, the best horse may not win. But it’s more likely to win than is the worst horse. And when the winning horse finishes in half the time of the second-place horse, you can be pretty sure the winning horse is also the best. But if instead it’s a photo-finish, with a near tie between first and second place, then it is much harder to be confident about which is the best horse. (p. 230) 7.6.2 Outliers and other illusions. Time to bring back the WaffleDivorce data. data(WaffleDivorce, package = &quot;rethinking&quot;) d &lt;- WaffleDivorce %&gt;% mutate(d = rethinking::standardize(Divorce), m = rethinking::standardize(Marriage), a = rethinking::standardize(MedianAgeMarriage)) rm(WaffleDivorce) Refit the divorce models from Section 5.1. b5.1 &lt;- brm(data = d, family = gaussian, d ~ 1 + a, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, sample_prior = T, file = &quot;fits/b05.01&quot;) b5.2 &lt;- brm(data = d, family = gaussian, d ~ 1 + m, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.02&quot;) b5.3 &lt;- brm(data = d, family = gaussian, d ~ 1 + m + a, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.03&quot;) Compute and save the LOO estimates for each. b5.1 &lt;- add_criterion(b5.1, criterion = &quot;loo&quot;) b5.2 &lt;- add_criterion(b5.2, criterion = &quot;loo&quot;) b5.3 &lt;- add_criterion(b5.3, criterion = &quot;loo&quot;) Now compare the models by the PSIS-LOO-CV. loo_compare(b5.1, b5.2, b5.3, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b5.1 0.0 0.0 -62.9 6.4 3.6 1.8 125.7 12.8 ## b5.3 -1.0 0.4 -63.8 6.4 4.8 1.9 127.7 12.9 ## b5.2 -6.8 4.6 -69.7 4.9 3.0 0.9 139.3 9.9 Like in the text, our b5.1 has the best LOO estimate, but only by a little bit when compared to b5.3. Unlike McElreath reported in the text, we did not get a warning message from loo_compare(). Let’s investigate more carefully with the loo() function. loo(b5.3) ## ## Computed from 4000 by 50 log-likelihood matrix ## ## Estimate SE ## elpd_loo -63.8 6.4 ## p_loo 4.8 1.9 ## looic 127.7 12.9 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 49 98.0% 901 ## (0.5, 0.7] (ok) 1 2.0% 127 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. One of the observations was in the (ok) range, but none were in the (bad) or (very bad) ranges. By opening the loo package directly, we gain access to the pareto_k_ids() function, which will help us identify which observation crossed out of the (good) range into the (ok). library(loo) loo(b5.3) %&gt;% pareto_k_ids(threshold = 0.5) ## [1] 13 The number 13 refers to the corresponding row in the data used to fit the model. We can access that row directly with the dplyr::slice() function. d %&gt;% slice(13) %&gt;% select(Location:Loc) ## Location Loc ## 1 Idaho ID Here we subset the 13th cell in the loo::pareto_k_values() output to see what that value was. pareto_k_values(loo(b5.3))[13] ## [1] 0.6813662 Alternatively, we could have extracted that value from our b5.3 fit object like so. b5.3$criteria$loo$diagnostics$pareto_k[13] ## [1] 0.6813662 0.68 is a little high, but not high enough to cause loo() to return a warning message. Once a Pareto \\(k\\) value crosses the 0.7 threshold, though, the loo package will bark. Before we make our version of Figure 7.10, we’ll want to compute the WAIC for b5.3. That will give us access to the \\(p_\\text{WAIC}\\). b5.3 &lt;- add_criterion(b5.3, &quot;waic&quot;, file = &quot;fits/b05.03&quot;) We’re ready to make our version of Figure 7.10. tibble(pareto_k = b5.3$criteria$loo$diagnostics$pareto_k, p_waic = b5.3$criteria$waic$pointwise[, &quot;p_waic&quot;], Loc = pull(d, Loc)) %&gt;% ggplot(aes(x = pareto_k, y = p_waic, color = Loc == &quot;ID&quot;)) + geom_vline(xintercept = .5, linetype = 2, color = &quot;black&quot;, alpha = 1/2) + geom_point(aes(shape = Loc == &quot;ID&quot;)) + geom_text(data = . %&gt;% filter(p_waic &gt; 0.5), aes(x = pareto_k - 0.03, label = Loc), hjust = 1) + scale_color_manual(values = carto_pal(7, &quot;BurgYl&quot;)[c(5, 7)]) + scale_shape_manual(values = c(1, 19)) + labs(subtitle = &quot;Gaussian model (b5.3)&quot;) + theme(legend.position = &quot;none&quot;) For both the Pareto \\(k\\) and the \\(p_\\text{WAIC}\\), our values are not as extreme as those McElreath reported in the text. I’m not sure if this is a consequence of us using HMC or due to recent algorithmic changes for the Stan/loo teams. But at least the overall pattern is the same. Idaho is the most extreme/influential case. In the text (p. 232), McElreath reported the effective number of parameters for b5.3 was nearly 6. We can look at this with the waic() function. waic(b5.3) ## ## Computed from 4000 by 50 log-likelihood matrix ## ## Estimate SE ## elpd_waic -63.7 6.3 ## p_waic 4.6 1.8 ## waic 127.3 12.7 ## ## 2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. Our \\(p_\\text{WAIC}\\) was about 4.6, which is still a little high due to Idaho but not quite as high as in the text. There is some concern that Idaho is putting us at risk of overfitting due to the large influence it has on the posterior. What can be done about this? There is a tradition of dropping outliers. People sometimes drop outliers even before a model is fit, based only on standard deviations from the mean outcome value. You should never do that–a point can only be unexpected and highly influential in light of a model. After you fit a model, the picture changes. If there are only a few outliers, and you are sure to report results both with and without them, dropping outliers might be okay. But if there are several outliers and we really need to model them, what then? A basic problem here is that the Gaussian error model is easily surprised. (p. 232) This surprise has to do with the thinness of its tails relative to those of the Student’s \\(t\\)-distribution. We can get a sense of this with Figure 7.11. tibble(x = seq(from = -6, to = 6, by = 0.01)) %&gt;% mutate(Gaussian = dnorm(x), `Student-t` = dstudent(x)) %&gt;% pivot_longer(-x, names_to = &quot;likelihood&quot;, values_to = &quot;density&quot;) %&gt;% mutate(`minus log density` = -log(density)) %&gt;% pivot_longer(contains(&quot;density&quot;)) %&gt;% ggplot(aes(x = x, y = value, group = likelihood, color = likelihood)) + geom_line() + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[6], &quot;black&quot;)) + ylim(0, NA) + labs(x = &quot;value&quot;, y = NULL) + theme(strip.background = element_blank()) + facet_wrap(~ name, scales = &quot;free_y&quot;) I’m a little baffled as to why our curves don’t quite match up with those in the text. The Gaussian curves were made using the dnorm() function with the default settings, which are \\(\\operatorname{Normal}(0, 1)\\). The Student-\\(t\\) curves were made with McElreath’s rethinking::dstudent()function with the default settings, which are \\(\\operatorname{Student-t}(2, 0, 1)\\)–you get the same results is you use dt(x, df = 2). Discrepancies aside, the main point in the text still stands. The \\(t\\) distribution, especially as \\(\\nu \\rightarrow 1\\), has thicker tails than the Gaussian. As a consequence, it is more robust to extreme values. The brms package will allow us to fit a Student \\(t\\) model. Just set family = student. We are at liberty to estimate \\(\\nu\\) along with the other parameters in the model or to set it to a constant. We’ll follow McElreath and fix nu = 2. Note that brms syntax requires we do so within a bf() statement. b5.3t &lt;- brm(data = d, family = student, bf(d ~ 1 + m + a, nu = 2), prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 5, file = &quot;fits/b05.03t&quot;) Check the summary. print(b5.3t) ## Family: student ## Links: mu = identity; sigma = identity; nu = identity ## Formula: d ~ 1 + m + a ## nu = 2 ## Data: d (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.02 0.10 -0.17 0.22 1.00 4082 3105 ## m 0.05 0.21 -0.33 0.47 1.00 3413 2675 ## a -0.70 0.15 -0.99 -0.41 1.00 3531 3041 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.58 0.09 0.43 0.77 1.00 4379 2728 ## nu 2.00 0.00 2.00 2.00 1.00 4000 4000 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we use the add_criterion() function compute both the LOO-CV and the WAIC and add them to the model fit object. b5.3t &lt;- add_criterion(b5.3t, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) Now we might remake the plot from Figure 7.10, this time based on the \\(t\\) model. tibble(pareto_k = b5.3t$criteria$loo$diagnostics$pareto_k, p_waic = b5.3t$criteria$waic$pointwise[, &quot;p_waic&quot;], Loc = pull(d, Loc)) %&gt;% ggplot(aes(x = pareto_k, y = p_waic, color = Loc == &quot;ID&quot;)) + geom_point(aes(shape = Loc == &quot;ID&quot;)) + geom_text(data = . %&gt;% filter(Loc %in% c(&quot;ID&quot;, &quot;ME&quot;)), aes(x = pareto_k - 0.01, label = Loc), hjust = 1) + scale_color_manual(values = carto_pal(7, &quot;BurgYl&quot;)[c(5, 7)]) + scale_shape_manual(values = c(1, 19)) + labs(subtitle = &quot;Student-t model (b5.3t)&quot;) + theme(legend.position = &quot;none&quot;) The high points in both the Pareto \\(k\\) and \\(p_\\text{WAIC}\\) are much smaller and both ID and ME are now closer to the center of the bivariate distribution. We can formally compare the models with the WAIC and the LOO. loo_compare(b5.3, b5.3t, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b5.3 0.0 0.0 -63.7 6.3 4.6 1.8 127.3 12.7 ## b5.3t -2.8 2.9 -66.5 5.8 6.3 1.0 133.0 11.6 loo_compare(b5.3, b5.3t, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b5.3 0.0 0.0 -63.8 6.4 4.8 1.9 127.7 12.9 ## b5.3t -2.6 3.0 -66.5 5.8 6.2 1.0 132.9 11.6 For both criteria, the standard error for the difference was about the same size as the difference itself. This suggests the models were close and hard to distinguish with respect to their fit to the data. This will not always be the case. Just for kicks and giggles, let’s compare the parameter summaries for the two models in a coefficient plot. bind_rows(posterior_samples(b5.3), posterior_samples(b5.3t)) %&gt;% mutate(fit = rep(c(&quot;Gaussian (b5.3)&quot;, &quot;Student-t (b5.3t)&quot;), each = n() / 2)) %&gt;% pivot_longer(b_Intercept:sigma) %&gt;% mutate(name = factor(name, levels = c(&quot;b_Intercept&quot;, &quot;b_a&quot;, &quot;b_m&quot;, &quot;sigma&quot;), labels = c(&quot;alpha&quot;, &quot;beta[a]&quot;, &quot;beta[m]&quot;, &quot;sigma&quot;))) %&gt;% ggplot(aes(x = value, y = fit, color = fit)) + stat_pointinterval(.width = .95, size = 1) + scale_color_manual(values = c(carto_pal(7, &quot;BurgYl&quot;)[6], &quot;black&quot;)) + labs(x = &quot;posterior&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, strip.background = element_rect(fill = alpha(carto_pal(7, &quot;BurgYl&quot;)[1], 1/4), color = &quot;transparent&quot;), strip.text = element_text(size = 12)) + facet_wrap(~ name, ncol = 1, labeller = label_parsed) Overall, the coefficients are very similar between the two models. The most notable difference is in \\(\\sigma\\). This is, in part, because \\(\\sigma\\) has a slightly different meaning for Student-\\(t\\) models than it does for the Gaussian. For both, we can refer to \\(\\sigma\\) as the scale parameter, with is a measure of spread or variability around the mean. However, the scale is the same thing as the standard deviation only for the Gaussian. Kruschke walked this out nicely: It is important to understand that the scale parameter \\(\\sigma\\) in the \\(t\\) distribution is not the standard deviation of the distribution. (Recall that the standard deviation is the square root of the variance, which is the expected value of the squared deviation from the mean, as defined back in Equation 4.8, p. 86.) The standard deviation is actually larger than \\(\\sigma\\) because of the heavy tails. In fact, when \\(\\nu\\) drops below 2 (but is still \\(\\geq 1\\)), the standard deviation of the mathematical \\(t\\) distribution goes to infinity. (2015, p. 159) The variance for \\(\\operatorname{Student-t}(\\nu, 0, 1)\\) is \\[\\frac{\\nu}{\\nu-2} \\text{ for } \\nu &gt;2, \\;\\;\\; \\infty \\text{ for } 1 &lt; \\nu \\leq 2,\\] and the standard deviation is the square of that. To give a sense of how that formula works, here are the standard deviation values when \\(\\nu\\) ranges from 2.1 to 20. # for the annotation text &lt;- tibble(nu = c(1.35, 20), sd = c(sqrt(2.1 / (2.1 - 2)), 0.875), angle = c(90, 0), hjust = 1, label = &quot;asymptote&quot;) # wrangle tibble(nu = seq(from = 2.1, to = 20, by = 0.01)) %&gt;% mutate(var = nu / (nu - 2)) %&gt;% mutate(sd = sqrt(var)) %&gt;% # plot ggplot(aes(x = nu, y = sd)) + geom_hline(yintercept = 1, color = carto_pal(7, &quot;BurgYl&quot;)[2], linetype = 2) + geom_vline(xintercept = 2, color = carto_pal(7, &quot;BurgYl&quot;)[2], linetype = 2) + geom_text(data = text, aes(label = label, angle = angle, hjust = hjust), color = carto_pal(7, &quot;BurgYl&quot;)[3]) + geom_line(color = carto_pal(7, &quot;BurgYl&quot;)[7]) + scale_x_continuous(expression(nu), breaks = c(2, 1:4 * 5)) + labs(subtitle = expression(Student-t(nu*&#39;, &#39;*0*&#39;, &#39;*1)), y = expression(standard~deviation*&quot;, &quot;*~sqrt(nu/(nu-2)))) As \\(\\nu \\rightarrow \\infty\\), the standard deviation approaches the scale, \\(\\sigma\\). We’ll have more practice with robust Student’s \\(t\\) regression later on. In the mean time, you might check out chapters 16, 17, 18, 19, and 20 from my (2020c) ebook translation of Kruschke’s text. I’ve also blogged on using the robust Student’s \\(t\\) for regression and correlations. Gelman et al. (2020) covered robust Student’s \\(t\\) regression in Section 15.6, too. 7.6.2.1 Rethinking: The Curse of Tippecanoe. One concern with model comparison is, if we try enough combinations and transformations of predictors, we might eventually find a model that fits any sample very well. But this fit will be badly overfit, unlikely to generalize…. Fiddling with and constructing many predictor variables is a great way to find coincidences, but not necessarily a great way to evaluate hypotheses. However, fitting many possible models isn’t always a dangerous idea, provided some judgment is exercised in weeding down the list of variables at the start. There are two scenarios in which this strategy appears defensible. First, sometimes all one wants to do is explore a set of data, because there are no clear hypotheses to evaluate. This is rightly labeled pejoratively as data dredging, when one does not admit to it. But when used together with model averaging, and freely admitted, it can be a way to stimulate future investigation. Second, sometimes we need to convince an audience that we have tried all of the combinations of predictors, because none of the variables seem to help much in prediction. (p. 234, emphasis in the original) 7.7 Summary Bonus: \\(R^2\\) talk At the beginning of the chapter (pp. 193–201), McElreath introduced \\(R^2\\) as a popular way to assess the variance explained in a model. He pooh-poohed it because of its tendency to overfit. It’s also limited in that it doesn’t generalize well outside of the single-level Gaussian framework, which will be a big deal for us starting in Chapter 10. However, if you should find yourself in a situation where \\(R^2\\) suits your purposes, the brms bayes_R2() function might be of use. Simply feeding a model brm() fit object into bayes_R2() will return the posterior mean, \\(SD\\), and 95% intervals. For example: bayes_R2(b5.3) %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## R2 0.332 0.088 0.147 0.481 With just a little data processing, you can get a tibble table of each of models’ \\(R^2\\) ‘Estimate.’ rbind(bayes_R2(b5.1), bayes_R2(b5.2), bayes_R2(b5.3)) %&gt;% data.frame() %&gt;% mutate(model = c(&quot;b5.1&quot;, &quot;b5.2&quot;, &quot;b5.3&quot;), r_square_posterior_mean = round(Estimate, digits = 3)) %&gt;% select(model, r_square_posterior_mean) ## model r_square_posterior_mean ## R2 b5.1 0.327 ## R2.1 b5.2 0.129 ## R2.2 b5.3 0.332 If you want the full distribution of the \\(R^2\\), you’ll need to add a summary = F argument. Note how this returns a numeric vector. r2_b5.1 &lt;- bayes_R2(b5.1, summary = F) r2_b5.1 %&gt;% glimpse() ## num [1:4000, 1] 0.258 0.281 0.217 0.225 0.463 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;R2&quot; If you want to use these in ggplot2, you’ll need to put them in tibbles or data frames. Here we do so for two of our model fits. r2 &lt;- cbind(bayes_R2(b5.1, summary = F), bayes_R2(b5.2, summary = F)) %&gt;% data.frame() %&gt;% set_names(str_c(&quot;b5.&quot;, 1:2)) r2 %&gt;% ggplot() + geom_density(aes(x = b5.1), alpha = 3/4, size = 0, fill = carto_pal(7, &quot;BurgYl&quot;)[4]) + geom_density(aes(x = b5.2), alpha = 3/4, size = 0, fill = carto_pal(7, &quot;BurgYl&quot;)[6]) + annotate(geom = &quot;text&quot;, x = c(.1, .34), y = 3.5, label = c(&quot;b5.2&quot;, &quot;b5.1&quot;), color = alpha(&quot;white&quot;, 3/4), family = &quot;Courier&quot;) + scale_x_continuous(NULL, limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(italic(R)^2~distributions)) If you do your work in a field where folks use \\(R^2\\) change, you might do that with a simple difference score. Here’s the \\(\\Delta R^2\\) plot. r2 %&gt;% mutate(diff = b5.2 - b5.1) %&gt;% ggplot(aes(x = diff, y = 0)) + stat_halfeye(point_interval = median_qi, .width = .95, fill = carto_pal(7, &quot;BurgYl&quot;)[5], color = carto_pal(7, &quot;BurgYl&quot;)[7]) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(Turns~out~b5.2~had~a~lower~italic(R)^2~than~b5.1), x = expression(Delta*italic(R)^2)) The brms package did not get these \\(R^2\\) values by traditional method used in, say, ordinary least squares estimation. To learn more about how the Bayesian \\(R^2\\) sausage is made, check out the paper by Gelman et al. (2019), R-squared for Bayesian regression models. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] loo_2.4.1 rethinking_2.13 rstan_2.21.2 StanHeaders_2.21.0-7 ## [5] patchwork_1.1.1 tidybayes_2.3.1 brms_2.15.0 Rcpp_1.0.6 ## [9] ggrepel_0.9.1 rcartocolor_2.0.0 forcats_0.5.1 stringr_1.4.0 ## [13] dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 ## [17] tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 RcppEigen_0.3.3.7.0 plyr_1.8.6 ## [5] igraph_1.2.6 svUnit_1.0.3 splines_4.0.4 crosstalk_1.1.0.1 ## [9] TH.data_1.0-10 rstantools_2.1.1 inline_0.3.17 digest_0.6.27 ## [13] htmltools_0.5.1.1 rsconnect_0.8.16 fansi_0.4.2 checkmate_2.0.0 ## [17] BH_1.75.0-0 magrittr_2.0.1 modelr_0.1.8 RcppParallel_5.0.2 ## [21] matrixStats_0.57.0 xts_0.12.1 sandwich_3.0-0 prettyunits_1.1.1 ## [25] colorspace_2.0-0 rvest_0.3.6 ggdist_2.4.0.9000 haven_2.3.1 ## [29] xfun_0.22 callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 ## [33] lme4_1.1-25 survival_3.2-7 zoo_1.8-8 glue_1.4.2 ## [37] gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 distributional_0.2.2 ## [41] pkgbuild_1.2.0 shape_1.4.5 abind_1.4-5 scales_1.1.1 ## [45] mvtnorm_1.1-1 DBI_1.1.0 miniUI_0.1.1.1 viridisLite_0.3.0 ## [49] xtable_1.8-4 stats4_4.0.4 DT_0.16 htmlwidgets_1.5.2 ## [53] httr_1.4.2 threejs_0.3.3 arrayhelpers_1.1-0 ellipsis_0.3.1 ## [57] pkgconfig_2.0.3 farver_2.0.3 dbplyr_2.0.0 utf8_1.1.4 ## [61] tidyselect_1.1.0 labeling_0.4.2 rlang_0.4.10 reshape2_1.4.4 ## [65] later_1.1.0.1 munsell_0.5.0 cellranger_1.1.0 tools_4.0.4 ## [69] cli_2.3.1 generics_0.1.0 broom_0.7.5 ggridges_0.5.2 ## [73] evaluate_0.14 fastmap_1.0.1 processx_3.4.5 knitr_1.31 ## [77] fs_1.5.0 nlme_3.1-152 mime_0.10 projpred_2.0.2 ## [81] xml2_1.3.2 compiler_4.0.4 bayesplot_1.8.0 shinythemes_1.1.2 ## [85] rstudioapi_0.13 gamm4_0.2-6 curl_4.3 reprex_0.3.0 ## [89] statmod_1.4.35 stringi_1.5.3 highr_0.8 ps_1.6.0 ## [93] Brobdingnag_1.2-6 lattice_0.20-41 Matrix_1.3-2 nloptr_1.2.2.2 ## [97] markdown_1.1 shinyjs_2.0.0 vctrs_0.3.6 pillar_1.5.1 ## [101] lifecycle_1.0.0 bridgesampling_1.0-0 estimability_1.3 httpuv_1.5.4 ## [105] R6_2.5.0 bookdown_0.21 promises_1.1.1 gridExtra_2.3 ## [109] codetools_0.2-18 boot_1.3-26 colourpicker_1.1.0 MASS_7.3-53 ## [113] gtools_3.8.2 assertthat_0.2.1 withr_2.4.1 shinystan_2.5.0 ## [117] multcomp_1.4-16 mgcv_1.8-33 hms_0.5.3 grid_4.0.4 ## [121] coda_0.19-4 minqa_1.2.4 rmarkdown_2.7 shiny_1.5.0 ## [125] lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 "],["conditional-manatees.html", "8 Conditional Manatees 8.1 Building an interaction 8.2 Symmetry of interactions 8.3 Continuous interactions 8.4 Summary Bonus: conditional_effects() Session info", " 8 Conditional Manatees Every model so far in this book has assumed that each predictor has an independent association with the mean of the outcome. What if we want to allow the association to be conditional?… To model deeper conditionality–where the importance of one predictor depends upon another predictor–we need interaction (also known as moderation). Interaction is a kind of conditioning, a way of allowing parameters (really their posterior distributions) to be conditional on further aspects of the data. The simplest kind of interaction, a linear interaction, is built by extending the linear modeling strategy to parameters within the linear model. So it is akin to placing epicycles on epicycles in the Ptolemaic and Kopernikan models. It is descriptive, but very powerful. More generally, interactions are central to most statistical models beyond the cozy world of Gaussian outcomes and linear models of the mean. In generalized linear models (GLMs, Chapter 10 and onwards), even when one does not explicitly define variables as interacting, they will always interact to some degree. Multilevel models induce similar effects. (McElreath, 2020a, p. 238, emphasis in the original) 8.1 Building an interaction “Africa is special” (p. 239). Let’s load the rugged data (Nunn &amp; Puga, 2012) to see one of the reasons why. data(rugged, package = &quot;rethinking&quot;) d &lt;- rugged rm(rugged) # may as well load this, too library(brms) For this chapter, we’ll take our plot theme from the ggthemes package (Arnold, 2019). library(tidyverse) library(ggthemes) theme_set( theme_pander() + theme(text = element_text(family = &quot;Times&quot;), panel.background = element_rect(color = &quot;black&quot;)) ) We’ll use the pander color scheme to help us make our first DAG. library(ggdag) dag_coords &lt;- tibble(name = c(&quot;R&quot;, &quot;G&quot;, &quot;C&quot;, &quot;U&quot;), x = c(1, 2, 3, 2), y = c(2, 2, 2, 1)) dagify(R ~ U, G ~ R + U + C, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;U&quot;), alpha = 1/2, size = 6, show.legend = F) + geom_point(x = 2, y = 1, size = 6, shape = 1, stroke = 3/4, color = palette_pander(n = 2)[2]) + geom_dag_text(color = &quot;black&quot;, family = &quot;Times&quot;) + geom_dag_edges() + scale_colour_pander() + theme_dag() Let’s ignore $Uv for now… Focus instead on the implication that \\(R\\) and \\(C\\) both influence \\(G\\). This could mean that they are independent influences or rather that they interact (one moderates the influence of the other). The DAG does not display an interaction. That’s because DAGs do not specify how variables combine to influence other variables. The DAG above implies only that there is some function that uses \\(R\\) and \\(C\\) to generate \\(G\\). In typical notation, \\(G = f(R, C)\\). (p. 240) It’s generally not a good idea to split up your data and run separate analyses when examining an interaction. McElreath listed four reasons why: “There are usually some parameters, such as \\(\\sigma\\), that the model says do not depend in any way upon continent. By splitting the data table, you are hurting the accuracy of the estimates for these parameters” (p. 241). “In order to acquire probability statements about the variable you used to split the data, cont_africa, in this case, you need to include it in the model” (p. 241). “We many want to use information criteria or another method to compare models” (p. 241). “Once you begin using multilevel models (Chapter 13), you’ll see that there are advantages to borrowing information across categories like ‘Africa’ and ‘not Africa’” (p. 241). 8.1.0.1 Overthinking: Not so simple causation. Here’s the DAG for a fuller model for the data. dag_coords &lt;- tibble(name = c(&quot;G&quot;, &quot;R&quot;, &quot;H&quot;, &quot;C&quot;, &quot;U&quot;), x = c(1, 1.5, 2.5, 3.5, 1), y = c(3, 2, 2, 2, 1)) dagify(G ~ R + U + H, R ~ U, H ~ R + U + C, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;U&quot;), alpha = 1/2, size = 6, show.legend = F) + geom_point(x = 1, y = 1, size = 6, shape = 1, stroke = 3/4, color = palette_pander(n = 2)[2]) + geom_dag_text(color = &quot;black&quot;, family = &quot;Times&quot;) + geom_dag_edges() + scale_colour_pander() + theme_dag() “The data contain a large number of potential confounds that you might consider. Natural systems like this are terrifyingly complex” (p. 241). In the words of the great Dan Simpson, “Pictures and fear–this is what we do [in statistics]; we draw pictures and have fear” (see here). 8.1.1 Making a rugged model. We’ll continue to use tidyverse-style syntax to wrangle the data. # make the log version of criterion d &lt;- d %&gt;% mutate(log_gdp = log(rgdppc_2000)) # extract countries with GDP data dd &lt;- d %&gt;% filter(complete.cases(rgdppc_2000)) %&gt;% # re-scale variables mutate(log_gdp_std = log_gdp / mean(log_gdp), rugged_std = rugged / max(rugged)) Before we fit our first Bayesian models, let’s back track a bit and make our version of Figure 8.2. In the title, McElreath indicated it was a depiction of two linear regressions separated by whether the nations were African. A fairly simple way to make those plots is to simultaneously fit and plot the two regression models using OLS via the geom_smooth() function using the method = \"lm\" argument. After dividing the data with cont_africa, make each plot separately and then combine them with patchwork syntax. library(ggrepel) library(patchwork) # African nations p1 &lt;- dd %&gt;% filter(cont_africa == 1) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, fill = palette_pander(n = 2)[1], color = palette_pander(n = 2)[1]) + geom_point(color = palette_pander(n = 2)[1]) + geom_text_repel(data = . %&gt;% filter(country %in% c(&quot;Lesotho&quot;, &quot;Seychelles&quot;)), aes(label = country), size = 3, family = &quot;Times&quot;, seed = 8) + labs(subtitle = &quot;African nations&quot;, x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) # Non-African nations p2 &lt;- dd %&gt;% filter(cont_africa == 0) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, fill = palette_pander(n = 2)[2], color = palette_pander(n = 2)[2]) + geom_point(color = palette_pander(n = 2)[2]) + geom_text_repel(data = . %&gt;% filter(country %in% c(&quot;Switzerland&quot;, &quot;Tajikistan&quot;)), aes(label = country), size = 3, family = &quot;Times&quot;, seed = 8) + xlim(0, 1) + labs(subtitle = &quot;Non-African nations&quot;, x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) # combine p1 + p2 + plot_annotation(title = &quot;Figure 8.2. Separate linear regressions inside and outside of Africa&quot;) Our first Bayesian model will follow the form \\[\\begin{align*} \\text{log_gdp_std}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta \\left (\\text{rugged_std}_i - \\overline{\\text{rugged_std}} \\right ) \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(1, 1) \\\\ \\beta &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Here we compute \\(\\overline{\\text{rugged_std}}\\). mean(dd$rugged_std) ## [1] 0.2149601 A naïve translation of McElreath’s rethinking code into a brms::brm() formula argument might be log_gdp_std ~ 1 + (rugged_std - 0.215 ). However, this kind of syntax will not work outside of the non-linear syntax. Our approach will be to make a mean-centered version of rugged_std. dd &lt;- dd %&gt;% mutate(rugged_std_c = rugged_std - mean(rugged_std)) Now fit the model. b8.1 &lt;- brm(data = dd, family = gaussian, log_gdp_std ~ 1 + rugged_std_c, prior = c(prior(normal(1, 1), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, sample_prior = T, file = &quot;fits/b08.01&quot;) Did you notice the sample_prior = T argument? Because of that, we can now use the prior_samples() function to help us plot the prior predictive distribution for m8.1 and make our version of the left panel of Figure 8.3. prior &lt;- prior_samples(b8.1) set.seed(8) p1 &lt;- prior %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), rugged_std_c = c(-2, 2)) %&gt;% mutate(log_gdp_std = Intercept + b * rugged_std_c, rugged_std = rugged_std_c + mean(dd$rugged_std)) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std, group = rowname)) + geom_hline(yintercept = range(dd$log_gdp_std), linetype = 2) + geom_line(color = palette_pander(n = 2)[2], alpha = .4) + geom_abline(intercept = 1.3, slope = -0.6, color = palette_pander(n = 2)[1], size = 2) + labs(subtitle = &quot;Intercept ~ dnorm(1, 1)\\nb ~ dnorm(0, 1)&quot;, x = &quot;ruggedness&quot;, y = &quot;log GDP (prop of mean)&quot;) + coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) p1 Toward the bottom of page 243, McElreath wrote: “The slope of such a line must be about \\(1.3 − 0.7 = 0.6\\), the difference between the maximum and minimum observed proportional log GDP.” The math appears backwards, there. Rather, the slope of our solid blue line is \\(0.7 - 1.3 = -0.6\\). But anyway, “under the \\(\\beta \\sim \\operatorname{Normal}(0, 1)\\) prior, more than half of all slopes will have [an] absolute value greater than \\(0.6\\)” (p. 244). prior %&gt;% summarise(a = sum(abs(b) &gt; abs(-0.6)) / nrow(prior)) ## a ## 1 0.55725 Our updated model is \\[\\begin{align*} \\text{log_gdp_std}_i &amp; \\sim \\operatorname{Normal} (\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta \\left (\\text{rugged_std}_i - \\overline{\\text{rugged_std}} \\right ) \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(1, 0.1) \\\\ \\beta &amp; \\sim \\operatorname{Normal}(0, 0.3) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Fit the model. b8.1b &lt;- brm(data = dd, family = gaussian, log_gdp_std ~ 1 + rugged_std_c, prior = c(prior(normal(1, 0.1), class = Intercept), prior(normal(0, 0.3), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, sample_prior = T, file = &quot;fits/b08.01b&quot;) Now we’ll use prior_samples(b8.1b) to make the left panel of Figure 8.3 and present both panels together with a little patchwork syntax. set.seed(8) p2 &lt;- prior_samples(b8.1b) %&gt;% slice_sample(n = 50) %&gt;% rownames_to_column() %&gt;% expand(nesting(rowname, Intercept, b), rugged_std_c = c(-2, 2)) %&gt;% mutate(log_gdp_std = Intercept + b * rugged_std_c, rugged_std = rugged_std_c + mean(dd$rugged_std)) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std, group = rowname)) + geom_hline(yintercept = range(dd$log_gdp_std), linetype = 2) + geom_line(color = palette_pander(n = 2)[2], alpha = .4) + scale_y_continuous(&quot;&quot;, breaks = NULL) + labs(subtitle = &quot;Intercept ~ dnorm(1, 0.1)\\nb ~ dnorm(0, 0.3)&quot;, x = &quot;ruggedness&quot;) + coord_cartesian(xlim = c(0, 1), ylim = c(0.5, 1.5)) p1 + p2 + plot_annotation(title = &quot;Simulating in search of reasonable priors for the terrain ruggedness example.&quot;, theme = theme(plot.title = element_text(size = 12))) Now check the summary for b8.1b. print(b8.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp_std ~ 1 + rugged_std_c ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.00 0.01 0.98 1.02 1.00 4316 2983 ## rugged_std_c 0.00 0.06 -0.11 0.12 1.00 3677 2896 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.14 0.01 0.12 0.15 1.00 3938 3028 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 8.1.2 Adding an indicator variable isn’t enough. When you’d like to allow a model intercept and slope to differ by levels of a dichotomous variable, a typical approach is to use a 0/1 coded dummy variable. In this section and throughout much of the text, McElreath opted to highlight the index variable approach, instead. We’ll follow along. But if you’d like to practice using brms to fit interaction models with dummy variables, see Section 7.1 of my (2020a) translation of McElreath’s (2015) first edition or Chapters 7 and beyond in my (2019) translation of Andrew Hayes’s (2017) text on mediation and moderation. Make the index variable. dd &lt;- dd %&gt;% mutate(cid = if_else(cont_africa == 1, &quot;1&quot;, &quot;2&quot;)) In case you were curious, here’s a plot showing how the cid index works. dd %&gt;% mutate(cid = str_c(&quot;cid: &quot;, cid)) %&gt;% arrange(cid, country) %&gt;% group_by(cid) %&gt;% mutate(rank = 1:n()) %&gt;% ggplot(aes(x = cid, y = rank, label = country)) + geom_text(size = 2, hjust = 0, family = &quot;Times&quot;) + scale_y_reverse() + theme_void() + facet_wrap(~ cid, scales = &quot;free_x&quot;) If you recall from the latter sections of Chapter 5, the conventional brms syntax can accommodate an index variable by simply suppressing the default intercept via the 0 + .... syntax. That will be our approach, here. b8.2 &lt;- brm(data = dd, family = gaussian, log_gdp_std ~ 0 + cid + rugged_std_c, prior = c(prior(normal(1, 0.1), class = b, coef = cid1), prior(normal(1, 0.1), class = b, coef = cid2), prior(normal(0, 0.3), class = b, coef = rugged_std_c), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;fits/b08.02&quot;) Use add_criterion() and loo_compare() to compare b8.1b and b8.2 with the WAIC. b8.1b &lt;- add_criterion(b8.1b, &quot;waic&quot;) b8.2 &lt;- add_criterion(b8.2, &quot;waic&quot;) loo_compare(b8.1b, b8.2, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b8.2 0.0 0.0 126.1 7.4 4.1 0.8 -252.3 14.8 ## b8.1b -31.7 7.3 94.4 6.5 2.6 0.3 -188.8 13.0 Here are the WAIC weights. model_weights(b8.1b, b8.2, weights = &quot;waic&quot;) %&gt;% round(digits = 3) ## b8.1b b8.2 ## 0 1 Here is the summary for the model with all the weight, b8.2. print(b8.2) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp_std ~ 0 + cid + rugged_std_c ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## cid1 0.88 0.02 0.85 0.91 1.00 4089 2944 ## cid2 1.05 0.01 1.03 1.07 1.00 4491 2987 ## rugged_std_c -0.05 0.05 -0.14 0.04 1.00 3484 2736 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.13 1.00 3981 3176 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now extract the posterior draws, make a difference score for the two intercepts, and use tidybayes::qi() to compute the percentile-based 89% intervals for the difference. post &lt;- posterior_samples(b8.2) %&gt;% mutate(diff = b_cid1 - b_cid2) library(tidybayes) qi(post$diff, .width = .89) ## [,1] [,2] ## [1,] -0.2004941 -0.1370836 Now it’s time to use fitted() to prepare to plot the implications of the model in Figure 8.4. nd &lt;- crossing(cid = 1:2, rugged_std = seq(from = -0.2, to = 1.2, length.out = 30)) %&gt;% mutate(rugged_std_c = rugged_std - mean(dd$rugged_std)) f &lt;- fitted(b8.2, newdata = nd, probs = c(.015, .985)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(cont_africa = ifelse(cid == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) # what did we do? head(f) ## Estimate Est.Error Q1.5 Q98.5 cid rugged_std rugged_std_c cont_africa ## 1 0.8999853 0.02380766 0.8493229 0.9514257 1 -0.200000000 -0.4149601 Africa ## 2 0.8977392 0.02231333 0.8501207 0.9451827 1 -0.151724138 -0.3666842 Africa ## 3 0.8954931 0.02093972 0.8501513 0.9397260 1 -0.103448276 -0.3184083 Africa ## 4 0.8932469 0.01971208 0.8517471 0.9355161 1 -0.055172414 -0.2701325 Africa ## 5 0.8910008 0.01865925 0.8511358 0.9314925 1 -0.006896552 -0.2218566 Africa ## 6 0.8887547 0.01781224 0.8504140 0.9274236 1 0.041379310 -0.1735808 Africa Behold our Figure 8.4. dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged_std, fill = cont_africa, color = cont_africa)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q1.5, ymax = Q98.5), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(aes(y = log_gdp_std), size = 2/3) + scale_fill_pander() + scale_colour_pander() + labs(subtitle = &quot;b8.2&quot;, x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) + coord_cartesian(xlim = c(0, 1)) + theme(legend.background = element_blank(), legend.direction = &quot;horizontal&quot;, legend.position = c(.67, .93), legend.title = element_blank()) Though adding our index variable cid to b8.2 allowed us to give the African nations a different intercept than the non-African nations, it did nothing for the slope. We need a better method. 8.1.2.1 Rethinking: Why 97%? Did you notice the probs = c(.015, .985) argument in our fitted() code, above? This is one of those rare moments when we went along with McElreath and used intervals other than the conventional 95%. In the code block just above, and therefore also in Figure 8.4, I used 97% intervals of the expected mean. This is a rather non-standard percentile interval. So why use 97%? In this book, [McElreath used] non-standard percents to constantly remind the reader that conventions like 95% and 5% are arbitrary. Furthermore, boundaries are meaningless. There is continuous change in probability as we move away from the expected value. So one side of the boundary is almost equally probable as the other side. (p. 247) Building off of McElreath’s “boundaries are meaningless” point, here we use a combination of summary = F within fitted() and a little tidybayes::stat_lineribbon() magic to re-imagine Figure 8.4. This time we use a sequence of overlapping semitransparent credible intervals to give the posterior a 3D-like appearance. fitted(b8.2, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols( expand(nd, iter = 1:4000, nesting(cid, rugged_std)) ) %&gt;% mutate(cont_africa = ifelse(cid == 1, &quot;Africa&quot;, &quot;not Africa&quot;)) %&gt;% ggplot(aes(x = rugged_std, y = value, fill = cont_africa, color = cont_africa)) + stat_lineribbon(.width = seq(from = .03, to = .99, by = .03), alpha = .1, size = 0) + geom_point(data = dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;Africa&quot;, &quot;not Africa&quot;)), aes(y = log_gdp_std), size = 2/3) + scale_fill_pander() + scale_colour_pander() + labs(subtitle = &quot;b8.2&quot;, x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) + coord_cartesian(xlim = c(0, 1)) + theme(legend.background = element_blank(), legend.direction = &quot;horizontal&quot;, legend.position = c(.67, .93), legend.title = element_blank()) 8.1.3 Adding an interaction does work. The 0 + ... syntax works fine when we just want to use an index variable to fit a model with multiple intercepts, this approach will not work for fitting brms models that apply the index variable to slopes. Happily, we have alternatives. If we’d like to use the cid index to make intercepts and slopes as in McElreath’s m8.3, we can use the brms non-linear syntax (Bürkner, 2021e). Here it is for b8.3. b8.3 &lt;- brm(data = dd, family = gaussian, bf(log_gdp_std ~ 0 + a + b * rugged_std_c, a ~ 0 + cid, b ~ 0 + cid, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a), prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a), prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b), prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;fits/b08.03&quot;) Check the summary of the marginal distributions. print(b8.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp_std ~ 0 + a + b * rugged_std_c ## a ~ 0 + cid ## b ~ 0 + cid ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_cid1 0.89 0.02 0.86 0.92 1.00 4808 3274 ## a_cid2 1.05 0.01 1.03 1.07 1.00 6227 3439 ## b_cid1 0.13 0.08 -0.02 0.28 1.00 5023 2991 ## b_cid2 -0.14 0.06 -0.25 -0.03 1.00 4639 2559 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.12 1.00 4913 2940 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Success! Our results look just like McElreath’s. Now make haste with add_criterion() so we can compare the models by the PSIS-LOO-CV. b8.1b &lt;- add_criterion(b8.1b, &quot;loo&quot;) b8.2 &lt;- add_criterion(b8.2, &quot;loo&quot;) b8.3 &lt;- add_criterion(b8.3, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(b8.1b, b8.2, b8.3, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b8.3 0.0 0.0 129.6 7.3 5.0 0.9 -259.2 14.7 ## b8.2 -3.5 3.2 126.1 7.4 4.1 0.8 -252.2 14.8 ## b8.1b -35.2 7.5 94.4 6.5 2.6 0.3 -188.8 13.0 Here are the LOO weights. model_weights(b8.1b, b8.2, b8.3, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## b8.1b b8.2 b8.3 ## 0.00 0.03 0.97 We can get a Pareto \\(k\\) diagnostic plot for b8.3 by feeding the results of the loo() function into plot(). loo(b8.3) %&gt;% plot() Unlike in the text, our results suggest all the cases had Pareto \\(k\\) values below the 0.5 threshold. We can confirm by rank ordering them and taking a look at the top values. tibble(k = b8.3$criteria$loo$diagnostics$pareto_k, row = 1:170) %&gt;% arrange(desc(k)) ## # A tibble: 170 x 2 ## k row ## &lt;dbl&gt; &lt;int&gt; ## 1 0.390 93 ## 2 0.365 27 ## 3 0.306 145 ## 4 0.277 150 ## 5 0.224 118 ## 6 0.222 8 ## 7 0.185 29 ## 8 0.169 84 ## 9 0.166 90 ## 10 0.164 57 ## # … with 160 more rows Yep, no big issues, here. 8.1.3.1 Bonus: Give me Student-\\(t\\). McElreath remarked: “This is possibly a good context for robust regression, like the Student-t regression we did in Chapter 7” (p. 249). Let’s practice fitting the alternative model using the Student-\\(t\\) likelihood for which \\(\\nu = 2\\). b8.3t &lt;- brm(data = dd, family = student, bf(log_gdp_std ~ 0 + a + b * rugged_std_c, a ~ 0 + cid, b ~ 0 + cid, nu = 2, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a), prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a), prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b), prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;fits/b08.03t&quot;) Use the LOO to compare this with the Gaussian model. b8.3t &lt;- add_criterion(b8.3t, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(b8.3, b8.3t, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b8.3 0.0 0.0 129.6 7.3 5.0 0.9 -259.2 14.7 ## b8.3t -19.1 2.7 110.5 8.8 6.2 0.7 -221.0 17.5 The PSIS-LOO-CV comparison suggests the robust Student-\\(t\\) model might be overfit. Just for kicks, we might make our own diagnostic plot to compare the two likelihoods by the Pareto \\(k\\) values. To get a nice fine-grain sense of the distributions, we’ll employ the handy tidybayes::stat_dots() function which will display each value as an individual dot. tibble(Normal = b8.3$criteria$loo$diagnostics$pareto_k, `Student-t` = b8.3t$criteria$loo$diagnostics$pareto_k) %&gt;% pivot_longer(everything(), values_to = &quot;pareto_k&quot;) %&gt;% ggplot(aes(x = pareto_k, y = name)) + geom_vline(xintercept = .5, linetype = 2, color = palette_pander(n = 5)[5]) + stat_dots(slab_fill = palette_pander(n = 4)[4], slab_color = palette_pander(n = 4)[4]) + annotate(geom = &quot;text&quot;, x = .485, y = 1.5, label = &quot;threshold&quot;, angle = 90, family = &quot;Times&quot;, color = palette_pander(n = 5)[5]) + ylab(NULL) + coord_cartesian(ylim = c(1.5, 2.4)) To close this exercise out, compare the \\(\\alpha\\) and \\(\\beta\\) parameters of the two models using fixef(). fixef(b8.3) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## a_cid1 0.89 0.02 0.86 0.92 ## a_cid2 1.05 0.01 1.03 1.07 ## b_cid1 0.13 0.08 -0.02 0.28 ## b_cid2 -0.14 0.06 -0.25 -0.03 fixef(b8.3t) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## a_cid1 0.87 0.02 0.84 0.90 ## a_cid2 1.05 0.01 1.02 1.07 ## b_cid1 0.13 0.09 -0.02 0.33 ## b_cid2 -0.20 0.07 -0.33 -0.07 8.1.4 Plotting the interaction. The code for Figure 8.5 is a minor extension of the code we used for Figure 8.4. Other than which fit we use, the code we use for fitted() is the same for both plots. Two of the largest changes are the addition of labels with ggrepel::geom_text_repel() and using facet_wrap() to split the plot into two panels. countries &lt;- c(&quot;Equatorial Guinea&quot;, &quot;South Africa&quot;, &quot;Seychelles&quot;, &quot;Swaziland&quot;, &quot;Lesotho&quot;, &quot;Rwanda&quot;, &quot;Burundi&quot;, &quot;Luxembourg&quot;, &quot;Greece&quot;, &quot;Switzerland&quot;, &quot;Lebanon&quot;, &quot;Yemen&quot;, &quot;Tajikistan&quot;, &quot;Nepal&quot;) f &lt;- fitted(b8.3, # we already defined `nd`, above newdata = nd, probs = c(.015, .985)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(cont_africa = ifelse(cid == 1, &quot;African nations&quot;, &quot;Non-African nations&quot;)) dd %&gt;% mutate(cont_africa = ifelse(cont_africa == 1, &quot;African nations&quot;, &quot;Non-African nations&quot;)) %&gt;% ggplot(aes(x = rugged_std, y = log_gdp_std, fill = cont_africa, color = cont_africa)) + geom_smooth(data = f, aes(y = Estimate, ymin = Q1.5, ymax = Q98.5), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_text_repel(data = . %&gt;% filter(country %in% countries), aes(label = country), size = 3, seed = 8, segment.color = &quot;grey25&quot;, min.segment.length = 0) + geom_point(aes(y = log_gdp_std), size = 2/3) + scale_fill_pander() + scale_colour_pander() + labs(x = &quot;ruggedness (standardized)&quot;, y = &quot;log GDP (as proportion of mean)&quot;) + coord_cartesian(xlim = c(0, 1)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ cont_africa) “Finally, the slope reverses direction inside and outside of Africa. And because we achieved this inside a single model, we could statistically evaluate the value of this reversal” (p. 250). 8.1.4.1 Rethinking: All Greek to me. We use these Greek symbols \\(\\alpha\\) and \\(\\beta\\) because it is conventional. They don’t have special meanings. If you prefer some other Greek symbol like \\(\\omega\\)–why should \\(\\alpha\\) get all the attention?–feel free to use that instead. It is conventional to use Greek letters for unobserved variables (parameters) and Roman letters for observed variables (data). That convention does have some value, because it helps others read your models. But breaking the convention is not an error, and sometimes it is better to use a familiar Roman symbol than an unfamiliar Greek one like \\(\\xi\\) or \\(\\zeta\\). If your readers cannot say the symbol’s name, it could make understanding the model harder. (p. 249) This topic is near and dear my heart. In certain areas of psychology, people presume symbols like \\(\\beta\\) and \\(b\\) have universal meanings. This presumption is a mistake and will not serve one well beyond a narrow section of the scientific literature. My recommendation is whatever notation you fancy in a given publication, clearly define your terms, especially if there could be any confusion over whether your results are standardized or not. 8.2 Symmetry of interactions If you’re unfamiliar with Buridan’s ass, here’s a brief clip to catch up up to speed. With that ass still on your mind, recall the model for \\(\\mu_i\\) from the last example, \\[\\mu_i = \\alpha_{\\text{cid}[i]} + \\beta_{\\text{cid}[i]} \\left (\\text{rugged_std}_i - \\overline{\\text{rugged_std}} \\right ).\\] With this model, it is equally true that that slope is conditional on the intercept as it is that the intercept is conditional on the slope. Another way to express the model is \\[\\begin{align*} \\mu_i &amp; = \\underbrace{(2 - \\text{cid}_{i}) \\left (\\alpha_1 + \\beta_1 \\left [\\text{rugged_std}_i - \\overline{\\text{rugged_std}} \\right ] \\right )}_{\\text{cid}[i] = 1} \\\\ &amp; \\;\\;\\; + \\underbrace{(\\text{cid}_{i} - 1) \\left (\\alpha_2 + \\beta_2 \\left [\\text{rugged_std}_i - \\overline{\\text{rugged_std}} \\right ] \\right )}_{\\text{cid}[i] = 2}, \\end{align*}\\] where the first term vanishes when \\(\\text{cid}_i = 2\\) and the second term vanishes when \\(\\text{cid}_i = 1\\). In contrast to the plots above, we can re-express this equation as saying “The association of being in Africa with log GDP depends upon terrain ruggedness” (p. 251, emphasis in the original). Here we follow McElreath’s Figure 8.6 and plot the difference between a nation in Africa and outside Africa, conditional on ruggedness. fitted(b8.3, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% pivot_longer(everything()) %&gt;% bind_cols(expand(nd, iter = 1:4000, nesting(cid, rugged_std))) %&gt;% select(-name) %&gt;% pivot_wider(names_from = cid, values_from = value) %&gt;% mutate(delta = `1` - `2`) %&gt;% ggplot(aes(x = rugged_std, y = delta)) + stat_lineribbon(.width = .95, fill = palette_pander(n = 8)[8], alpha = 3/4) + geom_hline(yintercept = 0, linetype = 2) + annotate(geom = &quot;text&quot;, x = .2, y = 0, label = &quot;Africa higher GDP\\nAfrica lower GDP&quot;, family = &quot;Times&quot;) + labs(x = &quot;ruggedness (standardized)&quot;, y = &quot;expected difference log GDP&quot;) + coord_cartesian(xlim = c(0, 1), ylim = c(-0.3, 0.2)) This perspective on the GDP and terrain ruggedness is completely consistent with the previous perspective. It’s simultaneously true in these data (and with this model) that (1) the influence of ruggedness depends upon continent and (2) the influence of continent depends upon ruggedness. Simple interactions are symmetric, just like the choice facing Buridan’s ass. Within the model, there’s no basis to prefer one interpretation over the other, because in fact they are the same interpretation. But when we reason causally about models, our minds tend to prefer one interpretation over the other, because it’s usually easier to imagine manipulating one of the predictor variables instead of the other. (pp. 251–252) 8.3 Continuous interactions It’s one thing to make a slope conditional upon a category. In such a context, the model reduces to estimating a different slope for each category. But it’s quite a lot harder to understand that a slope varies in a continuous fashion with a continuous variable. Interpretation is much harder in this case, even though the mathematics of the model are essentially the same. (p. 252, emphasis in the original) 8.3.1 A winter flower. Look at the tulips data, which were adapted from Grafen &amp; Hails (2002). data(tulips, package = &quot;rethinking&quot;) d &lt;- tulips rm(tulips) glimpse(d) ## Rows: 27 ## Columns: 4 ## $ bed &lt;fct&gt; a, a, a, a, a, a, a, a, a, b, b, b, b, b, b, b, b, b, c, c, c, c, c, c, c, c, c ## $ water &lt;int&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3 ## $ shade &lt;int&gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3 ## $ blooms &lt;dbl&gt; 0.00, 0.00, 111.04, 183.47, 59.16, 76.75, 224.97, 83.77, 134.95, 80.10, 85.95, 19.8… 8.3.2 The models. Wrangle a little. d &lt;- d %&gt;% mutate(blooms_std = blooms / max(blooms), water_cent = water - mean(water), shade_cent = shade - mean(shade)) With the variables in hand, the basic model is \\(B = f(W, S)\\), where \\(B\\) = blooms, \\(W\\) = water, \\(S\\) = shade, and \\(f(\\cdot)\\) indicates a function. We can also express this as \\(W \\rightarrow B \\leftarrow S\\). Neither expression clarifies whether the effects of \\(W\\) and \\(S\\) are additive or conditional on each other in some way. We might express an unconditional (additive) model as \\[\\begin{align*} \\text{blooms_std}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{water_cent}_i + \\beta_2 \\text{shade_cent}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0.5, 1) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where \\(\\text{water_cent}_i = \\left (\\text{water}_i - \\overline{\\text{water}} \\right )\\) and \\(\\text{shade_cent}_i = \\left (\\text{shade}_i - \\overline{\\text{shade}} \\right )\\). Even though “the intercept \\(\\alpha\\) must be greater than zero and less than one,… this prior assigns most of the probability outside that range” (p. 254). set.seed(8) tibble(a = rnorm(1e4, mean = 0.5, sd = 1)) %&gt;% summarise(proportion_outside_of_the_range = sum(a &lt; 0 | a &gt; 1) / n()) ## # A tibble: 1 x 1 ## proportion_outside_of_the_range ## &lt;dbl&gt; ## 1 0.621 Tightening up the prior to \\(\\operatorname{Normal}(0, 0.25)\\) helps. set.seed(8) tibble(a = rnorm(1e4, mean = 0.5, sd = 0.25)) %&gt;% summarise(proportion_outside_of_the_range = sum(a &lt; 0 | a &gt; 1) / n()) ## # A tibble: 1 x 1 ## proportion_outside_of_the_range ## &lt;dbl&gt; ## 1 0.0501 Here are the ranges for our two predictors. range(d$water_cent) ## [1] -1 1 range(d$shade_cent) ## [1] -1 1 Putting the same \\(\\operatorname{Normal}(0, 0.25)\\) prior on each would indicate a .95 probability each coefficient would be within -0.5 to 0.5. Since the total range for both is \\(1 - (-1) = 2\\), that would imply either could account for the full range of blooms_std because \\(0.5 \\cdot 2 = 1\\), which is the full range of blooms_std. Our first model, then, will be \\[\\begin{align*} \\text{blooms_std}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text{water_cent}_i + \\beta_2 \\text{shade_cent}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0.5, 0.25) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.25) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.25) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Fit the model. b8.4 &lt;- brm(data = d, family = gaussian, blooms_std ~ 1 + water_cent + shade_cent, prior = c(prior(normal(0.5, 0.25), class = Intercept), prior(normal(0, 0.25), class = b, coef = water_cent), prior(normal(0, 0.25), class = b, coef = shade_cent), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;fits/b08.04&quot;) Check the model summary. print(b8.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: blooms_std ~ 1 + water_cent + shade_cent ## Data: d (Number of observations: 27) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.36 0.03 0.29 0.43 1.00 4448 2585 ## water_cent 0.20 0.04 0.12 0.28 1.00 4299 2788 ## shade_cent -0.11 0.04 -0.20 -0.03 1.00 4390 3062 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.18 0.03 0.13 0.24 1.00 3063 2786 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Using the \\(\\gamma\\) notation, we can express an interaction between water_cent and shade_cent by \\[\\begin{align*} \\mu_i &amp; = \\alpha + \\color{#009E73}{\\gamma_{1, i}} \\text{water_cent}_i + \\beta_2 \\text{shade_cent}_i \\\\ \\color{#009E73}{\\gamma_{1, i}} &amp; \\color{#009E73}= \\color{#009E73}{\\beta_1 + \\beta_3 \\text{shade_cent}_i}, \\end{align*}\\] where both \\(\\mu_i\\) and \\(\\gamma_{1, i}\\) get a linear model. We could do the converse by switching the positions of water_cent and shade_cent. If we substitute the equation for \\(\\gamma_{1, i}\\) into the equation for \\(\\mu_i\\), we get \\[\\begin{align*} \\mu_i &amp; = \\alpha + \\color{#009E73}{\\underbrace{(\\beta_1 + \\beta_3 \\text{shade_cent}_i)}_{\\gamma_{1, i}}} \\text{water_cent}_i + \\beta_2 \\text{shade_cent}_i \\\\ &amp; = \\alpha + \\color{#009E73}{\\beta_1} \\text{water_cent}_i + (\\color{#009E73}{\\beta_3 \\text{shade_cent}_i} \\cdot \\text{water_cent}_i) + \\beta_2 \\text{shade_cent}_i \\\\ &amp; = \\alpha + \\color{#009E73}{\\beta_1} \\text{water_cent}_i + \\beta_2 \\text{shade_cent}_i + \\color{#009E73}{\\beta_3} (\\color{#009E73}{\\text{shade_cent}_i} \\cdot \\text{water_cent}_i), \\end{align*}\\] where \\(\\beta_3\\) is the interaction term which makes water_cent and shade_cent conditional on each other. If we use the same priors as before, we might write the full equation for our interaction model as \\[\\begin{align*} \\text{blooms_std}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\color{#009E73}{\\beta_1} \\text{water_cent}_i + \\beta_2 \\text{shade_cent}_i + \\color{#009E73}{\\beta_3 \\text{shade_cent}_i} \\cdot \\text{water_cent}_i\\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0.5, 0.25) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.25) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.25) \\\\ \\beta_3 &amp; \\sim \\operatorname{Normal}(0, 0.25) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Fit the model. b8.5 &lt;- brm(data = d, family = gaussian, blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent, prior = c(prior(normal(0.5, 0.25), class = Intercept), prior(normal(0, 0.25), class = b, coef = water_cent), prior(normal(0, 0.25), class = b, coef = shade_cent), prior(normal(0, 0.25), class = b, coef = &quot;water_cent:shade_cent&quot;), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;fits/b08.05&quot;) Check the summary. print(b8.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent ## Data: d (Number of observations: 27) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.36 0.03 0.30 0.41 1.00 4147 2853 ## water_cent 0.21 0.03 0.14 0.27 1.00 4603 2752 ## shade_cent -0.11 0.03 -0.18 -0.04 1.00 4625 2673 ## water_cent:shade_cent -0.14 0.04 -0.22 -0.06 1.00 4689 2968 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.14 0.02 0.11 0.19 1.00 3304 2710 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The row for the interaction term, water_cent:shade_cent, indicates the marginal posterior is negative. 8.3.3 Plotting posterior predictions. Now we’re ready for the top row of Figure 8.8. Here’s our variation on McElreath’s triptych loop code, adjusted for brms and ggplot2. # loop over values of `water_c` and plot predictions for(s in -1:1) { # define the subset of the original data dt &lt;- d[d$shade_cent == s, ] # defining our new data nd &lt;- tibble(shade_cent = s, water_cent = c(-1, 1)) # use our sampling skills, like before f &lt;- fitted(b8.4, newdata = nd, summary = F) %&gt;% data.frame() %&gt;% set_names(&quot;-1&quot;, &quot;1&quot;) %&gt;% slice_sample(n = 20) %&gt;% mutate(row = 1:n()) %&gt;% pivot_longer(-row, names_to = &quot;water_cent&quot;, values_to = &quot;blooms_std&quot;) %&gt;% mutate(water_cent = as.double(water_cent)) # specify our custom plot fig &lt;- ggplot(data = dt, aes(x = water_cent, y = blooms_std)) + geom_line(data = f, aes(group = row), color = palette_pander(n = 6)[6], alpha = 1/5, size = 1/2) + geom_point(color = palette_pander(n = 6)[6]) + scale_x_continuous(&quot;Water (centered)&quot;, breaks = c(-1, 0, 1)) + labs(title = paste(&quot;Shade (centered) =&quot;, s), y = &quot;Blooms (standardized)&quot;) + coord_cartesian(xlim = c(-1, 1), ylim = c(0, 1)) # plot that joint plot(fig) } We don’t necessarily need a loop. We can achieve all of McElreath’s Figure 8.8 with fitted(), some data wrangling, and a little help from ggplot2::facet_grid(). # augment the data points &lt;- d %&gt;% expand(fit = c(&quot;b8.4&quot;, &quot;b8.5&quot;), nesting(shade_cent, water_cent, blooms_std)) %&gt;% mutate(x_grid = str_c(&quot;shade_cent = &quot;, shade_cent), y_grid = fit) # redefine `nd` nd &lt;- crossing(shade_cent = -1:1, water_cent = c(-1, 1)) # use `fitted()` set.seed(8) rbind(fitted(b8.4, newdata = nd, summary = F, nsamples = 20), fitted(b8.5, newdata = nd, summary = F, nsamples = 20)) %&gt;% # wrangle data.frame() %&gt;% set_names(mutate(nd, name = str_c(shade_cent, water_cent, sep = &quot;_&quot;)) %&gt;% pull()) %&gt;% mutate(row = 1:n(), fit = rep(c(&quot;b8.4&quot;, &quot;b8.5&quot;), each = n() / 2)) %&gt;% pivot_longer(-c(row:fit), values_to = &quot;blooms_std&quot;) %&gt;% separate(name, into = c(&quot;shade_cent&quot;, &quot;water_cent&quot;), sep = &quot;_&quot;) %&gt;% mutate(shade_cent = shade_cent %&gt;% as.double(), water_cent = water_cent %&gt;% as.double()) %&gt;% # these will come in handy for `ggplot2::facet_grid()` mutate(x_grid = str_c(&quot;shade_cent = &quot;, shade_cent), y_grid = fit) %&gt;% # plot! ggplot(aes(x = water_cent, y = blooms_std)) + geom_line(aes(group = row), color = palette_pander(n = 6)[6], alpha = 1/5, size = 1/2) + geom_point(data = points, color = palette_pander(n = 6)[6]) + scale_x_continuous(&quot;Water (centered)&quot;, breaks = c(-1, 0, 1)) + scale_y_continuous(&quot;Blooms (standardized)&quot;, breaks = c(0, .5, 1)) + ggtitle(&quot;Posterior predicted blooms&quot;) + coord_cartesian(xlim = c(-1, 1), ylim = c(0, 1)) + theme(strip.background = element_rect(fill = alpha(palette_pander(n = 2)[2], 1/3))) + facet_grid(y_grid ~ x_grid) 8.3.4 Plotting prior predictions. In some of the earlier models in this book, we used the sample_prior = T argument within brm() to simultaneously sample from the posterior and prior distributions. As far as the priors go, we could then retrieve their draws from the prior_samples() function and plot as desired. And to be clear, we could use this method to remake Figure 8.8 with our brms fits. However, a limitation of the sample_prior = T method is it will not work if you’re trying to use a fitted()-oriented work flow. Happily, we have an alternative. Within brm(), set sample_prior = \"only\". The resulting fit object will be based solely on the priors. Here we’ll use this method within update() to refit the last two models. b8.4p &lt;- update(b8.4, sample_prior = &quot;only&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;fits/b08.04p&quot;) b8.5p &lt;- update(b8.5, sample_prior = &quot;only&quot;, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 8, file = &quot;fits/b08.05p&quot;) Now we can insert b8.4p and b8.5p into the fitted() function and plot the prior predictions we desire. set.seed(8) rbind(fitted(b8.4p, newdata = nd, summary = F, nsamples = 20), fitted(b8.5p, newdata = nd, summary = F, nsamples = 20)) %&gt;% # wrangle data.frame() %&gt;% set_names(mutate(nd, name = str_c(shade_cent, water_cent, sep = &quot;_&quot;)) %&gt;% pull()) %&gt;% mutate(row = rep(1:20, times = 2), fit = rep(c(&quot;b8.4&quot;, &quot;b8.5&quot;), each = n() / 2)) %&gt;% pivot_longer(-c(row:fit), values_to = &quot;blooms_std&quot;) %&gt;% separate(name, into = c(&quot;shade_cent&quot;, &quot;water_cent&quot;), sep = &quot;_&quot;) %&gt;% mutate(shade_cent = shade_cent %&gt;% as.double(), water_cent = water_cent %&gt;% as.double()) %&gt;% # these will come in handy for `ggplot2::facet_grid()` mutate(x_grid = str_c(&quot;shade_cent = &quot;, shade_cent), y_grid = fit) %&gt;% # plot! ggplot(aes(x = water_cent, y = blooms_std, group = row)) + geom_hline(yintercept = 0:1, linetype = 2) + geom_line(aes(alpha = row == 1, size = row == 1), color = palette_pander(n = 6)[6]) + scale_size_manual(values = c(1/2, 1)) + scale_alpha_manual(values = c(1/3, 1)) + scale_x_continuous(&quot;Water (centered)&quot;, breaks = c(-1, 0, 1)) + scale_y_continuous(&quot;Blooms (standardized)&quot;, breaks = c(0, .5, 1)) + ggtitle(&quot;Prior predicted blooms&quot;) + coord_cartesian(xlim = c(-1, 1), ylim = c(-0.5, 1.5)) + theme(legend.position = &quot;none&quot;, strip.background = element_rect(fill = alpha(palette_pander(n = 2)[2], 1/3))) + facet_grid(y_grid ~ x_grid) It was the aes() statement within geom_line() and the scale_size_manual() and scale_alpha_manual() lines that followed that allowed us to bold the one line in each panel. Relatedly, it was the set.seed() line at the top of the code block that made the results reproducible. What can we say about these priors, overall? They are harmless, but only weakly realistic. Most of the lines stay within the valid outcome space. But silly trends are not rare. We could do better. We could also do a lot worse, such as flat priors which would consider plausible that even a tiny increase in shade would kill all the tulips. If you displayed these priors to your colleagues, a reasonable summary might be, “These priors contain no bias towards positive or negative effects, and at the same time they very weakly bound the effects to realistic ranges.” (p. 260) 8.4 Summary Bonus: conditional_effects() The brms package includes the conditional_effects() function as a convenient way to look at simple effects and two-way interactions. Recall the simple univariable model, b8.1b. b8.1b$formula ## log_gdp_std ~ 1 + rugged_std_c We can look at the regression line and its percentile-based intervals like so. conditional_effects(b8.1b) If we feed the conditional_effects() output into the plot() function with a points = T argument, we can add the original data to the figure. conditional_effects(b8.1b) %&gt;% plot(points = T) We can further customize the plot. For example, we can replace the intervals with a spaghetti plot. While we’re at it, we can use point_args to adjust the geom_jitter() parameters and line_args() to adjust the line marking off the posterior median. conditional_effects(b8.1b, spaghetti = T, nsamples = 200) %&gt;% plot(points = T, point_args = c(alpha = 1/2, size = 1), line_args = c(colour = &quot;black&quot;)) With multiple predictors, things get more complicated. Consider our multivariable, non-interaction model, b8.2. b8.2$formula ## log_gdp_std ~ 0 + cid + rugged_std_c conditional_effects(b8.2) We got one plot for each predictor, controlling the other predictor at zero. Note how the plot for cid treated it as a categorical variable. This is because the variable was saved as a character in the original data set. b8.2$data %&gt;% glimpse() ## Rows: 170 ## Columns: 3 ## $ log_gdp_std &lt;dbl&gt; 0.8797119, 0.9647547, 1.1662705, 1.1044854, 0.9149038, 1.0816501, 1.1909183, … ## $ cid &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;1&quot;, &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2… ## $ rugged_std_c &lt;dbl&gt; -0.07661760, 0.33760362, -0.09096781, -0.09000038, 0.21844851, -0.21399264, -… The results would have been similar had we saved cid as a factor. Things get more complicated with the non-linear interaction model, b8.3. b8.3$formula ## log_gdp_std ~ 0 + a + b * rugged_std_c ## a ~ 0 + cid ## b ~ 0 + cid conditional_effects(b8.3) The conditional_effects() function correctly picked up on how we used the a parameter to make two intercepts according to the two levels of cid. However, it did not pick up our intent to use the b parameter as a stand-in for the two levels of the slope for rugged_std_c. Instead, it only showed the slope for rugged_std_c == 1. In GitHub issue #925, Bürkner clarified this is because “brms will only display interactions by default if the interactions are explictely provided within linear formulas. brms has no way of knowing what variables in non-linear models actually interact.” However, the effects argument provides a workaround. conditional_effects(b8.3, effects = &quot;rugged_std_c:cid&quot;) The conditional_effects() function defaults to expressing interactions such that the first variable in the term–in this case, rugged_std_c–is on the \\(x\\)-axis and the second variable in the term–cid, treated as an integer–is treated as a factor using different values for the fill and color of the trajectories. See what happens when we change the ordering. conditional_effects(b8.3, effects = &quot;cid:rugged_std_c&quot;) Now our binary index variable cid is on the \\(x\\)-axis and the error bars for the effects are now depicted by three levels of the continuous variable rugged_std_c. By default, those are the mean \\(\\pm\\) one standard deviation. We might confirm those values like this. b8.3$data %&gt;% summarize(mean = mean(rugged_std_c), `mean + 1 sd` = mean(rugged_std_c) + sd(rugged_std_c), `mean - 1 sd` = mean(rugged_std_c) - sd(rugged_std_c)) %&gt;% mutate_all(round, digits = 2) ## mean mean + 1 sd mean - 1 sd ## 1 0 0.19 -0.19 Now we might use b8.5, our interaction model with two continuous variables, to get a sense of how this behavior works in that context. b8.5$formula ## blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent conditional_effects(b8.5, effects = &quot;water_cent:shade_cent&quot;) Once again, the three levels of the second variable in the interaction term, shade_cent, are the mean \\(\\pm\\) one standard deviation. If you’d like to set these to different values, such as -1, 0, and 1, define those within a list and feed that list into the int_conditions argument within conditional_effects(). We’ll do that in the next plot in a bit. Though the paradigm of using the conditional_effects() and plot() functions allows users to augment the results with a variety of options, users to not have the full flexibility of ggplot2 with this approach. If you’re picky and want to augment the plot further with other ggplot2 settings, you need to: save the settings from conditional_effects() as an object, feed that object into plot(), set the plot argument to FALSE within the plot() function, index using bracket, and finally customize away with other ggplot2 functions. Here’s an example of what that might look like. p1 &lt;- conditional_effects(b8.5, effects = &quot;water_cent:shade_cent&quot;, int_conditions = list(shade_cent = -1:1)) plot(p1, points = T, plot = F)[[1]] + scale_fill_pander() + scale_colour_pander() + scale_x_continuous(breaks = -1:1) + theme(legend.position = &quot;none&quot;, panel.grid.minor = element_blank()) + facet_wrap(~ shade_cent, labeller = label_both) I tend to prefer using other plotting methods when visualizing models, so we won’t be seeing much more from the conditional_effects() function in this ebook. But if you like method, you can find more ideas by checking out the conditional_effects section of the brms reference manual (Bürkner, 2021i) or searching for “conditional_effects” under the “brms” tag on the Stan Forums. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_2.3.1 patchwork_1.1.1 ggrepel_0.9.1 ggdag_0.2.3 ggthemes_4.2.4 forcats_0.5.1 ## [7] stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 tibble_3.1.0 ## [13] ggplot2_3.3.3 tidyverse_1.3.0 brms_2.15.0 Rcpp_1.0.6 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 igraph_1.2.6 ## [5] svUnit_1.0.3 splines_4.0.4 crosstalk_1.1.0.1 TH.data_1.0-10 ## [9] rstantools_2.1.1 inline_0.3.17 digest_0.6.27 htmltools_0.5.1.1 ## [13] viridis_0.5.1 rsconnect_0.8.16 fansi_0.4.2 magrittr_2.0.1 ## [17] graphlayouts_0.7.1 modelr_0.1.8 RcppParallel_5.0.2 matrixStats_0.57.0 ## [21] xts_0.12.1 sandwich_3.0-0 prettyunits_1.1.1 colorspace_2.0-0 ## [25] rvest_0.3.6 ggdist_2.4.0.9000 haven_2.3.1 xfun_0.22 ## [29] callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 lme4_1.1-25 ## [33] survival_3.2-7 zoo_1.8-8 glue_1.4.2 polyclip_1.10-0 ## [37] gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 distributional_0.2.2 ## [41] pkgbuild_1.2.0 rstan_2.21.2 abind_1.4-5 scales_1.1.1 ## [45] mvtnorm_1.1-1 DBI_1.1.0 miniUI_0.1.1.1 viridisLite_0.3.0 ## [49] xtable_1.8-4 stats4_4.0.4 StanHeaders_2.21.0-7 DT_0.16 ## [53] htmlwidgets_1.5.2 httr_1.4.2 threejs_0.3.3 arrayhelpers_1.1-0 ## [57] ellipsis_0.3.1 farver_2.0.3 pkgconfig_2.0.3 loo_2.4.1 ## [61] dbplyr_2.0.0 utf8_1.1.4 labeling_0.4.2 tidyselect_1.1.0 ## [65] rlang_0.4.10 reshape2_1.4.4 later_1.1.0.1 dagitty_0.3-1 ## [69] munsell_0.5.0 cellranger_1.1.0 tools_4.0.4 cli_2.3.1 ## [73] generics_0.1.0 broom_0.7.5 ggridges_0.5.2 evaluate_0.14 ## [77] fastmap_1.0.1 processx_3.4.5 knitr_1.31 fs_1.5.0 ## [81] tidygraph_1.2.0 pander_0.6.3 ggraph_2.0.4 nlme_3.1-152 ## [85] mime_0.10 projpred_2.0.2 xml2_1.3.2 compiler_4.0.4 ## [89] bayesplot_1.8.0 shinythemes_1.1.2 rstudioapi_0.13 curl_4.3 ## [93] gamm4_0.2-6 reprex_0.3.0 tweenr_1.0.1 statmod_1.4.35 ## [97] stringi_1.5.3 highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 ## [101] lattice_0.20-41 Matrix_1.3-2 nloptr_1.2.2.2 markdown_1.1 ## [105] shinyjs_2.0.0 vctrs_0.3.6 pillar_1.5.1 lifecycle_1.0.0 ## [109] bridgesampling_1.0-0 estimability_1.3 httpuv_1.5.4 R6_2.5.0 ## [113] bookdown_0.21 promises_1.1.1 gridExtra_2.3 codetools_0.2-18 ## [117] boot_1.3-26 colourpicker_1.1.0 MASS_7.3-53 gtools_3.8.2 ## [121] assertthat_0.2.1 withr_2.4.1 shinystan_2.5.0 multcomp_1.4-16 ## [125] mgcv_1.8-33 parallel_4.0.4 hms_0.5.3 grid_4.0.4 ## [129] coda_0.19-4 minqa_1.2.4 rmarkdown_2.7 ggforce_0.3.2 ## [133] shiny_1.5.0 lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 "],["markov-chain-monte-carlo.html", "9 Markov Chain Monte Carlo 9.1 Good King Markov and his island kingdom 9.2 Metropolis algorithms 9.3 Hamiltonian Monte Carlo 9.4 Easy HMC: ulam brm() 9.5 Care and feeding of your Markov chain Session info", " 9 Markov Chain Monte Carlo This chapter introduces one commonplace example of Fortuna and Minerva’s cooperation: the estimation of posterior probability distributions using a stochastic process known as Markov chain Monte Carlo (MCMC)\" (McElreath, 2020a, p. 263, emphasis in the original). Though we’ve been using MCMC via the brms package for chapters, now, this chapter should clarify some of the questions you might have about the details. 9.0.0.1 Rethinking: Stan was a man. The Stan programming language is not an abbreviation or acronym. Rather, it is named after Stanisław Ulam. Ulam is credited as one of the inventors of Markov chain Monte Carlo. Together with Ed Teller, Ulam applied it to designing fusion bombs. But he and others soon applied the general Monte Carlo method to diverse problems of less monstrous nature. Ulam made important contributions in pure mathematics, chaos theory, and molecular and theoretical biology, as well. (p. 264) 9.1 Good King Markov and his island kingdom Here we simulate King Markov’s journey. In this version of the code, we’ve added set.seed(), which helps make the exact results reproducible. set.seed(9) num_weeks &lt;- 1e5 positions &lt;- rep(0, num_weeks) current &lt;- 10 for (i in 1:num_weeks) { # record current position positions[i] &lt;- current # flip coin to generate proposal proposal &lt;- current + sample(c(-1, 1), size = 1) # now make sure he loops around the archipelago if (proposal &lt; 1) proposal &lt;- 10 if (proposal &gt; 10) proposal &lt;- 1 # move? prob_move &lt;- proposal/current current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current) } In this chapter, we’ll take our plotting theme from the ggpomological package (Aden-Buie, 2020). library(ggpomological) To get the full benefits from ggpomological, you may need to download some fonts. Throughout this chapter, I make extensive use of Marck Script, which you find at https://fonts.google.com/specimen/Marck+Script. Once you’ve installed the font on your computer, you may also have to execute extrafont::font_import(). Here we get a sense of the colors we’ll be using with our dots and lines and so on. scales::show_col(ggpomological:::pomological_palette) This will make it easier to access those colors. pomological_palette &lt;- ggpomological:::pomological_palette Now make Figure 9.2.a. library(tidyverse) tibble(week = 1:1e5, island = positions) %&gt;% ggplot(aes(x = week, y = island)) + geom_point(shape = 1, color = pomological_palette[1]) + scale_x_continuous(breaks = seq(from = 0, to = 100, by = 20)) + scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) + coord_cartesian(xlim = c(0, 100)) + labs(title = &quot;Behold the Metropolis algorithm in action!&quot;, subtitle = &quot;The dots show the king&#39;s path over the first 100 weeks.&quot;) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) Figure 9.2.b. tibble(week = 1:1e5, island = positions) %&gt;% mutate(island = factor(island)) %&gt;% ggplot(aes(x = island)) + geom_bar(fill = pomological_palette[2]) + scale_y_continuous(&quot;number of weeks&quot;, expand = expansion(mult = c(0, 0.05))) + labs(title = &quot;Old Metropolis shines in the long run.&quot;, subtitle = &quot;Sure enough, the time the king spent on each island\\nwas proportional to its population size.&quot;) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) 9.2 Metropolis algorithms “The Metropolis algorithm is the grandparent of several different strategies for getting samples from unknown posterior distributions” (p. 267). If you’re interested, Robert &amp; Casella (2011) wrote a good historical overview of MCMC. 9.2.1 Gibbs sampling. The Gibbs sampler (Casella &amp; George, 1992; Geman &amp; Geman, 1984) uses conjugate pairs (i.e., pairs of priors and likelihoods that have analytic solutions for the posterior of an individual parameter) to efficiently sample from the posterior. Gibbs was the workhorse algorithm during the rise of Bayesian computation in the 1990s and it was highlighted in Bayesian software like BUGS (D. Spiegelhalter et al., 2003) and JAGS (Plummer, 2003). We will not be using the Gibbs sampler in this project. It’s available for use in R. For an extensive applied introduction, check out Kruschke’s (2015) text. 9.2.2 High-dimensional problems. The Gibbs sampler is limited in that (a) you might not want to use conjugate priors and (b) it can be quite inefficient with complex hierarchical models, which we’ll be fitting soon. Earlier versions of this ebook did not focus on McElreath’s example of the pathology high autocorrelations can create when using the Metropolis algorithm, which is depicted in Figure 9.3. However, James Henegan kindly reached out with a tidyverse workflow for reproducing this example. Here is a slightly amended version of that workflow. The first step is to simulate a bivariate distribution “with a strong negative correlation of -0.9” (p. 268). Henagen simulated the from a distribution where the two variables \\(\\text a_1\\) and \\(\\text a_2\\) followed the bivariate normal distribution \\[\\begin{align*} \\begin{bmatrix} \\text a_1 \\\\ \\text a_2 \\end{bmatrix} &amp; \\sim \\operatorname{MVNormal} \\left (\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma \\right) \\\\ \\mathbf \\Sigma &amp; = \\mathbf{SRS} \\\\ \\mathbf S &amp; = \\begin{bmatrix} 0.22 &amp; 0 \\\\ 0 &amp; 0.22 \\end{bmatrix} \\\\ \\mathbf R &amp; = \\begin{bmatrix} 1 &amp; -.9 \\\\ -.9 &amp; 1 \\end{bmatrix}, \\end{align*}\\] where the variance/covariance matrix is decomposed into a \\(2 \\times 2\\) matrix of standard deviations and a \\(2 \\times 2\\) correlation matrix. In this example, both variables (\\(\\text a_1\\) and \\(\\text a_2\\)) have standard deviations of 0.22. We’ll have more practice with data of this kind in Chapter 14. For now, just go with it. Here’s how to simulate from this distribution. # mean vector mu &lt;- c(0, 0) # variance/covariance matrix sd_a1 &lt;- 0.22 sd_a2 &lt;- 0.22 rho &lt;- -.9 Sigma &lt;- matrix(data = c(sd_a1^2, rho * sd_a1 * sd_a2, rho * sd_a1 * sd_a2, sd_a2^2), nrow = 2) # sample from the distribution with the `mvtnorm::rmvnorm()` function set.seed(9) my_samples &lt;- mvtnorm::rmvnorm(n = 1000, mean = mu, sigma = Sigma) Check the sample correlation. data.frame(my_samples) %&gt;% set_names(str_c(&quot;a&quot;, 1:2)) %&gt;% summarise(rho = cor(a1, a2)) ## rho ## 1 -0.9106559 We won’t actually be using the values from this simulation. Instead, we can evaluate the density function for this distribution using the mvtnorm::dmvnorm() function. But even before that, we’ll want to create a grid of values for the contour lines in Figure 9.3. Here we do so with a custom function called x_y_grid(). # define the function x_y_grid &lt;- function(x_start = -1.6, x_stop = 1.6, x_length = 100, y_start = -1.6, y_stop = 1.6, y_length = 100) { x_domain &lt;- seq(from = x_start, to = x_stop, length.out = x_length) y_domain &lt;- seq(from = y_start, to = y_stop, length.out = y_length) x_y_grid_tibble &lt;- tidyr::expand_grid(a1 = x_domain, a2 = y_domain) return(x_y_grid_tibble) } # simulate contour_plot_dat &lt;- x_y_grid() # what have we done? str(contour_plot_dat) ## tibble [10,000 × 2] (S3: tbl_df/tbl/data.frame) ## $ a1: num [1:10000] -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 ... ## $ a2: num [1:10000] -1.6 -1.57 -1.54 -1.5 -1.47 ... Now compute the density values for each combination of a1 and a2. contour_plot_dat &lt;- contour_plot_dat %&gt;% mutate(d = mvtnorm::dmvnorm(as.matrix(contour_plot_dat), mean = mu, sigma = Sigma)) head(contour_plot_dat) ## # A tibble: 6 x 3 ## a1 a2 d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.6 -1.6 1.47e-229 ## 2 -1.6 -1.57 6.08e-225 ## 3 -1.6 -1.54 2.24e-220 ## 4 -1.6 -1.50 7.38e-216 ## 5 -1.6 -1.47 2.17e-211 ## 6 -1.6 -1.44 5.68e-207 To get a sense of what we’ve done, here are those data as a 2D density plot. contour_plot_dat %&gt;% ggplot(aes(x = a1, y = a2, fill = d)) + geom_raster(interpolate = T) + scale_fill_gradientn(colors = pomological_palette[c(8, 2, 4)], limits = c(0, NA)) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) But we don’t want a density plot. We want contour lines! contour_plot &lt;- contour_plot_dat %&gt;% ggplot() + geom_contour(aes(x = a1, y = a2, z = d), size = 1/8, color = pomological_palette[4], breaks = 9^(-(10 * 1:25))) + scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) contour_plot Note how we saved that plot as contour_plot, which will serve as the base for the two panels in our Figure 9.3. Next we use the Metropolis algorithm to sample from the posterior defined, above. Henagen’s implementation is wrapped in a custom function he called metropolis(), which is designed to track coordinates of candidate (i.e., proposal) points, and whether or not the candidate points were accepted. Here we define metropolis() with slight amendments to Henagen’s original. metropolis &lt;- function(num_proposals = 50, step_size = 0.1, starting_point = c(-1, 1)) { # Initialize vectors where we will keep track of relevant candidate_x_history &lt;- rep(-Inf, num_proposals) candidate_y_history &lt;- rep(-Inf, num_proposals) did_move_history &lt;- rep(FALSE, num_proposals) # Prepare to begin the algorithm... current_point &lt;- starting_point for(i in 1:num_proposals) { # &quot;Proposals are generated by adding random Gaussian noise # to each parameter&quot; noise &lt;- rnorm(n = 2, mean = 0, sd = step_size) candidate_point &lt;- current_point + noise # store coordinates of the proposal point candidate_x_history[i] &lt;- candidate_point[1] candidate_y_history[i] &lt;- candidate_point[2] # evaluate the density of our posterior at the proposal point candidate_prob &lt;- mvtnorm::dmvnorm(candidate_point, mean = mu, sigma = Sigma) # evaluate the density of our posterior at the current point current_prob &lt;- mvtnorm::dmvnorm(current_point, mean = mu, sigma = Sigma) # Decide whether or not we should move to the candidate point acceptance_ratio &lt;- candidate_prob / current_prob should_move &lt;- ifelse(runif(n = 1) &lt; acceptance_ratio, TRUE, FALSE) # Keep track of the decision did_move_history[i] &lt;- should_move # Move if necessary if(should_move) { current_point &lt;- candidate_point } } # once the loop is complete, store the relevant results in a tibble results &lt;- tibble::tibble( candidate_x = candidate_x_history, candidate_y = candidate_y_history, accept = did_move_history ) # compute the &quot;acceptance rate&quot; by dividing the total number of &quot;moves&quot; # by the total number of proposals number_of_moves &lt;- results %&gt;% dplyr::pull(accept) %&gt;% sum(.) acceptance_rate &lt;- number_of_moves/num_proposals return(list(results = results, acceptance_rate = acceptance_rate)) } Run the algorithm for the panel on the left, which uses a step size of 0.1. set.seed(9) round_1 &lt;- metropolis(num_proposals = 50, step_size = 0.1, starting_point = c(-1,1)) Use round_1 to make Figure 9.3a. p1 &lt;- contour_plot + geom_point(data = round_1$results, aes(x = candidate_x, y = candidate_y, color = accept, shape = accept)) + labs(subtitle = str_c(&quot;step size 0.1,\\naccept rate &quot;, round_1$acceptance_rate), x = &quot;a1&quot;, y = &quot;a2&quot;) + scale_shape_manual(values = c(21, 19)) + scale_color_manual(values = pomological_palette[8:9]) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) p1 Now run the algorithm with step size set to 0.25. Then make Figure 9.3b, combine the two ggplots, and return the our full version of Figure 9.3. # simulate set.seed(9) round_2 &lt;- metropolis(num_proposals = 50, step_size = 0.25, starting_point = c(-1,1)) # plot p2 &lt;- contour_plot + geom_point(data = round_2$results, mapping = aes(x = candidate_x, y = candidate_y, color = accept, shape = accept)) + scale_shape_manual(values = c(21, 19)) + scale_color_manual(values = pomological_palette[8:9]) + scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) + labs(subtitle = str_c(&quot;step size 0.25,\\naccept rate &quot;, round_2$acceptance_rate), x = &quot;a1&quot;) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) # combine library(patchwork) (p1 + p2) + plot_annotation(theme = theme_pomological_fancy(base_family = &quot;Marck Script&quot;), title = &quot;Metropolis chains under high correlation&quot;) + plot_layout(guides = &quot;collect&quot;) Our acceptance rates differ from McElreath’s due to simulation variance. If you want to get a sense of stability in the acceptance rates, just simulate some more. For example, we might wrap metropolis() inside another function that takes a simulation seed value. metropolis_with_seed &lt;- function(seed, step_size = 0.1) { set.seed(seed) met &lt;- metropolis(num_proposals = 50, step_size = step_size, starting_point = c(-1,1)) return(met$acceptance_rate) } Kick the times and iterate 500 times. ars &lt;- tibble(seed = 1:500) %&gt;% mutate(acceptance_rate = map_dbl(seed, metropolis_with_seed)) Now summarize the results in a histogram. ars %&gt;% ggplot(aes(x = acceptance_rate)) + geom_histogram(binwidth = .025, fill = pomological_palette[5]) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) If you iterate with step_size = 0.25, instead, the resulting histogram will look very different. Now we turn our focus to Figure 9.4. McElreath threw us a bone and gave us the code in his R code 9.4. Here we’ll warp his code into a function called concentration_sim() which adds a seed argument for reproducibility. concentration_sim &lt;- function(d = 1, t = 1e3, seed = 9) { set.seed(seed) y &lt;- rethinking::rmvnorm(t, rep(0, d), diag(d)) rad_dist &lt;- function(y) sqrt(sum(y^2)) rd &lt;- sapply(1:t, function(i) rad_dist( y[i, ])) } Now run the simulation four times and plot. d &lt;- tibble(d = c(1, 10, 100, 1000)) %&gt;% mutate(con = map(d, concentration_sim)) %&gt;% unnest(con) %&gt;% mutate(`# dimensions` = factor(d)) d %&gt;% ggplot(aes(x = con, fill = `# dimensions`)) + geom_density(size = 0, alpha = 3/4) + scale_fill_pomological() + xlab(&quot;Radial distance from mode&quot;) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) + theme(legend.position = c(.7, .625)) With high-dimensional posteriors, sampled points are in a thin, high-dimensional shell very far from the mode. This shell can create very hard paths for a sampler to follow. This is why we need MCMC algorithms that focus on the entire posterior at once, instead of one or a few dimensions at a time like Metropolis and Gibbs. Otherwise we get stuck in a narrow, highly curving region of parameter space. (p. 270) 9.3 Hamiltonian Monte Carlo Hamiltonian Monte Carlo (HMC) is more computationally costly and more efficient than Gibbs at sampling from the posterior. It needs fewer samples, especially when fitting models with many parameters. To learn more about how HMC works, check out McElreath’s lecture on the topic from January 2019; his blog post, Markov chains: Why walk when you can flow?; or one of these lectures (here, here, or here) by Michael Betancourt. 9.3.1 Another parable. This section is beyond the scope of this project. 9.3.2 Particles in space. This section is beyond the scope of this project. 9.3.3 Limitations. As always, there are some limitations. HMC requires continuous parameters. It can’t glide through a discrete parameter. In practice, this means that certain techniques, like the imputation of discrete missing data, have to be done differently with HMC. HMC can certainly sample from such models, often much more efficiently than a Gibbs sampler could. But you have to change how you code them. (p. 278) 9.4 Easy HMC: ulam brm() Much like McElreath’s rethinking package, brms provides a convenient interface to HMC via Stan. Other packages providing Stan interfaces include rstanarm (Brilleman et al., 2018; Gabry &amp; Goodrich, 2020) and blavaan (Merkle et al., 2020; Merkle &amp; Rosseel, 2018). I’m not aware of any up-to-date comparisons across the packages. If you’re ever inclined to make one, let the rest of us know! Here we load the rugged data and brms. library(brms) data(rugged, package = &quot;rethinking&quot;) d &lt;- rugged rm(rugged) It takes just a sec to do a little data manipulation. d &lt;- d %&gt;% mutate(log_gdp = log(rgdppc_2000)) dd &lt;- d %&gt;% drop_na(rgdppc_2000) %&gt;% mutate(log_gdp_std = log_gdp / mean(log_gdp), rugged_std = rugged / max(rugged), cid = ifelse(cont_africa == 1, &quot;1&quot;, &quot;2&quot;)) %&gt;% mutate(rugged_std_c = rugged_std - mean(rugged_std)) In the context of this chapter, it doesn’t make sense to translate McElreath’s m8.3 quap() code to brm() code. Below, we’ll just go directly to the brm() variant of his m9.1. 9.4.1 Preparation. When working with brms, you don’t need to do the data processing McElreath did on page 280. If you wanted to, however, here’s how you might do it within the tidyverse. dat_slim &lt;- dd %&gt;% mutate(cid = as.integer(cid)) %&gt;% select(log_gdp_std, rugged_std, cid, rugged_std_c) %&gt;% list() str(dat_slim) 9.4.2 Sampling from the posterior. Finally, we get to work that sweet HMC via brms::brm(). b9.1 &lt;- brm(data = dd, family = gaussian, bf(log_gdp_std ~ 0 + a + b * (rugged_std - 0.215), a ~ 0 + cid, b ~ 0 + cid, nl = TRUE), prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a), prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a), prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b), prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b), prior(exponential(1), class = sigma)), chains = 1, cores = 1, seed = 9, file = &quot;fits/b09.01&quot;) This was another instance of the brms non-linear syntax, We’ve already introduced this in Section 4.4.2.1, Section 5.3.2, and Section 6.2.1. For even more details, you can always peruse Bürkner’s (2021e) vignette, Estimating non-linear models with brms. Here is a summary of the posterior. print(b9.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp_std ~ 0 + a + b * (rugged_std - 0.215) ## a ~ 0 + cid ## b ~ 0 + cid ## Data: dd (Number of observations: 170) ## Samples: 1 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 1000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_cid1 0.89 0.02 0.86 0.92 1.00 1114 721 ## a_cid2 1.05 0.01 1.03 1.07 1.00 1106 658 ## b_cid1 0.13 0.08 -0.01 0.29 1.00 1015 773 ## b_cid2 -0.14 0.06 -0.26 -0.03 1.01 1241 712 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.13 1.00 997 720 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Unlike McElreath’s precis() output, our output has an Rhat instead of Rhat4. McElreath’s documentation indicated his Rhat4 values are based on the \\(\\widehat R\\) from Gelman et al. (2013). To my knowledge, brms uses the same formula. McElreath also remarked he expected to update to an Rhat5 in the near future. I believe he was referencing Vehtari, Gelman, et al. (2019). I am under the impression this will be implemented at the level of the underlying Stan code, which means brms will get the update, too. To learn more, check out the New R-hat and ESS thread on the Stan Forums. Also of note, McElreath’s rethinking::precis() returns highest posterior density intervals (HPDIs) when summarizing ulam() models. Not so with brms. If you want HPDIs, you might use the convenience functions from the tidybayes package. Here’s an example. library(tidybayes) post &lt;- posterior_samples(b9.1) post %&gt;% pivot_longer(-lp__) %&gt;% group_by(name) %&gt;% mean_hdi(value, .width = .89) # note our rare use of 89% intervals ## # A tibble: 5 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 b_a_cid1 0.887 0.858 0.910 0.89 mean hdi ## 2 b_a_cid2 1.05 1.03 1.07 0.89 mean hdi ## 3 b_b_cid1 0.132 0.0101 0.245 0.89 mean hdi ## 4 b_b_cid2 -0.144 -0.228 -0.0505 0.89 mean hdi ## 5 sigma 0.112 0.102 0.123 0.89 mean hdi There’s one more important difference in our brms summary output compared to McElreath’s rethinking::precis() output. In the text we learn precis() returns n_eff values for each parameter. Earlier versions of brms used to have a direct analogue named Eff.Sample. Both were estimates of the effective number of samples (a.k.a. the effective sample size) for each parameter. As with typical sample size, the more the merrier. Starting with version 2.10.0, brms now returns two columns: Bulk_ESS and Tail_ESS. These originate from the same Vehtari, Gelman, et al. (2019) paper that introduced the upcoming change to the \\(\\widehat R\\). From the paper, we read: If you plan to report quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3 we show that convergence of Markov chains is not uniform across the parameter space and propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as the “bulk-ESS”), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (p. 5, emphasis in the original) For more technical details, see the paper. In short, Bulk_ESS in the output from brms 2.10.0+ is what was previously referred to as Eff.Sample in earlier versions. It’s also what corresponds to what McElreath calls n_eff. This indexed the number of effective samples in ‘the center of the’ posterior distribution (i.e., the posterior mean or median). But since we also care about uncertainty in our parameters, we care about stability in the 95% intervals and such. The new Tail_ESS in brms output allows us to gauge the effective sample size for those intervals. 9.4.3 Sampling again, in parallel. You can easily parallelize those chains… They can all run at the same time, instead of in sequence. So as long as your computer has four cores (it probably does), it won’t take longer to run four chains than one chain. To run four independent Markov chains for the model above, and to distribute them across separate cores in your computer, just increase the number of chains and add a cores argument. (p. 281) If you don’t know how many cores you have on your computer, you can always check with the parallel::detectCores() function. My current laptop has 16. parallel::detectCores() ## [1] 16 Here we sample from four HMC chains in parallel by adding cores = 4. b9.1b &lt;- update(b9.1, chains = 4, cores = 4, seed = 9, file = &quot;fits/b09.01b&quot;) This model sampled so fast that it really didn’t matter if we sampled in parallel or not. It will for others. print(b9.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp_std ~ 0 + a + b * (rugged_std - 0.215) ## a ~ 0 + cid ## b ~ 0 + cid ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_cid1 0.89 0.02 0.86 0.92 1.00 4613 2993 ## a_cid2 1.05 0.01 1.03 1.07 1.00 4782 3058 ## b_cid1 0.13 0.07 -0.01 0.28 1.00 4397 2724 ## b_cid2 -0.14 0.06 -0.25 -0.03 1.00 5029 3104 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.12 1.00 4693 3152 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The show() function does not work for brms models the same way it does with those from rethinking. Rather, show() returns the same information we’d get from print() or summary(). show(b9.1b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: log_gdp_std ~ 0 + a + b * (rugged_std - 0.215) ## a ~ 0 + cid ## b ~ 0 + cid ## Data: dd (Number of observations: 170) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_cid1 0.89 0.02 0.86 0.92 1.00 4613 2993 ## a_cid2 1.05 0.01 1.03 1.07 1.00 4782 3058 ## b_cid1 0.13 0.07 -0.01 0.28 1.00 4397 2724 ## b_cid2 -0.14 0.06 -0.25 -0.03 1.00 5029 3104 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.11 0.01 0.10 0.12 1.00 4693 3152 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). You can get a focused look at the formula and prior information from a brms fit object by subsetting them directly. b9.1b$formula ## log_gdp_std ~ 0 + a + b * (rugged_std - 0.215) ## a ~ 0 + cid ## b ~ 0 + cid b9.1b$prior ## prior class coef group resp dpar nlpar bound source ## (flat) b a default ## normal(1, 0.1) b cid1 a user ## normal(1, 0.1) b cid2 a user ## (flat) b b default ## normal(0, 0.3) b cid1 b user ## normal(0, 0.3) b cid2 b user ## exponential(1) sigma user You can get that information on the model priors with the prior_summary() function. prior_summary(b9.1b) ## prior class coef group resp dpar nlpar bound source ## (flat) b a default ## normal(1, 0.1) b cid1 a user ## normal(1, 0.1) b cid2 a user ## (flat) b b default ## normal(0, 0.3) b cid1 b user ## normal(0, 0.3) b cid2 b user ## exponential(1) sigma user I am not aware of a convenient way to pull the information on how long each chain ran for. As to the sample size, our output is a little different than McElreath’s. The brms default is to run 4 chains, each containing 2,000 total samples, the first 1,000 of which are warmups. Since we used those defaults, we ended up with \\((2{,}000 - 1{,}000) \\times 4 = 4{,}000\\) post-warmup HMC samples. But anyways, just like all of McElreath’s n_eff values were above 2,000, most of our Bulk_ESS values were above 4,000, which is no mistake. The adaptive sampler that Stan uses is so good, it can actually produce sequential samples that are better than uncorrelated. They are anti-correlated. This means it can explore the posterior distribution so efficiently that it can beat random. (p. 282) In addition to sampling HMC chains in parallel, the Stan team have been working on within-chain parallelization, too. This is a new development and was just made available to brms users with the release of brms version 2.14.0. I don’t plan on covering within-chain parallelization in this ebook, but you can learn more in Weber and Bürkner’s (2021) vignette, Running brms models with within-chain parallelization, or Weber’s guest post on Gelman’s blog, Stan’s within-chain parallelization now available with brms. If you have some large rough model that’s taking hours upon hours to fit, it might be worth your while. 9.4.4 Visualization. As with McElreath’s rethinking, brms allows users to put the fit object directly into the pairs() function. pairs(b9.1b, off_diag_args = list(size = 1/5, alpha = 1/5)) Our output is a little different in that we don’t get a lower-triangle of Pearson’s correlation coefficients. If you’d like those values, use vcov(). vcov(b9.1b, correlation = T) %&gt;% round(digits = 2) ## a_cid1 a_cid2 b_cid1 b_cid2 ## a_cid1 1.00 -0.03 0.17 0.05 ## a_cid2 -0.03 1.00 0.01 -0.09 ## b_cid1 0.17 0.01 1.00 -0.03 ## b_cid2 0.05 -0.09 -0.03 1.00 Note, however, that this will only return the correlations among the ‘Population-Level Effects.’ Within brms, \\(\\sigma\\) is classified among the ‘Family Specific Parameters.’ We have more options still. As a first step, use the brms::posterior_samples() function to extract the posterior samples within a data frame. post &lt;- posterior_samples(b9.1b) glimpse(post) ## Rows: 4,000 ## Columns: 6 ## $ b_a_cid1 &lt;dbl&gt; 0.9064987, 0.8718809, 0.9012252, 0.8977892, 0.8703916, 0.9253578, 0.9209231, 0.85… ## $ b_a_cid2 &lt;dbl&gt; 1.050241, 1.060212, 1.044918, 1.047780, 1.041505, 1.035713, 1.045927, 1.058108, 1… ## $ b_b_cid1 &lt;dbl&gt; 0.05395168, 0.17354574, 0.10481150, 0.12121863, 0.24473996, 0.20813709, 0.2348308… ## $ b_b_cid2 &lt;dbl&gt; -0.12886759, -0.12364289, -0.19158438, -0.18779887, -0.17745017, -0.09887813, -0.… ## $ sigma &lt;dbl&gt; 0.11045080, 0.11337407, 0.10222348, 0.10314139, 0.09620476, 0.12221787, 0.1165980… ## $ lp__ &lt;dbl&gt; 133.1505, 133.4317, 132.5007, 133.3072, 128.2402, 129.3591, 131.5271, 132.3988, 1… Another nice way to customize your pairs plot is with the GGally package. For this approach, you feed the post data into the ggpairs() function. library(GGally) post %&gt;% select(-lp__ ) %&gt;% ggpairs() Now we get the pairs plot on the lower triangle and the Pearson’s correlation coefficients in the upper. Since GGally::ggpairs() returns a ggplot2 object, you can customize it as you please. # make the custom functions my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(fill = pomological_palette[7], color = pomological_palette[6]) } my_upper &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_bin2d() + scale_fill_gradient(low = pomological_palette[4], high = pomological_palette[1]) } # plot! post %&gt;% select(-lp__ ) %&gt;% ggpairs(lower = list(continuous = wrap(&quot;cor&quot;, family = &quot;Marck Script&quot;, color = &quot;black&quot;)), diag = list(continuous = my_diag), upper = list(continuous = my_upper)) + labs(subtitle = &quot;My custom pairs plot&quot;) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) For more ideas on customizing a ggpairs() plot, go here. 9.4.5 Checking the chain. Using plot() for a brm() fit returns both density and trace lots for the parameters. plot(b9.1b) The bayesplot package allows a little more control. Here, we use the bayesplot::mcmc_trace() function to show only trace plots with our custom theme. Note that mcmc_trace() works with data frames, not brmfit objects. There’s a further complication. Recall how we made post (i.e., post &lt;- posterior_samples(b8.1)). Our post data frame carries no information on chains. To retain that information, we’ll need to add an add_chain = T argument to our posterior_samples() function. library(bayesplot) post &lt;- posterior_samples(b9.1b, add_chain = T) mcmc_trace(post[, c(1:5, 7)], # we need to include column 7 because it contains the chain info facet_args = list(ncol = 3), size = .15) + scale_color_pomological() + labs(title = &quot;My custom trace plots&quot;) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) + theme(legend.position = c(.95, .2)) The bayesplot package offers a variety of diagnostic plots. Here we make autocorrelation plots for all model parameters, one for each HMC chain. post %&gt;% mcmc_acf(pars = vars(b_a_cid1:sigma), lags = 5) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) That’s just what we like to see–nice L-shaped autocorrelation plots. Those are the kinds of shapes you’d expect when you have reasonably large effective samples. Before we move on, there’s an important difference between the trace plots McElreath showed in the text and the ones we just made. McElreath’s trace plots include the warmup iterations. Ours did not. To my knowledge, neither the brms::plot() nor the bayesplot::mcmc_trace() functions support including warmups in their trace plots. One quick way to get them is with the ggmcmc package (Fernández i Marín, 2016, 2020). library(ggmcmc) The ggmcmc package has a variety of convenience functions for working with MCMC chains. The ggs() function extracts the posterior draws, including warmup, and arranges them in a tidy tibble. ggs(b9.1b) %&gt;% str() ## tibble [40,000 × 4] (S3: tbl_df/tbl/data.frame) ## $ Iteration: int [1:40000] 1 2 3 4 5 6 7 8 9 10 ... ## $ Chain : int [1:40000] 1 1 1 1 1 1 1 1 1 1 ... ## $ Parameter: Factor w/ 5 levels &quot;b_a_cid1&quot;,&quot;b_a_cid2&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ value : num [1:40000] 0.771 0.771 0.771 0.771 0.836 ... ## - attr(*, &quot;nChains&quot;)= int 4 ## - attr(*, &quot;nParameters&quot;)= int 5 ## - attr(*, &quot;nIterations&quot;)= int 2000 ## - attr(*, &quot;nBurnin&quot;)= num 1000 ## - attr(*, &quot;nThin&quot;)= num 1 ## - attr(*, &quot;description&quot;)= chr &quot;80cd85644da91dd98f7c7533c1ed79b9&quot; With this in hand, we can now include those warmup draws in our trace plots. Here’s how to do so without convenience functions like bayesplot::mcmc_trace(). ggs(b9.1b) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value)) + # this marks off the warmups annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf, fill = pomological_palette[9], alpha = 1/6, size = 0) + geom_line(aes(color = chain), size = .15) + scale_color_pomological() + labs(title = &quot;My custom trace plots with warmups via ggmcmc::ggs()&quot;, x = NULL, y = NULL) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) + theme(legend.position = c(.95, .18)) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) Following brms defaults, we won’t include warmup iterations in the trace plots for other models in this book. A nice thing about plots that do contain them, though, is they reveal how quickly our HMC chains transition away from their start values into the posterior. To get a better sense of this, let’s make those trace plots once more, but this time zooming in on the first 50 iterations. ggs(b9.1b) %&gt;% mutate(chain = factor(Chain)) %&gt;% ggplot(aes(x = Iteration, y = value, color = chain)) + annotate(geom = &quot;rect&quot;, xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf, fill = pomological_palette[9], alpha = 1/6, size = 0) + geom_line(size = .5) + scale_color_pomological() + labs(title = &quot;Another custom trace plots with warmups via ggmcmc::ggs()&quot;, x = NULL, y = NULL) + coord_cartesian(xlim = c(1, 50)) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) + theme(legend.position = c(.95, .18)) + facet_wrap(~ Parameter, scales = &quot;free_y&quot;) For each parameter, the all four chains had moved away from their starting values to converge on the marginal posteriors by the 30th iteration or so. But anyway, we’ve veered a bit from the text. McElreath pointed out a second way to visualize the chains is by the distribution of the ranked samples, which he called a trank plot (short for trace rank plot). I’m not aware that brms has a built-in function for that. Happily, we can make them with mcmc_rank_overlay(). post %&gt;% mcmc_rank_overlay(pars = vars(b_a_cid1:sigma)) + scale_color_pomological() + ggtitle(&quot;My custom trank plots&quot;) + coord_cartesian(ylim = c(25, NA)) + theme_pomological_fancy(base_family = &quot;Marck Script&quot;) + theme(legend.position = c(.95, .2)) What this means is to take all the samples for each individual parameter and rank them. The lowest sample gets rank 1. The largest gets the maximum rank (the number of samples across all chains). Then we draw a histogram of these ranks for each individual chain. Why do this? Because if the chains are exploring the same space efficiently, the histograms should be similar to one another and largely overlapping. …The horizontal is rank, from 1 to the number of samples across all chains (2000 in this example). The vertical axis is the frequency of ranks in each bin of the histogram. This trank plot is what we hope for: Histograms that overlap and stay within the same range. (pp. 284–285) 9.4.5.1 Overthinking: Raw Stan model code. The stancode() function works with brms much like it does with rethinking. brms::stancode(b9.1b) ## // generated with brms 2.15.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## vector[N] Y; // response variable ## int&lt;lower=1&gt; K_a; // number of population-level effects ## matrix[N, K_a] X_a; // population-level design matrix ## int&lt;lower=1&gt; K_b; // number of population-level effects ## matrix[N, K_b] X_b; // population-level design matrix ## // covariate vectors for non-linear functions ## vector[N] C_1; ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## vector[K_a] b_a; // population-level effects ## vector[K_b] b_b; // population-level effects ## real&lt;lower=0&gt; sigma; // residual SD ## } ## transformed parameters { ## } ## model { ## // likelihood including constants ## if (!prior_only) { ## // initialize linear predictor term ## vector[N] nlp_a = X_a * b_a; ## // initialize linear predictor term ## vector[N] nlp_b = X_b * b_b; ## // initialize non-linear predictor term ## vector[N] mu; ## for (n in 1:N) { ## // compute non-linear predictor values ## mu[n] = 0 + nlp_a[n] + nlp_b[n] * (C_1[n] - 0.215); ## } ## target += normal_lpdf(Y | mu, sigma); ## } ## // priors including constants ## target += normal_lpdf(b_a[1] | 1, 0.1); ## target += normal_lpdf(b_a[2] | 1, 0.1); ## target += normal_lpdf(b_b[1] | 0, 0.3); ## target += normal_lpdf(b_b[2] | 0, 0.3); ## target += exponential_lpdf(sigma | 1); ## } ## generated quantities { ## } You can also get that information by executing b9.1b$model or b9.1b$fit@stanmodel. 9.5 Care and feeding of your Markov chain Markov chain Monte Carlo is a highly technical and usually automated procedure. You might write your own MCMC code, for the sake of learning. But it is very easy to introduce subtle biases. A package like Stan, in contrast, is continuously tested against expected output. Most people who use Stan don’t really understand what it is doing, under the hood. That’s okay. Science requires division of labor, and if every one of us had to write our own Markov chains from scratch, a lot less research would get done in the aggregate. (p. 287) If you do want to learn more about HMC, McElreath has some nice introductory lectures on the topic (see here and here). To dive even deeper, Michael Betancourt from the Stan team has given many lectures on the topic (e.g., here and here). 9.5.1 How many samples do you need? The brms defaults are iter = 2000 and warmup = 1000, which are twice the number as in McElreath’s rethinking package. If all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do. But if you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many more. So there is no universally useful number of samples to aim for. In most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think more about which region of the distribution interests you. Stan will sometimes warn you about “tail ESS,” the effective sample size (similar to n_eff) in the tails of the posterior. In those cases, it is nervous about the quality of extreme intervals, like 95%. Sampling more usually helps. (pp. 287-288) And remember, with changes from brms version 2.10.0, we now have both Bulk_ESS and Tail_ESS to consult when thinking about the effective sample size. What McElreath referred to as n_eff is what we now think of as Bulk_ESS when using brms. When McElreath referred to the “tail ESS” in the end of that block quote, that’s our brms Tail_ESS number. 9.5.1.1 Rethinking: Warmup is not burn-in. Other MCMC algorithms and software often discuss burn-in…. But Stan’s sampling algorithms use a different approach. What Stan does during warmup is quite different from what it does after warmup. The warmup samples are used to adapt sampling, to find good values for the step size and the number of steps. Warmup samples are not representative of the target posterior distribution, no matter how long warmup continues. They are not burning in, but rather more like cycling the motor to heat things up and get ready for sampling. When real sampling begins, the samples will be immediately from the target distribution, assuming adaptation was successful. (p. 288) 9.5.2 How many chains do you need? It is very common to run more than one Markov chain, when estimating a single model. To do this with [brms], the chains argument specifies the number of independent Markov chains to sample from. And the optional cores argument lets you distribute the chains across different processors, so they can run simultaneously, rather than sequentially…. for typical regression models, you can live by the motto one short chain to debug, four chains for verification and inference. (pp. 288–289, emphasis in the original) 9.5.2.1 Rethinking: Convergence diagnostics. We’ve already covered how brms has expanded the traditional notion of effective samples (i.e., n_eff) to Bulk_ESS and Tail_ESS. Times are changing for the \\(\\widehat R\\), too. However, it turns out the Stan team has found some deficiencies with the \\(\\widehat R\\), for which they’ve made recommendations that will be implemented in the Stan ecosystem sometime soon (see here for a related thread on the Stan Forums). In the meantime, you can read all about it in Vehtari, Gelman, et al. (2019) and in one of Dan Simpson’s blog posts. If you learn best by sassy twitter banter, click through this interchange among some of our Stan team all-stars. For more on these topics, you might also check out Gabry and Modrák’s (2021) vignette, Visual MCMC diagnostics using the bayesplot package. 9.5.3 Taming a wild chain. As with rethinking, brms can take data in the form of a list. Recall however, that in order to specify starting values, you need to specify a list of lists with an inits argument rather than with start. b9.2 &lt;- brm(data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(normal(0, 1000), class = Intercept), prior(exponential(0.0001), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 9, file = &quot;fits/b09.02&quot;) Let’s peek at the summary. print(b9.2) ## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be careful when analysing ## the results! We recommend running more iterations and/or setting stronger priors. ## Warning: There were 393 divergent transitions after warmup. Increasing adapt_delta above 0.8 may ## help. See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Samples: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 10.68 270.90 -579.92 700.75 1.05 475 297 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 453.49 1148.02 7.77 3254.50 1.06 63 50 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Much like in the text, this summary is a disaster. Note the warning about divergent transitions. The brms::nuts_params() function allows use to pull a wealth of diagnostic information for the chains from a brms fit. The different kinds of diagnostics are listed in the Parameter column. nuts_params(b9.2) %&gt;% distinct(Parameter) ## Parameter ## 1 accept_stat__ ## 2 stepsize__ ## 3 treedepth__ ## 4 n_leapfrog__ ## 5 divergent__ ## 6 energy__ Our interest is for when Parameter == \"divergent__\". nuts_params(b9.2) %&gt;% filter(Parameter == &quot;divergent__&quot;) %&gt;% count(Value) ## Value n ## 1 0 2607 ## 2 1 393 This indicates that among the 3,000 post-warmup draws, 393 were classified as divergent transitions. We can use the np argument within brms::pairs() to include this information in the pairs() plot. pairs(b9.2, np = nuts_params(b9.2), off_diag_args = list(size = 1/4)) That np = nuts_params(b9.2) trick will work in a similar way with bayesplot functions like mcmc_pairs() and mcmc_trace(). The red x marks show us where the divergent transitions are within the bivariate posterior. To my eye, the pattern in this plot isn’t very strong. Sometimes the pattern of divergent transitions can give you clear clues about where the problems are in the model. Let’s further inspect the damage by making the top two rows of Figure 9.9. post &lt;- posterior_samples(b9.2, add_chain = T) p1 &lt;- post %&gt;% mcmc_trace(pars = vars(b_Intercept:sigma), size = .25) p2 &lt;- post %&gt;% mcmc_rank_overlay(pars = vars(b_Intercept:sigma)) ( (p1 / p2) &amp; scale_color_pomological() &amp; theme_pomological_fancy(base_family = &quot;Marck Script&quot;) &amp; theme(legend.position = &quot;none&quot;) ) + plot_annotation(subtitle = &quot;These chains are not healthy&quot;) Okay, that’s enough disaster. Let’s try a model that adds just a little information by way of weakly-regularizing priors: \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Normal}(\\mu, \\sigma) \\\\ \\mu &amp; = \\alpha \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(1, 10) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Watch our new priors save the day. b9.3 &lt;- brm(data = list(y = c(-1, 1)), family = gaussian, y ~ 1, prior = c(prior(normal(1, 10), class = Intercept), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 9, file = &quot;fits/b09.03&quot;) print(b9.3) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 1 ## Data: list(y = c(-1, 1)) (Number of observations: 2) ## Samples: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.04 1.18 -2.55 2.43 1.00 1126 1158 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.54 0.78 0.59 3.60 1.00 961 994 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As in the text, no more warning signs and no more silly estimates. The trace and trank plots look better, too. post &lt;- posterior_samples(b9.3, add_chain = T) p1 &lt;- post %&gt;% mcmc_trace(pars = vars(b_Intercept:sigma), size = .25) p2 &lt;- post %&gt;% mcmc_rank_overlay(pars = vars(b_Intercept:sigma)) + ylim(35, NA) ( (p1 / p2) &amp; scale_color_pomological() &amp; theme_pomological_fancy(base_family = &quot;Marck Script&quot;) &amp; theme(legend.position = &quot;none&quot;) ) + plot_annotation(subtitle = &quot;Weakly informative priors cleared up the condition right away&quot;) Now behold our version of Figure 9.10. # left p1 &lt;- post %&gt;% select(b_Intercept) %&gt;% ggplot(aes(x = b_Intercept)) + geom_density(trim = T) + geom_line(data = tibble(x = seq(from = -15, to = 15, length.out = 50)), aes(x = x, y = dnorm(x = x, mean = 0, sd = 10)), color = pomological_palette[5], linetype = 2) + xlab(expression(alpha)) # right p2 &lt;- post %&gt;% select(sigma) %&gt;% ggplot(aes(x = sigma)) + geom_density(trim = T) + geom_line(data = tibble(x = seq(from = 0, to = 10, length.out = 50)), aes(x = x, y = dexp(x = x, rate = 1)), color = pomological_palette[9], linetype = 2) + labs(x = expression(sigma), y = NULL) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 0.7)) # combine ( (p1 + p2) &amp; theme_pomological_fancy(base_family = &quot;Marck Script&quot;) ) + plot_annotation(subtitle = &quot;Prior (dashed) and posterior (solid) distributions for the\\nmodel with weakly-informative priors, b9.3&quot;) These weakly informative priors have helped by providing a very gentle nudge towards reasonable values of the parameters. Now values like 30 million are no longer equally plausible as small values like 1 or 2. Lots of problematic chains want subtle priors like these, designed to tune estimation by assuming a tiny bit of prior information about each parameter. And even though the priors end up getting washed out right away–two observations were enough here–they still have a big effect on inference, by allowing us to get an answer. (pp. 292–293) 9.5.3.1 Rethinking: The folk theorem of statistical computing. The example above illustrates Andrew Gelman’s folk theorem of statistical computing: When you have computational problems, often there’s a problem with your model. Before we begin to tune the software and pour more computer power into a problem, it can be useful to go over the model specification again, and the data itself, to make sure the problem isn’t in the pre-sampling stage. (p. 293) 9.5.3.2 Overthinking: Divergent transitions are your friend. You’ll see divergent transition warnings often in using [brms::brm()] and Stan. They are your friend, providing a helpful warning. These warnings arise when the numerical simulation that HMC uses is inaccurate. HMC can detect these inaccuracies. That is one of its major advantages over other sampling approaches, most of which provide few automatic ways to discover bad chains. (p. 293) This is an issue that comes up frequently on the Stan Forums (here’s a link to the brms section). It’s a good idea to take divergent transitions seriously. Reaching out to others in the community is a great way to get guidance. But before you start a new post, make sure you look through the previous threads to keep from posting redundant questions. 9.5.4 Non-identifiable parameters. It appears that the only way to get a brms version of McElreath’s m9.4 and m9.5 is to augment the data. In addition to the Gaussian y vector, we’ll add two constants to the data, intercept_1 = 1 and intercept_2 = 1. set.seed(9) y &lt;- rnorm(100, mean = 0, sd = 1) b9.4 &lt;- brm(data = list(y = y, a1 = 1, a2 = 1), family = gaussian, y ~ 0 + a1 + a2, prior = c(prior(normal(0, 1000), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 9, file = &quot;fits/b09.04&quot;) Our model results don’t perfectly mirror McElreath’s, but they’re identical in spirit. print(b9.4) ## Warning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be careful when analysing ## the results! We recommend running more iterations and/or setting stronger priors. ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + a1 + a2 ## Data: list(y = y, a1 = 1, a2 = 1) (Number of observations: 100) ## Samples: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a1 113.93 563.81 -599.69 1303.63 1.87 5 17 ## a2 -113.99 563.81 -1303.73 599.49 1.87 5 17 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.00 0.08 0.86 1.13 1.41 6 28 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Note the frightening warning message. Those results are a mess! Let’s try again. b9.5 &lt;- brm(data = list(y = y, a1 = 1, a2 = 1), family = gaussian, y ~ 0 + a1 + a2, prior = c(prior(normal(0, 10), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 3, seed = 9, file = &quot;fits/b09.05&quot;) print(b9.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: y ~ 0 + a1 + a2 ## Data: list(y = y, a1 = 1, a2 = 1) (Number of observations: 100) ## Samples: 3 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 3000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a1 -0.09 7.46 -14.99 14.38 1.00 574 576 ## a2 0.04 7.47 -14.46 14.92 1.00 574 568 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.97 0.07 0.85 1.11 1.00 1065 1077 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). “The estimates for a1 and a2 are better identified now. Well, they still aren’t individually identified. But their sum is identified” (p. 296). Now it’s time to make our version of Figure 9.11. Before we do, one of the challenges we’ll have to overcome is the bayesplot::mcmc_rank_overlay() doesn’t seem to give users an easy way to control the faceting behavior of the plotting function. If you try to simultaneously plot all three model parameters with mcmc_rank_overlay(), you’ll end up with the one row and three columns. But I want the reverse. One solution is to make a custom plotting function. Since we’re juggling both mcmc_trace() and mcmc_rank_overlay(), we’ll make the function do both at once. The trick will be to set the function and workflow up so that we only enter in one parameter at a time. Here’s the function. trace_rank &lt;- function(data, var, subtitle = NULL, ymin = NA) { p1 &lt;- data %&gt;% mcmc_trace(pars = var, size = .25, facet_args = list(ncol = 1)) + labs(subtitle = subtitle, y = NULL) + facet_wrap(~ parameter) p2 &lt;- data %&gt;% mcmc_rank_overlay(pars = var) + coord_cartesian(ylim = c(ymin, NA)) + xlab(NULL) tr &lt;- p1 + p2 tr } Now use our custom trace_rank() function to make the six rows one at a time and then combine them with a little patchwork at the end to make our version of Figure 9.11. # b9.4 post &lt;- posterior_samples(b9.4, add_chain = T) p1 &lt;- trace_rank(data = post, var = &quot;b_a1&quot;, subtitle = &quot;b9.4 (bad priors)&quot;) p2 &lt;- trace_rank(data = post, var = &quot;b_a2&quot;) p3 &lt;- trace_rank(data = post, var = &quot;sigma&quot;) # b9.5 post &lt;- posterior_samples(b9.5, add_chain = T) p4 &lt;- trace_rank(data = post, var = &quot;b_a1&quot;, subtitle = &quot;b9.5 (good priors)&quot;, ymin = 30) p5 &lt;- trace_rank(data = post, var = &quot;b_a2&quot;, ymin = 30) p6 &lt;- trace_rank(data = post, var = &quot;sigma&quot;, ymin = 30) # combine! (p1 / p2 / p3 / p4 / p5 / p6) &amp; scale_color_pomological() &amp; theme_pomological_fancy(base_family = &quot;Marck Script&quot;) &amp; theme(legend.position = &quot;none&quot;) The central message in the text, default to weakly-regularizing priors, holds for brms just as it does for rethinking. For more on the topic, see the recommendations from the Stan team. If you want to dive deeper, check out Simpson’s post on Gelman’s blog, (It’s never a) total eclipse of the prior and their corresponding (2017) paper with Betancourt, The prior can often only be understood in the context of the likelihood. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ggmcmc_1.5.1.1 bayesplot_1.8.0 GGally_2.1.1 tidybayes_2.3.1 ## [5] brms_2.15.0 Rcpp_1.0.6 patchwork_1.1.1 forcats_0.5.1 ## [9] stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 ## [13] tidyr_1.1.3 tibble_3.1.0 tidyverse_1.3.0 ggpomological_0.1.2 ## [17] ggplot2_3.3.3 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 igraph_1.2.6 ## [5] svUnit_1.0.3 splines_4.0.4 crosstalk_1.1.0.1 TH.data_1.0-10 ## [9] rstantools_2.1.1 inline_0.3.17 digest_0.6.27 htmltools_0.5.1.1 ## [13] rethinking_2.13 rsconnect_0.8.16 fansi_0.4.2 magrittr_2.0.1 ## [17] modelr_0.1.8 extrafont_0.17 RcppParallel_5.0.2 matrixStats_0.57.0 ## [21] xts_0.12.1 sandwich_3.0-0 extrafontdb_1.0 prettyunits_1.1.1 ## [25] colorspace_2.0-0 rvest_0.3.6 ggdist_2.4.0.9000 haven_2.3.1 ## [29] xfun_0.22 callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 ## [33] lme4_1.1-25 survival_3.2-7 zoo_1.8-8 glue_1.4.2 ## [37] gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 distributional_0.2.2 ## [41] pkgbuild_1.2.0 Rttf2pt1_1.3.8 rstan_2.21.2 shape_1.4.5 ## [45] abind_1.4-5 scales_1.1.1 mvtnorm_1.1-1 DBI_1.1.0 ## [49] miniUI_0.1.1.1 isoband_0.2.3 xtable_1.8-4 HDInterval_0.2.2 ## [53] stats4_4.0.4 StanHeaders_2.21.0-7 DT_0.16 htmlwidgets_1.5.2 ## [57] httr_1.4.2 threejs_0.3.3 RColorBrewer_1.1-2 arrayhelpers_1.1-0 ## [61] ellipsis_0.3.1 reshape_0.8.8 pkgconfig_2.0.3 loo_2.4.1 ## [65] farver_2.0.3 dbplyr_2.0.0 utf8_1.1.4 tidyselect_1.1.0 ## [69] labeling_0.4.2 rlang_0.4.10 reshape2_1.4.4 later_1.1.0.1 ## [73] munsell_0.5.0 cellranger_1.1.0 tools_4.0.4 cli_2.3.1 ## [77] generics_0.1.0 broom_0.7.5 ggridges_0.5.2 evaluate_0.14 ## [81] fastmap_1.0.1 processx_3.4.5 knitr_1.31 fs_1.5.0 ## [85] nlme_3.1-152 mime_0.10 projpred_2.0.2 xml2_1.3.2 ## [89] compiler_4.0.4 shinythemes_1.1.2 rstudioapi_0.13 curl_4.3 ## [93] gamm4_0.2-6 reprex_0.3.0 statmod_1.4.35 stringi_1.5.3 ## [97] highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 lattice_0.20-41 ## [101] Matrix_1.3-2 nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 ## [105] vctrs_0.3.6 pillar_1.5.1 lifecycle_1.0.0 bridgesampling_1.0-0 ## [109] estimability_1.3 httpuv_1.5.4 R6_2.5.0 bookdown_0.21 ## [113] promises_1.1.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-26 ## [117] colourpicker_1.1.0 MASS_7.3-53 gtools_3.8.2 assertthat_0.2.1 ## [121] withr_2.4.1 shinystan_2.5.0 multcomp_1.4-16 mgcv_1.8-33 ## [125] parallel_4.0.4 hms_0.5.3 grid_4.0.4 coda_0.19-4 ## [129] minqa_1.2.4 rmarkdown_2.7 shiny_1.5.0 lubridate_1.7.9.2 ## [133] base64enc_0.1-3 dygraphs_1.1.1.6 "],["big-entropy-and-the-generalized-linear-model.html", "10 Big Entropy and the Generalized Linear Model 10.1 Maximum entropy 10.2 Generalized linear models Session info", " 10 Big Entropy and the Generalized Linear Model Statistical models force many choices upon us. Some of these choices are distributions that represent uncertainty. We must choose, for each parameter, a prior distribution. And we must choose a likelihood function, which serves as a distribution of data. There are conventional choices, such as wide Gaussian priors and the Gaussian likelihood of linear regression. These conventional choices work unreasonably well in many circumstances. But very often the conventional choices are not the best choices. Inference can be more powerful when we use all of the information, and doing so usually requires going beyond convention. To go beyond convention, it helps to have some principles to guide choice. When an engineer wants to make an unconventional bridge, engineering principles help guide choice. When a researcher wants to build an unconventional model, entropy provides one useful principle to guide choice of probability distributions: Bet on the distribution with the biggest entropy. (McElreath, 2020a, p. 299) 10.0.0.1 Rethinking: Bayesian updating is entropy maximization. Another kind of probability distribution, the posterior distribution deduced by Bayesian updating, is also a case of maximizing entropy. The posterior distribution has the greatest entropy relative to the prior (the smallest cross entropy) among all distributions consistent with the assumed constraints and the observed data. (p. 300) 10.1 Maximum entropy In Chapter 7, you met the basics of information theory. In brief, we seek a measure of uncertainty that satisfies three criteria: (1) the measure should be continuous; (2) it should increase as the number of possible events increases; and (3) it should be additive. The resulting unique measure of the uncertainty of a probability distribution \\(p\\) with probabilities \\(p_i\\) for each possible event \\(i\\) turns out to be just the average log-probability: \\[H(p) = - \\sum_i p_i \\log p_i\\] This function is known as information entropy. The principle of maximum entropy applies this measure of uncertainty to the problem of choosing among probability distributions. Perhaps the simplest way to state the maximum entropy principle is: The distribution that can happen the most ways is also the distribution with the biggest information entropy. The distribution with the biggest entropy is the most conservative distribution that obeys its constraints. There’s nothing intuitive about this idea, so if it seems weird, you are normal. (pp. 300–301, emphasis in the original) Let’s execute the code for the pebbles-in-buckets example. library(tidyverse) d &lt;- tibble(a = c(0, 0, 10, 0, 0), b = c(0, 1, 8, 1, 0), c = c(0, 2, 6, 2, 0), d = c(1, 2, 4, 2, 1), e = 2) # this is our analogue to McElreath&#39;s `lapply()` code d %&gt;% mutate_all(~ . / sum(.)) %&gt;% # the next few lines constitute our analogue to his `sapply()` code pivot_longer(everything(), names_to = &quot;plot&quot;) %&gt;% group_by(plot) %&gt;% summarise(h = -sum(ifelse(value == 0, 0, value * log(value)))) ## # A tibble: 5 x 2 ## plot h ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 0 ## 2 b 0.639 ## 3 c 0.950 ## 4 d 1.47 ## 5 e 1.61 For more on the formula syntax we used within mutate_all(), you might check out this or this. Anyway, we’re almost ready to plot, which brings us to color. For the plots in this chapter, we’ll be taking our color palettes from the ghibli package (Henderson, 2020), which provides palettes based on scenes from anime films by the Studio Ghibli. library(ghibli) The main function is ghibli_palette() which you can use to both preview the palettes before using them and also index in order to use specific colors. For example, we’ll play with “MarnieMedium1,” first. ghibli_palette(&quot;MarnieMedium1&quot;) ghibli_palette(&quot;MarnieMedium1&quot;)[1:7] ## [1] &quot;#28231DFF&quot; &quot;#5E2D30FF&quot; &quot;#008E90FF&quot; &quot;#1C77A3FF&quot; &quot;#C5A387FF&quot; &quot;#67B8D6FF&quot; &quot;#E9D097FF&quot; Now we’re ready to plot five of the six panels of Figure 10.1. d %&gt;% mutate(bucket = 1:5) %&gt;% pivot_longer(-bucket, names_to = &quot;letter&quot;, values_to = &quot;pebbles&quot;) %&gt;% ggplot(aes(x = bucket, y = pebbles)) + geom_col(width = 1/5, fill = ghibli_palette(&quot;MarnieMedium1&quot;)[2]) + geom_text(aes(y = pebbles + 1, label = pebbles)) + geom_text(data = tibble( letter = letters[1:5], bucket = 5.5, pebbles = 10.5, label = str_c(c(1, 90, 1260, 37800, 113400), rep(c(&quot; way&quot;, &quot; ways&quot;), times = c(1, 4)))), aes(label = label), hjust = 1) + scale_y_continuous(breaks = c(0, 5, 10), limits = c(0, 12)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[6]), panel.grid = element_blank(), strip.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[7])) + facet_wrap(~ letter, ncol = 2) We might plot our version of the final panel like so. d %&gt;% # the next four lines are the same from above mutate_all(~ . / sum(.)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(h = -sum(ifelse(value == 0, 0, value * log(value)))) %&gt;% # here&#39;s the R code 9.4 stuff mutate(n_ways = c(1, 90, 1260, 37800, 113400)) %&gt;% group_by(name) %&gt;% mutate(log_ways = log(n_ways) / 10, text_y = ifelse(name &lt; &quot;c&quot;, h + .15, h - .15)) %&gt;% # plot ggplot(aes(x = log_ways, y = h)) + geom_abline(intercept = 0, slope = 1.37, color = &quot;white&quot;) + geom_point(size = 2.5, color = ghibli_palette(&quot;MarnieMedium1&quot;)[7]) + geom_text(aes(y = text_y, label = name)) + labs(x = &quot;log(ways) per pebble&quot;, y = &quot;entropy&quot;) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium1&quot;)[6]), panel.grid = element_blank()) “The distribution that can happen the greatest number of ways is the most plausible distribution. Call this distribution the maximum entropy distribution” (p. 303, emphasis in the original). Among the pebbles, the maximum entropy distribution was e (i.e., the uniform). 10.1.0.1 Rethinking: What good is intuition? “Like many aspects of information theory, maximum entropy is not very intuitive. But note that intuition is just a guide to developing methods. When a method works, it hardly matters whether our intuition agrees” (p. 303). 10.1.1 Gaussian. Behold the probability density for the generalized normal distribution: \\[\\text{Pr} (y | \\mu, \\alpha, \\beta) = \\frac{\\beta}{2 \\alpha \\Gamma \\left (\\frac{1}{\\beta} \\right )} e ^ {- \\left (\\frac{|y - \\mu|}{\\alpha} \\right ) ^ {\\beta}},\\] where \\(\\alpha =\\) the scale, \\(\\beta =\\) the shape, \\(\\mu =\\) the location, and \\(\\Gamma =\\) the gamma function. If you read closely in the text, you’ll discover that the densities in the right panel of Figure 10.2 were all created with the constraint \\(\\sigma^2 = 1\\). But \\(\\sigma^2 \\neq \\alpha\\) and there’s no \\(\\sigma\\) in the equations in the text. However, it appears the variance for the generalized normal distribution follows the form \\[\\sigma^2 = \\frac{\\alpha^2 \\Gamma (3/\\beta)}{\\Gamma (1/\\beta)}.\\] So if you do the algebra, you’ll see that you can compute \\(\\alpha\\) for a given \\(\\sigma^2\\) and \\(\\beta\\) with the equation \\[\\alpha = \\sqrt{ \\frac{\\sigma^2 \\Gamma (1/\\beta)}{\\Gamma (3/\\beta)} }.\\] I got the formula from Wikipedia.com. Don’t judge. We can wrap that formula in a custom function, alpha_per_beta(), use it to solve for the desired \\(\\beta\\) values, and plot. But one more thing: McElreath didn’t tell us exactly which \\(\\beta\\) values the left panel of Figure 10.2 was based on. So the plot below is my best guess. alpha_per_beta &lt;- function(beta, variance = 1) { sqrt((variance * gamma(1 / beta)) / gamma(3 / beta)) } crossing(value = seq(from = -5, to = 5, by = .1), # I arrived at these values by trial and error beta = c(1, 1.5, 2, 4)) %&gt;% mutate(mu = 0, alpha = alpha_per_beta(beta)) %&gt;% # behold the formula for the generalized normal distribution in code! mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% # plot ggplot(aes(x = value, y = density, group = beta)) + geom_line(aes(color = beta == 2, size = beta == 2)) + scale_color_manual(values = c(ghibli_palette(&quot;MarnieMedium2&quot;)[c(2, 4)])) + scale_size_manual(values = c(1/4, 1.25)) + labs(subtitle = &quot;Guess which color denotes the Gaussian.&quot;) + coord_cartesian(xlim = c(-4, 4)) + theme(legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium2&quot;)[7]), panel.grid = element_blank()) Here’s right panel of Figure 10.2. crossing(value = -8:8, # this time we need a more densely-packed sequence of `beta` values beta = seq(from = 1, to = 4, length.out = 100)) %&gt;% mutate(mu = 0, alpha = alpha_per_beta(beta)) %&gt;% mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) %&gt;% group_by(beta) %&gt;% # this is just an abbreviated version of the formula we used in our first code block summarise(entropy = -sum(density * log(density))) %&gt;% ggplot(aes(x = beta, y = entropy)) + geom_vline(xintercept = 2, color = &quot;white&quot;) + geom_line(size = 2, color = ghibli_palette(&quot;MarnieMedium2&quot;)[6]) + xlab(expression(beta(i.e.*&quot;, &quot;*shape))) + coord_cartesian(ylim = c(1.34, 1.42)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;MarnieMedium2&quot;)[7]), panel.grid = element_blank()) If you look closely, you’ll see our version doesn’t quite match up with McElreath’s. Over \\(x\\)-axis values of 2 to 4, they match up pretty well. But as you go from 2 to 1, you’ll see our line drops off more steeply than his did. [And no, coord_cartesian() isn’t the problem.] If you can figure out why our numbers diverged, please share the answer. But getting back on track: The take-home lesson from all of this is that, if all we are willing to assume about a collection of measurements is that they have a finite variance, then the Gaussian distribution represents the most conservative probability distribution to assign to those measurements. But very often we are comfortable assuming something more. And in those cases, provided our assumptions are good ones, the principle of maximum entropy leads to distributions other than the Gaussian. (p. 306) 10.1.2 Binomial. The binomial likelihood entails counting the numbers of ways that a given observation could arise, according to our assumptions. The resulting distribution is known as the binomial distribution. If only two things can happen (blue or white marble, for example), and there’s a constant chance \\(p\\) of each across \\(n\\) trials, then the probability of observing \\(y\\) events of type 1 and \\(n - y\\) events of type 2 is: \\[\\text{Pr} (y | n, p) = \\frac{n!}{y! (n - y)!} p^y (1 - p)^{n - y}\\] It may help to note that the fraction with the factorials is just saying how many different ordered sequences of \\(n\\) outcomes have a count of \\(y\\). (p. 307, emphasis in the original) For me, that last sentence made more sense when I walked it out in an example. To do so, let’s wrap that fraction of factorials into a function. count_ways &lt;- function(n, y) { # n = the total number of trials (i.e., the number of rows in your vector) # y = the total number of 1s (i.e., successes) in your vector (factorial(n) / (factorial(y) * factorial(n - y))) } Now consider three sequences: 0, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 0\\)) 1, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 1\\)) 1, 1, 0, 0 (i.e., \\(n = 4\\) and \\(y = 2\\)) We can organize that information in a little tibble and then demo our count_ways() function. tibble(sequence = 1:3, n = 4, y = c(0, 1, 2)) %&gt;% mutate(n_ways = count_ways(n = n, y = y)) ## # A tibble: 3 x 4 ## sequence n y n_ways ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4 0 1 ## 2 2 4 1 4 ## 3 3 4 2 6 Here’s the pre-Figure 10.3 data McElreath presented on page 308. # data d &lt;- tibble(distribution = letters[1:4], ww = c(1/4, 2/6, 1/6, 1/8), bw = c(1/4, 1/6, 2/6, 4/8), wb = c(1/4, 1/6, 2/6, 2/8), bb = c(1/4, 2/6, 1/6, 1/8)) # table d %&gt;% mutate_if(is.numeric, ~MASS::fractions(.) %&gt;% as.character()) %&gt;% flextable::flextable() .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-cdd03a32{border-collapse:collapse;}.cl-cdcb0c74{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-cdcb209c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-cdcb4cc0{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cdcb4cd4{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-cdcb4cde{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}distributionwwbwwbbba1/41/41/41/4b1/31/61/61/3c1/61/31/31/6d1/81/21/41/8 Those data take just a tiny bit of wrangling before they’re ready to plot in our version of Figure 10.3. d &lt;- d %&gt;% pivot_longer(-distribution, names_to = &quot;sequence&quot;, values_to = &quot;probability&quot;) %&gt;% mutate(sequence = factor(sequence, levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) d %&gt;% ggplot(aes(x = sequence, y = probability, group = 1)) + geom_point(size = 2, color = ghibli_palette(&quot;PonyoMedium&quot;)[4]) + geom_line(color = ghibli_palette(&quot;PonyoMedium&quot;)[5]) + labs(x = NULL, y = NULL) + coord_cartesian(ylim = 0:1) + theme(axis.ticks.x = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;PonyoMedium&quot;)[2]), panel.grid = element_blank(), strip.background = element_rect(fill = ghibli_palette(&quot;PonyoMedium&quot;)[6])) + facet_wrap(~ distribution) If we go step by step, we might count the expected value for each distribution like follows. d %&gt;% # `str_count()` will count the number of times &quot;b&quot; occurs within a given row of `sequence` mutate(n_b = str_count(sequence, &quot;b&quot;)) %&gt;% mutate(product = probability * n_b) %&gt;% group_by(distribution) %&gt;% summarise(expected_value = sum(product)) ## # A tibble: 4 x 2 ## distribution expected_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1 ## 2 b 1 ## 3 c 1 ## 4 d 1 We can use the same group_by() strategy on the way to computing the entropies. d %&gt;% group_by(distribution) %&gt;% summarise(entropy = -sum(probability * log(probability))) ## # A tibble: 4 x 2 ## distribution entropy ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1.39 ## 2 b 1.33 ## 3 c 1.33 ## 4 d 1.21 Like in the text, distribution == \"a\" had the largest entropy of the four. In the next example, the \\(\\text{expected value} = 1.4\\) and \\(p = .7\\). p &lt;- 0.7 ( a &lt;- c((1 - p)^2, p * (1 - p), (1 - p) * p, p^2) ) ## [1] 0.09 0.21 0.21 0.49 Here’s the entropy for our distribution a. -sum(a * log(a)) ## [1] 1.221729 I’m going to alter McElreath’s simulation function from R code 10.9 to take a seed argument. In addition, I altered the names of the objects within the function and changed the output to a tibble that will also include the conditions “ww,” “bw,” “wb,” and “bb.” sim_p &lt;- function(seed, g = 1.4) { set.seed(seed) x_123 &lt;- runif(3) x_4 &lt;- ((g) * sum(x_123) - x_123[2] - x_123[3]) / (2 - g) z &lt;- sum(c(x_123, x_4)) p &lt;- c(x_123, x_4) / z tibble(h = -sum(p * log(p)), p = p, key = factor(c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;), levels = c(&quot;ww&quot;, &quot;bw&quot;, &quot;wb&quot;, &quot;bb&quot;))) } For a given seed and g value, our augmented sim_p() function returns a \\(4 \\times 3\\) tibble. sim_p(seed = 9.9, g = 1.4) ## # A tibble: 4 x 3 ## h p key ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1.02 0.197 ww ## 2 1.02 0.0216 bw ## 3 1.02 0.184 wb ## 4 1.02 0.597 bb So the next step is to determine how many replications we’d like, create a tibble with seed values ranging from 1 to that number, and then feed those seed values into sim_p() via purrr::map2(), which will return a nested tibble. We’ll then unnest() and take a peek. # how many replications would you like? n_rep &lt;- 1e5 d &lt;- tibble(seed = 1:n_rep) %&gt;% mutate(sim = map2(seed, 1.4, sim_p)) %&gt;% unnest(sim) Take a look. head(d) ## # A tibble: 6 x 4 ## seed h p key ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1 1.21 0.108 ww ## 2 1 1.21 0.151 bw ## 3 1 1.21 0.233 wb ## 4 1 1.21 0.508 bb ## 5 2 1.21 0.0674 ww ## 6 2 1.21 0.256 bw In order to intelligently choose which four replications we want to highlight in Figure 10.4, we’ll want to rank order them by entropy, h. ranked_d &lt;- d %&gt;% group_by(seed) %&gt;% arrange(desc(h)) %&gt;% ungroup() %&gt;% # here&#39;s the rank order step mutate(rank = rep(1:n_rep, each = 4)) head(ranked_d) ## # A tibble: 6 x 5 ## seed h p key rank ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 55665 1.22 0.0903 ww 1 ## 2 55665 1.22 0.209 bw 1 ## 3 55665 1.22 0.210 wb 1 ## 4 55665 1.22 0.490 bb 1 ## 5 71132 1.22 0.0902 ww 2 ## 6 71132 1.22 0.210 bw 2 And we’ll also want a subset of the data to correspond to McElreath’s “A” through “D” distributions. subset_d &lt;- ranked_d %&gt;% # I arrived at these `rank` values by trial and error filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) %&gt;% # I arrived at the `height` values by trial and error, too mutate(height = rep(c(8, 2.25, .75, .5), each = 4), distribution = rep(letters[1:4], each = 4)) head(subset_d) ## # A tibble: 6 x 7 ## seed h p key rank height distribution ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 55665 1.22 0.0903 ww 1 8 a ## 2 55665 1.22 0.209 bw 1 8 a ## 3 55665 1.22 0.210 wb 1 8 a ## 4 55665 1.22 0.490 bb 1 8 a ## 5 50981 1.00 0.0459 ww 87373 2.25 b ## 6 50981 1.00 0.0459 bw 87373 2.25 b We’re finally ready to make our version of the left panel of Figure 10.4. p1 &lt;- d %&gt;% ggplot(aes(x = h)) + geom_density(size = 0, fill = ghibli_palette(&quot;LaputaMedium&quot;)[3], adjust = 1/4) + # note the data statements for the next two geoms geom_linerange(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(ymin = 0, ymax = height), color = ghibli_palette(&quot;LaputaMedium&quot;)[5]) + geom_text(data = subset_d %&gt;% group_by(seed) %&gt;% slice(1), aes(y = height + .5, label = distribution)) + scale_x_continuous(&quot;Entropy&quot;, breaks = seq(from = .7, to = 1.2, by = .1)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[7]), panel.grid = element_blank()) Did you notice how our adjust = 1/4 with geom_density() served a similar function to the adj=0.1 in McElreath’s dens() code? Anyways, here we make the right panel and combine the two with patchwork. p2 &lt;- ranked_d %&gt;% filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) %&gt;% mutate(distribution = rep(letters[1:4], each = 4)) %&gt;% ggplot(aes(x = key, y = p, group = 1)) + geom_line(color = ghibli_palette(&quot;LaputaMedium&quot;)[5]) + geom_point(size = 2, color = ghibli_palette(&quot;LaputaMedium&quot;)[4]) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, .75)) + xlab(NULL) + theme(axis.ticks.x = element_blank(), panel.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[7]), panel.grid = element_blank(), strip.background = element_rect(fill = ghibli_palette(&quot;LaputaMedium&quot;)[6])) + facet_wrap(~ distribution) # combine and plot library(patchwork) p1 | p2 Because we simulated, our values won’t match up identically with those in the text. We got pretty close, eh? Since we saved our sim_p() output in a nested tibble, which we then unnested(), there’s no need to separate the entropy values from the distributional values the way McElreath did in his R code 10.11. If we wanted to determine our highest entropy value–and the corresponding seed and p values, while we’re at it–, we might execute something like this. ranked_d %&gt;% group_by(key) %&gt;% arrange(desc(h)) %&gt;% slice(1) ## # A tibble: 4 x 5 ## # Groups: key [4] ## seed h p key rank ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 55665 1.22 0.0903 ww 1 ## 2 55665 1.22 0.209 bw 1 ## 3 55665 1.22 0.210 wb 1 ## 4 55665 1.22 0.490 bb 1 That maximum h value matched up nicely with the one in the text. If you look at the p column, you’ll see our values approximated McElreath’s distribution values, too. In both cases, they’re real close to the a values we computed, above. a ## [1] 0.09 0.21 0.21 0.49 “All four of these distributions really do have expected value 1.4. But among the infinite distributions that satisfy this constraint, it is only the most even distribution, the exact one nominated by the binomial distribution, that has greatest entropy” (p. 310). 10.2 Generalized linear models For an outcome variable that is continuous and far from any theoretical maximum or minimum, [a simple] Gaussian model has maximum entropy. But when the outcome variable is either discrete or bounded, a Gaussian likelihood is not the most powerful choice. (p. 312) I winged the values for our Figure 10.5. tibble(x = seq(from = -1, to = 3, by = .01)) %&gt;% mutate(probability = .35 + x * .5) %&gt;% ggplot(aes(x = x, y = probability)) + geom_rect(xmin = -1, xmax = 3, ymin = 0, ymax = 1, fill = ghibli_palette(&quot;MononokeMedium&quot;)[5]) + geom_hline(yintercept = 0:1, linetype = 2, color = ghibli_palette(&quot;MononokeMedium&quot;)[7]) + geom_line(aes(linetype = probability &gt; 1, color = probability &gt; 1), size = 1) + geom_segment(x = 1.3, xend = 3, y = 1, yend = 1, size = 2/3, color = ghibli_palette(&quot;MononokeMedium&quot;)[3]) + annotate(geom = &quot;text&quot;, x = 1.28, y = 1.04, hjust = 1, label = &quot;This is why we need link functions&quot;, color = ghibli_palette(&quot;MononokeMedium&quot;)[4], size = 2.6) + scale_color_manual(values = c(ghibli_palette(&quot;MononokeMedium&quot;)[3:4])) + scale_y_continuous(breaks = c(0, .5, 1)) + coord_cartesian(xlim = c(0, 2), ylim = c(0, 1.2)) + theme(legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;MononokeMedium&quot;)[1]), panel.grid = element_blank()) Luckily, it’s easy to do better. By using all of our prior knowledge about the outcome variable, usually in the form of constraints on the possible values it can take, we can appeal to maximum entropy for the choice of distribution. Then all we have to do is generalize the linear regression strategy–replace a parameter describing the shape of the likelihood with a linear model–to probability distributions other than the Gaussian. (p. 313) As we will see, doing better will often involve using link functions. 10.2.0.1 Rethinking: The scourge of Histomancy. One strategy for choosing an outcome distribution is to plot the histogram of the outcome variable and, by gazing into its soul, decide what sort of distribution function to use. Call this strategy Histomancy, the ancient art of divining likelihood functions from empirical histograms. This sorcery is used, for example, when testing for normality before deciding whether or not to use a non-parametric procedure. Histomancy is a false god. (p. 314, emphasis in the original) Stop worshiping at alter of this false god. Use domain knowledge and principles maximum entropy to pick your likelihoods. 10.2.1 Meet the family. The most common distributions used in statistical modeling are members of a family known as the exponential family. Every member of this family is a maximum entropy distribution, for some set of constraints. And conveniently, just about every other statistical modeling tradition employs the exact same distributions, even though they arrive at them via justifications other than maximum entropy. (p. 314, emphasis in the original) Here are the Gamma and Exponential panels for Figure 10.6. length_out &lt;- 100 tibble(x = seq(from = 0, to = 5, length.out = length_out)) %&gt;% mutate(Gamma = dgamma(x, 2, 2), Exponential = dexp(x)) %&gt;% pivot_longer(-x, values_to = &quot;density&quot;) %&gt;% mutate(label = ifelse(name == &quot;Gamma&quot;, &quot;y %~% Gamma(lambda, kappa)&quot;, &quot;y %~% Exponential(lambda)&quot;)) %&gt;% ggplot(aes(x = x, y = density)) + geom_area(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 4)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), panel.grid = element_blank(), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~ label, scales = &quot;free_y&quot;, labeller = label_parsed) The Gaussian: tibble(x = seq(from = -5, to = 5, length.out = length_out)) %&gt;% mutate(density = dnorm(x), strip = &quot;y %~% Normal(mu, sigma)&quot;) %&gt;% ggplot(aes(x = x, y = density)) + geom_area(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-4, 4)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), panel.grid = element_blank(), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~ strip, labeller = label_parsed) Here is the Poisson. tibble(x = 0:20) %&gt;% mutate(density = dpois(x, lambda = 2.5), strip = &quot;y %~% Poisson(lambda)&quot;) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[2], width = 1/2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 10)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), panel.grid = element_blank(), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~ strip, labeller = label_parsed) Finally, the Binomial: tibble(x = 0:10) %&gt;% mutate(density = dbinom(x, size = 10, prob = .85), strip = &quot;y %~% Binomial(n, p)&quot;) %&gt;% ggplot(aes(x = x, y = density)) + geom_col(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[2], width = 1/2) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 10)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[5]), panel.grid = element_blank(), strip.background = element_rect(fill = ghibli_palette(&quot;SpiritedMedium&quot;)[7])) + facet_wrap(~ strip, labeller = label_parsed) 10.2.1.1 Rethinking: A likelihood is a prior. In traditional statistics, likelihood functions are “objective” and prior distributions “subjective.” In Bayesian statistics, likelihoods are deeply related to prior probability distributions: They are priors for the data, conditional on the parameters. And just like with other priors, there is no correct likelihood. But there are better and worse likelihoods, depending upon the context. (p. 316) For a little more in this, check out McElreath’s great lecture, Bayesian statistics without frequentist language. This subsection also reminds me of the title of one of Gelman’s blog posts, “It is perhaps merely an accident of history that skeptics and subjectivists alike strain on the gnat of the prior distribution while swallowing the camel that is the likelihood”. The title, which itself is a quote, comes from one of his papers, which he linked to in the blog, along with several related papers. It’s taken some time for the weight of that quote to sink in with me, and indeed it’s still sinking. Perhaps you’ll benefit from it, too. 10.2.2 Linking linear models to distributions. To build a regression model from any of the exponential family distributions is just a matter of attaching one or more linear models to one or more of the parameters that describe the distribution’s shape. But as hinted at earlier, usually we require a link function to prevent mathematical accidents like negative distances or probability masses that exceed 1. (p. 316, emphasis in the original) These models generally follow the form \\[\\begin{align*} y_i &amp; \\sim \\color{#4D6D93}{\\operatorname{Some distribution}} (\\theta_i, \\phi) \\\\ \\color{#4D6D93}{f(\\theta_i)} &amp; = \\alpha + \\beta (x_i - \\bar x), \\end{align*}\\] where \\(\\theta_i\\) is a parameter of central interest (e.g., the probability of 1 in a Binomial distribution) and \\(\\phi\\) is a placeholder for any other parameters necessary for the likelihood but not typically of primary substantive interest (e.g., \\(\\sigma\\) in conventional Gaussian models). The \\(f(\\cdot)\\) portion is the link function. Speaking about links, the logit link maps a parameter that is defined as a probability mass, and therefore constrained to lie between zero and one, onto a linear model that can take on any real value. This link is extremely common when working with binomial GLMs. In the context of a model definition, it looks like this: \\[\\begin{align*} y_i &amp; \\sim \\color{#4D6D93}{\\operatorname{Binomial}}(n, p_i) \\\\ \\color{#4D6D93}{\\operatorname{logit}}(p_i) &amp; = \\alpha + \\beta x_i \\end{align*}\\] And the logit function itself is defined as the log-odds: \\[\\operatorname{logit}(p_i) = \\log \\frac{p_i}{1 - p_i}\\] The “odds” of an event are just the probability it happens divided by the probability it does not happen. So really all that is being stated here is: \\[\\log \\frac{p_i}{1 - p_i} = \\alpha + \\beta x_i\\] If we do the final algebraic manipulation on page 317, we can solve for \\(p_i\\) in terms of the linear model \\[p_i = \\frac{\\exp (\\alpha + \\beta x_i)}{1 + \\exp (\\alpha + \\beta x_i)}.\\] As we’ll see later, we will make great use of this formula via the brms::inv_logit_scaled() when making sense of logistic regression models. Now we have that last formula in hand, we can make the data necessary for Figure 10.7. # first, we&#39;ll make data for the horizontal lines alpha &lt;- 0 beta &lt;- 4 lines &lt;- tibble(x = seq(from = -1, to = 1, by = .25)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) # now we&#39;re ready to make the primary data beta &lt;- 2 d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-odds` = alpha + x * beta, probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) # now we make the individual plots p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-odds`)) + geom_hline(data = lines, aes(yintercept = `log-odds`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = c(-1, 1)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[5]), panel.grid = element_blank()) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = probability)) + geom_hline(data = lines, aes(yintercept = probability), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = c(-1, 1)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[7]), panel.grid = element_blank()) # finally, we&#39;re ready to mash the plots together and behold their nerdy glory (p1 | p2) + plot_annotation(subtitle = &quot;The logit link transforms a linear model (left) into a probability (right).&quot;) The key lesson for now is just that no regression coefficient, such as \\(\\beta\\), from a GLM ever produces a constant change on the outcome scale. Recall that we defined interaction (Chapter 8) as a situation in which the effect of a predictor depends upon the value of another predictor. Well now every predictor essentially interacts with itself, because the impact of a change in a predictor depends upon the value of the predictor before the change…. The second very common link function is the log link. This link function maps a parameter that is defined over only positive real values onto a linear model. For example, suppose we want to model the standard deviation \\(\\sigma\\) of a Gaussian distribution so it is a function of a predictor variable \\(x\\). The parameter \\(\\sigma\\) must be positive, because a standard deviation cannot be negative nor can it be zero. The model might look like: \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Normal}(\\mu, \\color{#0E84B4}{\\sigma_i}) \\\\ \\color{#0E84B4}{\\log (\\sigma_i)} &amp; = \\alpha + \\beta x_i \\end{align*}\\] In this model, the mean \\(\\mu\\) is constant, but the standard deviation scales with the value \\(x_i\\). (p. 318, emphasis in the original) This kind of model is trivial in the brms framework, which you can learn more about in Bürkner’s (2021b) vignette, Estimating distributional models with brms. Before moving on with the text, let’s detour and see how we might fit such a model. First, we’ll simulate some continuous data y for which the \\(SD\\) is affected by a dummy variable x. set.seed(10) ( d &lt;- tibble(x = rep(0:1, each = 100)) %&gt;% mutate(y = rnorm(n = n(), mean = 100, sd = 10 + x * 10)) ) ## # A tibble: 200 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 0 100. ## 2 0 98.2 ## 3 0 86.3 ## 4 0 94.0 ## 5 0 103. ## 6 0 104. ## 7 0 87.9 ## 8 0 96.4 ## 9 0 83.7 ## 10 0 97.4 ## # … with 190 more rows These data are based on IQ data. In psychology, general intelligence is often operationalized by and measured with standardized intelligence tests. The results form these tests are often summarized with an a single intelligence quotient (IQ) score, often called the full-scale IQ score. For many years now, the convention within among IQ test developers is to scale full-scale IQ scores so they have a population mean of 100 and a standard deviation of 15. One of the old and continuing controversies in the literature is whether men and women differ not in their means–they don’t–but in their standard deviations (e.g., Johnson et al., 2008). To give a sense of how one might explore such a controversy, we simulated data where the y variables have a mean of 100 and standard deviations of either 10 or 20, depending on one’s status on x. We can view what data like these look like with aid from tidybayes::stat_halfeye(). library(tidybayes) d %&gt;% mutate(x = x %&gt;% as.character()) %&gt;% ggplot(aes(x = y, y = x, fill = x)) + stat_halfeye(point_interval = mean_qi, .width = .68, color = ghibli_palette(&quot;KikiMedium&quot;)[2]) + scale_fill_manual(values = c(ghibli_palette(&quot;KikiMedium&quot;)[c(4, 6)])) + coord_cartesian(ylim = c(1.5, 2)) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;KikiMedium&quot;)[7]), panel.grid = element_blank()) Even though the means of y are the same for both levels of the x dummy, the variance for x == 1 is substantially larger than that for x == 0. Let’s open brms. library(brms) For such a model, we have two formulas: one for \\(\\mu\\) and one for \\(\\sigma\\). We wrap both within the bf() function. b10.1 &lt;- brm(data = d, family = gaussian, bf(y ~ 1, sigma ~ 1 + x), prior = c(prior(normal(100, 5), class = Intercept), prior(normal(2.70805, 0.5), class = Intercept, dpar = sigma), prior(normal(0, 0.5), class = b, dpar = sigma)), seed = 10, file = &quot;fits/b10.01&quot;) Do note our use of the dpar arguments in the prior() functions Here’s the summary. print(b10.1) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: y ~ 1 ## sigma ~ 1 + x ## Data: d (Number of observations: 200) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 98.57 0.86 96.88 100.26 1.00 3982 2819 ## sigma_Intercept 2.26 0.07 2.13 2.41 1.00 3368 2777 ## sigma_x 0.69 0.10 0.50 0.88 1.00 4052 3091 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we get an intercept for both \\(\\mu\\) and \\(\\sigma\\), with the intercept for sigma labeled as sigma_Intercept. And note the \\(\\beta\\) coefficient for \\(\\sigma\\) was named sigma_x. Also notice the scale the sigma_i coefficients are on. These are not in the original metric, but rather based on a logarithmic transformation of \\(\\sigma\\). You can confirm that by the second line of the print() output: Links: mu = identity; sigma = log. So if you want to get a sense of the effects of x on the \\(\\sigma\\) for y, you have to exponentiate the formula. Here we’ll do so with the posterior_samples(). post &lt;- posterior_samples(b10.1) head(post) ## b_Intercept b_sigma_Intercept b_sigma_x lp__ ## 1 98.70843 2.240895 0.7359405 -807.6554 ## 2 99.25465 2.364534 0.6248927 -809.1533 ## 3 99.73125 2.274513 0.7635480 -809.2723 ## 4 100.59124 2.363369 0.5976097 -811.1601 ## 5 97.55274 2.235885 0.8263562 -809.6148 ## 6 98.62746 2.288775 0.6987624 -807.8320 With the samples in hand, we’ll use the model formula to compute the model-implied standard deviations of y based on the x dummy and then examine them in a plot. post %&gt;% mutate(`x == 0` = exp(b_sigma_Intercept + b_sigma_x * 0), `x == 1` = exp(b_sigma_Intercept + b_sigma_x * 1)) %&gt;% pivot_longer(contains(&quot;==&quot;)) %&gt;% ggplot(aes(x = value, y = name, fill = name)) + stat_halfeye(point_interval = median_qi, .width = .95, color = ghibli_palette(&quot;KikiMedium&quot;)[2]) + scale_fill_manual(values = c(ghibli_palette(&quot;KikiMedium&quot;)[c(4, 6)])) + labs(subtitle = &quot;Model-implied standard deviations by group&quot;, x = expression(sigma[x]), y = NULL) + coord_cartesian(ylim = c(1.5, 2)) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;, panel.background = element_rect(fill = ghibli_palette(&quot;KikiMedium&quot;)[7]), panel.grid = element_blank()) If we looked back at the data, those \\(SD\\) estimates are right about what we’d expect. d %&gt;% group_by(x) %&gt;% summarise(sd = sd(y) %&gt;% round(digits = 1)) ## # A tibble: 2 x 2 ## x sd ## &lt;int&gt; &lt;dbl&gt; ## 1 0 9.4 ## 2 1 19.4 For more on models like this, check out Christakis’s blog post, 2014: What scientific idea is ready for retirement?, or his paper with Subramanian and Kim, The “average” treatment effect: A construct ripe for retirement. A commentary on Deaton and Cartwright, (Subramanian et al., 2018). Kruschke covered modeling \\(\\sigma\\) a bit in his (2015) Doing Bayesian data analysis, second edition: A tutorial with R, JAGS, and Stan, my (2020c) translation for which lives here. Finally, this is foreshadowing a bit because it requires the multilevel model (see Chapters 13 and 14), but you might also check out the (2019) preprint by Williams, Liu, Martin, and Rast, Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity or Williams’s blog post, A defining feature of cognitive interference tasks: Heterogeneous within-person variance. But getting back to the text, What the log link effectively assumes is that the parameter’s value is the exponentiation of the linear model. Solving \\(\\log (\\sigma_i) = \\alpha + \\beta x_i\\) for \\(\\sigma_i\\) yields the inverse link: \\[\\sigma_i = \\exp (\\alpha + \\beta x_i)\\] The impact of this assumption can be seen in [our version of] Figure 10.8. (pp. 318–319) # first, we&#39;ll make data that&#39;ll be make the horizontal lines alpha &lt;- 0 beta &lt;- 2 lines &lt;- tibble(`log-measurement` = -3:3, `original measurement` = exp(-3:3)) # now we&#39;re ready to make the primary data d &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) %&gt;% mutate(`log-measurement` = alpha + x * beta, `original measurement` = exp(alpha + x * beta)) # now we make the individual plots p1 &lt;- d %&gt;% ggplot(aes(x = x, y = `log-measurement`)) + geom_hline(data = lines, aes(yintercept = `log-measurement`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + coord_cartesian(xlim = c(-1, 1)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[5]), panel.grid = element_blank()) p2 &lt;- d %&gt;% ggplot(aes(x = x, y = `original measurement`)) + geom_hline(data = lines, aes(yintercept = `original measurement`), color = ghibli_palette(&quot;YesterdayMedium&quot;)[6]) + geom_line(size = 1.5, color = ghibli_palette(&quot;YesterdayMedium&quot;)[3]) + scale_y_continuous(position = &quot;right&quot;, limits = c(0, 10)) + coord_cartesian(xlim = c(-1, 1)) + theme(panel.background = element_rect(fill = ghibli_palette(&quot;YesterdayMedium&quot;)[7]), panel.grid = element_blank()) # combine the ggplots p1 | p2 Using a log link for a linear model (left) implies an exponential scaling of the outcome with the predictor variable (right). Another way to think of this relationship is to remember that logarithms are magnitudes. An increase of one unit on the log scale means an increase of an order of magnitude on the untransformed scale. And this fact is reflected in the widening intervals between the horizontal lines in the right-hand plot of Figure 10.8. (p. 319, emphasis in the original) 10.2.2.1 Rethinking: When in doubt, play with assumptions. Link functions are assumptions. And like all assumptions, they are useful in different contexts. The conventional logit and log links are widely useful, but they can sometimes distort inference. If you ever have doubts, and want to reassure yourself that your conclusions are not sensitive to choice of link function, then you can use sensitivity analysis. A sensitivity analysis explores how changes in assumptions influence inference. (p. 319, emphasis in the original) As an example, a common alternative to the logit link is the probit. Both are available with brms. 10.2.3 Omitted variable bias again. Back in Chapters 5 and 6, you saw some examples of omitted variable bias, where leaving a causally important variable out of a model leads to biased inference. The same thing can of course happen in GLMs. But it can be worse in GLMs, because even a variable that isn’t technically a confounder can bias inference, once we have a link function. The reason is that the ceiling and floor effects described above can distort estimates by suppressing the causal influence of a variable. (p. 320) 10.2.4 Absolute and relative differences. Within the context of GLMs with non-identity link functions, parameter estimates do not by themselves tell you the importance of a predictor on the outcome. The reason is that each parameter represents a relative difference on the scale of the linear model, ignoring other parameters, while we are really interested in absolute differences in outcomes that must incorporate all parameters. (p. 320, emphasis in the original) This will make more sense after we start playing around with logistic regression, count regression, and so on. For now, just file it away. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.15.0 Rcpp_1.0.6 tidybayes_2.3.1 patchwork_1.1.1 ghibli_0.3.2 forcats_0.5.1 ## [7] stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 tibble_3.1.0 ## [13] ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 uuid_0.1-4 backports_1.2.1 systemfonts_1.0.1 ## [5] plyr_1.8.6 igraph_1.2.6 splines_4.0.4 svUnit_1.0.3 ## [9] crosstalk_1.1.0.1 TH.data_1.0-10 rstantools_2.1.1 inline_0.3.17 ## [13] digest_0.6.27 htmltools_0.5.1.1 rsconnect_0.8.16 fansi_0.4.2 ## [17] magrittr_2.0.1 modelr_0.1.8 RcppParallel_5.0.2 matrixStats_0.57.0 ## [21] officer_0.3.17 sandwich_3.0-0 xts_0.12.1 prettyunits_1.1.1 ## [25] colorspace_2.0-0 rvest_0.3.6 ggdist_2.4.0.9000 haven_2.3.1 ## [29] xfun_0.22 callr_3.5.1 crayon_1.4.1 prismatic_0.2.0 ## [33] jsonlite_1.7.2 lme4_1.1-25 survival_3.2-7 zoo_1.8-8 ## [37] glue_1.4.2 gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 ## [41] distributional_0.2.2 pkgbuild_1.2.0 rstan_2.21.2 abind_1.4-5 ## [45] scales_1.1.1 mvtnorm_1.1-1 DBI_1.1.0 miniUI_0.1.1.1 ## [49] xtable_1.8-4 StanHeaders_2.21.0-7 stats4_4.0.4 DT_0.16 ## [53] htmlwidgets_1.5.2 httr_1.4.2 threejs_0.3.3 arrayhelpers_1.1-0 ## [57] ellipsis_0.3.1 pkgconfig_2.0.3 loo_2.4.1 farver_2.0.3 ## [61] dbplyr_2.0.0 utf8_1.1.4 tidyselect_1.1.0 labeling_0.4.2 ## [65] rlang_0.4.10 reshape2_1.4.4 later_1.1.0.1 munsell_0.5.0 ## [69] cellranger_1.1.0 tools_4.0.4 cli_2.3.1 generics_0.1.0 ## [73] broom_0.7.5 ggridges_0.5.2 evaluate_0.14 fastmap_1.0.1 ## [77] processx_3.4.5 knitr_1.31 fs_1.5.0 zip_2.1.1 ## [81] nlme_3.1-152 mime_0.10 projpred_2.0.2 xml2_1.3.2 ## [85] compiler_4.0.4 bayesplot_1.8.0 shinythemes_1.1.2 rstudioapi_0.13 ## [89] curl_4.3 gamm4_0.2-6 reprex_0.3.0 statmod_1.4.35 ## [93] stringi_1.5.3 highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 ## [97] gdtools_0.2.2 lattice_0.20-41 Matrix_1.3-2 nloptr_1.2.2.2 ## [101] markdown_1.1 shinyjs_2.0.0 vctrs_0.3.6 pillar_1.5.1 ## [105] lifecycle_1.0.0 bridgesampling_1.0-0 estimability_1.3 data.table_1.14.0 ## [109] flextable_0.6.4 httpuv_1.5.4 R6_2.5.0 bookdown_0.21 ## [113] promises_1.1.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-26 ## [117] colourpicker_1.1.0 MASS_7.3-53 gtools_3.8.2 assertthat_0.2.1 ## [121] withr_2.4.1 shinystan_2.5.0 multcomp_1.4-16 mgcv_1.8-33 ## [125] parallel_4.0.4 hms_0.5.3 grid_4.0.4 coda_0.19-4 ## [129] minqa_1.2.4 rmarkdown_2.7 shiny_1.5.0 lubridate_1.7.9.2 ## [133] base64enc_0.1-3 dygraphs_1.1.1.6 "],["god-spiked-the-integers.html", "11 God Spiked the Integers 11.1 Binomial regression 11.2 Poisson regression 11.3 Multinomial and categorical models 11.4 Summary 11.5 Bonus: Survival analysis Session info", " 11 God Spiked the Integers The most common and useful generalized linear models are models for counts. Counts are non-negative integers–0, 1, 2, and so on. They are the basis of all mathematics, the first bits that children learn. But they are also intoxicatingly complicated to model–hence the apocryphal slogan that titles this chapter. The essential problem is this: When what we wish to predict is a count, the scale of the parameters is never the same as the scale of the outcome. A count golem, like a tide prediction engine, has a whirring machinery underneath that doesn’t resemble the output. Keeping the tide engine in mind, you can master these models and use them responsibly. We will engineer complete examples of the two most common types of count model. Binomial regression is the name we’ll use for a family of related procedures that all model a binary classification–alive/dead, accept/reject, left/right–for which the total of both categories is known. This is like the marble and globe tossing examples from Chapter 2. But now you get to incorporate predictor variables. Poisson regression is a GLM that models a count with an unknown maximum—number of elephants in Kenya, number of applications to a PhD program, number of significance tests in an issue of Psychological Science. As described in Chapter 10, the Poisson model is a special case of binomial. At the end, the chapter describes some other count regressions. (McElreath, 2020a, p. 323, emphasis in the original) In this chapter, we focus on the two most common types of count models: the binomial and the Poisson. 11.1 Binomial regression The basic binomial model follows the form \\[y \\sim \\operatorname{Binomial}(n, p),\\] where \\(y\\) is some count variable, \\(n\\) is the number of trials, and \\(p\\) it the probability a given trial was a 1, which is sometimes termed a success. When \\(n = 1\\), then \\(y\\) is a vector of 0’s and 1’s. Presuming the logit link3, which we just covered in Chapter 10, models of this type are commonly termed logistic regression. When \\(n &gt; 1\\), and still presuming the logit link, we might call our model an aggregated logistic regression model, or more generally an aggregated binomial regression model. 11.1.1 Logistic regression: Prosocial chimpanzees. Load the Silk et al. (2005) chimpanzees data. data(chimpanzees, package = &quot;rethinking&quot;) d &lt;- chimpanzees rm(chimpanzees) The data include two experimental conditions, prosoc_left and condition, each of which has two levels. This results in four combinations. library(tidyverse) library(flextable) d %&gt;% distinct(prosoc_left, condition) %&gt;% mutate(description = c(&quot;Two food items on right and no partner&quot;, &quot;Two food items on left and no partner&quot;, &quot;Two food items on right and partner present&quot;, &quot;Two food items on left and partner present&quot;)) %&gt;% flextable() %&gt;% width(width = c(1, 1, 4)) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-d33b5038{border-collapse:collapse;}.cl-d3347dda{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-d3349676{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d334968a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-d334d924{width:288pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d334d956{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d334d960{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d334d96a{width:288pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d334d974{width:72pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-d334d97e{width:288pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}conditionprosoc_leftdescription00Two food items on right and no partner01Two food items on left and no partner10Two food items on right and partner present11Two food items on left and partner present It would be conventional to include these two variables and their interaction using dummy variables. We’re going to follow McElreath and use an index variable approach, instead. If you’d like to see what this would look like using the dummy variable approach, check out my (2020a) translation of the corresponding section from McElreath’s first (2015) edition. For now, make the index, which we’ll be saving as a factor. d &lt;- d %&gt;% mutate(treatment = factor(1 + prosoc_left + 2 * condition)) %&gt;% # this will come in handy, later mutate(labels = factor(treatment, levels = 1:4, labels = c(&quot;r/n&quot;, &quot;l/n&quot;, &quot;r/p&quot;, &quot;l/p&quot;))) We can use the dplyr::count() function to get a sense of the distribution of the conditions in the data. d %&gt;% count(condition, treatment, prosoc_left) ## condition treatment prosoc_left n ## 1 0 1 0 126 ## 2 0 2 1 126 ## 3 1 3 0 126 ## 4 1 4 1 126 Fire up brms. library(brms) We start with the simple intercept-only logistic regression model, which follows the statistical formula \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\operatorname{Binomial}(1, p_i) \\\\ \\operatorname{logit}(p_i) &amp; = \\alpha \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, w), \\end{align*}\\] where \\(w\\) is the hyperparameter for \\(\\sigma\\) the value for which we have yet to choose. To start things off, we’ll set \\(w = 10\\), fit a model with where we set sample_prior = T, and get a sense of the prior on a plot. In the brm() formula syntax, including a | bar on the left side of a formula indicates we have extra supplementary information about our criterion. In this case, that information is that each pulled_left value corresponds to a single trial (i.e., trials(1)), which itself corresponds to the \\(n = 1\\) portion of the statistical formula, above. b11.1 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1, prior(normal(0, 10), class = Intercept), seed = 11, sample_prior = T, file = &quot;fits/b11.01&quot;) Before we go any further, let’s discuss the plot theme. For this chapter, we’ll take our color scheme from the \"Moonrise2\" palette from the wesanderson package (Ram &amp; Wickham, 2018). library(wesanderson) wes_palette(&quot;Moonrise2&quot;) wes_palette(&quot;Moonrise2&quot;)[1:4] ## [1] &quot;#798E87&quot; &quot;#C27D38&quot; &quot;#CCC591&quot; &quot;#29211F&quot; We’ll also take a few formatting cues from Edward Tufte (2001), courtesy of the ggthemes package. The theme_tufte() function will change the default font and remove some chart junk. The theme_set() function, below, will make these adjustments the default for all subsequent ggplot2 plots. To undo this, just execute theme_set(theme_default()). library(ggthemes) theme_set( theme_default() + theme_tufte() + theme(plot.background = element_rect(fill = wes_palette(&quot;Moonrise2&quot;)[3], color = wes_palette(&quot;Moonrise2&quot;)[3])) ) Now we’re ready to plot. We’ll extract the prior draws with prior_samples(), convert them from the log-odds metric to the probability metric with the brms::inv_logit_scaled() function, and adjust the bandwidth of the density plot with the adjust argument within geom_density(). prior_samples(b11.1) %&gt;% mutate(p = inv_logit_scaled(Intercept)) %&gt;% ggplot(aes(x = p)) + geom_density(fill = wes_palette(&quot;Moonrise2&quot;)[4], size = 0, adjust = 0.1) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;prior prob pull left&quot;) At this point in the analysis, we were only able to make part of the left panel of McElreath’s Figure 11.3. We’ll add to it in a bit. Now update the model so that \\(w = 1.5\\). b11.1b &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1, prior(normal(0, 1.5), class = Intercept), seed = 11, sample_prior = T, file = &quot;fits/b11.01b&quot;) Now we can make the full version of the left panel of Figure 11.3. # wrangle bind_rows(prior_samples(b11.1), prior_samples(b11.1b)) %&gt;% mutate(p = inv_logit_scaled(Intercept), w = factor(rep(c(10, 1.5), each = n() / 2), levels = c(10, 1.5))) %&gt;% # plot ggplot(aes(x = p, fill = w)) + geom_density(size = 0, alpha = 3/4, adjust = 0.1) + scale_fill_manual(expression(italic(w)), values = wes_palette(&quot;Moonrise2&quot;)[c(4, 1)]) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(alpha%~%Normal(0*&quot;, &quot;*italic(w))), x = &quot;prior prob pull left&quot;) If we’d like to fit a model that includes an overall intercept and uses McElreath’d index variable approach for the predictor variable treatment, we’ll have to switch to the brms non-linear syntax. Here it is for the models using \\(w = 10\\) and then \\(w = 0.5\\). # w = 10 b11.2 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 1, b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 10), nlpar = b, coef = treatment1), prior(normal(0, 10), nlpar = b, coef = treatment2), prior(normal(0, 10), nlpar = b, coef = treatment3), prior(normal(0, 10), nlpar = b, coef = treatment4)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, sample_prior = T, file = &quot;fits/b11.02&quot;) # w = 0.5 b11.3 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 1, b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = b, coef = treatment1), prior(normal(0, 0.5), nlpar = b, coef = treatment2), prior(normal(0, 0.5), nlpar = b, coef = treatment3), prior(normal(0, 0.5), nlpar = b, coef = treatment4)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, sample_prior = T, file = &quot;fits/b11.03&quot;) If all you want to do is fit the models, you wouldn’t have to add a separate prior() statement for each level of treatment. You could have just included a single line, prior(normal(0, 0.5), nlpar = b), that did not include a coef argument. The problem with this approach is we’d only get one column for treatment when using the prior_samples() function to retrieve the prior samples. To get separate columns for the prior samples of each of the levels of treatment, you need to take the verbose approach, above. Anyway, here’s how to make a version of the right panel of Figure 11.3. # wrangle prior &lt;- bind_rows(prior_samples(b11.2), prior_samples(b11.3)) %&gt;% mutate(w = factor(rep(c(10, 0.5), each = n() / 2), levels = c(10, 0.5)), p1 = inv_logit_scaled(b_a + b_b_treatment1), p2 = inv_logit_scaled(b_a + b_b_treatment2)) %&gt;% mutate(diff = abs(p1 - p2)) # plot prior %&gt;% ggplot(aes(x = diff, fill = w)) + geom_density(size = 0, alpha = 3/4, adjust = 0.1) + scale_fill_manual(expression(italic(w)), values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2)]) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(alpha%~%Normal(0*&quot;, &quot;*italic(w))), x = &quot;prior diff between treatments&quot;) Here are the averages of the two prior-predictive difference distributions. prior %&gt;% group_by(w) %&gt;% summarise(mean = mean(diff)) ## # A tibble: 2 x 2 ## w mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 10 0.489 ## 2 0.5 0.0971 Before we move on to fit the full model, it might be useful to linger here and examine the nature of the model we just fit. Here’s the parameter summary for b11.3. print(b11.3) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ a + b ## a ~ 1 ## b ~ 0 + treatment ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_Intercept 0.31 0.25 -0.18 0.80 1.00 870 1309 ## b_treatment1 -0.10 0.28 -0.66 0.44 1.00 939 1535 ## b_treatment2 0.31 0.28 -0.24 0.88 1.00 1177 1913 ## b_treatment3 -0.36 0.28 -0.91 0.15 1.00 1088 1619 ## b_treatment4 0.22 0.28 -0.32 0.77 1.00 1278 1810 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now focus on the likelihood portion of the model formula, \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\operatorname{Binomial}(1, p_i) \\\\ \\operatorname{logit}(p_i) &amp; = \\alpha + \\beta_\\text{treatment} . \\end{align*}\\] When you have one overall intercept \\(\\alpha\\) and then use the non-linear approach for the treatment index, you end up with as many \\(\\beta\\) parameters as there levels for treatment. This means the formula for treatment == 1 is \\(\\alpha + \\beta_{\\text{treatment}[1]}\\), the formula for treatment == 2 is \\(\\alpha + \\beta_{\\text{treatment}[2]}\\), and so on. This also effectively makes \\(\\alpha\\) the grand mean. Here’s the empirical grand mean. d %&gt;% summarise(grand_mean = mean(pulled_left)) ## grand_mean ## 1 0.5793651 Now here’s the summary of \\(\\alpha\\) after transforming it back into the probability metric with the inv_logit_scaled() function. library(tidybayes) posterior_samples(b11.3) %&gt;% transmute(alpha = inv_logit_scaled(b_a_Intercept)) %&gt;% mean_qi() ## alpha .lower .upper .width .point .interval ## 1 0.5751639 0.4546315 0.6891985 0.95 mean qi Here are the empirical probabilities for each of the four levels of treatment. d %&gt;% group_by(treatment) %&gt;% summarise(mean = mean(pulled_left)) ## # A tibble: 4 x 2 ## treatment mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 0.548 ## 2 2 0.659 ## 3 3 0.476 ## 4 4 0.635 Here are the corresponding posteriors. posterior_samples(b11.3) %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(treatment = str_remove(name, &quot;b_b_treatment&quot;), mean = inv_logit_scaled(b_a_Intercept + value)) %&gt;% group_by(treatment) %&gt;% mean_qi(mean) ## # A tibble: 4 x 7 ## treatment mean .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0.551 0.467 0.635 0.95 mean qi ## 2 2 0.650 0.567 0.728 0.95 mean qi ## 3 3 0.486 0.403 0.571 0.95 mean qi ## 4 4 0.628 0.547 0.704 0.95 mean qi Okay, let’s get back on track with the text. Now we’re ready to fit the full model, which follows the form \\[\\begin{align*} \\text{pulled_left}_i &amp; \\sim \\operatorname{Binomial}(1, p_i) \\\\ \\operatorname{logit}(p_i) &amp; = \\alpha_{\\color{#54635e}{\\text{actor}}[i]} + \\beta_{\\color{#a4692f}{\\text{treatment}}[i]} \\\\ \\alpha_{\\color{#54635e}j} &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\beta_{\\color{#a4692f}k} &amp; \\sim \\operatorname{Normal}(0, 0.5). \\end{align*}\\] Before fitting the model, we should save actor as a factor. d &lt;- d %&gt;% mutate(actor = factor(actor)) Now fit the model. b11.4 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 0 + actor, b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, file = &quot;fits/b11.04&quot;) Inspect the parameter summary. print(b11.4) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ a + b ## a ~ 0 + actor ## b ~ 0 + treatment ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_actor1 -0.46 0.34 -1.11 0.20 1.00 1301 2137 ## a_actor2 3.88 0.74 2.54 5.46 1.00 3918 2735 ## a_actor3 -0.75 0.33 -1.41 -0.11 1.00 1349 2298 ## a_actor4 -0.76 0.33 -1.38 -0.10 1.00 1443 2531 ## a_actor5 -0.45 0.33 -1.07 0.23 1.00 1432 2274 ## a_actor6 0.46 0.33 -0.16 1.12 1.00 1370 2222 ## a_actor7 1.95 0.42 1.15 2.80 1.00 1829 2515 ## b_treatment1 -0.03 0.28 -0.60 0.52 1.00 1196 2175 ## b_treatment2 0.48 0.29 -0.08 1.03 1.00 1231 2117 ## b_treatment3 -0.38 0.29 -0.95 0.19 1.00 1248 1966 ## b_treatment4 0.37 0.28 -0.20 0.92 1.00 1141 1950 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s how we might make our version of McElreath’s coefficient plot of the \\(\\alpha\\) parameters. library(tidybayes) post &lt;- posterior_samples(b11.4) post %&gt;% pivot_longer(contains(&quot;actor&quot;)) %&gt;% mutate(probability = inv_logit_scaled(value), actor = factor(str_remove(name, &quot;b_a_actor&quot;), levels = 7:1)) %&gt;% ggplot(aes(x = probability, y = actor)) + geom_vline(xintercept = .5, color = wes_palette(&quot;Moonrise2&quot;)[1], linetype = 3) + stat_pointinterval(.width = .95, size = 1/2, color = wes_palette(&quot;Moonrise2&quot;)[4]) + scale_x_continuous(expression(alpha[actor]), limits = 0:1) + ylab(NULL) + theme(axis.ticks.y = element_blank()) Here’s the corresponding coefficient plot of the \\(\\beta\\) parameters. tx &lt;- c(&quot;R/N&quot;, &quot;L/N&quot;, &quot;R/P&quot;, &quot;L/P&quot;) post %&gt;% select(contains(&quot;treatment&quot;)) %&gt;% set_names(&quot;R/N&quot;,&quot;L/N&quot;,&quot;R/P&quot;,&quot;L/P&quot;) %&gt;% pivot_longer(everything()) %&gt;% mutate(probability = inv_logit_scaled(value), treatment = factor(name, levels = tx)) %&gt;% mutate(treatment = fct_rev(treatment)) %&gt;% ggplot(aes(x = value, y = treatment)) + geom_vline(xintercept = 0, color = wes_palette(&quot;Moonrise2&quot;)[2], linetype = 3) + stat_pointinterval(.width = .95, size = 1/2, color = wes_palette(&quot;Moonrise2&quot;)[4]) + labs(x = expression(beta[treatment]), y = NULL) + theme(axis.ticks.y = element_blank()) Now make the coefficient plot for the primary contrasts of interest. post %&gt;% mutate(db13 = b_b_treatment1 - b_b_treatment3, db24 = b_b_treatment2 - b_b_treatment4) %&gt;% pivot_longer(db13:db24) %&gt;% mutate(diffs = factor(name, levels = c(&quot;db24&quot;, &quot;db13&quot;))) %&gt;% ggplot(aes(x = value, y = diffs)) + geom_vline(xintercept = 0, color = wes_palette(&quot;Moonrise2&quot;)[2], linetype = 3) + stat_pointinterval(.width = .95, size = 1/2, color = wes_palette(&quot;Moonrise2&quot;)[4]) + labs(x = &quot;difference&quot;, y = NULL) + theme(axis.ticks.y = element_blank()) “These are the contrasts between the no-partner/partner treatments” (p. 331). Next, we prepare for the posterior predictive check. McElreath showed how to compute empirical proportions by the levels of actor and treatment with the by() function. Our approach will be with a combination of group_by() and summarise(). Here’s what that looks like for actor == 1. d %&gt;% group_by(actor, treatment) %&gt;% summarise(proportion = mean(pulled_left)) %&gt;% filter(actor == 1) ## # A tibble: 4 x 3 ## # Groups: actor [1] ## actor treatment proportion ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 1 0.333 ## 2 1 2 0.5 ## 3 1 3 0.278 ## 4 1 4 0.556 Now we’ll follow that through to make the top panel of Figure 11.4. Instead of showing the plot, we’ll save it for the next code block. p1 &lt;- d %&gt;% group_by(actor, treatment) %&gt;% summarise(proportion = mean(pulled_left)) %&gt;% left_join(d %&gt;% distinct(actor, treatment, labels, condition, prosoc_left), by = c(&quot;actor&quot;, &quot;treatment&quot;)) %&gt;% mutate(condition = factor(condition)) %&gt;% ggplot(aes(x = labels, y = proportion)) + geom_hline(yintercept = .5, color = wes_palette(&quot;Moonrise2&quot;)[3]) + geom_line(aes(group = prosoc_left), size = 1/4, color = wes_palette(&quot;Moonrise2&quot;)[4]) + geom_point(aes(color = condition), size = 2.5, show.legend = F) + labs(subtitle = &quot;observed proportions&quot;) Next we use brms() fitted to get the posterior predictive distributions for each unique combination of actor and treatment, wrangle, and plot. First, we save the plot as p2 and then we use patchwork syntax to combine the two subplots. nd &lt;- d %&gt;% distinct(actor, treatment, labels, condition, prosoc_left) p2 &lt;- fitted(b11.4, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(condition = factor(condition)) %&gt;% ggplot(aes(x = labels, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = .5, color = wes_palette(&quot;Moonrise2&quot;)[3]) + geom_line(aes(group = prosoc_left), size = 1/4, color = wes_palette(&quot;Moonrise2&quot;)[4]) + geom_pointrange(aes(color = condition), fatten = 2.5, show.legend = F) + labs(subtitle = &quot;posterior predictions&quot;) # combine the two ggplots library(patchwork) (p1 / p2) &amp; scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(2:1)]) &amp; scale_y_continuous(&quot;proportion left lever&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) &amp; xlab(NULL) &amp; theme(axis.ticks.x = element_blank(), panel.background = element_rect(fill = alpha(&quot;white&quot;, 1/10), size = 0)) &amp; facet_wrap(~ actor, nrow = 1, labeller = label_both) Let’s make two more index variables. d &lt;- d %&gt;% mutate(side = factor(prosoc_left + 1), # right 1, left 2 cond = factor(condition + 1)) # no partner 1, partner 2 Now fit the model without the interaction between prosoc_left and condition. b11.5 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + bs + bc, a ~ 0 + actor, bs ~ 0 + side, bc ~ 0 + cond, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = bs), prior(normal(0, 0.5), nlpar = bc)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, file = &quot;fits/b11.05&quot;) Compare b11.4 and b11.5 by the PSIS-LOO and the WAIC. b11.4 &lt;- add_criterion(b11.4, c(&quot;loo&quot;, &quot;waic&quot;)) b11.5 &lt;- add_criterion(b11.5, c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(b11.4, b11.5, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.5 0.0 0.0 -265.4 9.6 7.8 0.4 530.8 19.2 ## b11.4 -0.6 0.7 -266.0 9.4 8.3 0.4 532.0 18.9 loo_compare(b11.4, b11.5, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b11.5 0.0 0.0 -265.4 9.6 7.8 0.4 530.8 19.2 ## b11.4 -0.6 0.7 -266.0 9.4 8.3 0.4 532.0 18.9 Here are the weights. model_weights(b11.4, b11.5, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## b11.4 b11.5 ## 0.35 0.65 model_weights(b11.4, b11.5, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b11.4 b11.5 ## 0.35 0.65 Here’s a quick check of the parameter summary for the non-interaction model, b11.5. print(b11.5) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ a + bs + bc ## a ~ 0 + actor ## bs ~ 0 + side ## bc ~ 0 + cond ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_actor1 -0.65 0.46 -1.57 0.27 1.00 997 1473 ## a_actor2 3.76 0.81 2.30 5.52 1.00 1943 1954 ## a_actor3 -0.95 0.46 -1.84 -0.06 1.00 1045 1731 ## a_actor4 -0.95 0.46 -1.86 -0.01 1.00 987 1652 ## a_actor5 -0.65 0.45 -1.52 0.24 1.00 1057 1780 ## a_actor6 0.27 0.46 -0.63 1.18 1.00 1035 1730 ## a_actor7 1.77 0.53 0.73 2.83 1.00 1212 2182 ## bs_side1 -0.19 0.34 -0.85 0.51 1.00 1130 1800 ## bs_side2 0.50 0.35 -0.16 1.19 1.00 1134 1500 ## bc_cond1 0.28 0.33 -0.37 0.92 1.00 1351 1692 ## bc_cond2 0.03 0.33 -0.63 0.66 1.00 1387 1847 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Because it’s good practice, here’s the b11.5 version of the bottom panel of Figure 11.4. nd &lt;- d %&gt;% distinct(actor, treatment, labels, cond, side) fitted(b11.5, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = labels, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_hline(yintercept = .5, color = wes_palette(&quot;Moonrise2&quot;)[3]) + geom_line(aes(group = side), size = 1/4, color = wes_palette(&quot;Moonrise2&quot;)[4]) + geom_pointrange(aes(color = cond), fatten = 2.5, show.legend = F) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(2:1)]) + scale_y_continuous(&quot;proportion left lever&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + labs(subtitle = &quot;posterior predictions for b11.5&quot;, x = NULL) + theme(axis.ticks.x = element_blank(), panel.background = element_rect(fill = alpha(&quot;white&quot;, 1/10), size = 0)) + facet_wrap(~ actor, nrow = 1, labeller = label_both) 11.1.1.1 Overthinking: Adding log-probability calculations to a Stan model. For retrieving log-probability summaries, our approach with brms is a little different than the one you might take with McElreath’s rethinking. Rather than adding a log_lik=TRUE argument within rethinking::ulam(), we just use the log_lik() function after fitting a brms model. You may recall we already practiced this way back in Section 7.2.4.1. Here’s a quick example of what that looks like for b11.5. log_lik(b11.5) %&gt;% str() ## num [1:4000, 1:504] -0.323 -0.427 -0.515 -0.584 -0.429 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL 11.1.2 Relative shark and absolute deer. Based on the full model, b11.4, here’s how you might compute the posterior mean and 95% intervals for the proportional odds of switching from treatment == 2 to treatment == 4. posterior_samples(b11.4) %&gt;% mutate(proportional_odds = exp(b_b_treatment4 - b_b_treatment2)) %&gt;% mean_qi(proportional_odds) ## proportional_odds .lower .upper .width .point .interval ## 1 0.9315351 0.5183732 1.536444 0.95 mean qi On average, the switch multiplies the odds of pulling the left lever by 0.92, an 8% reduction in odds. This is what is meant by proportional odds. The new odds are calculated by taking the old odds and multiplying them by the proportional odds, which is 0.92 in this example. (p. 336) A limitation of relative measures measures like proportional odds is they ignore what you might think of as the reference or the baseline. Consider for example a rare disease which occurs in 1 per 10-million people. Suppose also that reading this textbook increased the odds of the disease 5-fold. That would mean approximately 4 more cases of the disease per 10-million people. So only 5-in-10-million chance now. The book is safe. (p. 336) Here that is in code. tibble(disease_rate = 1/1e7, fold_increase = 5) %&gt;% mutate(new_disease_rate = disease_rate * fold_increase) ## # A tibble: 1 x 3 ## disease_rate fold_increase new_disease_rate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0000001 5 0.0000005 The hard part, though, is that “neither absolute nor relative risk is sufficient for all purposes” (p. 337). Each provides its own unique perspective on the data. Again, welcome to applied statistics. 🤷♂ 11.1.3 Aggregated binomial: Chimpanzees again, condensed. With the tidyverse, we can use group_by() and summarise() to achieve what McElreath did with aggregate(). d_aggregated &lt;- d %&gt;% group_by(treatment, actor, side, cond) %&gt;% summarise(left_pulls = sum(pulled_left)) %&gt;% ungroup() d_aggregated %&gt;% head(n = 8) ## # A tibble: 8 x 5 ## treatment actor side cond left_pulls ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 1 1 1 1 6 ## 2 1 2 1 1 18 ## 3 1 3 1 1 5 ## 4 1 4 1 1 6 ## 5 1 5 1 1 6 ## 6 1 6 1 1 14 ## 7 1 7 1 1 14 ## 8 2 1 2 1 9 To fit an aggregated binomial model with brms, we augment the &lt;criterion&gt; | trials() syntax where the value that goes in trials() is either a fixed number, as in this case, or variable in the data indexing \\(n\\). Either way, at least some of those trials will have an \\(n &gt; 1\\). Here we’ll use the hard-code method, just like McElreath did in the text. b11.6 &lt;- brm(data = d_aggregated, family = binomial, bf(left_pulls | trials(18) ~ a + b, a ~ 0 + actor, b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 0.5), nlpar = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, file = &quot;fits/b11.06&quot;) Check the posterior summary. print(b11.6) ## Family: binomial ## Links: mu = logit ## Formula: left_pulls | trials(18) ~ a + b ## a ~ 0 + actor ## b ~ 0 + treatment ## Data: d_aggregated (Number of observations: 28) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_actor1 -0.44 0.33 -1.08 0.20 1.00 1323 2122 ## a_actor2 3.89 0.77 2.56 5.61 1.00 3794 2058 ## a_actor3 -0.74 0.34 -1.43 -0.09 1.00 1220 2095 ## a_actor4 -0.75 0.34 -1.43 -0.08 1.00 1265 1944 ## a_actor5 -0.44 0.34 -1.11 0.21 1.01 1263 1958 ## a_actor6 0.49 0.34 -0.16 1.13 1.00 1357 2446 ## a_actor7 1.96 0.42 1.16 2.82 1.00 1671 2621 ## b_treatment1 -0.05 0.29 -0.59 0.53 1.00 1072 1942 ## b_treatment2 0.48 0.29 -0.08 1.05 1.00 1076 1749 ## b_treatment3 -0.39 0.29 -0.94 0.17 1.01 1091 1875 ## b_treatment4 0.36 0.29 -0.19 0.93 1.00 984 1933 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). It might be easiest to compare b11.4 and b11.6 with a coefficient plot. # this is just for fancy annotation text &lt;- tibble(value = c(1.4, 2.6), name = &quot;b_a_actor7&quot;, fit = c(&quot;b11.6&quot;, &quot;b11.4&quot;)) # rope in the posterior draws and wrangle bind_rows(posterior_samples(b11.4), posterior_samples(b11.6)) %&gt;% mutate(fit = rep(c(&quot;b11.4&quot;, &quot;b11.6&quot;), each = n() / 2)) %&gt;% pivot_longer(b_a_actor1:b_b_treatment4) %&gt;% # plot ggplot(aes(x = value, y = name, color = fit)) + stat_pointinterval(.width = .95, size = 2/3, position = position_dodge(width = 0.5)) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[2:1]) + geom_text(data = text, aes(label = fit), family = &quot;Times&quot;, position = position_dodge(width = 2.25)) + labs(x = &quot;posterior (log-odds scale)&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) Did you catch our position = position_dodge() tricks? Try executing the plot without those parts of the code to get a sense of what they did. Now compute and save the PSIS-LOO estimates for the two models so we might compare them. b11.4 &lt;- add_criterion(b11.4, &quot;loo&quot;) b11.6 &lt;- add_criterion(b11.6, &quot;loo&quot;) Here’s how we might attempt the comparison. loo_compare(b11.4, b11.6, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) Unlike with McElreath’s compare() code in the text, loo_compare() wouldn’t even give us the results. All we get is the warning message informing us that because these two models are not based on the same data, comparing them with the LOO is invalid and brms refuses to let us do it. We can, however, look at their LOO summaries separately. loo(b11.4) ## ## Computed from 4000 by 504 log-likelihood matrix ## ## Estimate SE ## elpd_loo -266.0 9.4 ## p_loo 8.3 0.4 ## looic 532.0 18.9 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. loo(b11.6) ## ## Computed from 4000 by 28 log-likelihood matrix ## ## Estimate SE ## elpd_loo -57.7 4.3 ## p_loo 8.9 1.7 ## looic 115.3 8.6 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 17 60.7% 1412 ## (0.5, 0.7] (ok) 8 28.6% 768 ## (0.7, 1] (bad) 3 10.7% 243 ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## See help(&#39;pareto-k-diagnostic&#39;) for details. To understand what’s going on, consider how you might describe six 1’s out of nine trials in the aggregated form, \\[\\text{Pr}(6|9, p) = \\frac{6!}{6!(9 - 6)!} p^6 (1 - p)^{9 - 6}.\\] If we still stick with the same data, but this time re-express those as nine dichotomous data points, we now describe their joint probability as \\[\\text{Pr}(1, 1, 1, 1, 1, 1, 0, 0, 0 | p) = p^6 (1 - p)^{9 - 6}.\\] Let’s work this out in code. # deviance of aggregated 6-in-9 -2 * dbinom(6, size = 9, prob = 0.2, log = TRUE) ## [1] 11.79048 # deviance of dis-aggregated -2 * sum(dbinom(c(1, 1, 1, 1, 1, 1, 0, 0, 0), size = 1, prob = 0.2, log = TRUE)) ## [1] 20.65212 But this difference is entirely meaningless. It is just a side effect of how we organized the data. The posterior distribution for the probability of success on each trial will end up the same, either way. (p. 339) This is what our coefficient plot showed us, above. The posterior distribution was the same within simulation variance for b11.4 and b11.6. Just like McElreath reported in the text, we also got a warning about high Pareto \\(k\\) values from the aggregated binomial model, b11.6. To access the message and its associated table directly, we can feed the results of loo() into the loo::pareto_k_table function. loo(b11.6) %&gt;% loo::pareto_k_table() ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 17 60.7% 1412 ## (0.5, 0.7] (ok) 8 28.6% 768 ## (0.7, 1] (bad) 3 10.7% 243 ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; Before looking at the Pareto \\(k\\) values, you might have noticed already that we didn’t get a similar warning before in the disaggregated logistic models of the same data. Why not? Because when we aggregated the data by actor-treatment, we forced PSIS (and WAIC) to imagine cross-validation that leaves out all 18 observations in each actor-treatment combination. So instead of leave-one-out cross-validation, it is more like leave-eighteen-out. This makes some observations more influential, because they are really now 18 observations. What’s the bottom line? If you want to calculate WAIC or PSIS, you should use a logistic regression data format, not an aggregated format. Otherwise you are implicitly assuming that only large chunks of the data are separable. (p. 340) 11.1.4 Aggregated binomial: Graduate school admissions. Load the infamous UCBadmit data (see Bickel et al., 1975). data(UCBadmit, package = &quot;rethinking&quot;) d &lt;- UCBadmit rm(UCBadmit) d ## dept applicant.gender admit reject applications ## 1 A male 512 313 825 ## 2 A female 89 19 108 ## 3 B male 353 207 560 ## 4 B female 17 8 25 ## 5 C male 120 205 325 ## 6 C female 202 391 593 ## 7 D male 138 279 417 ## 8 D female 131 244 375 ## 9 E male 53 138 191 ## 10 E female 94 299 393 ## 11 F male 22 351 373 ## 12 F female 24 317 341 Now compute our new index variable, gid. We’ll also slip in a case variable that saves the row numbers as a factor. That’ll come in handy later when we plot. d &lt;- d %&gt;% mutate(gid = factor(applicant.gender, levels = c(&quot;male&quot;, &quot;female&quot;)), case = factor(1:n())) Note the difference in how we defined out gid. Whereas McElreath used numeral indices, we retained the text within an ordered factor. brms can handle either approach just fine. The advantage of the factor approach is it will be easier to understand the output. You’ll see in just a bit. The univariable logistic model with male as the sole predictor of admit follows the form \\[\\begin{align*} \\text{admit}_i &amp; \\sim \\operatorname{Binomial}(n_i, p_i) \\\\ \\text{logit}(p_i) &amp; = \\alpha_{\\text{gid}[i]} \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal}(0, 1.5), \\end{align*}\\] where \\(n_i = \\text{applications}_i\\), the rows are indexed by \\(i\\), and the two levels of \\(\\text{gid}\\) are indexed by \\(j\\). Since we’re only using our index variable gid to model two intercepts with no further complications, we don’t need to use the verbose non-linear syntax to fit this model with brms. b11.7 &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 0 + gid, prior(normal(0, 1.5), class = b), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.07&quot;) print(b11.7) ## Family: binomial ## Links: mu = logit ## Formula: admit | trials(applications) ~ 0 + gid ## Data: d (Number of observations: 12) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## gidmale -0.22 0.04 -0.30 -0.14 1.00 2684 2468 ## gidfemale -0.83 0.05 -0.94 -0.73 1.00 3289 2318 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our results are very similar to those in the text. But notice how our two rows have more informative row names than a[1] and a[2]. This is why you might consider using the ordered factor approach rather than using numeral indices. Anyway, here we’ll compute the difference score in two metrics and summarize them with a little help from mean_qi(). posterior_samples(b11.7) %&gt;% mutate(diff_a = b_gidmale - b_gidfemale, diff_p = inv_logit_scaled(b_gidmale) - inv_logit_scaled(b_gidfemale)) %&gt;% pivot_longer(contains(&quot;diff&quot;)) %&gt;% group_by(name) %&gt;% mean_qi(value, .width = .89) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 diff_a 0.609 0.504 0.715 0.89 mean qi ## 2 diff_p 0.141 0.118 0.165 0.89 mean qi brms doesn’t have a convenience function that works quite like rethinking::postcheck(). But we have options, the most handy of which in this case is probably predict(). p &lt;- predict(b11.7) %&gt;% data.frame() %&gt;% bind_cols(d) text &lt;- d %&gt;% group_by(dept) %&gt;% summarise(case = mean(as.numeric(case)), admit = mean(admit / applications) + .05) p %&gt;% ggplot(aes(x = case, y = admit / applications)) + geom_pointrange(aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = wes_palette(&quot;Moonrise2&quot;)[1], shape = 1, alpha = 1/3) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line(aes(group = dept), color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_text(data = text, aes(y = admit, label = dept), color = wes_palette(&quot;Moonrise2&quot;)[2], family = &quot;serif&quot;) + scale_y_continuous(&quot;Proportion admitted&quot;, limits = 0:1) + ggtitle(&quot;Posterior validation check&quot;) + theme(axis.ticks.x = element_blank()) Sometimes a fit this bad is the result of a coding mistake. In this case, it is not. The model did correctly answer the question we asked of it: What are the average probabilities of admission for women and men, across all departments? The problem in this case is that men and women did not apply to the same departments, and departments vary in their rates of admission. This makes the answer misleading…. Instead of asking “What are the average probabilities of admission for women and men across all departments?” we want to ask “What is the average difference in probability of admission between women and men within departments?” (pp. 342–343, emphasis in the original). The model better suited to answer that question follows the form \\[\\begin{align*} \\text{admit}_i &amp; \\sim \\operatorname{Binomial} (n_i, p_i) \\\\ \\text{logit}(p_i) &amp; = \\alpha_{\\text{gid}[i]} + \\delta_{\\text{dept}[i]} \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal} (0, 1.5) \\\\ \\delta_k &amp; \\sim \\operatorname{Normal} (0, 1.5), \\end{align*}\\] where departments are indexed by \\(k\\). To fit a model including two index variables like this in brms, we’ll need to switch back to the non-linear syntax. Though if you’d like to see an analogous approach using conventional brms syntax, check out model b10.9 in Section 10.1.3 of my translation of McElreath’s first edition. b11.8 &lt;- brm(data = d, family = binomial, bf(admit | trials(applications) ~ a + d, a ~ 0 + gid, d ~ 0 + dept, nl = TRUE), prior = c(prior(normal(0, 1.5), nlpar = a), prior(normal(0, 1.5), nlpar = d)), iter = 4000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.08&quot;) print(b11.8) ## Family: binomial ## Links: mu = logit ## Formula: admit | trials(applications) ~ a + d ## a ~ 0 + gid ## d ~ 0 + dept ## Data: d (Number of observations: 12) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_gidmale -0.56 0.53 -1.64 0.45 1.00 1014 1353 ## a_gidfemale -0.46 0.53 -1.55 0.56 1.00 1013 1399 ## d_deptA 1.14 0.53 0.12 2.22 1.00 1020 1390 ## d_deptB 1.10 0.54 0.08 2.19 1.00 1024 1431 ## d_deptC -0.12 0.53 -1.15 0.97 1.00 1024 1402 ## d_deptD -0.15 0.53 -1.17 0.93 1.00 1022 1447 ## d_deptE -0.59 0.54 -1.63 0.50 1.00 1038 1495 ## d_deptF -2.15 0.54 -3.19 -1.06 1.00 1057 1566 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Like with the earlier model, here we compute the difference score for \\(\\alpha\\) in two metrics. posterior_samples(b11.8) %&gt;% mutate(diff_a = b_a_gidmale - b_a_gidfemale, diff_p = inv_logit_scaled(b_a_gidmale) - inv_logit_scaled(b_a_gidfemale)) %&gt;% pivot_longer(contains(&quot;diff&quot;)) %&gt;% group_by(name) %&gt;% mean_qi(value, .width = .89) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 diff_a -0.0968 -0.227 0.0320 0.89 mean qi ## 2 diff_p -0.0215 -0.0514 0.00693 0.89 mean qi Why did adding departments to the model change the inference about gender so much? The earlier figure gives you a hint–the rates of admission vary a lot across departments. Furthermore, women and men applied to different departments. Let’s do a quick tabulation to show that: (p. 344) Here’s our tidyverse-style tabulation of the proportions of applicants in each department by gid. d %&gt;% group_by(dept) %&gt;% mutate(proportion = applications / sum(applications)) %&gt;% select(dept, gid, proportion) %&gt;% pivot_wider(names_from = dept, values_from = proportion) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 7 ## gid A B C D E F ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 male 0.88 0.96 0.35 0.53 0.33 0.52 ## 2 female 0.12 0.04 0.65 0.47 0.67 0.48 To make it even easier to see, we’ll depict it in a tile plot. d %&gt;% group_by(dept) %&gt;% mutate(proportion = applications / sum(applications)) %&gt;% mutate(label = round(proportion, digits = 2), gid = fct_rev(gid)) %&gt;% ggplot(aes(x = dept, y = gid, fill = proportion, label = label)) + geom_tile() + geom_text(aes(color = proportion &gt; .25), family = &quot;serif&quot;) + scale_fill_gradient(low = wes_palette(&quot;Moonrise2&quot;)[4], high = wes_palette(&quot;Moonrise2&quot;)[1], limits = c(0, 1)) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(1, 4)]) + scale_x_discrete(NULL, position = &quot;top&quot;) + ylab(NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks = element_blank(), legend.position = &quot;none&quot;) As it turns out, “The departments with a larger proportion of women applicants are also those with lower overall admissions rates” (p. 344). If we presume gender influences both choice of department and admission rates, we might depict that in a simple DAG where \\(G\\) is applicant gender, \\(D\\) is department, and \\(A\\) is acceptance into grad school. library(ggdag) dag_coords &lt;- tibble(name = c(&quot;G&quot;, &quot;D&quot;, &quot;A&quot;), x = c(1, 2, 3), y = c(1, 2, 1)) dagify(D ~ G, A ~ D + G, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_text(color = wes_palette(&quot;Moonrise2&quot;)[4], family = &quot;serif&quot;) + geom_dag_edges(edge_color = wes_palette(&quot;Moonrise2&quot;)[4]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) Although our b11.8 model did not contain a parameter corresponding to the \\(G \\rightarrow D\\) pathway, it did condition on both \\(G\\) and \\(D\\). If we make another Figure like 11.5, we’ll see conditioning on both substantially improved the posterior predictive distribution. predict(b11.8) %&gt;% data.frame() %&gt;% bind_cols(d) %&gt;% ggplot(aes(x = case, y = admit / applications)) + geom_pointrange(aes(y = Estimate / applications, ymin = Q2.5 / applications , ymax = Q97.5 / applications), color = wes_palette(&quot;Moonrise2&quot;)[1], shape = 1, alpha = 1/3) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_line(aes(group = dept), color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_text(data = text, aes(y = admit, label = dept), color = wes_palette(&quot;Moonrise2&quot;)[2], family = &quot;serif&quot;) + scale_y_continuous(&quot;Proportion admitted&quot;, limits = 0:1) + labs(title = &quot;Posterior validation check&quot;, subtitle = &quot;Though imperfect, this model is a big improvement&quot;) + theme(axis.ticks.x = element_blank()) Here’s the DAG that proposes an unobserved confound, \\(U\\), that might better explain the \\(D \\rightarrow A\\) pathway. dag_coords &lt;- tibble(name = c(&quot;G&quot;, &quot;D&quot;, &quot;A&quot;, &quot;U&quot;), x = c(1, 2, 3, 3), y = c(1, 2, 1, 2)) dagify(D ~ G + U, A ~ D + G + U, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_point(x = 3, y = 2, size = 5, color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_dag_text(color = wes_palette(&quot;Moonrise2&quot;)[4], family = &quot;serif&quot;) + geom_dag_edges(edge_color = wes_palette(&quot;Moonrise2&quot;)[4]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) McElreath recommended we look at the pairs() plot to get a sense of how highly correlated the parameters in our b11.8 model are. Why not get a little extra about it and use custom settings the upper triangle, the diagonal, and the lower triangle with a GGally::ggpairs() plot? First we save our custom settings. my_upper &lt;- function(data, mapping, ...) { # get the x and y data to use the other code x &lt;- eval_data_col(data, mapping$x) y &lt;- eval_data_col(data, mapping$y) r &lt;- unname(cor.test(x, y)$estimate) rt &lt;- format(r, digits = 2)[1] tt &lt;- as.character(rt) # plot the cor value ggally_text( label = tt, mapping = aes(), size = 4, color = wes_palette(&quot;Moonrise2&quot;)[4], alpha = 4/5, family = &quot;Times&quot;) + theme_void() } my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(fill = wes_palette(&quot;Moonrise2&quot;)[2], size = 0) + theme_void() } my_lower &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_point(color = wes_palette(&quot;Moonrise2&quot;)[1], size = 1/10, alpha = 1/10) + theme_void() } To learn more about the nature of the code for the my_upper() function, check out Issue #139 in the GGally GitHub repository. Here is the plot. library(GGally) posterior_samples(b11.8) %&gt;% select(-lp__) %&gt;% set_names(c(&quot;alpha[male]&quot;, &quot;alpha[female]&quot;, str_c(&quot;delta[&quot;, LETTERS[1:6], &quot;]&quot;))) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = &quot;label_parsed&quot;) + labs(title = &quot;Model: b11.8&quot;, subtitle = &quot;The parameters are strongly correlated.&quot;) + theme(strip.text = element_text(size = 11)) Why might we want to over-parameterize the model? Because it makes it easier to assign priors. If we made one of the genders baseline and measured the other as a deviation from it, we would stumble into the issue of assuming that the acceptance rate for one of the genders is pre-data more uncertain than the other. This isn’t to say that over-parameterizing a model is always a good idea. But it isn’t a violation of any statistical principle. You can always convert the posterior, post sampling, to any alternative parameterization. The only limitation is whether the algorithm we use to approximate the posterior can handle the high correlations. In this case, it can. (p. 345) 11.1.4.1 Rethinking: Simpson’s paradox is not a paradox. This empirical example is a famous one in statistical teaching. It is often used to illustrate a phenomenon known as Simpson’s paradox. Like most paradoxes, there is no violation of logic, just of intuition. And since different people have different intuition, Simpson’s paradox means different things to different people. The poor intuition being violated in this case is that a positive association in the entire population should also hold within each department. (p. 345, emphasis in the original) In my field of clinical psychology, Simpson’s paradox is an important, if under-appreciated, phenomenon. If you’re in the social sciences as well, I highly recommend spending more time thinking about it. To get you started, I blogged about it here and Kievit et al. (2013) wrote a great tutorial paper called Simpson’s paradox in psychological science: a practical guide. 11.2 Poisson regression When a binomial distribution has a very small probability of an event \\(p\\) and a very large number of trials \\(N\\), then it takes on a special shape. The expected value of a binomial distribution is just \\(Np\\), and its variance is \\(Np(1 - p)\\). But when \\(N\\) is very large and \\(p\\) is very small, then these are approximately the same. (p. 346) Data of this kind are often called count data. Here we simulate some. set.seed(11) tibble(y = rbinom(1e5, 1000, 1/1000)) %&gt;% summarise(y_mean = mean(y), y_variance = var(y)) ## # A tibble: 1 x 2 ## y_mean y_variance ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.00 1.01 Yes, those statistics are virtually the same. When dealing with pure Poisson data, \\(\\mu = \\sigma^2\\). When you have a number of trials for which \\(n\\) is unknown or much larger than seen in the data, the Poisson likelihood is a useful tool. We define it as \\[y_i \\sim \\text{Poisson}(\\lambda),\\] where \\(\\lambda\\) expresses both mean and variance because within this model, the variance scales right along with the mean. Since \\(\\lambda\\) is constrained to be positive, we typically use the log link. Thus the basic Poisson regression model is \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Poisson}(\\lambda_i) \\\\ \\log(\\lambda_i) &amp; = \\alpha + \\beta (x_i - \\bar x), \\end{align*}\\] where all model parameters receive priors following the forms we’ve been practicing. 11.2.1 Example: Oceanic tool complexity. Load the Kline data (see Kline &amp; Boyd, 2010). data(Kline, package = &quot;rethinking&quot;) d &lt;- Kline rm(Kline) d ## culture population contact total_tools mean_TU ## 1 Malekula 1100 low 13 3.2 ## 2 Tikopia 1500 low 22 4.7 ## 3 Santa Cruz 3600 low 24 4.0 ## 4 Yap 4791 high 43 5.0 ## 5 Lau Fiji 7400 high 33 5.0 ## 6 Trobriand 8000 high 19 4.0 ## 7 Chuuk 9200 high 40 3.8 ## 8 Manus 13000 low 28 6.6 ## 9 Tonga 17500 high 55 5.4 ## 10 Hawaii 275000 low 71 6.6 Here are our new columns. d &lt;- d %&gt;% mutate(log_pop_std = (log(population) - mean(log(population))) / sd(log(population)), cid = contact) Our statistical model will follow the form \\[\\begin{align*} \\text{total_tools}_i &amp; \\sim \\operatorname{Poisson}(\\lambda_i) \\\\ \\log(\\lambda_i) &amp; = \\alpha_{\\text{cid}[i]} + \\beta_{\\text{cid}[i]} \\text{log_pop_std}_i \\\\ \\alpha_j &amp; \\sim \\; ? \\\\ \\beta_j &amp; \\sim \\; ?, \\end{align*}\\] where the priors for \\(\\alpha_j\\) and \\(\\beta_j\\) have yet be defined. If we continue our convention of using a Normal prior on the \\(\\alpha\\) parameters, we should recognize those will be log-Normal distributed on the outcome scale. Why? Because we’re modeling \\(\\lambda\\) with the log link. Here’s our version of Figure 11.7, depicting the two log-Normal priors considered in the text. tibble(x = c(3, 22), y = c(0.055, 0.04), meanlog = c(0, 3), sdlog = c(10, 0.5)) %&gt;% expand(nesting(x, y, meanlog, sdlog), number = seq(from = 0, to = 100, length.out = 200)) %&gt;% mutate(density = dlnorm(number, meanlog, sdlog), group = str_c(&quot;alpha%~%Normal(&quot;, meanlog, &quot;, &quot;, sdlog, &quot;)&quot;)) %&gt;% ggplot(aes(fill = group, color = group)) + geom_area(aes(x = number, y = density), alpha = 3/4, size = 0, position = &quot;identity&quot;) + geom_text(data = . %&gt;% group_by(group) %&gt;% slice(1), aes(x = x, y = y, label = group), family = &quot;Times&quot;, parse = T, hjust = 0) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[1:2]) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[1:2]) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;mean number of tools&quot;) + theme(legend.position = &quot;none&quot;) In this context, \\(\\alpha \\sim \\operatorname{Normal}(0, 10)\\) has a very long tail on the outcome scale. The mean of the log-Normal distribution, recall, is \\(\\exp (\\mu + \\sigma^2/2)\\). Here that is in code. exp(0 + 10^2 / 2) ## [1] 5.184706e+21 That is very large. Here’s the same thing in a simulation. set.seed(11) rnorm(1e4, 0, 10) %&gt;% exp() %&gt;% mean() ## [1] 1.61276e+12 Now compute the mean for the other prior under consideration, \\(\\alpha \\sim \\operatorname{Normal}(3, 0.5)\\). exp(3 + 0.5^2 / 2) ## [1] 22.7599 This is much smaller and more reasonable. In case you were curious, here are the same priors, this time on the scale of \\(\\lambda\\). tibble(x = c(10, 4), y = c(0.05, 0.5), mean = c(0, 3), sd = c(10, 0.5)) %&gt;% expand(nesting(x, y, mean, sd), number = seq(from = -25, to = 25, length.out = 500)) %&gt;% mutate(density = dnorm(number, mean, sd), group = str_c(&quot;alpha%~%Normal(&quot;, mean, &quot;, &quot;, sd, &quot;)&quot;)) %&gt;% ggplot(aes(fill = group, color = group)) + geom_area(aes(x = number, y = density), alpha = 3/4, size = 0, position = &quot;identity&quot;) + geom_text(data = . %&gt;% group_by(group) %&gt;% slice(1), aes(x = x, y = y, label = group), family = &quot;Times&quot;, parse = T, hjust = 0) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[1:2]) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[1:2]) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(lambda~scale)) + theme(legend.position = &quot;none&quot;) Now let’s prepare to make the top row of Figure 11.8. In this portion of the figure, we consider the implications of two competing priors for \\(\\beta\\) while holding the prior for \\(\\alpha\\) at \\(\\operatorname{Normal}(3, 0.5)\\). The two \\(\\beta\\) priors under consideration are \\(\\operatorname{Normal}(0, 10)\\) and \\(\\operatorname{Normal}(0, 0.2)\\). set.seed(11) # how many lines would you like? n &lt;- 100 # simulate and wrangle tibble(i = 1:n, a = rnorm(n, mean = 3, sd = 0.5)) %&gt;% mutate(`beta%~%Normal(0*&#39;, &#39;*10)` = rnorm(n, mean = 0 , sd = 10), `beta%~%Normal(0*&#39;, &#39;*0.2)` = rnorm(n, mean = 0 , sd = 0.2)) %&gt;% pivot_longer(contains(&quot;beta&quot;), values_to = &quot;b&quot;, names_to = &quot;prior&quot;) %&gt;% expand(nesting(i, a, b, prior), x = seq(from = -2, to = 2, length.out = 100)) %&gt;% # plot ggplot(aes(x = x, y = exp(a + b * x), group = i)) + geom_line(size = 1/4, alpha = 2/3, color = wes_palette(&quot;Moonrise2&quot;)[4]) + labs(x = &quot;log population (std)&quot;, y = &quot;total tools&quot;) + coord_cartesian(ylim = c(0, 100)) + facet_wrap(~ prior, labeller = label_parsed) It turns out that many of the lines considered plausible under \\(\\operatorname{Normal}(0, 10)\\) are disturbingly extreme. Here is what \\(\\alpha \\sim \\operatorname{Normal}(3, 0.5)\\) and \\(\\beta \\sim \\operatorname{Normal}(0, 0.2)\\) would mean when the \\(x\\)-axis is on the log population scale and the population scale. set.seed(11) prior &lt;- tibble(i = 1:n, a = rnorm(n, mean = 3, sd = 0.5), b = rnorm(n, mean = 0, sd = 0.2)) %&gt;% expand(nesting(i, a, b), x = seq(from = log(100), to = log(200000), length.out = 100)) # left p1 &lt;- prior %&gt;% ggplot(aes(x = x, y = exp(a + b * x), group = i)) + geom_line(size = 1/4, alpha = 2/3, color = wes_palette(&quot;Moonrise2&quot;)[4]) + labs(subtitle = expression(beta%~%Normal(0*&#39;, &#39;*0.2)), x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = c(log(100), log(200000)), ylim = c(0, 500)) # right p2 &lt;- prior %&gt;% ggplot(aes(x = exp(x), y = exp(a + b * x), group = i)) + geom_line(size = 1/4, alpha = 2/3, color = wes_palette(&quot;Moonrise2&quot;)[4]) + labs(subtitle = expression(beta%~%Normal(0*&#39;, &#39;*0.2)), x = &quot;population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = c(100, 200000), ylim = c(0, 500)) # combine p1 | p2 Okay, after settling on our two priors, the updated model formula is \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Poisson}(\\lambda_i) \\\\ \\log(\\lambda_i) &amp; = \\alpha + \\beta (x_i - \\bar x) \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(3, 0.5) \\\\ \\beta &amp; \\sim \\operatorname{Normal}(0, 0.2). \\end{align*}\\] We’re finally ready to fit the model. The only new thing in our model code is family = poisson. In this case, brms defaults to the log() link. We’ll fit both an intercept-only Poisson model and an interaction model. # intercept only b11.9 &lt;- brm(data = d, family = poisson, total_tools ~ 1, prior(normal(3, 0.5), class = Intercept), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, file = &quot;fits/b11.09&quot;) # interaction model b11.10 &lt;- brm(data = d, family = poisson, bf(total_tools ~ a + b * log_pop_std, a + b ~ 0 + cid, nl = TRUE), prior = c(prior(normal(3, 0.5), nlpar = a), prior(normal(0, 0.2), nlpar = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, file = &quot;fits/b11.10&quot;) Check the model summaries. print(b11.9) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ 1 ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.54 0.05 3.44 3.65 1.00 1815 1498 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b11.10) ## Family: poisson ## Links: mu = log ## Formula: total_tools ~ a + b * log_pop_std ## a ~ 0 + cid ## b ~ 0 + cid ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_cidhigh 3.61 0.07 3.47 3.75 1.00 3941 2746 ## a_cidlow 3.32 0.09 3.14 3.48 1.00 3300 2786 ## b_cidhigh 0.19 0.16 -0.13 0.50 1.00 3907 2481 ## b_cidlow 0.38 0.05 0.27 0.48 1.00 3051 2756 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now compute the LOO estimates and compare the models by the LOO. b11.9 &lt;- add_criterion(b11.9, &quot;loo&quot;) b11.10 &lt;- add_criterion(b11.10, &quot;loo&quot;) loo_compare(b11.9, b11.10, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.10 0.0 0.0 -42.7 6.6 7.0 2.6 85.4 13.2 ## b11.9 -27.8 16.3 -70.5 16.6 8.0 3.5 141.0 33.3 Here’s the LOO weight. model_weights(b11.9, b11.10, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## b11.9 b11.10 ## 0 1 McElreath reported getting a warning from his rethinking::compare(). Our warning came from the add_criterion() function. We can inspect the Pareto \\(k\\) values with loo::pareto_k_table(). loo(b11.10) %&gt;% loo::pareto_k_table() ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 6 60.0% 1225 ## (0.5, 0.7] (ok) 2 20.0% 233 ## (0.7, 1] (bad) 2 20.0% 40 ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; Let’s take a closer look. tibble(culture = d$culture, k = b11.10$criteria$loo$diagnostics$pareto_k) %&gt;% arrange(desc(k)) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 10 x 2 ## culture k ## &lt;fct&gt; &lt;dbl&gt; ## 1 Hawaii 0.96 ## 2 Tonga 0.77 ## 3 Yap 0.6 ## 4 Trobriand 0.56 ## 5 Tikopia 0.39 ## 6 Malekula 0.37 ## 7 Santa Cruz 0.32 ## 8 Lau Fiji 0.31 ## 9 Chuuk 0.09 ## 10 Manus 0.08 It turns out Hawaii is very influential. Figure 11.9 will clarify why. Here we make the left panel. cultures &lt;- c(&quot;Hawaii&quot;, &quot;Tonga&quot;, &quot;Trobriand&quot;, &quot;Yap&quot;) library(ggrepel) nd &lt;- distinct(d, cid) %&gt;% expand(cid, log_pop_std = seq(from = -4.5, to = 2.5, length.out = 100)) f &lt;- fitted(b11.10, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) p1 &lt;- f %&gt;% ggplot(aes(x = log_pop_std, group = cid, color = cid)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(d, b11.10$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k), alpha = 4/5) + geom_text_repel(data = bind_cols(d, b11.10$criteria$loo$diagnostics) %&gt;% filter(culture %in% cultures) %&gt;% mutate(label = str_c(culture, &quot; (&quot;, round(pareto_k, digits = 2), &quot;)&quot;)), aes(y = total_tools, label = label), size = 3, seed = 11, color = &quot;black&quot;, family = &quot;Times&quot;) + labs(x = &quot;log population (std)&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = range(b11.10$data$log_pop_std), ylim = c(0, 80)) Now make the right panel of Figure 11.9. p2 &lt;- f %&gt;% mutate(population = exp((log_pop_std * sd(log(d$population))) + mean(log(d$population)))) %&gt;% ggplot(aes(x = population, group = cid, color = cid)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(d, b11.10$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k), alpha = 4/5) + scale_x_continuous(&quot;population&quot;, breaks = c(0, 50000, 150000, 250000)) + ylab(&quot;total tools&quot;) + coord_cartesian(xlim = range(d$population), ylim = c(0, 80)) Combine the two subplots with patchwork and adjust the settings a little. (p1 | p2) &amp; scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[1:2]) &amp; scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[1:2]) &amp; scale_size(range = c(2, 5)) &amp; theme(legend.position = &quot;none&quot;) Hawaii is influential in that it has a very large population relative to the other islands. 11.2.1.1 Overthinking: Modeling tool innovation. McElreath’s theoretical, or scientific, model for total_tools is \\[\\widehat{\\text{total_tools}} = \\frac{\\alpha_{\\text{cid}[i]} \\: \\text{population}^{\\beta_{\\text{cid}[i]}}}{\\gamma}.\\] We can use the Poisson likelihood to express this in a Bayesian model as \\[\\begin{align*} \\text{total_tools} &amp; \\sim \\operatorname{Poisson}(\\lambda_i) \\\\ \\lambda_i &amp; = \\left[ \\exp (\\alpha_{\\text{cid}[i]}) \\text{population}_i^{\\beta_{\\text{cid}[i]}} \\right] / \\gamma \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal}(1, 1) \\\\ \\beta_j &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\gamma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where we exponentiate \\(\\alpha_{\\text{cid}[i]}\\) to restrict the posterior to zero and above. Here’s how we might fit that model with brms. b11.11 &lt;- brm(data = d, family = poisson(link = &quot;identity&quot;), bf(total_tools ~ exp(a) * population^b / g, a + b ~ 0 + cid, g ~ 1, nl = TRUE), prior = c(prior(normal(1, 1), nlpar = a), prior(exponential(1), nlpar = b, lb = 0), prior(exponential(1), nlpar = g, lb = 0)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, control = list(adapt_delta = .95), file = &quot;fits/b11.11&quot;) Did you notice the family = poisson(link = \"identity\") part of the code? Yes, it’s possible to use the Poisson likelihood without the log link. However, if you’re going to buck tradition and use some other link, make sure you know what you’re doing. Check the model summary. print(b11.11) ## Family: poisson ## Links: mu = identity ## Formula: total_tools ~ exp(a) * population^b/g ## a ~ 0 + cid ## b ~ 0 + cid ## g ~ 1 ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_cidhigh 0.95 0.85 -0.69 2.64 1.00 1565 1367 ## a_cidlow 0.87 0.70 -0.59 2.19 1.00 1488 1419 ## b_cidhigh 0.28 0.10 0.08 0.49 1.00 1376 998 ## b_cidlow 0.26 0.03 0.19 0.33 1.00 2137 1753 ## g_Intercept 1.13 0.78 0.20 3.16 1.00 1395 1373 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Compute and check the PSIS-LOO estimates along with their diagnostic Pareto \\(k\\) values. b11.11 &lt;- add_criterion(b11.11, criterion = &quot;loo&quot;, moment_match = T) loo(b11.11) ## ## Computed from 4000 by 10 log-likelihood matrix ## ## Estimate SE ## elpd_loo -40.6 5.9 ## p_loo 5.1 1.9 ## looic 81.3 11.9 ## ------ ## Monte Carlo SE of elpd_loo is 0.1. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 8 80.0% 249 ## (0.5, 0.7] (ok) 2 20.0% 219 ## (0.7, 1] (bad) 0 0.0% &lt;NA&gt; ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## ## All Pareto k estimates are ok (k &lt; 0.7). ## See help(&#39;pareto-k-diagnostic&#39;) for details. The first time through, we still had Pareto high \\(k\\) values. Recall that due to the very small sample size, this isn’t entirely surprising. Newer versions of brms might prompt you to set moment_match = TRUE, which is what I did, here. You might perform the operation both ways to get a sense of the difference. Okay, it’s time to make Figure 11.10. # for the annotation text &lt;- distinct(d, cid) %&gt;% mutate(population = c(210000, 72500), total_tools = c(59, 68), label = str_c(cid, &quot; contact&quot;)) # redifine the new data nd &lt;- distinct(d, cid) %&gt;% expand(cid, population = seq(from = 0, to = 300000, length.out = 100)) # compute the poster predictions for lambda fitted(b11.11, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # plot! ggplot(aes(x = population, group = cid, color = cid)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(d, b11.11$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k), alpha = 4/5) + geom_text(data = text, aes(y = total_tools, label = label), family = &quot;serif&quot;) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[1:2]) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[1:2]) + scale_size(range = c(2, 5)) + scale_x_continuous(&quot;population&quot;, breaks = c(0, 50000, 150000, 250000)) + ylab(&quot;total tools&quot;) + coord_cartesian(xlim = range(d$population), ylim = range(d$total_tools)) + theme(legend.position = &quot;none&quot;) In case you were curious, here are the results if we compare b11.10 and b11.11 by the PSIS-LOO. loo_compare(b11.10, b11.11, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.11 0.0 0.0 -40.6 5.9 5.1 1.9 81.3 11.9 ## b11.10 -2.1 2.7 -42.7 6.6 7.0 2.6 85.4 13.2 model_weights(b11.10, b11.11, weights = &quot;loo&quot;) %&gt;% round(digits = 3) ## b11.10 b11.11 ## 0.111 0.889 Finally, here’s a comparison of the two models by the Pareto \\(k\\) values. tibble(b11.10 = b11.10$criteria$loo$diagnostics$pareto_k, b11.11 = b11.11$criteria$loo$diagnostics$pareto_k) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = c(.5, .7, 1), linetype = 3, color = wes_palette(&quot;Moonrise2&quot;)[2]) + stat_dots(slab_fill = wes_palette(&quot;Moonrise2&quot;)[1], slab_color = wes_palette(&quot;Moonrise2&quot;)[1]) + scale_x_continuous(expression(Pareto~italic(k)), breaks = c(.5, .7, 1)) + ylab(NULL) + coord_cartesian(ylim = c(1.5, 2.4)) 11.2.2 Negative binomial (gamma-Poisson) models. Typically there is a lot of unexplained variation in Poisson models. Presumably this additional variation arises from unobserved influences that vary from case to case, generating variation in the true \\(\\lambda\\)’s. Ignoring this variation, or rate heterogeneity, can cause confounds just like it can for binomial models. So a very common extension of Poisson GLMs is to swap the Poisson distribution for something called the negative binomial distribution. This is really a Poisson distribution in disguise, and it is also sometimes called the gamma-Poisson distribution for this reason. It is a Poisson in disguise, because it is a mixture of different Poisson distributions. This is the Poisson analogue of the Student-t model, which is a mixture of different normal distributions. We’ll work with mixtures in the next chapter. (p. 357, emphasis in the original) 11.2.3 Example: Exposure and the offset. For the last Poisson example, we’ll look at a case where the exposure varies across observations. When the length of observation, area of sampling, or intensity of sampling varies, the counts we observe also naturally vary. Since a Poisson distribution assumes that the rate of events is constant in time (or space), it’s easy to handle this. All we need to do, as explained above, is to add the logarithm of the exposure to the linear model. The term we add is typically called an offset. (p. 357, emphasis in the original) Here we simulate our data. set.seed(11) num_days &lt;- 30 y &lt;- rpois(num_days, lambda = 1.5) num_weeks &lt;- 4 y_new &lt;- rpois(num_weeks, lambda = 0.5 * 7) Now tidy the data and add log_days. ( d &lt;- tibble(y = c(y, y_new), days = rep(c(1, 7), times = c(num_days, num_weeks)), # this is the exposure monastery = rep(0:1, times = c(num_days, num_weeks))) %&gt;% mutate(log_days = log(days)) ) ## # A tibble: 34 x 4 ## y days monastery log_days ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 0 0 ## 2 0 1 0 0 ## 3 1 1 0 0 ## 4 0 1 0 0 ## 5 0 1 0 0 ## 6 4 1 0 0 ## 7 0 1 0 0 ## 8 1 1 0 0 ## 9 3 1 0 0 ## 10 0 1 0 0 ## # … with 24 more rows Within the context of the Poisson likelihood, we can decompose \\(\\lambda\\) into two parts, \\(\\mu\\) (mean) and \\(\\tau\\) (exposure), like this: \\[ y_i \\sim \\operatorname{Poisson}(\\lambda_i) \\\\ \\log \\lambda_i = \\log \\frac{\\mu_i}{\\tau_i} = \\log \\mu_i - \\log \\tau_i. \\] Therefore, you can rewrite the equation if the exposure (\\(\\tau\\)) varies in your data and you still want to model the mean (\\(\\mu\\)). Using the model we’re about to fit as an example, here’s what that might look like: \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Poisson}(\\mu_i) \\\\ \\log \\mu_i &amp; = \\color{#a4692f}{\\log \\tau_i} + \\alpha + \\beta \\text{monastery}_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\beta &amp; \\sim \\operatorname{Normal}(0, 1), \\end{align*}\\] where the offset \\(\\log \\tau_i\\) does not get a prior. In this context, its value is added directly to the right side of the formula. With the brms package, you use the offset() function in the formula syntax. You just insert a pre-processed variable like log_days or the log of a variable, such as log(days). Fit the model. b11.12 &lt;- brm(data = d, family = poisson, y ~ 1 + offset(log_days) + monastery, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 1), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.12&quot;) As we look at the model summary, keep in mind that the parameters are on the per-one-unit-of-time scale. Since we simulated the data based on summary information from two units of time–one day and seven days–, this means the parameters are in the scale of \\(\\log (\\lambda)\\) per one day. print(b11.12) ## Family: poisson ## Links: mu = log ## Formula: y ~ 1 + offset(log_days) + monastery ## Data: d (Number of observations: 34) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.01 0.18 -0.38 0.33 1.00 2350 1804 ## monastery -0.89 0.33 -1.56 -0.26 1.00 2443 2306 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The model summary helps clarify that when you use offset(), brm() fixes the value. Thus there is no parameter estimate for the offset(). It’s a fixed part of the model not unlike the \\(\\nu\\) parameter of the Student-\\(t\\) distribution gets fixed to infinity when you use the Gaussian likelihood. To get the posterior distributions for average daily outputs for the old and new monasteries, respectively, we’ll use use the formulas \\[\\begin{align*} \\lambda_\\text{old} &amp; = \\exp (\\alpha) \\;\\;\\; \\text{and} \\\\ \\lambda_\\text{new} &amp; = \\exp (\\alpha + \\beta_\\text{monastery}). \\end{align*}\\] Following those transformations, we’ll summarize the \\(\\lambda\\) distributions with medians and 89% HDIs with help from the tidybayes::mean_hdi() function. posterior_samples(b11.12) %&gt;% mutate(lambda_old = exp(b_Intercept), lambda_new = exp(b_Intercept + b_monastery)) %&gt;% pivot_longer(contains(&quot;lambda&quot;)) %&gt;% mutate(name = factor(name, levels = c(&quot;lambda_old&quot;, &quot;lambda_new&quot;))) %&gt;% group_by(name) %&gt;% mean_hdi(value, .width = .89) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 2 x 7 ## name value .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 lambda_old 1 0.69 1.25 0.89 mean hdi ## 2 lambda_new 0.42 0.23 0.6 0.89 mean hdi Because we don’t know what seed McElreath used to simulate his data, our simulated data differed a little from his and, as a consequence, our results differ a little, too. 11.3 Multinomial and categorical models When more than two types of unordered events are possible, and the probability of each type of event is constant across trials, then the maximum entropy distribution is the multinomial distribution. [We] already met the multinomial, implicitly, in Chapter 10 when we tossed pebbles into buckets as an introduction to maximum entropy. The binomial is really a special case of this distribution. And so its distribution formula resembles the binomial, just extrapolated out to three or more types of events. If there are \\(K\\) types of events with probabilities \\(p_1, \\dots, p_K\\), then the probability of observing \\(y_1, \\dots, y_K\\) events of each type out of n total trials is: \\[\\operatorname{Pr} (y_1, \\dots, y_K | n, p_1, \\dots, p_K) = \\frac{n!}{\\prod_i y_i!} \\prod_{i = 1}^K p_i^{y_i}\\] The fraction with \\(n!\\) on top just expresses the number of different orderings that give the same counts \\(y_1, \\dots, y_K\\). It’s the famous multiplicity from the previous chapter…. The conventional and natural link in this context is the multinomial logit, also known as the softmax function. This link function takes a vector of scores, one for each of \\(K\\) event types, and computes the probability of a particular type of event \\(k\\) as \\[\\text{Pr} (k |s_1, s_2, \\dots, s_K) = \\frac{\\exp (s_k)}{\\sum_{i = 1}^K \\exp (s_i)}\\] (p. 359, emphasis in the original) McElreath then went on to explain how multinomial logistic regression models are among the more difficult of the GLMs to master. He wasn’t kidding. To get a grasp on these, we’ll cover them in a little more detail than he did in the text. Before we begin, I’d like to give a big shout out to Adam Bear, whose initial comment on a GitHub issue turned into a friendly and productive email collaboration on what, exactly, is going on with this section. Hopefully we got it. 11.3.1 Predictors matched to outcomes. To begin, let’s simulate the data just like McElreath did in the R code 11.55 block. library(rethinking) # simulate career choices among 500 individuals n &lt;- 500 # number of individuals income &lt;- c(1, 2, 5) # expected income of each career score &lt;- 0.5 * income # scores for each career, based on income # next line converts scores to probabilities p &lt;- softmax(score[1], score[2], score[3]) # now simulate choice # outcome career holds event type values, not counts career &lt;- rep(NA, n) # empty vector of choices for each individual # sample chosen career for each individual set.seed(34302) # sample chosen career for each individual for(i in 1:n) career[i] &lt;- sample(1:3, size = 1, prob = p) Before moving on, it might be useful to examine what we just did. With the three lines below the “# simulate career choices among 500 individuals” comment, we defined the formulas for three scores. Those were \\[\\begin{align*} s_1 &amp; = 0.5 \\times \\text{income}_1 \\\\ s_2 &amp; = 0.5 \\times \\text{income}_2 \\\\ s_3 &amp; = 0.5 \\times \\text{income}_3, \\end{align*}\\] where \\(\\text{income}_1 = 1\\), \\(\\text{income}_2 = 2\\), and \\(\\text{income}_3 = 5\\). What’s a little odd about this setup and conceptually important to get is that although \\(\\text{income}_i\\) varies across the three levels of \\(s\\), the \\(\\text{income}_i\\) value is constant within each level of \\(s\\). E.g., \\(\\text{income}_1\\) is not a variable within the context of \\(s_1\\). Therefore, we could also write the above as \\[\\begin{align*} s_1 &amp; = 0.5 \\cdot 1 = 0.5 \\\\ s_2 &amp; = 0.5 \\cdot 2 = 1.0 \\\\ s_3 &amp; = 0.5 \\cdot 5 = 2.5. \\end{align*}\\] Let’s confirm. print(score) ## [1] 0.5 1.0 2.5 We then converted those score values to probabilities with the softmax() function. This will become important when we set up the model code. For now, here’s what the data look like. # put them in a tibble d &lt;- tibble(career = career) %&gt;% mutate(career_income = ifelse(career == 3, 5, career)) # plot d %&gt;% ggplot(aes(x = career)) + geom_bar(size = 0, fill = wes_palette(&quot;Moonrise2&quot;)[2]) Our career variable is composed of three categories, 1:3, with each category more likely than the one before. Here’s a breakdown of the counts, percentages, and probabilities of each category. d %&gt;% count(career) %&gt;% mutate(percent = (100 * n / sum(n)), probability = n / sum(n)) ## # A tibble: 3 x 4 ## career n percent probability ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 48 9.6 0.096 ## 2 2 79 15.8 0.158 ## 3 3 373 74.6 0.746 To further build an appreciation for how we simulated data with these proportions and how the process links in with the formulas, above, we’ll retrace the first few simulation steps within a tidyverse-centric workflow. Recall how in those first few steps we defined values for income, score, and p. Here they are again in a tibble. tibble(income = c(1, 2, 5)) %&gt;% mutate(score = 0.5 * income) %&gt;% mutate(p = exp(score) / sum(exp(score))) ## # A tibble: 3 x 3 ## income score p ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.5 0.0996 ## 2 2 1 0.164 ## 3 5 2.5 0.736 Notice how the values in the p column match up well with the probability values from the output from the block just above. Our simulation successfully produces data corresponding to the data-generating values. Woot! Also note how the code we just used to compute those p values, p = exp(score) / sum(exp(score)), corresponds nicely with the formula from above, \\[\\text{Pr} (k |s_1, s_2, \\dots, s_K) = \\frac{\\exp (s_k)}{\\sum_{i = 1}^K \\exp (s_i)}.\\] What still might seem mysterious is what those \\(s\\) values in the equation are. In the simulation and in the prose, McElreath called them scores. Another way to think about them is as weights. The thing to get is that their exact values aren’t important so much as their difference one from another. You’ll note that score for income == 2 was 0.5 larger than that of income == 1. The same was true for income == 3 and income == 2. So if we add an arbitrary constant to each of those score values, like 11, we’ll get the same p values. tibble(income = c(1, 2, 5), some_constant = 11) %&gt;% mutate(score = (0.5 * income) + some_constant) %&gt;% mutate(p = exp(score) / sum(exp(score))) ## # A tibble: 3 x 4 ## income some_constant score p ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 11 11.5 0.0996 ## 2 2 11 12 0.164 ## 3 5 11 13.5 0.736 Now keeping that in mind, recall how McElreath said that though we have \\(K\\) categories, \\(K = 3\\) in this case, we only estimate \\(K - 1\\) linear models. “In a multinomial (or categorical) GLM, you need \\(K - 1\\) linear models for \\(K\\) types of events. One of the outcome values is chosen as a ‘pivot’ and the others are modeled relative to it.” (p. 360). You could also think of the pivot category as the reference category. Before we practice fitting multinomial models with brms, it’ll be helpful if we first follow along with the text and fit the model directly in Stan. We will be working directly with Stan very infrequently in this ebook. If you’re interested in learning more about modeling directly with Stan, you might check out the Stan user’s guide (Stan Development Team, 2021c), the Stan reference manual (Stan Development Team, 2021b), and the Stan functions reference (Stan Development Team, 2021a). Fit the model with Stan. # define the model code_m11.13 &lt;- &quot; data{ int N; // number of individuals int K; // number of possible careers int career[N]; // outcome vector[K] career_income; } parameters{ vector[K - 1] a; // intercepts real&lt;lower=0&gt; b; // association of income with choice } model{ vector[K] p; vector[K] s; a ~ normal(0, 1); b ~ normal(0, 0.5); s[1] = a[1] + b * career_income[1]; s[2] = a[2] + b * career_income[2]; s[3] = 0; // pivot p = softmax(s); career ~ categorical(p); } &quot; # wrangle the data dat_list &lt;- list(N = n, K = 3, career = career, career_income = income) # fit the model m11.13 &lt;- stan(data = dat_list, model_code = code_m11.13, chains = 4) Check the summary. precis(m11.13, depth = 2) %&gt;% round(digits = 2) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a[1] -2.14 0.18 -2.44 -1.87 647.49 1 ## a[2] -1.79 0.25 -2.25 -1.45 501.46 1 ## b 0.13 0.11 0.01 0.36 497.10 1 One of the primary reasons we went through this exercise is to show that McElreath’s R code 11.56 and 11.57 do not return the results he reported on page 361. The plot thickens when we attempt the counterfactual simulation on page 362, as reported in R code 11.58. post &lt;- extract.samples(m11.13) # set up logit scores s1 &lt;- with(post, a[, 1] + b * income[1]) s2_orig &lt;- with(post, a[, 2] + b * income[2]) s2_new &lt;- with(post, a[, 2] + b * income[2] * 2) # compute probabilities for original and counterfactual p_orig &lt;- sapply(1:length(post$b), function(i) softmax(c(s1[i], s2_orig[i], 0))) p_new &lt;- sapply(1:length(post$b), function(i) softmax(c(s1[i], s2_new[i], 0))) # summarize p_diff &lt;- p_new[2, ] - p_orig[2, ] precis(p_diff) ## mean sd 5.5% 94.5% histogram ## p_diff 0.0419429 0.03941889 0.002461482 0.1214324 ▇▃▂▂▁▁▁▁▁▁▁ Even though we used the same code, our counterfactual simulation doesn’t match up with the results McElreath reported in the text, either. Keep this all in mind as we switch to brms. But before we move on to brms, check this out. data.frame(s1 = score[3] + s1, s2 = score[3] + s2_orig, s3 = score[3] + 0) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% mean_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 3 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 s1 0.49 0.19 0.77 0.95 mean qi ## 2 s2 0.98 0.73 1.21 0.95 mean qi ## 3 s3 2.5 2.5 2.5 0.95 mean qi In his Stan code (R code 11.56), you’ll see McElreath chose the third category to be his pivot and that he used zero as a constant value. As it turns out, it is common practice to set the score value for the reference category to zero. It’s also a common practice to use the first event type as the reference category. Importantly, in his (2021h) vignette, Parameterization of response distributions in brms, Bürkner clarified the brms default is to use the first response category as the reference and set it to a zero as well. However, we can control this behavior with the refcat argument. In the examples to follow, we’ll follow McElreath and use the third event type as the reference category by setting refcat = 3. In addition to the discrepancies with the code and results in the text, one of the things I don’t care for in this section is how fast McElreath covered the material. Our approach will be to slow down a little and start off by fitting a intercepts-only model before adding the covariate. Before we fit the model, we might take a quick look at the prior structure with brms::get_prior(). get_prior(data = d, family = categorical(link = logit, refcat = 3), career ~ 1) ## prior class coef group resp dpar nlpar bound source ## (flat) Intercept default ## student_t(3, 3, 2.5) Intercept mu1 default ## student_t(3, 3, 2.5) Intercept mu2 default We have two “intercepts,” which are differentiated in the dpar column. We’ll talk more about what these are in just a bit; don’t worry. I show this here because as of brms 2.12.0, “specifying global priors for regression coefficients in categorical models is deprecated.” The upshot is even if we want to use the same prior for both, we need to use the dpar argument for each. With that in mind, here’s our multinomial model in brms. Do note the specification family = categorical(link = logit, refcat = 3). The categorical part is what instructs brms to use the multinomial likelihood and the refcat = 3 part will allow us to use the third event type as the pivot. b11.13io &lt;- brm(data = d, family = categorical(link = logit, refcat = 3), career ~ 1, prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1), prior(normal(0, 1), class = Intercept, dpar = mu2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.13io&quot;) The summary can be difficult to interpret. print(b11.13io) ## Family: categorical ## Links: mu1 = logit; mu2 = logit ## Formula: career ~ 1 ## Data: d (Number of observations: 500) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## mu1_Intercept -2.01 0.15 -2.30 -1.73 1.00 3324 2631 ## mu2_Intercept -1.53 0.12 -1.77 -1.29 1.00 2993 2768 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). brms::brm() referred to the \\(K\\) categories as mu1, mu2, and mu3. Since career == 3 is the reference category, the score for which was set to zero, there is no parameter for mu3_Intercept. That’s a zero. Now notice how mu1_Intercept is about -2 and mu2_Intercept is about -1.5. If we double back to the income and score values we played with at the beginning of this section, you’ll notice that the score for the reference category was 2.5. Here’s what happens if we rescale the three scores such that the score value for the reference category is 0. tibble(income = c(1, 2, 5)) %&gt;% mutate(score = 0.5 * income) %&gt;% mutate(rescaled_score = score - 2.5) ## # A tibble: 3 x 3 ## income score rescaled_score ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.5 -2 ## 2 2 1 -1.5 ## 3 5 2.5 0 Now notice how the rescaled_score values for the first two rows correspond nicely to mu1_Intercept and mu2_Intercept from our model. What I hope this clarifies is that our statistical model returned the scores. But recall these are not quite probabilities. Why? Because the weights are all relative to one another. The easiest way to get what we want, the probabilities for the three categories, is with brms::fitted(). Since this model has no predictors, only intercepts, we won’t specify any newdata. In such a case, fitted() will return fitted values for each case in the data. Going slow, let’s take a look at the structure of the output. fitted(b11.13io) %&gt;% str() ## num [1:500, 1:4, 1:3] 0.0999 0.0999 0.0999 0.0999 0.0999 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## ..$ : chr [1:3] &quot;P(Y = 1)&quot; &quot;P(Y = 2)&quot; &quot;P(Y = 3)&quot; Just as expected, we have 500 rows–one for each case in the original data. We have four summary columns, the typical Estimate, Est.Error, Q2.5, and Q97.5. We also have third dimension composed of three levels, P(Y = 1), P(Y = 2), and P(Y = 3). Those index which of the three career categories each probability summary is for. Since the results are identical for each row, we’ll simplify the output by only keeping the first row. fitted(b11.13io)[1, , ] %&gt;% round(digits = 2) ## P(Y = 1) P(Y = 2) P(Y = 3) ## Estimate 0.10 0.16 0.74 ## Est.Error 0.01 0.02 0.02 ## Q2.5 0.08 0.13 0.70 ## Q97.5 0.13 0.19 0.77 If we take the transpose of that, it will put the results in the format we’re more accustomed to. fitted(b11.13io)[1, , ] %&gt;% round(digits = 2) %&gt;% t() ## Estimate Est.Error Q2.5 Q97.5 ## P(Y = 1) 0.10 0.01 0.08 0.13 ## P(Y = 2) 0.16 0.02 0.13 0.19 ## P(Y = 3) 0.74 0.02 0.70 0.77 Now compare those summaries with the empirically-derived percent and probability values we computed earlier. tibble(income = c(1, 2, 5)) %&gt;% mutate(score = 0.5 * income) %&gt;% mutate(p = exp(score) / sum(exp(score))) ## # A tibble: 3 x 3 ## income score p ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.5 0.0996 ## 2 2 1 0.164 ## 3 5 2.5 0.736 Now here’s how to make use of the formula from the last mutate() line, \\(\\frac{\\exp (s_k)}{\\sum_{i = 1}^K \\exp (s_i)}\\), to compute the marginal probabilities from b11.13io by hand. posterior_samples(b11.13io) %&gt;% mutate(b_mu3_Intercept = 0) %&gt;% mutate(p1 = exp(b_mu1_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)), p2 = exp(b_mu2_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)), p3 = exp(b_mu3_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept))) %&gt;% pivot_longer(p1:p3) %&gt;% group_by(name) %&gt;% mean_qi(value) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 3 x 7 ## name value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 p1 0.1 0.08 0.13 0.95 mean qi ## 2 p2 0.16 0.13 0.19 0.95 mean qi ## 3 p3 0.74 0.7 0.77 0.95 mean qi Hurray; we did it! Not only did we fit a simple multinomial model with brms, we actually made sense of the parameters by connecting them to the original data-generating values. We’re almost ready to contend with the model McElreath fit with stan(). But before we do, it’ll be helpful to show alternative ways to fit these models. We used conventional style syntax when we fit b11.13io. There are at least two alternative ways to fit the model: # verbose syntax b11.13io_verbose &lt;- brm(data = d, family = categorical(link = logit, refcat = 3), bf(career ~ 1, mu1 ~ 1, mu2 ~ 1), prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1), prior(normal(0, 1), class = Intercept, dpar = mu2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.13io_verbose&quot;) # nonlinear syntax b11.13io_nonlinear &lt;- brm(data = d, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1), nlf(mu2 ~ a2), a1 + a2 ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.13io_nonlinear&quot;) For the sake of space, I’m not going to show the results for those two models. If you fit them yourself, you’ll see the results for b11.13io and b11.13io_verbose are exactly the same and b11.13io_nonlinear differs from them only within simulation variation. I point this out because it’s the nonlinear approach that will allow us to fit a model like McElreath’s m11.13. My hope is the syntax we used in the b11.13io_verbose model will help clarify what’s going on with the non-linear syntax. When we fit multinomial models with brms, the terse conventional formula syntax might not make clear how there are actually \\(K - 1\\) formulas. The more verbose syntax of our b11.13io_verbose model shows how we can specify those models directly. In our case, that was with those mu1 ~ 1, mu2 ~ 1 lines. Had we used the brms default and used the first level of career as the pivot, those lines would have instead been mu2 ~ 1, mu3 ~ 1. So anyway, when we switch to the non-linear syntax, we explicitly model mu1 and mu2 and, as is typical of the non-linear syntax, we name our parameters. You can see another comparison of these three ways of fitting a multinomial model at the Nonlinear syntax with a multinomial model? thread on the Stan Forums. Now it’s time to focus on the brms version of McElreath’s m11.13. To my eye, McElreath’s model has two odd features. First, though he has two intercepts, he only has one \\(\\beta\\) parameter. Second, if you look at McElreath’s parameters block, you’ll see that he restricted his \\(\\beta\\) parameter to be zero and above (real&lt;lower=0&gt; b;). With the brms non-linear syntax, we can fit the model with one \\(\\beta\\) parameter or allow the one \\(\\beta\\) parameter to differ for mu1 and mu2. As to setting a lower bound to the b parameter[s], we can do that with the lb argument within the prior() function. If we fit our version of m11.13 by systemically varying these two features, we’ll end up with the four versions listed in the table below. crossing(b = factor(c(&quot;b1 &amp; b2&quot;, &quot;b&quot;), levels = c(&quot;b1 &amp; b2&quot;, &quot;b&quot;)), lb = factor(c(&quot;NA&quot;, 0), levels = c(&quot;NA&quot;, 0))) %&gt;% mutate(fit = str_c(&quot;b11.13&quot;, letters[1:n()])) %&gt;% select(fit, everything()) %&gt;% flextable() %&gt;% width(width = 1.25) .tabwid table{ border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-spacing: 0; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-e952b62c{border-collapse:collapse;}.cl-e94d76da{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-e94d83d2{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-e94da7a4{width:90pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e94da7c2{width:90pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-e94da7c3{width:90pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}fitblbb11.13ab1 &amp; b2NAb11.13bb1 &amp; b20b11.13cbNAb11.13db0 Fit b11.13a through b11.13d, the four variants on the model. b11.13a &lt;- brm(data = d, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b1 * 1), nlf(mu2 ~ a2 + b2 * 2), a1 + a2 + b1 + b2 ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2), prior(normal(0, 0.5), class = b, nlpar = b1), prior(normal(0, 0.5), class = b, nlpar = b2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.13a&quot;) b11.13b &lt;- brm(data = d, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b1 * 1), nlf(mu2 ~ a2 + b2 * 2), a1 + a2 + b1 + b2 ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2), prior(normal(0, 0.5), class = b, nlpar = b1, lb = 0), prior(normal(0, 0.5), class = b, nlpar = b2, lb = 0)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, control = list(adapt_delta = .99), file = &quot;fits/b11.13b&quot;) b11.13c &lt;- brm(data = d, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b * 1), nlf(mu2 ~ a2 + b * 2), a1 + a2 + b ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2), prior(normal(0, 0.5), class = b, nlpar = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.13c&quot;) b11.13d &lt;- brm(data = d, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b * 1), nlf(mu2 ~ a2 + b * 2), a1 + a2 + b ~ 1), prior = c(prior(normal(0, 1), class = b, nlpar = a1), prior(normal(0, 1), class = b, nlpar = a2), prior(normal(0, 0.5), class = b, nlpar = b, lb = 0)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, control = list(adapt_delta = .99), file = &quot;fits/b11.13d&quot;) I’m not going to exhaustively show the print() output for each. If you check, you’ll see they all fit reasonably well. Here we’ll look at their parameter summaries in bulk with a coefficient plot. tibble(fit = str_c(&quot;b11.13&quot;, letters[1:4])) %&gt;% mutate(fixef = purrr::map(fit, ~get(.) %&gt;% fixef() %&gt;% data.frame() %&gt;% rownames_to_column(&quot;parameter&quot;))) %&gt;% unnest(fixef) %&gt;% mutate(parameter = str_remove(parameter, &quot;_Intercept&quot;), fit = factor(fit, levels = str_c(&quot;b11.13&quot;, letters[4:1]))) %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = fit)) + geom_vline(xintercept = 0, color = wes_palette(&quot;Moonrise2&quot;)[3]) + geom_pointrange(fatten = 3/2, color = wes_palette(&quot;Moonrise2&quot;)[4]) + ylab(NULL) + theme(axis.ticks.y = element_blank(), panel.background = element_rect(fill = alpha(&quot;white&quot;, 1/8), size = 0)) + facet_wrap(~ parameter, nrow = 1) The results differed across models. None of them match up with the results McElreath reported in the text. However, the parameters from b11.13d are very close to those from our m11.13. precis(m11.13, depth = 2) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a[1] -2.1410295 0.1774365 -2.439962491 -1.8725250 647.4891 1.003762 ## a[2] -1.7875347 0.2483372 -2.250217501 -1.4535626 501.4588 1.003349 ## b 0.1331626 0.1126707 0.009310701 0.3569797 497.0989 1.003801 fixef(b11.13d) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## a1_Intercept -2.14 0.19 -2.53 -1.80 ## a2_Intercept -1.79 0.25 -2.39 -1.40 ## b_Intercept 0.13 0.11 0.00 0.42 It might be instructive to compare b11.13a through b11.13d with the PSIS-LOO. b11.13a &lt;- add_criterion(b11.13a, &quot;loo&quot;) b11.13b &lt;- add_criterion(b11.13b, &quot;loo&quot;) b11.13c &lt;- add_criterion(b11.13c, &quot;loo&quot;) b11.13d &lt;- add_criterion(b11.13d, &quot;loo&quot;) loo_compare(b11.13a, b11.13b, b11.13c, b11.13d, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.13a 0.0 0.0 -369.5 17.0 1.9 0.1 739.0 34.1 ## b11.13d 0.0 0.2 -369.5 16.9 1.9 0.1 739.0 33.8 ## b11.13c -0.1 0.1 -369.6 17.1 2.0 0.1 739.2 34.2 ## b11.13b -0.1 0.2 -369.6 16.9 2.0 0.1 739.3 33.7 model_weights(b11.13a, b11.13b, b11.13c, b11.13d, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## b11.13a b11.13b b11.13c b11.13d ## 0.27 0.23 0.24 0.26 Two things pop out, here. First, all models are essentially equivalent in terms of LOO estimates and LOO weights. Second, the effective number of parameters (\\(p_\\text{LOO}\\)) is about 2 for each model. At first glance, this might be surprising given that b11.13a and b11.13b both have 4 parameters and b11.13c and b11.13d both have three parameters. But recall that none of these models contain predictor variables from the data. All those \\(\\beta\\) parameters, whether they’re held equal or allowed to vary across \\(s_1\\) and \\(s_2\\), are just constants. In the absence of actual income values that vary within the data, those \\(\\beta\\) parameters are kinda like extra intercepts. For context, go back and review our multicollinear legs from Section 6.1.1 or our double intercepts from Section 9.5.4. Now see what happens when we compare these four models with our intercepts-only model, b11.13io. b11.13io &lt;- add_criterion(b11.13io, &quot;loo&quot;) loo_compare(b11.13io, b11.13a, b11.13b, b11.13c, b11.13d, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b11.13a 0.0 0.0 -369.5 17.0 1.9 0.1 739.0 34.1 ## b11.13d 0.0 0.2 -369.5 16.9 1.9 0.1 739.0 33.8 ## b11.13io 0.0 0.1 -369.5 16.9 1.9 0.1 739.1 33.9 ## b11.13c -0.1 0.1 -369.6 17.1 2.0 0.1 739.2 34.2 ## b11.13b -0.1 0.2 -369.6 16.9 2.0 0.1 739.3 33.7 model_weights(b11.13io, b11.13a, b11.13b, b11.13c, b11.13d, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## b11.13io b11.13a b11.13b b11.13c b11.13d ## 0.21 0.21 0.18 0.19 0.21 They’re all the same. Each model effectively has 2 parameters. Though it doesn’t do much by way of cross-validation, McElreath’s extra \\(\\beta\\) parameter will let us perform a counterfactual simulation. Here is a brms/tidyverse workflow to make a counterfactual simulation for two levels of income based on our b11.13d, the brms model most closely corresponding to our rethinking-based m11.13. posterior_samples(b11.13d) %&gt;% transmute(s1 = b_a1_Intercept + b_b_Intercept * income[1], s2_orig = b_a2_Intercept + b_b_Intercept * income[2], s2_new = b_a2_Intercept + b_b_Intercept * income[2] * 2) %&gt;% mutate(p_orig = purrr::map2_dbl(s1, s2_orig, ~softmax(.x, .y, 0)[2]), p_new = purrr::map2_dbl(s1, s2_new, ~softmax(.x, .y, 0)[2])) %&gt;% mutate(p_diff = p_new - p_orig) %&gt;% mean_qi(p_diff) %&gt;% mutate_if(is.double, round, digits = 2) ## p_diff .lower .upper .width .point .interval ## 1 0.04 0 0.15 0.95 mean qi Now let’s build. 11.3.2 Predictors matched to observations. Now consider an example in which each observed outcome has unique predictor values. Suppose you are still modeling career choice. But now you want to estimate the association between each person’s family income and which career they choose. So the predictor variable must have the same value in each linear model, for each row in the data. But now there is a unique parameter multiplying it in each linear model. This provides an estimate of the impact of family income on choice, for each type of career. (p. 362) n &lt;- 500 set.seed(11) # simulate family incomes for each individual family_income &lt;- runif(n) # assign a unique coefficient for each type of event b &lt;- c(-2, 0, 2) career &lt;- rep(NA, n) # empty vector of choices for each individual for (i in 1:n) { score &lt;- 0.5 * (1:3) + b * family_income[i] p &lt;- softmax(score[1], score[2], score[3]) career[i] &lt;- sample(1:3, size = 1, prob = p) } In effect, we now have three data-generating equations: \\[\\begin{align*} s_1 &amp; = 0.5 + -2 \\cdot \\text{family_income}_i \\\\ s_2 &amp; = 1.0 + 0 \\cdot \\text{family_income}_i \\\\ s_3 &amp; = 1.5 + 2 \\cdot \\text{family_income}_i, \\end{align*}\\] where, because family_income is an actual variable that can take on unique values for each row in the data, we can call the first term in each equation the \\(\\alpha\\) parameter and the second term in each equation the \\(\\beta\\) parameter AND those \\(\\beta\\) parameters will be more than odd double intercepts. We might examine what the family_income distributions look like across the three levels of career. We’ll do it in two plots and combine them with the patchwork syntax. The first will be overlapping densities. For the second, we’ll display the proportions of career across a discretized version of family_income in a stacked area plot. # put the data in a tibble d &lt;- tibble(career = career) %&gt;% mutate(family_income = family_income) p1 &lt;- d %&gt;% mutate(career = as.factor(career)) %&gt;% ggplot(aes(x = family_income, fill = career)) + geom_density(size = 0, alpha = 3/4) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + theme(legend.position = &quot;none&quot;) p2 &lt;- d %&gt;% mutate(career = as.factor(career)) %&gt;% mutate(fi = santoku::chop_width(family_income, width = .1, start = 0, labels = 1:10)) %&gt;% count(fi, career) %&gt;% group_by(fi) %&gt;% mutate(proportion = n / sum(n)) %&gt;% mutate(f = as.double(fi)) %&gt;% ggplot(aes(x = (f - 1) / 9, y = proportion, fill = career)) + geom_area() + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + xlab(&quot;family_income, descritized&quot;) p1 + p2 Since Mcelreath’s simulation code in McElreath’s R code 11.59 did not contain a set.seed() line, it won’t be possible to exactly reproduce his results. Happily, though, it appears that this time the results he reported in the text to cohere reasonably well with I ran the code on my computer. They weren’t identical, but there were much closer that for m11.13 from the last section. Since things are working more smoothly, here, I’m going to jump directly to brms code. b11.14 &lt;- brm(data = d, family = categorical(link = logit, refcat = 3), bf(career ~ 1, nlf(mu1 ~ a1 + b1 * family_income), nlf(mu2 ~ a2 + b2 * family_income), a1 + a2 + b1 + b2 ~ 1), prior = c(prior(normal(0, 1.5), class = b, nlpar = a1), prior(normal(0, 1.5), class = b, nlpar = a2), prior(normal(0, 1), class = b, nlpar = b1), prior(normal(0, 1), class = b, nlpar = b2)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 11, file = &quot;fits/b11.14&quot;) print(b11.14) ## Family: categorical ## Links: mu1 = logit; mu2 = logit ## Formula: career ~ 1 ## mu1 ~ a1 + b1 * family_income ## mu2 ~ a2 + b2 * family_income ## a1 ~ 1 ## a2 ~ 1 ## b1 ~ 1 ## b2 ~ 1 ## Data: d (Number of observations: 500) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a1_Intercept -1.29 0.26 -1.80 -0.80 1.00 2090 1831 ## a2_Intercept -1.02 0.22 -1.46 -0.60 1.00 1965 1881 ## b1_Intercept -2.50 0.56 -3.61 -1.44 1.00 2172 2176 ## b2_Intercept -1.21 0.41 -2.01 -0.41 1.00 1988 2100 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Check the PSIS-LOO. b11.14 &lt;- add_criterion(b11.14, &quot;loo&quot;) loo(b11.14) ## ## Computed from 4000 by 500 log-likelihood matrix ## ## Estimate SE ## elpd_loo -330.3 17.0 ## p_loo 3.2 0.3 ## looic 660.6 34.0 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. Now that we actually have predictor variables with which we might estimate conventional \\(\\beta\\) parameters, we finally have more than 2 effective parameters (\\(p_\\text{LOO}\\)). “Again, computing implied predictions is the safest way to interpret these models. They do a great job of classifying discrete, unordered events. But the parameters are on a scale that is very hard to interpret” (p. 325). Like before, we’ll do that with fitted(). Now we have a predictor, this time we will use the newdata argument. nd &lt;- tibble(family_income = seq(from = 0, to = 1, length.out = 60)) f &lt;- fitted(b11.14, newdata = nd) First we’ll plot the fitted probabilities for each career level across the full range of family_income values. # wrangle rbind(f[, , 1], f[, , 2], f[, , 3]) %&gt;% data.frame() %&gt;% bind_cols(nd %&gt;% expand(career = 1:3, family_income)) %&gt;% mutate(career = str_c(&quot;career: &quot;, career)) %&gt;% # plot ggplot(aes(x = family_income, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = career, color = career)) + geom_ribbon(alpha = 2/3, size = 0) + geom_line(size = 3/4) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + scale_x_continuous(breaks = 0:2 / 2) + scale_y_continuous(&quot;probability&quot;, limits = c(0, 1), breaks = 0:3 / 3, labels = c(&quot;0&quot;, &quot;.33&quot;, &quot;.67&quot;, &quot;1&quot;)) + theme(axis.text.y = element_text(hjust = 0), legend.position = &quot;none&quot;) + facet_wrap(~ career) If we’re willing to summarize those fitted lines by their posterior means, we could also make a model-implied version of the stacked area plot from above. # annotation text &lt;- tibble(family_income = c(.45, .3, .15), proportion = c(.65, .8, .95), label = str_c(&quot;career: &quot;, 3:1), color = c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;)) # wrangle rbind(f[, , 1], f[, , 2], f[, , 3]) %&gt;% data.frame() %&gt;% bind_cols(nd %&gt;% expand(career = 1:3, family_income)) %&gt;% group_by(family_income) %&gt;% mutate(proportion = Estimate / sum(Estimate), career = factor(career)) %&gt;% # plot! ggplot(aes(x = family_income, y = proportion)) + geom_area(aes(fill = career)) + geom_text(data = text, aes(label = label, color = color), family = &quot;Times&quot;, size = 4.25) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[4:3]) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 2, 1)]) + theme(legend.position = &quot;none&quot;) For more practice fitting multinomial models with brms, check out Chapter 22 of my (2020c) translation of Kruschke’s (2015) text. 11.3.2.1 Multinomial in disguise as Poisson. Here we fit a multinomial likelihood by refactoring it to a series of Poissons. Let’s retrieve the Berkeley data. data(UCBadmit, package = &quot;rethinking&quot;) d &lt;- UCBadmit rm(UCBadmit) Fit the models. # binomial model of overall admission probability b11.binom &lt;- brm(data = d, family = binomial, admit | trials(applications) ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 11, file = &quot;fits/b11.binom&quot;) # Poisson model of overall admission rate and rejection rate b11.pois &lt;- brm(data = d %&gt;% mutate(rej = reject), # &#39;reject&#39; is a reserved word family = poisson, mvbind(admit, rej) ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 2000, warmup = 1000, cores = 3, chains = 3, seed = 11, file = &quot;fits/b11.pois&quot;) Note, the mvbind() syntax made b11.pois a multivariate Poisson model. Starting with version 2.0.0, brms supports a variety of multivariate models, which you might learn more about with Bürkner’s (2021d) vignette, Estimating multivariate models with brms. Anyway, here are the implications of b11.pois. # extract the samples post &lt;- posterior_samples(b11.pois) # wrangle post %&gt;% mutate(admit = exp(b_admit_Intercept), reject = exp(b_rej_Intercept)) %&gt;% pivot_longer(admit:reject) %&gt;% # plot ggplot(aes(x = value, y = name, fill = name)) + stat_halfeye(point_interval = median_qi, .width = .95, color = wes_palette(&quot;Moonrise2&quot;)[4]) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[1:2]) + labs(title = &quot; Mean admit/reject rates across departments&quot;, x = &quot;# applications&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) We might compare the model summaries. summary(b11.binom)$fixed ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.4556919 0.02999385 -0.5123193 -0.3941477 1.00131 1245 1403 summary(b11.pois)$fixed ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## admit_Intercept 4.984153 0.02393449 4.937026 5.029603 1.001556 2534 1733 ## rej_Intercept 5.440899 0.01910679 5.403373 5.478298 1.004164 2747 2061 Here’s the posterior mean for the probability of admission, based on b11.binom. fixef(b11.binom)[, &quot;Estimate&quot;] %&gt;% inv_logit_scaled() ## [1] 0.3880083 Happily, we get the same value within simulation error from model b11.pois. k &lt;- fixef(b11.pois) %&gt;% as.numeric() exp(k[1]) / (exp(k[1]) + exp(k[2])) ## [1] 0.3877581 The formula for what we just did in code is \\[p_\\text{admit} = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} = \\frac{\\exp (\\alpha_1)}{\\exp (\\alpha_1) + \\exp (\\alpha_2)}.\\] To get a better appreciation on how well the two model types converge on the same solution, we might plot the full poster for admissions probability from each. # wrangle bind_cols( posterior_samples(b11.pois) %&gt;% mutate(`the Poisson` = exp(b_admit_Intercept) / (exp(b_admit_Intercept) + exp(b_rej_Intercept))), posterior_samples(b11.binom) %&gt;% mutate(`the binomial` = inv_logit_scaled(b_Intercept)) ) %&gt;% pivot_longer(starts_with(&quot;the&quot;)) %&gt;% # plot ggplot(aes(x = value, y = name, fill = name)) + stat_halfeye(point_interval = median_qi, .width = c(.95, .5), color = wes_palette(&quot;Moonrise2&quot;)[4]) + scale_fill_manual(values = c(wes_palette(&quot;Moonrise2&quot;)[2:1])) + labs(title = &quot;Two models, same marginal posterior&quot;, x = &quot;admissions probability&quot;, y = NULL) + coord_cartesian(ylim = c(1.5, 2.25)) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) 11.4 Summary This chapter described some of the most common generalized linear models, those used to model counts. It is important to never convert counts to proportions before analysis, because doing so destroys information about sample size. A fundamental difficulty with these models is that parameters are on a different scale, typically log-odds (for binomial) or log-rate (for Poisson), than the outcome variable they describe. Therefore computing implied predictions is even more important than before. (p. 365) 11.5 Bonus: Survival analysis In the middle of the thirteenth lecture of his 2019 lecture series, McElreath briefly covered continuous-time survival analysis. Sadly, the problem didn’t make it into the text. Here we’ll slip it in as a bonus section. To fully understand this section, do listen to this section of the lecture. It’s only about ten minutes. Now let’s load the AustinCats data. data(AustinCats, package = &quot;rethinking&quot;) d &lt;- AustinCats rm(AustinCats) glimpse(d) ## Rows: 22,356 ## Columns: 9 ## $ id &lt;fct&gt; A730601, A679549, A683656, A709749, A733551, A756485, A732960, A664571, A727402, A7495… ## $ days_to_event &lt;int&gt; 1, 25, 4, 41, 9, 4, 4, 5, 24, 2, 34, 27, 3, 151, 106, 4, 55, 1, 4, 30, 18, 5, 34, 1, 1… ## $ date_out &lt;fct&gt; 07/08/2016 09:00:00 AM, 06/16/2014 01:54:00 PM, 07/17/2014 04:57:00 PM, 09/22/2015 12:… ## $ out_event &lt;fct&gt; Transfer, Transfer, Adoption, Transfer, Transfer, Adoption, Adoption, Adoption, Adopti… ## $ date_in &lt;fct&gt; 07/07/2016 12:11:00 PM, 05/22/2014 03:43:00 PM, 07/13/2014 01:20:00 PM, 08/12/2015 06:… ## $ in_event &lt;fct&gt; Stray, Stray, Stray, Stray, Stray, Stray, Stray, Owner Surrender, Stray, Stray, Stray,… ## $ breed &lt;fct&gt; Domestic Shorthair Mix, Domestic Shorthair Mix, Snowshoe Mix, Domestic Shorthair Mix, … ## $ color &lt;fct&gt; Blue Tabby, Black/White, Lynx Point, Calico, Brown Tabby/White, Blue Tabby, Calico, To… ## $ intake_age &lt;int&gt; 7, 1, 2, 12, 1, 1, 2, 24, 1, 3, 4, 12, 1, 7, 0, 12, 1, 12, 1, 24, 24, 3, 1, 24, 1, 24,… At the moment, it doesn’t look like the rethinking package contains documentation about the AustinCats. Based on McElreath’s lecture, he downloaded them from the website of an animal shelter in Austin, TX. We have data on 22,356 cats on whether they were adopted and how long it took. The cats came in a variety of colors. Here are the first ten. d %&gt;% count(color) %&gt;% slice(1:10) ## color n ## 1 Agouti 3 ## 2 Agouti/Brown Tabby 1 ## 3 Agouti/White 1 ## 4 Apricot 1 ## 5 Black 2965 ## 6 Black Smoke 83 ## 7 Black Smoke/Black Tiger 1 ## 8 Black Smoke/White 21 ## 9 Black Tabby 119 ## 10 Black Tabby/White 43 McElreath wondered whether it took longer for black cats to be adopted. If you look at the color categories, above, you’ll see the people doing the data entry were creative with their descriptions. To keep things simple, we’ll just be comparing cats for whom color == \"Black\" to all the others. d &lt;- d %&gt;% mutate(black = ifelse(color == &quot;Black&quot;, &quot;black&quot;, &quot;other&quot;)) d %&gt;% count(black) %&gt;% mutate(percent = 100 * n / sum(n)) %&gt;% mutate(label = str_c(round(percent, digits = 1), &quot;%&quot;)) %&gt;% ggplot(aes(y = black)) + geom_col(aes(x = n, fill = black)) + geom_text(aes(x = n - 250, label = label), color = wes_palette(&quot;Moonrise2&quot;)[3], family = &quot;Times&quot;, hjust = 1) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 1)], breaks = NULL) + scale_x_continuous(expression(italic(n)), breaks = c(0, count(d, black) %&gt;% pull(n))) + labs(title = &quot;Cat color&quot;, y = NULL) + theme(axis.ticks.y = element_blank()) Another variable we need to consider is the out_event. d %&gt;% count(out_event) ## out_event n ## 1 Adoption 11351 ## 2 Censored 549 ## 3 Died 369 ## 4 Disposal 9 ## 5 Euthanasia 636 ## 6 Missing 28 ## 7 Transfer 9414 Happily, most of the cats had Adoption as their out_event. For our purposes, all of the other options are the same as if they were Censored. We’ll make a new variable to indicate that. d &lt;- d %&gt;% mutate(adopted = ifelse(out_event == &quot;Adoption&quot;, 1, 0), censored = ifelse(out_event != &quot;Adoption&quot;, 1, 0)) glimpse(d) ## Rows: 22,356 ## Columns: 12 ## $ id &lt;fct&gt; A730601, A679549, A683656, A709749, A733551, A756485, A732960, A664571, A727402, A7495… ## $ days_to_event &lt;int&gt; 1, 25, 4, 41, 9, 4, 4, 5, 24, 2, 34, 27, 3, 151, 106, 4, 55, 1, 4, 30, 18, 5, 34, 1, 1… ## $ date_out &lt;fct&gt; 07/08/2016 09:00:00 AM, 06/16/2014 01:54:00 PM, 07/17/2014 04:57:00 PM, 09/22/2015 12:… ## $ out_event &lt;fct&gt; Transfer, Transfer, Adoption, Transfer, Transfer, Adoption, Adoption, Adoption, Adopti… ## $ date_in &lt;fct&gt; 07/07/2016 12:11:00 PM, 05/22/2014 03:43:00 PM, 07/13/2014 01:20:00 PM, 08/12/2015 06:… ## $ in_event &lt;fct&gt; Stray, Stray, Stray, Stray, Stray, Stray, Stray, Owner Surrender, Stray, Stray, Stray,… ## $ breed &lt;fct&gt; Domestic Shorthair Mix, Domestic Shorthair Mix, Snowshoe Mix, Domestic Shorthair Mix, … ## $ color &lt;fct&gt; Blue Tabby, Black/White, Lynx Point, Calico, Brown Tabby/White, Blue Tabby, Calico, To… ## $ intake_age &lt;int&gt; 7, 1, 2, 12, 1, 1, 2, 24, 1, 3, 4, 12, 1, 7, 0, 12, 1, 12, 1, 24, 24, 3, 1, 24, 1, 24,… ## $ black &lt;chr&gt; &quot;other&quot;, &quot;other&quot;, &quot;other&quot;, &quot;other&quot;, &quot;other&quot;, &quot;other&quot;, &quot;other&quot;, &quot;other&quot;, &quot;other&quot;, &quot;othe… ## $ adopted &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,… ## $ censored &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,… Here’s what the distribution of days_to_event looks like, when grouped by our new censored variable. d %&gt;% mutate(censored = factor(censored)) %&gt;% filter(days_to_event &lt; 300) %&gt;% ggplot(aes(x = days_to_event, y = censored)) + # let&#39;s just mark off the 50% intervals stat_halfeye(.width = .5, fill = wes_palette(&quot;Moonrise2&quot;)[2], height = 4) + scale_y_discrete(NULL, labels = c(&quot;censored == 0&quot;, &quot;censored == 1&quot;)) + coord_cartesian(ylim = c(1.5, 5.1)) + theme(axis.ticks.y = element_blank()) Do note there is a very long right tail that we’ve cut off for the sake of the plot. Anyway, the point of this plot is to show that the distribution for our primary variable, days_to_event, looks very different conditional on whether the data were censored. As McElreath covered in the lecture, we definitely don’t want to loose that information by excluding the censored cases from the analysis. McElreath fit his survival model using the exponential likelihood. We briefly met the exponential likelihood in Chapter 10. As McElreath wrote: It is a fundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space. If the probability of an event is constant in time or across space, then the distribution of events tends towards exponential. The exponential distribution has maximum entropy among all non-negative continuous distributions with the same average displacement. (p. 314) If we let \\(y\\) be a non-negative continuous variable, the probability density function for the exponential distribution is \\[f(y) = \\lambda e^{-\\lambda y},\\] where \\(\\lambda\\) is called the rate. The mean of the exponential distribution is the inverse of the rate \\[\\operatorname{E}[y] = \\frac{1}{\\lambda}.\\] Importantly, brms paramaterizes exponential models in terms of \\(\\operatorname{E}[y]\\). By default, it uses the log link. The is the same set-up McElreath used for rethinking in his lecture. To get a sense of how this all works, we can write our continuous-time survival model as \\[\\begin{align*} \\text{days_to_event}_i | \\text{censored}_i = 0 &amp; \\sim \\operatorname{Exponential}(\\lambda_i) \\\\ \\text{days_to_event}_i | \\text{censored}_i = 1 &amp; \\sim \\operatorname{Exponential-CCDF}(\\lambda_i) \\\\ \\lambda_i &amp; = 1 / \\mu_i \\\\ \\log \\mu_i &amp; = \\alpha_{\\text{black}[i]} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 1). \\end{align*}\\] This is the same model McElreath discussed in the lecture. We’ve just renamed a couple variables. When you fit a continuous-time survival analysis with brm(), you’ll want to tell the software about how the data have been censored with help from the cens() function. For many of the models in this chapter, we used the trials() function to include the \\(n_i\\) information into our binomial models. Both trials() and cens() are members of a class of functions designed to provide supplemental information about our criterion variables to brm(). The cens() function lets us add in information about censoring. In his lecture, McElreath mentioned there can be different kinds of censoring. brms can handle variables with left, right, or interval censoring. In the case of our days_to_event data, some of the values have been right censored, which is typical in survival models. We will feed this information into the model with the formula code days_to_event | cens(censored), where censored is the name of the variable in our data that indexes the censoring. The cens() function has been set up to expect our data to be coded as either 'left', 'none', 'right', and/or 'interval'; or -1, 0, 1, and/or 2. Since we coded our censored variable as censored = 1 and not censored = 0, we have followed the second coding scheme. For more on the topic, see the Additional response information subsection within the brmsformula section of brms reference manual (Bürkner, 2021i). Here’s how to fit our survival model with brms. b11.15 &lt;- brm(data = d, family = exponential, days_to_event | cens(censored) ~ 0 + black, prior(normal(0, 1), class = b), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 11, file = &quot;fits/b11.15&quot;) Check the summary. print(b11.15) ## Family: exponential ## Links: mu = log ## Formula: days_to_event | cens(censored) ~ 0 + black ## Data: d (Number of observations: 22356) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## blackblack 4.05 0.03 4.00 4.10 1.00 3256 2435 ## blackother 3.88 0.01 3.86 3.90 1.00 3965 2441 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Since we modeled \\(\\log \\mu_i\\), we need to transform our \\(\\alpha\\) parameters back into the \\(\\lambda\\) metric using the formula \\[\\begin{align*} \\log \\mu &amp; = \\alpha_\\text{black}, &amp;&amp; \\text{and} \\\\ \\lambda &amp; = 1 / \\mu, &amp;&amp; \\text{therefore} \\\\ \\lambda_\\text{black} &amp; = 1 / \\exp(\\alpha_\\text{black}). \\end{align*}\\] Here are the posterior means for our two \\(\\lambda\\)’s. 1 / exp(fixef(b11.15)[, -2]) ## Estimate Q2.5 Q97.5 ## blackblack 0.01740042 0.01832528 0.01650372 ## blackother 0.02065009 0.02105913 0.02023061 It still might not be clear what any of this all means. To get a better sense, let’s make our version of one of the plots from McElreath’s lecture. # annotation text &lt;- tibble(color = c(&quot;black&quot;, &quot;other&quot;), days = c(40, 34), p = c(.55, .45), label = c(&quot;black cats&quot;, &quot;other cats&quot;), hjust = c(0, 1)) # wrangle f &lt;- fixef(b11.15) %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% mutate(color = str_remove(rowname, &quot;black&quot;)) %&gt;% expand(nesting(Estimate, Q2.5, Q97.5, color), days = 0:100) %&gt;% mutate(m = 1 - pexp(days, rate = 1 / exp(Estimate)), ll = 1 - pexp(days, rate = 1 / exp(Q2.5)), ul = 1 - pexp(days, rate = 1 / exp(Q97.5))) # plot! f %&gt;% ggplot(aes(x = days)) + geom_hline(yintercept = .5, linetype = 3, color = wes_palette(&quot;Moonrise2&quot;)[2]) + geom_ribbon(aes(ymin = ll, ymax = ul, fill = color), alpha = 1/2) + geom_line(aes(y = m, color = color)) + geom_text(data = text, aes(y = p, label = label, hjust = hjust, color = color), family = &quot;Times&quot;) + scale_fill_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 1)], breaks = NULL) + scale_color_manual(values = wes_palette(&quot;Moonrise2&quot;)[c(4, 1)], breaks = NULL) + scale_y_continuous(&quot;proportion remaining&quot;, breaks = c(0, .5, 1), limits = 0:1) + xlab(&quot;days to adoption&quot;) McElreath’s hypothesis is correct: Black cats are adopted a lower rates than cats of other colors. Another way to explore this model is to ask: About how many days would it take for half of the cats of a given color to be adopted? We can do this with help from the qexp() function. For example: qexp(p = .5, rate = 1 / exp(fixef(b11.15)[1, 1])) ## [1] 39.83509 But that’s just using one of the posterior means. Here’s that information using the full posterior distributions for our two levels of black. # wrangle post &lt;- posterior_samples(b11.15) %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% mutate(color = str_remove(name, &quot;b_black&quot;), days = qexp(p = .5, rate = 1 / exp(value))) # axis breaks medians &lt;- group_by(post, color) %&gt;% summarise(med = median(days)) %&gt;% pull(med) %&gt;% round(., digits = 1) # plot! post %&gt;% ggplot(aes(x = days, y = color)) + stat_halfeye(.width = .95, fill = wes_palette(&quot;Moonrise2&quot;)[2], height = 4) + scale_x_continuous(&quot;days untill 50% are adopted&quot;, breaks = c(30, medians, 45), labels = c(&quot;30&quot;, medians, &quot;45&quot;), limits = c(30, 45)) + ylab(NULL) + coord_cartesian(ylim = c(1.5, 5.1)) + theme(axis.ticks.y = element_blank()) The model suggests it takes about six days longer for the half of the black cats to be adopted. 11.5.1 Survival summary. We’ve really just scratched the surface on survival models. In addition to those which use the exponential likelihood, brms supports a variety of survival models. Some of the more popular likelihoods are the log-Normal, the gamma, and the Weibull. For details, see the Time-to-event models section of Bürkner’s (2021h) vignette, Parameterization of response distributions in brms. Starting with the release of version 2.13.5, brms now supports the Cox proportional hazards model via family = cox. If you’re tricky with your coding, you can also fit discrete-time survival models with the binomial likelihood (see here). For some examples of discrete and continuous-time survival models, you might check out my (2020b) ebook translation of Singer and Willett’s (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence, the later chapters of which provide an exhaustive introduction to survival analysis. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] rethinking_2.13 rstan_2.21.2 StanHeaders_2.21.0-7 ggrepel_0.9.1 GGally_2.1.1 ## [6] ggdag_0.2.3 patchwork_1.1.1 tidybayes_2.3.1 ggthemes_4.2.4 wesanderson_0.3.6 ## [11] brms_2.15.0 Rcpp_1.0.6 flextable_0.6.4 forcats_0.5.1 stringr_1.4.0 ## [16] dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 tibble_3.1.0 ## [21] ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 uuid_0.1-4 backports_1.2.1 systemfonts_1.0.1 plyr_1.8.6 ## [6] igraph_1.2.6 svUnit_1.0.3 splines_4.0.4 crosstalk_1.1.0.1 TH.data_1.0-10 ## [11] rstantools_2.1.1 inline_0.3.17 digest_0.6.27 htmltools_0.5.1.1 viridis_0.5.1 ## [16] rsconnect_0.8.16 fansi_0.4.2 magrittr_2.0.1 vembedr_0.1.4 graphlayouts_0.7.1 ## [21] modelr_0.1.8 RcppParallel_5.0.2 matrixStats_0.57.0 officer_0.3.17 xts_0.12.1 ## [26] sandwich_3.0-0 prettyunits_1.1.1 colorspace_2.0-0 rvest_0.3.6 ggdist_2.4.0.9000 ## [31] haven_2.3.1 xfun_0.22 callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 ## [36] lme4_1.1-25 survival_3.2-7 zoo_1.8-8 glue_1.4.2 polyclip_1.10-0 ## [41] gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 distributional_0.2.2 pkgbuild_1.2.0 ## [46] shape_1.4.5 abind_1.4-5 scales_1.1.1 mvtnorm_1.1-1 emo_0.0.0.9000 ## [51] DBI_1.1.0 miniUI_0.1.1.1 viridisLite_0.3.0 xtable_1.8-4 HDInterval_0.2.2 ## [56] stats4_4.0.4 DT_0.16 htmlwidgets_1.5.2 httr_1.4.2 threejs_0.3.3 ## [61] RColorBrewer_1.1-2 arrayhelpers_1.1-0 ellipsis_0.3.1 santoku_0.5.0 reshape_0.8.8 ## [66] farver_2.0.3 pkgconfig_2.0.3 loo_2.4.1 dbplyr_2.0.0 utf8_1.1.4 ## [71] labeling_0.4.2 tidyselect_1.1.0 rlang_0.4.10 reshape2_1.4.4 later_1.1.0.1 ## [76] dagitty_0.3-1 munsell_0.5.0 cellranger_1.1.0 tools_4.0.4 cli_2.3.1 ## [81] generics_0.1.0 broom_0.7.5 ggridges_0.5.2 evaluate_0.14 fastmap_1.0.1 ## [86] processx_3.4.5 knitr_1.31 fs_1.5.0 tidygraph_1.2.0 zip_2.1.1 ## [91] ggraph_2.0.4 nlme_3.1-152 mime_0.10 projpred_2.0.2 xml2_1.3.2 ## [96] compiler_4.0.4 bayesplot_1.8.0 shinythemes_1.1.2 rstudioapi_0.13 gamm4_0.2-6 ## [101] curl_4.3 reprex_0.3.0 tweenr_1.0.1 statmod_1.4.35 stringi_1.5.3 ## [106] highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 gdtools_0.2.2 lattice_0.20-41 ## [111] Matrix_1.3-2 nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 vctrs_0.3.6 ## [116] pillar_1.5.1 lifecycle_1.0.0 bridgesampling_1.0-0 estimability_1.3 data.table_1.14.0 ## [121] httpuv_1.5.4 R6_2.5.0 bookdown_0.21 promises_1.1.1 gridExtra_2.3 ## [126] codetools_0.2-18 boot_1.3-26 colourpicker_1.1.0 MASS_7.3-53 gtools_3.8.2 ## [131] assertthat_0.2.1 withr_2.4.1 shinystan_2.5.0 multcomp_1.4-16 mgcv_1.8-33 ## [136] hms_0.5.3 grid_4.0.4 coda_0.19-4 minqa_1.2.4 rmarkdown_2.7 ## [141] ggforce_0.3.2 shiny_1.5.0 lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 Though McElreath didn’t cover it, here, it’s also fine to fit binomial models using the probit link. Gelman et al. (2020) covered probit regression in Section 15.4. With brms, it’s simply a matter of setting family = binomial(link = \"probit\") within brm().↩︎ "],["monsters-and-mixtures.html", "12 Monsters and Mixtures 12.1 Over-dispersed counts 12.2 Zero-inflated outcomes 12.3 Ordered categorical outcomes 12.4 Ordered categorical predictors 12.5 Summary Session info", " 12 Monsters and Mixtures Many monsters are hybrids. Many statistical models are too. This chapter is about constructing likelihood and link functions by piecing together the simpler components of previous chapters. Like legendary monsters, these hybrid likelihoods contain pieces of other model types. Endowed with some properties of each piece, they help us model outcome variables with inconvenient, but common, properties…. We’ll consider three common and useful examples. The first are models for handling over-dispersion. These models extend the binomial and Poisson models of the previous chapter to cope a bit with unmeasured sources of variation. The second type is a family of zero-inflated and zero-augmented models, each of which mixes a binary event with an ordinary GLM likelihood like a Poisson or binomial. The third type is the ordered categorical model, useful for categorical outcomes with a fixed ordering. This model is built by merging a categorical likelihood function with a special kind of link function, usually a cumulative link. We’ll also learn how to construct ordered categorical predictors. These model types help us transform our modeling to cope with the inconvenient realities of measurement, rather than transforming measurements to cope with the constraints of our models. (McElreath, 2020a, p. 369, emphasis in the original) 12.1 Over-dispersed counts When counts arise from a mixture of different processes, then there may be more variation–thicker tails–than a pure count model expects. This can again lead to overly excited models. When counts are more variable than a pure process, they exhibit over-dispersion. The variance of a variable is sometimes called its dispersion. For a counting process like a binomial, the variance is a function of the same parameters as the expected value. For example, the expected value of a binomial is \\(Np\\) and its variance is \\(Np(1 - p)\\). When the observed variance exceeds this amount–after conditioning on all the predictor variables–this implies that some omitted variable is producing additional dispersion in the observed counts. That isn’t necessarily bad. Such a model could still produce perfectly good inferences. But ignoring over-dispersion can also lead to all of the same problems as ignoring any predictor variable. Heterogeneity in counts can be a confound, hiding effects of interest or producing spurious inferences. (p, 370, emphasis in the original) In this chapter we’ll cope with the problem using continuous mixture models, “models in which a linear model is attached not to the observations themselves but rather to a distribution of observations” (p. 370). 12.1.1 Beta-binomial. A beta-binomial model is a mixture of binomial distributions. It assumes that each binomial count observation has its own probability of success. We estimate the distribution of probabilities of success instead of a single probability of success. Any predictor variables describe the shape of this distribution. (p, 370, emphasis in the original) Unfortunately, we need to digress. As it turns out, there are multiple ways to parameterize the beta distribution and we’ve run square into two. In the text, McElreath described the beta distribution with two parameters, an average probability \\(\\bar p\\) and a shape parameter \\(\\theta\\). In his R code 12.1, which we’ll reproduce in a bit, he demonstrated that parameterization with the rethinking::dbeta2() function. The nice thing about this parameterization is how intuitive the pbar parameter is. If you want a beta with an average of .2, you set pbar = .2. If you want the distribution to be more or less certain, make the theta argument more or less large, respectively. However, the beta density is often defined in terms of \\(\\alpha\\) and \\(\\beta\\). If you denote the data as \\(y\\), this follows the form \\[\\operatorname{Beta}(y | \\alpha, \\beta) = \\frac{y^{\\alpha - 1} (1 - y)^{\\beta - 1}}{\\text B (\\alpha, \\beta)},\\] which you can verify in the Continuous distributions on [0, 1] section of the Stan functions reference (Stan Development Team, 2021a). In the formula, \\(\\text B\\) stands for the Beta function, which computes a normalizing constant, which you can learn about in the Mathematical functions chapter of the Stan functions reference. If you look at the base R dbeta() function, you’ll learn it takes two parameters, shape1 and shape2. Those uncreatively-named parameters are the same \\(\\alpha\\) and \\(\\beta\\) from the density, above. They do not correspond to the pbar and theta parameters of McEreath’s rethinking::dbeta2() function. McElreath had good reason for using dbeta2(). Beta’s typical \\(\\alpha\\) and \\(\\beta\\) parameters aren’t the most intuitive to use; the parameters in McElreath’s dbeta2() are much nicer. If you dive a little deeper, it turns out you can find the mean of a beta distribution in terms of \\(\\alpha\\) and \\(\\beta\\) like this: \\[\\mu = \\frac{\\alpha}{\\alpha + \\beta}.\\] We can talk about the spread of the distribution, sometimes called \\(\\kappa\\), in terms \\(\\alpha\\) and \\(\\beta\\) like this: \\[\\kappa = \\alpha + \\beta.\\] With \\(\\mu\\) and \\(\\kappa\\) in hand, we can even find the \\(SD\\) of a beta distribution with the formula \\[\\sigma = \\sqrt{\\mu (1 - \\mu) / (\\kappa + 1)}.\\] I explicate all this because McElreath’s pbar is \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) and his theta is \\(\\kappa = \\alpha + \\beta\\), which is great news because it means that we can understand what McElreath did with his beta2() function in terms of the base R dbeta() function. This also sets us up to understand the distribution of the beta parameters used in brms::brm(). To demonstrate, let’s walk through McElreath’s R code 12.1. Before we get to R code 12.1 and our version of the resulting plot, we should discuss themes. In this chapter we’ll use theme settings and a color palette from the ggthemes package. library(ggthemes) We’ll take our basic theme settings from the theme_hc() function. We’ll use the Green fields color palette, which we can inspect with the canva_pal() function and a little help from scales::show_col(). scales::show_col(canva_pal(&quot;Green fields&quot;)(4)) canva_pal(&quot;Green fields&quot;)(4) ## [1] &quot;#919636&quot; &quot;#524a3a&quot; &quot;#fffae1&quot; &quot;#5a5f37&quot; canva_pal(&quot;Green fields&quot;)(4)[3] ## [1] &quot;#fffae1&quot; Now we finally get to R code 12.1. library(tidyverse) theme_set( theme_hc() + theme(axis.ticks.y = element_blank(), plot.background = element_rect(fill = &quot;grey92&quot;)) ) pbar &lt;- .5 theta &lt;- 5 ggplot(data = tibble(x = seq(from = 0, to = 1, by = .01)), aes(x = x, y = rethinking::dbeta2(x, pbar, theta))) + geom_area(fill = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(The~beta~distribution), subtitle = expression(&quot;Defined in terms of &quot;*mu*&quot; (i.e., pbar) and &quot;*kappa*&quot; (i.e., theta)&quot;)) In his (2015) text, Doing Bayesian data analysis, Kruschke provided code for a convenience function that takes pbar and theta as inputs and returns the corresponding \\(\\alpha\\) and \\(\\beta\\) values. Here’s the function: betaABfromMeanKappa &lt;- function(mean, kappa) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (kappa &lt;= 0) stop(&quot;kappa must be &gt; 0&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } Now we can use Kruschke’s betaABfromMeanKappa() to find the \\(\\alpha\\) and \\(\\beta\\) values corresponding to pbar and theta. betaABfromMeanKappa(mean = pbar, kappa = theta) ## $a ## [1] 2.5 ## ## $b ## [1] 2.5 And finally, we can double check that all of this works. Here’s the same distribution but defined in terms of \\(\\alpha\\) and \\(\\beta\\). ggplot(data = tibble(x = seq(from = 0, to = 1, by = .01)), aes(x = x, y = dbeta(x, 2.5, 2.5))) + geom_area(fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1)) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(The~beta~distribution), subtitle = expression(&quot;This time defined in terms of &quot;*alpha*&quot; and &quot;*beta)) McElreath encouraged us to “explore different values for pbar and theta” (p. 371). Here’s a grid of plots with pbar = c(.25, .5, .75) and theta = c(5, 10, 15). # data crossing(pbar = c(.25, .5, .75), theta = c(5, 15, 30)) %&gt;% expand(nesting(pbar, theta), x = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(density = rethinking::dbeta2(x, pbar, theta), mu = str_c(&quot;mu == &quot;, pbar %&gt;% str_remove(., &quot;0&quot;)), kappa = factor(str_c(&quot;kappa == &quot;, theta), levels = c(&quot;kappa == 30&quot;, &quot;kappa == 15&quot;, &quot;kappa == 5&quot;))) %&gt;% # plot ggplot(aes(x = x, y = density)) + geom_area(fill = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_x_continuous(&quot;probability space&quot;, breaks = c(0, .5, 1), labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, labels = NULL) + theme(axis.ticks.y = element_blank()) + facet_grid(kappa ~ mu, labeller = label_parsed) If you’d like to see how to make a similar plot in terms of \\(\\alpha\\) and \\(\\beta\\), see Chapter 6 of my (2020c) ebook wherein I translated Kruschke’s text into tidyverse and brms code. But remember, we’re not fitting a beta model. We’re using the beta-binomial. “We’re going to bind our linear model to \\(\\bar p\\), so that changes in predictor variables change the central tendency of the distribution” (p. 371). The statistical model we’ll be fitting follows the form \\[\\begin{align*} \\text{admit}_i &amp; \\sim \\operatorname{BetaBinomial}(n_i, \\bar p_i, \\phi)\\\\ \\operatorname{logit}(\\bar p_i) &amp; = \\alpha_{\\text{gid}[i]} \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\phi &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Here the size, \\(n\\), is defined in the applications column in the data we’ll load in just a moment. In case you’re confused, yes, our statistical model is not quite the same as the one McElreath presented on page 371 in the text. If you look closely, we dropped all mention of \\(\\theta\\) and jumped directly to \\(\\phi\\). Instead of implementing McElreath’s \\(\\theta = \\phi + 2\\) trick, we’re going to set the lower bound for \\(\\phi\\) directly. Which brings us to the next issue: We have an additional complication. The beta-binomial likelihood is not natively supported in brms at this time (see GitHub issue #144). However, brms versions 2.2.0 and above allow users to define custom distributions. To get all the details, you might check out Bürkner’s (2021a) vignette, Define custom response distributions with brms. Happily, Bürkner even used the beta-binomial distribution as the exemplar in the vignette. Before we get carried away, let’s load the data and brms. data(UCBadmit, package = &quot;rethinking&quot;) d &lt;- UCBadmit %&gt;% mutate(gid = ifelse(applicant.gender == &quot;male&quot;, &quot;1&quot;, &quot;2&quot;)) rm(UCBadmit) library(brms) I’m not going to go into great detail explaining the ins and outs of making custom distributions for brm(). You’ve got Bürkner’s vignette for that. For our purposes, we need a few preparatory steps. First, we need to use the custom_family() function to define the name and parameters of the beta-binomial distribution for use in brm(). Second, we have to define some functions for Stan which are not defined in Stan itself. We’ll save them as stan_funs. Third, we’ll make a stanvar() statement which will allow us to pass our stan_funs to brm(). beta_binomial2 &lt;- custom_family( &quot;beta_binomial2&quot;, dpars = c(&quot;mu&quot;, &quot;phi&quot;), links = c(&quot;logit&quot;, &quot;log&quot;), lb = c(NA, 2), type = &quot;int&quot;, vars = &quot;vint1[n]&quot; ) stan_funs &lt;- &quot; real beta_binomial2_lpmf(int y, real mu, real phi, int T) { return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi); } int beta_binomial2_rng(real mu, real phi, int T) { return beta_binomial_rng(T, mu * phi, (1 - mu) * phi); } &quot; stanvars &lt;- stanvar(scode = stan_funs, block = &quot;functions&quot;) Did you notice the lb = c(NA, 2) portion of the code defining beta_binomial2()? In Bürkner’s vignette, he set the lower bound of phi to zero. Since McElreath wanted the lower bound for \\(\\phi\\) to be 2, we just set that as the default in the likelihood. We should clarify two more points: First, what McElreath referred to as the shape parameter, \\(\\theta\\), Bürkner called the precision parameter, \\(\\phi\\). In our exposition, above, we followed Kruschke’s convention and called it \\(\\kappa\\). These are all the same thing: \\(\\theta\\), \\(\\phi\\), and \\(\\kappa\\) are all the same thing. Perhaps less confusingly, what McElreath called the pbar parameter, \\(\\bar p\\), Bürkner simply refers to as \\(\\mu\\). Second, we’ve become accustomed to using the y | trials() ~ ... syntax when defining our formula arguments for binomial models. Here we are replacing trials() with vint(). From Bürkner’s Define custom response distributions with brms vignette, we read: To provide information about the number of trials (an integer variable), we are going to use the addition argument vint(), which can only be used in custom families. Similarly, if we needed to include additional vectors of real data, we would use vreal(). Actually, for this particular example, we could more elegantly apply the addition argument trials() instead of vint() as in the basic binomial model. However, since the present vignette is meant to give a general overview of the topic, we will go with the more general method. We now have all components together to fit our custom beta-binomial model: b12.1 &lt;- brm(data = d, family = beta_binomial2, # here&#39;s our custom likelihood admit | vint(applications) ~ 0 + gid, prior = c(prior(normal(0, 1.5), class = b), prior(exponential(1), class = phi)), iter = 2000, warmup = 1000, cores = 4, chains = 4, stanvars = stanvars, # note our `stanvars` seed = 12, file = &quot;fits/b12.01&quot;) Success, our results look a lot like those in the text! print(b12.1) ## Family: beta_binomial2 ## Links: mu = logit; phi = identity ## Formula: admit | vint(applications) ~ 0 + gid ## Data: d (Number of observations: 12) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## gid1 -0.43 0.41 -1.25 0.38 1.00 2993 2311 ## gid2 -0.32 0.41 -1.13 0.46 1.00 3187 2426 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## phi 3.00 0.77 2.04 4.90 1.00 2194 1561 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Just remember that, perhaps confusingly, what McElreath’s output called theta, our brms output is calling phi. I know; this section is a lot. Keep your chin up! Here’s what the corresponding posterior_samples() data object looks like. post &lt;- posterior_samples(b12.1) head(post) ## b_gid1 b_gid2 phi lp__ ## 1 -0.7111732 -0.2315126 3.978483 -72.62151 ## 2 -0.1013704 0.1953892 2.489657 -73.74827 ## 3 -0.7666380 -0.5098164 2.527412 -73.24252 ## 4 -0.8413891 -0.9727541 2.363326 -74.95696 ## 5 -0.7283230 -1.1329282 2.315965 -75.48158 ## 6 -0.3496499 -0.6758734 2.268871 -73.86028 Now we can compute and summarize a contrast between the two genders, what McElreath called da. library(tidybayes) post %&gt;% transmute(da = b_gid1 - b_gid2) %&gt;% mean_qi(.width = .89) %&gt;% mutate_if(is.double, round, digits = 3) ## da .lower .upper .width .point .interval ## 1 -0.105 -1.001 0.814 0.89 mean qi Much like in the text, the difference between genders on admission rates is near zero with wide uncertainty intervals spanning in either direction. To stay within the tidyverse while making the many thin lines in Figure 12.1.a, we’re going to need to do a bit of data processing. First, we’ll want a variable to index the rows in post (i.e., to index the posterior draws). And we’ll want to convert the b_gid2 to the \\(\\bar p\\) metric with the inv_logit_scaled() function. Then we’ll use slice_sample() to randomly draw a subset of the posterior draws. Then with the expand() function, we’ll insert a dense sequence of x values ranging between 0 and 1–the parameter space of beta distribution. Finally, we’ll use pmap_dbl() to compute the density values for the rethinking::dbeta2 distribution corresponding to the unique combination of x, p_bar, and phi values in each row. set.seed(12) lines &lt;- post %&gt;% mutate(iter = 1:n(), p_bar = inv_logit_scaled(b_gid2)) %&gt;% slice_sample(n = 100) %&gt;% expand(nesting(iter, p_bar, phi), x = seq(from = 0, to = 1, by = .005)) %&gt;% mutate(density = pmap_dbl(list(x, p_bar, phi), rethinking::dbeta2)) str(lines) ## tibble [20,100 × 5] (S3: tbl_df/tbl/data.frame) ## $ iter : int [1:20100] 72 72 72 72 72 72 72 72 72 72 ... ## $ p_bar : num [1:20100] 0.291 0.291 0.291 0.291 0.291 ... ## $ phi : num [1:20100] 2.92 2.92 2.92 2.92 2.92 ... ## $ x : num [1:20100] 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 ... ## $ density: num [1:20100] Inf 3.58 3.21 3 2.86 ... All that was just for the thin lines. To make the thicker line for the posterior mean, we’ll get tricky with stat_function(). lines %&gt;% ggplot(aes(x = x, y = density)) + stat_function(fun = rethinking::dbeta2, args = list(prob = mean(inv_logit_scaled(post[, &quot;b_gid2&quot;])), theta = mean(post[, &quot;phi&quot;])), size = 1.5, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + geom_line(aes(group = iter), alpha = .2, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_y_continuous(NULL, breaks = NULL, limits = c(0, 3)) + labs(subtitle = &quot;distribution of female admission rates&quot;, x = &quot;probability admit&quot;) There are other ways to do this. For ideas, check out my blog post, Make rotated Gaussians, Kruschke style. Before we can do our variant of Figure 12.1.b, we’ll need to define a few more custom functions. The log_lik_beta_binomial2() and posterior_predict_beta_binomial2() functions are required for brms::predict() to work with our family = beta_binomial2 brmfit object. Similarly, posterior_epred_beta_binomial2() is required for brms::fitted() to work properly. And before all that, we need to throw in a line with the expose_functions() function. Just go with it. expose_functions(b12.1, vectorize = TRUE) # required to use `predict()` log_lik_beta_binomial2 &lt;- function(i, prep) { mu &lt;- prep$dpars$mu[, i] phi &lt;- prep$dpars$phi trials &lt;- prep$data$vint1[i] y &lt;- prep$data$Y[i] beta_binomial2_lpmf(y, mu, phi, trials) } posterior_predict_beta_binomial2 &lt;- function(i, prep, ...) { mu &lt;- prep$dpars$mu[, i] phi &lt;- prep$dpars$phi trials &lt;- prep$data$vint1[i] beta_binomial2_rng(mu, phi, trials) } # required to use `fitted()` posterior_epred_beta_binomial2 &lt;- function(prep) { mu &lt;- prep$dpars$mu trials &lt;- prep$data$vint1 trials &lt;- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE) mu * trials } With those intermediary steps out of the way, we’re ready to make Figure 12.1.b. # the prediction intervals predict(b12.1) %&gt;% data.frame() %&gt;% transmute(ll = Q2.5, ul = Q97.5) %&gt;% bind_cols( # the fitted intervals fitted(b12.1) %&gt;% data.frame(), # the original data used to fit the model) %&gt;% b12.1$data ) %&gt;% mutate(case = 1:12) %&gt;% # plot! ggplot(aes(x = case)) + geom_linerange(aes(ymin = ll / applications, ymax = ul / applications), color = canva_pal(&quot;Green fields&quot;)(4)[1], size = 2.5, alpha = 1/4) + geom_pointrange(aes(ymin = Q2.5 / applications, ymax = Q97.5 / applications, y = Estimate/applications), color = canva_pal(&quot;Green fields&quot;)(4)[4], size = 1/2, shape = 1) + geom_point(aes(y = admit/applications), color = canva_pal(&quot;Green fields&quot;)(4)[2], size = 2) + scale_x_continuous(breaks = 1:12) + scale_y_continuous(breaks = 0:5 / 5, limits = c(0, 1)) + labs(subtitle = &quot;Posterior validation check&quot;, caption = expression(italic(Note.)*&quot; A = admittance probability&quot;), y = &quot;A&quot;) + theme(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;) As in the text, the raw data are consistent with the prediction intervals. But those intervals are so incredibly wide, they’re hardly an endorsement of the model. Once we learn about hierarchical models, we’ll be able to do much better. 12.1.2 Negative-binomial or gamma-Poisson. Recall from the last chapter how the Poisson distribution presumes \\(\\sigma^2\\) scales with \\(\\mu\\). The negative binomial distribution relaxes this assumption and presumes “each Poisson count observation has its own rate. It estimates the shape of a gamma distribution to describe the Poisson rates across cases” (p. 373). If you execute ?dgamma, you’ll see that base R will allow you to define the gamma distribution with either the shape and rate or the shape and scale. If we define gamma in terms of shape and rate, it follows the formula \\[\\operatorname{Gamma}(y | \\alpha, \\beta) = \\frac{\\beta^\\alpha y^{\\alpha - 1} e^{-\\beta y}}{\\Gamma (\\alpha)},\\] where \\(\\alpha\\) is the shape, \\(\\beta\\) is the rate, \\(e\\) is base of the natural logarithm, and \\(\\Gamma\\) is the gamma function. It turns out the rate and scale parameters are the reciprocals of each other. Thus if you’d like to define a gamma distribution in terms of shape and scale, it would follow the formula \\[\\operatorname{Gamma}(y | \\alpha, \\theta) = \\frac{y^{\\alpha - 1} e^{-x /\\theta}}{\\theta^\\alpha \\Gamma (\\alpha)},\\] where \\(\\alpha\\), \\(e\\), and \\(\\Gamma\\) are all as they were before and \\(\\theta\\) is the scale parameter. If that all wasn’t complicated enough, it turns out there’s one more way to define a gamma distribution. You can use the mean and shape. This would follow the formula \\[\\operatorname{Gamma}(y | \\mu, \\alpha) = \\frac{ \\big (\\frac{\\alpha}{\\mu} \\big)^\\alpha}{\\Gamma (\\alpha)} y^{\\alpha - 1} \\exp (- \\frac{\\alpha y}{\\mu}),\\] where \\(\\alpha\\) and \\(\\Gamma\\) are still the shape and gamma function, respectively, and \\(\\mu\\) is the mean. I know, this is a lot and it probably all seems really abstract, right now. Think of this section as a reference. As you’ll see after we fit our model, you may well need it. Returning to the content in the text, we might express the gamma-Poisson (negative binomial) as \\[y_i \\sim \\operatorname{Gamma-Poisson}(\\mu, \\alpha),\\] where \\(\\mu\\) is the mean or rate, taking the place of \\(\\lambda\\) from the Poisson distribution, and \\(\\alpha\\) is the shape. data(Kline, package = &quot;rethinking&quot;) d &lt;- Kline %&gt;% mutate(p = rethinking::standardize(log(population)), contact_id = ifelse(contact == &quot;high&quot;, 2L, 1L), cid = contact) rm(Kline) print(d) ## culture population contact total_tools mean_TU p contact_id cid ## 1 Malekula 1100 low 13 3.2 -1.291473310 1 low ## 2 Tikopia 1500 low 22 4.7 -1.088550750 1 low ## 3 Santa Cruz 3600 low 24 4.0 -0.515764892 1 low ## 4 Yap 4791 high 43 5.0 -0.328773359 2 high ## 5 Lau Fiji 7400 high 33 5.0 -0.044338980 2 high ## 6 Trobriand 8000 high 19 4.0 0.006668287 2 high ## 7 Chuuk 9200 high 40 3.8 0.098109204 2 high ## 8 Manus 13000 low 28 6.6 0.324317564 1 low ## 9 Tonga 17500 high 55 5.4 0.518797917 2 high ## 10 Hawaii 275000 low 71 6.6 2.321008320 1 low If you take a look of McElreath’s m12.2, you’ll see it’s a gamma-Poisson version of the non-linear model he fit in last chapter, m11.11. You might also recall that we had to employ somewhat complicated non-linear syntax to translate that model into brms. Instead of jumping straight into a similarly complicated gamma-Poisson version of that model, I’m going to warm us up with a simple intercept-only model of the data. The formula will be \\[\\begin{align*} \\text{total_tools}_i &amp; \\sim \\operatorname{Gamma-Poisson} (\\mu, \\alpha) \\\\ \\text{log}(\\mu) &amp; = \\beta_0 \\\\ \\beta_0 &amp; \\sim \\operatorname{Normal}(3, 0.5) \\\\ \\alpha &amp; \\sim \\operatorname{Gamma}(0.01, 0.01), \\end{align*}\\] where we have deviated from McElreath’s convention of using \\(\\alpha\\) for the model in favor \\(\\beta_0\\). This is because brms parameterizes the gamma likelihood in terms of \\(\\mu\\) and shape and, as we discussed above, shape is typically denoted as \\(\\alpha\\). I mean, technically we could refer to the shape parameter as \\(\\psi\\) or \\(\\xi\\) or whatever, but then we’d just be abandoning one convention for another. There’s no to win, here. Sigh. The prior for \\(\\beta_0\\) is the same one we used for the intercept way back for model b11.9. We have assigned a gamma prior for our troublesome new \\(\\alpha\\) (shape) parameter. Here’s where that prior came from. get_prior(data = d, family = negbinomial, total_tools ~ 1) ## prior class coef group resp dpar nlpar bound source ## student_t(3, 3.4, 2.5) Intercept default ## gamma(0.01, 0.01) shape default gamma(0.01, 0.01) is the brms default for the shape parameter in this model. Within brms, priors using the gamma distribution are based on the shape-rate (\\(\\alpha\\)-\\(\\theta\\)) parameterization. This is what \\(\\operatorname{Gamma}(0.01, 0.01)\\) looks like. ggplot(data = tibble(x = seq(from = 0, to = 60, by = .1)), aes(x = x, y = dgamma(x, 0.01, 0.01))) + geom_area(color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_x_continuous(NULL) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 50)) + ggtitle(expression(brms~default~gamma(0.01*&quot;, &quot;*0.01)~shape~prior)) Let’s fit the model. b12.2a &lt;- brm(data = d, family = negbinomial, total_tools ~ 1, prior = c(prior(normal(3, 0.5), class = Intercept), # beta_0 prior(gamma(0.01, 0.01), class = shape)), # alpha iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 12, file = &quot;fits/b12.02a&quot;) Notice how you use the language of family = negbinomial to fit these models with brms. Here’s the summary. print(b12.2a) ## Family: negbinomial ## Links: mu = log; shape = identity ## Formula: total_tools ~ 1 ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 3.51 0.16 3.19 3.82 1.00 2063 2047 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## shape 4.90 2.70 1.42 11.50 1.00 2254 2201 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The intercept is our estimate of \\(\\log \\mu\\), similar to \\(\\log \\lambda\\) from a simple Poisson model. The shape is our estimate of, well, the shape (\\(\\alpha\\)). To help us get a sense of what this model is, let’s use the brms::predict() function to return random samples of the poster predictive distribution. Because we want random samples instead of summary values, we will specify summary = F. Let’s take a look of what this returns. p &lt;- predict(b12.2a, summary = F) p %&gt;% str() ## num [1:4000, 1:10] 40 73 28 18 3 43 40 43 29 37 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL Because we have 4,000 posterior iterations, we also get back 4,000 rows. We have 10 columns, which correspond to the 10 rows (i.e., cultures) in the original data. In the next block, we’ll put convert that output to a data frame and wrangle a little before plotting the results. p %&gt;% data.frame() %&gt;% set_names(d$culture) %&gt;% pivot_longer(everything(), names_to = &quot;culture&quot;, values_to = &quot;lambda&quot;) %&gt;% ggplot(aes(x = lambda)) + geom_density(color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_x_continuous(expression(lambda[&quot;[culture]&quot;]), breaks = 0:2 * 100) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 210)) + facet_wrap(~ culture, nrow = 2) Because this model had no predictors, we have similar posterior-predictive distributions for each case in the data. It’s important, however, to be very clear of what these posterior-predictive distributions are of. They are not of the data, per se. Let’s look back at the text: A negative-binomial model, more usefully called a gamma-Poisson model, assumes that each Poisson count observation has its own rate. It estimates the shape of the gamma distribution to describe the Poisson rates across cases. (p. 373, emphasis in the original) As a reminder, the “rate” for the Poisson distribution is just another word for the mean, also called \\(\\lambda\\). So unlike a simple Poisson model where we use the individual cases to estimate one overall \\(\\lambda\\), here we’re presuming each case has it’s own \\(\\lambda_i\\). There are 10 \\(\\lambda_i\\) values that generated our data and if we look at those \\(\\lambda_i\\) values on the whole, their distribution can be described with a gamma distribution. And again, this is not a gamma distribution for our data. This is a gamma distribution of the \\(\\lambda_i\\) values from the 10 separate Poisson distributions that presumably made our data. After exponentiating the intercept parameter (\\(\\log \\mu\\)), here are the posterior distributions for those two gamma parameters. post &lt;- posterior_samples(b12.2a) post %&gt;% mutate(mu = exp(b_Intercept), alpha = shape) %&gt;% pivot_longer(mu:alpha, names_to = &quot;parameter&quot;) %&gt;% ggplot(aes(x = value)) + geom_density(color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Behold our gamma parameters!&quot;, x = &quot;posterior&quot;) + facet_wrap(~ parameter, scales = &quot;free&quot;, labeller = label_parsed) We might want to use these parameters estimates to visualize the model-implied gamma distribution of \\(\\lambda\\) parameters. But recall that the base R dgamma() function doesnt’ take the mean. It is based on either the shape and rate or the shape andscale`. Since we already have the shape (\\(\\alpha\\)), we need a way to compute the scale or rate. Happily, we can define the scale in terms of the mean and the shape with the equation \\[\\theta = \\frac{\\mu}{\\alpha}.\\] Behold \\(\\theta\\) in a plot. post %&gt;% mutate(mu = exp(b_Intercept), alpha = shape) %&gt;% mutate(theta = mu / alpha) %&gt;% ggplot(aes(x = theta)) + geom_density(color = &quot;transparent&quot;, fill = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(We~define~the~scale~as~theta==mu/alpha), x = &quot;posterior&quot;) + coord_cartesian(xlim = c(0, 40)) Now we know how to get both \\(\\alpha\\) and \\(\\theta\\) from the model, we can pump them into dgamma() to get a sense of the model-implied gamma distribution, the presumed underlying distribution of \\(\\lambda\\) values that generated the total_tools data. set.seed(12) # wrangle to get 200 draws post %&gt;% mutate(iter = 1:n(), alpha = shape, theta = exp(b_Intercept) / shape) %&gt;% slice_sample(n = 200) %&gt;% expand(nesting(iter, alpha, theta), x = 0:250) %&gt;% mutate(density = dgamma(x, shape = alpha, scale = theta)) %&gt;% # plot ggplot(aes(x = x, y = density)) + geom_line(aes(group = iter), alpha = .1, color = canva_pal(&quot;Green fields&quot;)(4)[4]) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(&quot;200 credible gamma densities for &quot;*lambda), x = expression(lambda)) + coord_cartesian(xlim = c(0, 170), ylim = c(0, 0.045)) Now we’ve warmed up with an intercept-only gamma-Poisson, it’s time to fit a brms version of McElreath’s m12.2. Our model formula will be \\[\\begin{align*} \\text{total_tools_i} &amp; \\sim \\operatorname{Gamma-Poisson} (\\mu_i, \\alpha) \\\\ \\mu_i &amp; = \\exp (\\beta_{0,\\text{cid}[i]}) \\text{population}_i^{\\beta_{1,\\text{cid}[i]}} / \\gamma \\\\ \\beta_{0,j} &amp; \\sim \\operatorname{Normal}(1, 1) \\\\ \\beta_{1,j} &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\gamma &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\alpha &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where \\(\\mu\\) and \\(\\alpha\\) and the mean and shape of the gamma distribution for the case-specific \\(\\lambda\\) parameters. Here’s how we might fit that model with brms. b12.2b &lt;- brm(data = d, family = negbinomial(link = &quot;identity&quot;), bf(total_tools ~ exp(b0) * population^b1 / g, b0 + b1 ~ 0 + cid, g ~ 1, nl = TRUE), prior = c(prior(normal(1, 1), nlpar = b0), prior(exponential(1), nlpar = b1, lb = 0), prior(exponential(1), nlpar = g, lb = 0), prior(exponential(1), class = shape)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 12, control = list(adapt_delta = .95), file = &quot;fits/b12.02b&quot;) Here is the model summary. print(b12.2b) ## Family: negbinomial ## Links: mu = identity; shape = identity ## Formula: total_tools ~ exp(b0) * population^b1/g ## b0 ~ 0 + cid ## b1 ~ 0 + cid ## g ~ 1 ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## b0_cidhigh 1.04 0.95 -0.81 2.90 1.00 1947 1686 ## b0_cidlow 0.93 0.85 -0.70 2.60 1.00 1702 2117 ## b1_cidhigh 0.26 0.13 0.03 0.52 1.00 1320 899 ## b1_cidlow 0.24 0.10 0.06 0.44 1.00 1647 1231 ## g_Intercept 1.04 0.84 0.13 3.18 1.00 1532 1441 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## shape 3.71 1.75 1.24 7.79 1.00 2077 2173 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Compute and check the PSIS-LOO estimates along with their diagnostic Pareto \\(k\\) values. b12.2b &lt;- add_criterion(b12.2b, &quot;loo&quot;) loo(b12.2b) ## ## Computed from 4000 by 10 log-likelihood matrix ## ## Estimate SE ## elpd_loo -41.5 1.7 ## p_loo 1.3 0.3 ## looic 82.9 3.4 ## ------ ## Monte Carlo SE of elpd_loo is NA. ## ## Pareto k diagnostic values: ## Count Pct. Min. n_eff ## (-Inf, 0.5] (good) 8 80.0% 1497 ## (0.5, 0.7] (ok) 1 10.0% 1818 ## (0.7, 1] (bad) 1 10.0% 230 ## (1, Inf) (very bad) 0 0.0% &lt;NA&gt; ## See help(&#39;pareto-k-diagnostic&#39;) for details. One of those Pareto \\(k\\) values is still on the high side. Can you guess which one that is? d %&gt;% mutate(k = b12.2b$criteria$loo$diagnostics$pareto_k) %&gt;% filter(k &gt; .7) %&gt;% select(culture, k) ## culture k ## 1 Hawaii 0.7521461 Before we can make our version of Figure 12.2, we’ll need to reload b11.11 from last chapter. One way is with the readRDS() function. b11.11 &lt;- readRDS(&quot;fits/b11.11.rds&quot;) Here we make the left panel of the figure. # the new data nd &lt;- distinct(d, cid) %&gt;% expand(cid, population = seq(from = 0, to = 300000, length.out = 100)) p1 &lt;- # compute the expected trajectories fitted(b11.11, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # plot ggplot(aes(x = population, group = cid, color = cid)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(d, b11.11$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k), alpha = 4/5) + labs(subtitle = &quot;pure Poisson model&quot;, y = &quot;total tools&quot;) Now make the right panel. # for the annotation text &lt;- distinct(d, cid) %&gt;% mutate(population = c(150000, 110000), total_tools = c(57, 69), label = str_c(cid, &quot; contact&quot;)) p2 &lt;- fitted(b12.2b, newdata = nd, probs = c(.055, .945)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = population, group = cid, color = cid)) + geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid), stat = &quot;identity&quot;, alpha = 1/4, size = 1/2) + geom_point(data = bind_cols(d, b12.2b$criteria$loo$diagnostics), aes(y = total_tools, size = pareto_k), alpha = 4/5) + geom_text(data = text, aes(y = total_tools, label = label)) + scale_y_continuous(NULL, labels = NULL) + labs(subtitle = &quot;gamma-Poisson model&quot;) Combine the two ggplots with patchwork syntax to make the full version of Figure 12.2. library(patchwork) (p1 | p2) &amp; scale_fill_manual(values = canva_pal(&quot;Green fields&quot;)(4)[c(4, 1)]) &amp; scale_color_manual(values = canva_pal(&quot;Green fields&quot;)(4)[c(4, 1)]) &amp; scale_size(range = c(2, 5)) &amp; scale_x_continuous(&quot;population&quot;, breaks = c(0, 50000, 150000, 250000)) &amp; coord_cartesian(xlim = range(d$population), ylim = range(d$total_tools)) &amp; theme(axis.ticks = element_blank(), legend.position = &quot;none&quot;) Oh man! Recall that Hawaii was a highly influential point in the pure Poisson model. It does all the work of pulling the low-contact trend down. In this new model, Hawaii is still influential, but it exerts a lot less influence on the trends. Now the high and low contact trends are much more similar, very hard to reliably distinguish. This is because the gamma-Poisson model expects rate variation, and the estimated amount of variation is quite large. Population is still strongly related to the total tools, but the influence of contact rate has greatly diminished. (p. 374) Before we move on, let’s use predict() to generate posterior predictive distributions for each of our 10 cultures. predict(b12.2b, summary = F) %&gt;% data.frame() %&gt;% set_names(d$culture) %&gt;% pivot_longer(everything(), names_to = &quot;culture&quot;, values_to = &quot;lambda&quot;) %&gt;% left_join(d) %&gt;% ggplot(aes(x = lambda, y = 0)) + stat_halfeye(point_interval = mean_qi, .width = .5, fill = canva_pal(&quot;Green fields&quot;)(4)[2], color = canva_pal(&quot;Green fields&quot;)(4)[1]) + geom_vline(aes(xintercept = total_tools), color = canva_pal(&quot;Green fields&quot;)(4)[3]) + scale_x_continuous(expression(lambda[&quot;[culture]&quot;]), breaks = 0:2 * 100) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 210)) + facet_wrap(~ culture, nrow = 2) Because we used predictors in the model, this time the posterior predictive distributions differ across the cultures. The mean and interquartile range of each distribution are marked off by the light-green dot and horizontal line below each. The vertical lines in the foreground mark off the corresponding total_tools values from the data. Recall that these distributions are not based on the total_tools values themselves, but rather are estimates of the \\(\\lambda\\) values from the underlying Poisson distributions that might have generated such total_tools values. 12.1.3 Over-dispersion, entropy, and information criteria. In terms of model comparison using information criteria, a beta-binomial model is a binomial model, and a gamma-Poisson (negative-binomial) is a Poisson model. You should not use WAIC and PSIS with these models, however, unless you are very sure of what you are doing. The reason is that while ordinary binomial and Poisson models can be aggregated and disaggregated across rows in the data, without changing any causal assumptions, the same is not true of beta-binomial and gamma-Poisson models. The reason is that a beta-binomial or gamma-Poisson likelihood applies an unobserved parameter to each row in the data. When we then go to calculate log-likelihoods, how the data are structured will determine how the beta-distributed or gamma-distributed variation enters the model. (pp. 374–375) 12.2 Zero-inflated outcomes Very often, the things we can measure are not emissions from any pure process. Instead, they are mixtures of multiple processes. Whenever there are different causes for the same observation, then a mixture model may be useful. A mixture model uses more than one simple probability distribution to model a mixture of causes. In effect, these models use more than one likelihood for the same outcome variable. Count variables are especially prone to needing a mixture treatment. The reason is that a count of zero can often arise more than one way. A “zero” means that nothing happened, and nothing can happen either because the rate of events is low or rather because the process that generates events failed to get started. (p. 376, emphasis in the original) 12.2.0.1 Rethinking: Breaking the law. McElreath discussed how advances in computing have made it possible for working scientists to define their own data generating models. If you’d like to dive deeper into the topic, check out Bürkner’s (2021a) vignette, Define custom response distributions with brms. 12.2.1 Example: Zero-inflated Poisson. Do you remember the monk data from back in Chapter 11? Here we simulate some more. This time we’ll work in a little alcohol. # define parameters prob_drink &lt;- 0.2 # 20% of days rate_work &lt;- 1 # average 1 manuscript per day # sample one year of production n &lt;- 365 # simulate days monks drink set.seed(365) drink &lt;- rbinom(n, size = 1, prob = prob_drink) # simulate manuscripts completed y &lt;- (1 - drink) * rpois(n, lambda = rate_work) We’ll put those data in a tidy tibble before plotting. d &lt;- tibble(drink = factor(drink, levels = 1:0), y = y) d %&gt;% ggplot(aes(x = y)) + geom_histogram(aes(fill = drink), binwidth = 1, size = 1/10, color = &quot;grey92&quot;) + scale_fill_manual(values = canva_pal(&quot;Green fields&quot;)(4)[1:2]) + xlab(&quot;Manuscripts completed&quot;) + theme(legend.position = &quot;none&quot;) With these data, the likelihood of observing zero on y, (i.e., the likelihood zero manuscripts were completed on a given occasion) is \\[\\begin{align*} \\operatorname{Pr}(0 | p, \\lambda) &amp; = \\operatorname{Pr}(\\text{drink} | p) + \\operatorname{Pr}(\\text{work} | p) \\times \\operatorname{Pr}(0 | \\lambda) \\\\ &amp; = p + (1 - p) \\exp (- \\lambda). \\end{align*}\\] And since the Poisson likelihood of \\(y\\) is \\(\\operatorname{Pr}(y | \\lambda) = \\lambda^y \\exp (- \\lambda) / y!\\), the likelihood of \\(y = 0\\) is just \\(\\exp (- \\lambda)\\). The above is just the mathematics for: The probability of observing a zero is the probability that the monks didn’t drink OR (\\(+\\)) the probability that the monks worked AND (\\(\\times\\)) failed to finish anything. And the likelihood of a non-zero value \\(y\\) is: \\[\\operatorname{Pr}(y | y &gt; 0, p, \\lambda) = \\operatorname{Pr}(\\text{drink} | p) (0) + \\operatorname{Pr}(\\text{work} | p) \\operatorname{Pr}(y | \\lambda) = (1 - p) \\frac {\\lambda^y \\exp (- \\lambda)}{y!}\\] Since drinking monks never produce \\(y &gt; 0\\), the expression above is just the chance the monks both work \\(1 - p\\), and finish \\(y\\) manuscripts. (pp. 377–378, emphasis in the original) So letting \\(p\\) be the probability \\(y\\) is zero and \\(\\lambda\\) be the shape of the distribution, the zero-inflated Poisson (\\(\\operatorname{ZIPoisson}\\)) regression model might take the basic form \\[\\begin{align*} y_i &amp; \\sim \\operatorname{ZIPoisson}(\\color{#5a5f37}{p_i}, \\color{#524a3a}{\\lambda_i})\\\\ \\color{#5a5f37}{\\operatorname{logit}(p_i)} &amp; \\color{#5a5f37}= \\color{#5a5f37}{\\alpha_p + \\beta_p x_i} \\\\ \\color{#524a3a}{\\log (\\lambda_i)} &amp; \\color{#524a3a}= \\color{#524a3a}{\\alpha_\\lambda + \\beta_\\lambda x_i}, \\end{align*}\\] where both parameters in the likelihood, \\(p_i\\) and \\(\\lambda_i\\) might get their own statistical model, making this a special case of what Bürkner (2021b) calls distributional models. One last thing to note is that in brms, \\(p_i\\) is denoted zi. To fit a zero-inflated Poisson model with brms, make sure to specify the correct likelihood with family = zero_inflated_poisson. To use a non-default prior for zi, make sure to indicate class = zi within the prior() function. b12.3 &lt;- brm(data = d, family = zero_inflated_poisson, y ~ 1, prior = c(prior(normal(1, 0.5), class = Intercept), prior(beta(2, 6), class = zi)), # the brms default is beta(1, 1) iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 12, file = &quot;fits/b12.03&quot;) print(b12.3) ## Family: zero_inflated_poisson ## Links: mu = log; zi = identity ## Formula: y ~ 1 ## Data: d (Number of observations: 365) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.02 0.09 -0.15 0.19 1.00 1321 1791 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## zi 0.23 0.05 0.12 0.34 1.00 1415 1558 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you look at the Zero-inflated and hurdle models section of Bürkner’s (2021h) Parameterization of response distributions in brms document, you’ll see the zero-inflated Poisson is set up a little differently in brms than it is in rethinking. The difference did not influence the estimate for the intercept, \\(\\lambda\\). In both here and in the text, \\(\\lambda\\) was about zero. However, it did influence the summary of zi. Note how McElreath’s mean( inv_logit( post$ap ) ) returned 0.2241255, which seems rather close to our zi estimate of 0.23. Hopefully it’s clear that zi in brms is already in the probability metric. There’s no need to convert it. You can further confirm this by looking at the second line from the print() output, Links: mu = log; zi = identity. When there are no predictors for zi, the brms default is to use the identity link. In the text, however, McElreath used the logit link for p. In the prior argument, we used beta(2, 6) for zi and also mentioned in the margin that the brms default is beta(1, 1). The beta distribution ranges from 0 to 1, making it a natural distribution to use for priors on probabilities when using the identity link. To give you a sense of what those two versions of the beta look like, let’s plot. A typical way to plot a beta distribution would be to use the base R dbeta() function. Let’s try a different approach, instead. The tidybayes package includes a parse_dist() function which takes the kind of string specifications you would usually include in the brms::prior() function and converts them into a format one can plot with. For example, here’s what the preparatory work would be in our case. priors &lt;- c(prior(beta(1, 1), class = zi), prior(beta(2, 6), class = zi)) priors %&gt;% parse_dist(prior) ## prior class coef group resp dpar nlpar bound source .dist .args ## 1 beta(1, 1) zi user beta 1, 1 ## 2 beta(2, 6) zi user beta 2, 6 The first several columns look a lot like the kind of output we’d get from the brms::get_prior() function. The parse_dist() function added those last two columns. Here we put them to work by feeding them into a ggplot. priors %&gt;% parse_dist(prior) %&gt;% ggplot(aes(y = prior, dist = .dist, args = .args, fill = prior)) + stat_dist_halfeye(.width = .95) + scale_fill_manual(values = canva_pal(&quot;Green fields&quot;)(4)[c(4, 1)]) + scale_x_continuous(&quot;zi&quot;, breaks = c(0, .5, 1)) + ylab(NULL) + theme(legend.position = &quot;none&quot;) Whereas the brms default is flat, our prior guided the posterior a bit toward 0. In case you were curious, we might write our statistical model for b12.3 as \\[\\begin{align*} y_i &amp; \\sim \\operatorname{ZIPoisson} (p, \\lambda) \\\\ p &amp; = \\alpha_p \\\\ \\log \\lambda &amp; = \\alpha_\\lambda \\\\ \\alpha_p &amp; \\sim \\operatorname{Beta}(2, 6) \\\\ \\alpha_\\lambda &amp; \\sim \\operatorname{Normal}(1, 0.5). \\end{align*}\\] Anyway, here’s that exponentiated \\(\\alpha_\\lambda\\). fixef(b12.3)[1, ] %&gt;% exp() ## Estimate Est.Error Q2.5 Q97.5 ## 1.017924 1.091458 0.858510 1.205584 12.2.1.1 Overthinking: Zero-inflated Poisson calculations in Stan. If you’re curious, here’s the Stan code underlying our brms fit, b12.3. b12.3$model ## // generated with brms 2.15.0 ## functions { ## /* zero-inflated poisson log-PDF of a single response ## * Args: ## * y: the response value ## * lambda: mean parameter of the poisson distribution ## * zi: zero-inflation probability ## * Returns: ## * a scalar to be added to the log posterior ## */ ## real zero_inflated_poisson_lpmf(int y, real lambda, real zi) { ## if (y == 0) { ## return log_sum_exp(bernoulli_lpmf(1 | zi), ## bernoulli_lpmf(0 | zi) + ## poisson_lpmf(0 | lambda)); ## } else { ## return bernoulli_lpmf(0 | zi) + ## poisson_lpmf(y | lambda); ## } ## } ## /* zero-inflated poisson log-PDF of a single response ## * logit parameterization of the zero-inflation part ## * Args: ## * y: the response value ## * lambda: mean parameter of the poisson distribution ## * zi: linear predictor for zero-inflation part ## * Returns: ## * a scalar to be added to the log posterior ## */ ## real zero_inflated_poisson_logit_lpmf(int y, real lambda, real zi) { ## if (y == 0) { ## return log_sum_exp(bernoulli_logit_lpmf(1 | zi), ## bernoulli_logit_lpmf(0 | zi) + ## poisson_lpmf(0 | lambda)); ## } else { ## return bernoulli_logit_lpmf(0 | zi) + ## poisson_lpmf(y | lambda); ## } ## } ## /* zero-inflated poisson log-PDF of a single response ## * log parameterization for the poisson part ## * Args: ## * y: the response value ## * eta: linear predictor for poisson distribution ## * zi: zero-inflation probability ## * Returns: ## * a scalar to be added to the log posterior ## */ ## real zero_inflated_poisson_log_lpmf(int y, real eta, real zi) { ## if (y == 0) { ## return log_sum_exp(bernoulli_lpmf(1 | zi), ## bernoulli_lpmf(0 | zi) + ## poisson_log_lpmf(0 | eta)); ## } else { ## return bernoulli_lpmf(0 | zi) + ## poisson_log_lpmf(y | eta); ## } ## } ## /* zero-inflated poisson log-PDF of a single response ## * log parameterization for the poisson part ## * logit parameterization of the zero-inflation part ## * Args: ## * y: the response value ## * eta: linear predictor for poisson distribution ## * zi: linear predictor for zero-inflation part ## * Returns: ## * a scalar to be added to the log posterior ## */ ## real zero_inflated_poisson_log_logit_lpmf(int y, real eta, real zi) { ## if (y == 0) { ## return log_sum_exp(bernoulli_logit_lpmf(1 | zi), ## bernoulli_logit_lpmf(0 | zi) + ## poisson_log_lpmf(0 | eta)); ## } else { ## return bernoulli_logit_lpmf(0 | zi) + ## poisson_log_lpmf(y | eta); ## } ## } ## // zero-inflated poisson log-CCDF and log-CDF functions ## real zero_inflated_poisson_lccdf(int y, real lambda, real zi) { ## return bernoulli_lpmf(0 | zi) + poisson_lccdf(y | lambda); ## } ## real zero_inflated_poisson_lcdf(int y, real lambda, real zi) { ## return log1m_exp(zero_inflated_poisson_lccdf(y | lambda, zi)); ## } ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## int Y[N]; // response variable ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## real Intercept; // temporary intercept for centered predictors ## real&lt;lower=0,upper=1&gt; zi; // zero-inflation probability ## } ## transformed parameters { ## } ## model { ## // likelihood including constants ## if (!prior_only) { ## // initialize linear predictor term ## vector[N] mu = Intercept + rep_vector(0.0, N); ## for (n in 1:N) { ## target += zero_inflated_poisson_log_lpmf(Y[n] | mu[n], zi); ## } ## } ## // priors including constants ## target += normal_lpdf(Intercept | 1, 0.5); ## target += beta_lpdf(zi | 2, 6); ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept; ## } 12.3 Ordered categorical outcomes It is very common in the social sciences, and occasional in the natural sciences, to have an outcome variable that is discrete, like a count, but in which the values merely indicate different ordered levels along some dimension. For example, if I were to ask you how much you like to eat fish,on a scale from 1 to 7, you might say 5. If I were to ask 100 people the same question, I’d end up with 100 values between 1 and 7. In modeling each outcome value, I’d have to keep in mind that these values are ordered, because 7 is greater than 6, which is greater than 5, and so on. The result is a set of ordered categories. Unlike a count, the differences in value are not necessarily equal…. In principle, an ordered categorical variable is just a multinomial prediction problem (page 359). But the constraint that the categories be ordered demands special treatment…. The conventional solution is to use a cumulative link function. The cumulative probability of a value is the probability of that value or any smaller value. In the context of ordered categories, the cumulative probability of 3 is the sum of the probabilities of 3, 2, and 1. Ordered categories by convention begin at 1, so a result less than 1 has no probability at all. By linking a linear model to cumulative probability, it is possible to guarantee the ordering of the outcomes. (p. 380, emphasis in the original) 12.3.1 Example: Moral intuition. Let’s get the Trolley data from rethinking (see Cushman et al., 2006). data(Trolley, package = &quot;rethinking&quot;) d &lt;- Trolley rm(Trolley) Use the dplyr::glimpse() to get a sense of the dimensions of the data. glimpse(d) ## Rows: 9,930 ## Columns: 12 ## $ case &lt;fct&gt; cfaqu, cfbur, cfrub, cibox, cibur, cispe, fkaqu, fkboa, fkbox, fkbur, fkcar, fkspe, fkswi,… ## $ response &lt;int&gt; 4, 3, 4, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 5, 4, 4, 3, 4, 4, … ## $ order &lt;int&gt; 2, 31, 16, 32, 4, 9, 29, 12, 23, 22, 27, 19, 14, 3, 18, 15, 30, 5, 1, 13, 20, 17, 28, 10, … ## $ id &lt;fct&gt; 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96… ## $ age &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14… ## $ male &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ edu &lt;fct&gt; Middle School, Middle School, Middle School, Middle School, Middle School, Middle School, … ## $ action &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, … ## $ intention &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, … ## $ contact &lt;int&gt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ story &lt;fct&gt; aqu, bur, rub, box, bur, spe, aqu, boa, box, bur, car, spe, swi, boa, car, che, sha, swi, … ## $ action2 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, … Though we have 9,930 rows, we only have 331 unique individuals. d %&gt;% distinct(id) %&gt;% count() ## n ## 1 331 12.3.2 Describing an ordered distribution with intercepts. Make our version of the simple Figure 12.4 histogram of our primary variable, response. p1 &lt;- d %&gt;% ggplot(aes(x = response, fill = ..x..)) + geom_histogram(binwidth = 1/4, size = 0) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(breaks = 1:7) + theme(axis.ticks = element_blank(), axis.title.y = element_text(angle = 90), legend.position = &quot;none&quot;) p1 Our cumulative proportion plot, Figure 12.4.b, will require some pre-plot wrangling. p2 &lt;- d %&gt;% count(response) %&gt;% mutate(pr_k = n / nrow(d), cum_pr_k = cumsum(pr_k)) %&gt;% ggplot(aes(x = response, y = cum_pr_k, fill = response)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + geom_point(shape = 21, color = &quot;grey92&quot;, size = 2.5, stroke = 1) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;cumulative proportion&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + theme(axis.ticks = element_blank(), axis.title.y = element_text(angle = 90), legend.position = &quot;none&quot;) p2 Then to re-describe the histogram as log-cumulative odds, we’ll need a series of intercept parameters. Each intercept will be on the log-cumulative-odds scale and stand in for the cumulative probability of each outcome. So this is just the application of the link function. The log-cumulative-odds that a response value \\(y_i\\) is equal-to-or-less-than some possible outcome value \\(k\\) is: \\[\\log \\frac{\\operatorname{Pr}(y_i \\leq k)}{1 - \\operatorname{Pr}(y_i \\leq k)} = \\alpha_k\\] where \\(\\alpha_k\\) is an “intercept” unique to each possible outcome value \\(k\\). (p. 383) We can compute the \\(\\alpha_k\\) estimates directly with a little help from McElreath’s custom logit() function. logit &lt;- function(x) log(x / (1 - x)) # convenience function d %&gt;% count(response) %&gt;% mutate(pr_k = n / nrow(d), cum_pr_k = cumsum(n / nrow(d))) %&gt;% mutate(alpha = logit(cum_pr_k) %&gt;% round(digits = 2)) ## response n pr_k cum_pr_k alpha ## 1 1 1274 0.12829809 0.1282981 -1.92 ## 2 2 909 0.09154079 0.2198389 -1.27 ## 3 3 1071 0.10785498 0.3276939 -0.72 ## 4 4 2323 0.23393756 0.5616314 0.25 ## 5 5 1462 0.14723061 0.7088620 0.89 ## 6 6 1445 0.14551863 0.8543807 1.77 ## 7 7 1446 0.14561934 1.0000000 Inf Now we plot those joints to make our version of Figure 12.4.c. p3 &lt;- d %&gt;% count(response) %&gt;% mutate(cum_pr_k = cumsum(n / nrow(d))) %&gt;% filter(response &lt; 7) %&gt;% # we can do the `logit()` conversion right in `ggplot() ggplot(aes(x = response, y = logit(cum_pr_k), fill = response)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(breaks = 1:7, limits = c(1, 7)) + ylab(&quot;log-cumulative-odds&quot;) + theme(axis.ticks = element_blank(), axis.title.y = element_text(angle = 90), legend.position = &quot;none&quot;) p3 Why not combine the three subplots with patchwork? (p1 | p2 | p3) + plot_annotation(title = &quot;Re-describing a discrete distribution using log-cumulative-odds.&quot;) The code for Figure 12.5 is itself something of a monster. # primary data d_plot &lt;- d %&gt;% count(response) %&gt;% mutate(pr_k = n / nrow(d), cum_pr_k = cumsum(n / nrow(d))) %&gt;% mutate(discrete_probability = ifelse(response == 1, cum_pr_k, cum_pr_k - pr_k)) # annotation text &lt;- tibble(text = 1:7, response = seq(from = 1.25, to = 7.25, by = 1), cum_pr_k = d_plot$cum_pr_k - .065) d_plot %&gt;% ggplot(aes(x = response, y = cum_pr_k, color = cum_pr_k, fill = cum_pr_k)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[1]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 2.5, stroke = 1) + geom_linerange(aes(ymin = 0, ymax = cum_pr_k), alpha = 1/2, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + geom_linerange(aes(x = response + .025, ymin = ifelse(response == 1, 0, discrete_probability), ymax = cum_pr_k), color = &quot;black&quot;) + # number annotation geom_text(data = text, aes(label = text), size = 4) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;cumulative proportion&quot;, breaks = c(0, .5, 1), limits = c(0, 1)) + theme(axis.ticks = element_blank(), axis.title.y = element_text(angle = 90), legend.position = &quot;none&quot;) A compact way to express the formula for this first type of statistical model is \\[\\begin{align*} \\text{response}_i &amp; \\sim \\operatorname{Categorical} (\\mathbf p) \\\\ \\operatorname{logit}(p_k) &amp; = \\alpha_k - \\phi \\\\ \\phi &amp; = 0 \\\\ \\alpha_k &amp; \\sim \\operatorname{Normal}(0, 1.5), \\end{align*}\\] where the \\(\\alpha_k\\) term denotes the \\(K - 1\\) intercepts (cut points or thresholds) we use to describe each possible outcome value \\(k\\) and. The mysterious looking \\(\\phi\\) term is a stand-in for the potential terms of the linear model. In the case where we have no predictors, it’s just 0. Just hold on to your hats; this will make more sense in the next section. An ordered-logit distribution is really just a categorical distribution that takes a vector \\(\\mathbf p = \\{p_1, p_2, p_3, p_4, p_5, p_6\\}\\) of probabilities of each response value below the maximum response (7 in this example). Each response value \\(k\\) in this vector is defined by its link to an intercept parameter, \\(\\alpha_k\\). Finally, some weakly regularizing priors are placed on these intercepts. (p. 385) Whereas in rethinking::ulam() you indicate the likelihood by &lt;criterion&gt; ~ dordlogit(0 , c(&lt;the thresholds&gt;), in brms::brm() you code family = cumulative. Here’s how to fit the intercepts-only model. # define the start values inits &lt;- list(`Intercept[1]` = -2, `Intercept[2]` = -1, `Intercept[3]` = 0, `Intercept[4]` = 1, `Intercept[5]` = 2, `Intercept[6]` = 2.5) inits_list &lt;- list(inits, inits, inits, inits) b12.4 &lt;- brm(data = d, family = cumulative, response ~ 1, prior(normal(0, 1.5), class = Intercept), iter = 2000, warmup = 1000, cores = 4, chains = 4, inits = inits_list, # here we add our start values file = &quot;fits/b12.04&quot;) McElreath needed to include the depth=2 argument in the rethinking::precis() function to show the threshold parameters from his m11.1stan model (R code 12.24). With a brm() fit, we just use print() or summary() as usual. print(b12.4) ## Family: cumulative ## Links: mu = logit; disc = identity ## Formula: response ~ 1 ## Data: d (Number of observations: 9930) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -1.92 0.03 -1.98 -1.86 1.00 2667 2679 ## Intercept[2] -1.27 0.02 -1.31 -1.22 1.00 3787 3333 ## Intercept[3] -0.72 0.02 -0.76 -0.68 1.00 4319 3563 ## Intercept[4] 0.25 0.02 0.21 0.29 1.00 4859 3437 ## Intercept[5] 0.89 0.02 0.85 0.93 1.00 4731 3274 ## Intercept[6] 1.77 0.03 1.72 1.83 1.00 5131 3576 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 1.00 4000 4000 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). What McElreath’s m12.4 summary termed cutpoints[k], our brms summary termed Intercept[k]. In both cases, these are the \\(\\alpha_k\\) parameters from the formula, above (i.e., the thresholds). The summaries look like those in the text, the \\(\\widehat R\\) values are great, and both measures of effective sample size are high. The results looks good. We can use the brms::inv_logit_scaled() function to get these into the probability metric. b12.4 %&gt;% fixef() %&gt;% inv_logit_scaled() %&gt;% round(digits = 3) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept[1] 0.128 0.508 0.122 0.135 ## Intercept[2] 0.220 0.506 0.212 0.229 ## Intercept[3] 0.328 0.505 0.318 0.337 ## Intercept[4] 0.562 0.505 0.552 0.571 ## Intercept[5] 0.709 0.505 0.700 0.718 ## Intercept[6] 0.854 0.507 0.848 0.861 But recall that the posterior \\(SD\\) (i.e., the ‘Est.Error’ values) are not valid using that approach. If you really care about them, you’ll need to work with the posterior_samples(). posterior_samples(b12.4) %&gt;% mutate_all(inv_logit_scaled) %&gt;% pivot_longer(starts_with(&quot;b_&quot;)) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value), sd = sd(value), ll = quantile(value, probs = .025), ul = quantile(value, probs = .975)) ## # A tibble: 6 x 5 ## name mean sd ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_Intercept[1] 0.128 0.00339 0.122 0.135 ## 2 b_Intercept[2] 0.220 0.00426 0.212 0.229 ## 3 b_Intercept[3] 0.328 0.00465 0.318 0.337 ## 4 b_Intercept[4] 0.562 0.00481 0.552 0.571 ## 5 b_Intercept[5] 0.709 0.00444 0.700 0.718 ## 6 b_Intercept[6] 0.854 0.00350 0.848 0.861 Just to confirm, those posterior means are centered right around the cum_pr_k we computed for Figure 12.4. d_plot %&gt;% select(response, cum_pr_k) ## response cum_pr_k ## 1 1 0.1282981 ## 2 2 0.2198389 ## 3 3 0.3276939 ## 4 4 0.5616314 ## 5 5 0.7088620 ## 6 6 0.8543807 ## 7 7 1.0000000 To walk out our results even further, we can make b12.4-based version of Figure 12.4.c after formatting our posterior summary a little. fixef(b12.4) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;intercept&quot;) %&gt;% mutate(response = str_extract(intercept, &quot;\\\\d&quot;) %&gt;% as.double()) %&gt;% ggplot(aes(x = response, y = Estimate, ymin = Q2.5, ymax = Q97.5, fill = response)) + geom_line(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + geom_point(shape = 21, colour = &quot;grey92&quot;, size = 1.5, stroke = 1) + geom_linerange(color = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(breaks = 1:7, limits = c(1, 7)) + ylab(&quot;log-cumulative-odds&quot;) + theme(axis.ticks = element_blank(), axis.title.y = element_text(angle = 90), legend.position = &quot;none&quot;) Now the dots are the posterior means and the vertical lines layered on top of them are their 95% posterior intervals. Given the large amount of data, the posteriors for our \\(\\alpha_k\\) parameters are rather narrow, which is expressed in tightness of those vertical lines. 12.3.3 Adding predictor variables. Now we define the generic linear model as \\(\\phi_i = \\beta x_i\\). Accordingly, the formula for our cumulative logit model becomes \\[\\begin{align*} \\log \\frac{\\operatorname{Pr}(y_i \\leq k)}{1 - \\operatorname{Pr}(y_i \\leq k)} &amp; = \\alpha_k - \\phi_i \\\\ \\phi_i &amp; = \\beta x_i. \\end{align*}\\] This form automatically ensures the correct ordering of the outcome values, while still morphing the likelihood of each individual value as the predictor \\(x_i\\) changes value. Why is the linear model \\(\\phi\\) subtracted from each intercept? Because if we decrease the log-cumulative-odds of every outcome value \\(k\\) below the maximum, this necessarily shifts probability mass upwards towards higher outcome values. So then positive values of \\(\\beta\\) mean increasing \\(x\\) also increases the mean \\(y\\). You could add \\(\\phi\\) instead like \\(\\alpha_k + \\phi_i\\). But then \\(\\beta &gt; 0\\) would indicate increasing \\(x\\) decreases the mean. (p. 386) I’m not aware that brms has an equivalent to the rethinking::dordlogit() function. So here we’ll make it by hand. The code comes from McElreath’s GitHub repo for rethinking. logistic &lt;- function(x) { p &lt;- 1 / (1 + exp(-x)) p &lt;- ifelse(x == Inf, 1, p) p } # now we get down to it dordlogit &lt;- function(x, phi, a, log = FALSE) { a &lt;- c(as.numeric(a), Inf) p &lt;- logistic(a[x] - phi) na &lt;- c(-Inf, a) np &lt;- logistic(na[x] - phi) p &lt;- p - np if (log == TRUE) p &lt;- log(p) p } The dordlogit() function works like this: pk &lt;- dordlogit(1:7, 0, fixef(b12.4)[, 1]) pk %&gt;% round(digits = 2) ## [1] 0.13 0.09 0.11 0.23 0.15 0.15 0.15 Note the slight difference in how we used dordlogit() with a brm() fit summarized by fixef() than the way McElreath did with a ulam() fit summarized by coef(). McElreath just put coef(m12.4) into dordlogit(). We, however, more specifically placed fixef(b12.4)[, 1] into the function. With the [, 1] part, we specified that we were working with the posterior means (i.e., Estimate) and neglecting the other summaries (i.e., the posterior SDs and 95% intervals). If you forget to subset, chaos ensues. Next, as McElreath further noted on page 338, “these probabilities imply an average outcome of:” sum(pk * (1:7)) ## [1] 4.198906 I found that a bit abstract. Here’s the thing in a more elaborate tibble format. ( explicit_example &lt;- tibble(probability_of_a_response = pk) %&gt;% mutate(the_response = 1:7) %&gt;% mutate(their_product = probability_of_a_response * the_response) ) ## # A tibble: 7 x 3 ## probability_of_a_response the_response their_product ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.128 1 0.128 ## 2 0.0916 2 0.183 ## 3 0.108 3 0.323 ## 4 0.234 4 0.936 ## 5 0.147 5 0.736 ## 6 0.146 6 0.873 ## 7 0.146 7 1.02 explicit_example %&gt;% summarise(average_outcome_value = sum(their_product)) ## # A tibble: 1 x 1 ## average_outcome_value ## &lt;dbl&gt; ## 1 4.20 Now we’ll try it by subtracting .5 from each. # the probabilities of a given response pk &lt;- dordlogit(1:7, 0, fixef(b12.4)[, 1] - .5) pk %&gt;% round(digits = 2) ## [1] 0.08 0.06 0.08 0.21 0.16 0.18 0.22 # the average rating sum(pk * (1:7)) ## [1] 4.729349 So the rule is we subtract the linear model from each intercept. “This way, a positive \\(\\beta\\) value indicates that an increase in the predictor variable \\(x\\) results in an increase in the average response” (p. 387). Happily, even though this makes for a somewhat confusing statistical formula, we still enter our predictor terms into the brm() formula argument much the same way we always have. As to our upcoming model, we might express the statistical formula as \\[\\begin{align*} \\text{response}_i &amp; \\sim \\operatorname{Categorical} (\\mathbf p) \\\\ \\text{logit}(p_k) &amp; = \\alpha_k - \\phi_i \\\\ \\phi_i &amp; = \\beta_1 \\text{action}_i + \\beta_2 \\text{contact}_i + (\\beta_3 + \\beta_4 \\text{action}_i + \\beta_5 \\text{contact}_i) \\text{intention}_i \\\\ \\alpha_k &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\beta_1, \\dots, \\beta_5 &amp; \\sim \\operatorname{Normal}(0, 0.5), \\end{align*}\\] where, because we have included predictors, \\(\\phi\\) is no longer set to 0. Using our skills from back in Chapter 8, we might also rewrite the linear model for \\(\\phi\\) as \\[\\phi_i = \\beta_1 \\text{action}_i + \\beta_2 \\text{contact}_i + \\beta_3 \\text{intention}_i + \\beta_4 (\\text{action}_i \\times \\text{intention}_i) + \\beta_5 (\\text{contact}_i \\times \\text{intention}_i).\\] Let’s fit the model. b12.5 &lt;- brm(data = d, family = cumulative, response ~ 1 + action + contact + intention + intention:action + intention:contact, prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 12, file = &quot;fits/b12.05&quot;) There are ways with brms to mirror how McElreath coded his phi &lt;- bA*A + bC*C + BI*I and BI &lt;- bI + bIA*A + bIC*C. Here we just used a more conventional style of syntax. Behold the summary. print(b12.5) ## Family: cumulative ## Links: mu = logit; disc = identity ## Formula: response ~ 1 + action + contact + intention + intention:action + intention:contact ## Data: d (Number of observations: 9930) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -2.64 0.05 -2.74 -2.54 1.00 3416 2886 ## Intercept[2] -1.94 0.05 -2.04 -1.85 1.00 3378 3244 ## Intercept[3] -1.35 0.05 -1.44 -1.26 1.00 3028 3135 ## Intercept[4] -0.31 0.04 -0.40 -0.23 1.00 3048 3253 ## Intercept[5] 0.36 0.04 0.27 0.45 1.00 2884 2963 ## Intercept[6] 1.26 0.05 1.17 1.36 1.00 3188 3102 ## action -0.48 0.05 -0.59 -0.37 1.00 3008 3148 ## contact -0.35 0.07 -0.48 -0.21 1.00 3241 2961 ## intention -0.30 0.06 -0.41 -0.19 1.00 2880 3121 ## action:intention -0.43 0.08 -0.58 -0.27 1.01 3054 2819 ## contact:intention -1.23 0.10 -1.42 -1.04 1.00 2960 2746 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 1.00 4000 4000 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). For a little variety, we’ll make our coefficient plot with a little help from the tidybayes::stat_gradientinterval() function. labs &lt;- str_c(&quot;beta[&quot;, 1:5, &quot;]&quot;) posterior_samples(b12.5) %&gt;% select(b_action:`b_contact:intention`) %&gt;% set_names(labs) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = 0, alpha = 1/5, linetype = 3) + stat_gradientinterval(.width = .5, size = 1, point_size = 3/2, shape = 21, point_fill = canva_pal(&quot;Green fields&quot;)(4)[3], fill = canva_pal(&quot;Green fields&quot;)(4)[1], color = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_x_continuous(&quot;marginal posterior&quot;, breaks = -5:0 / 4) + scale_y_discrete(NULL, labels = parse(text = labs)) + coord_cartesian(xlim = c(-1.4, 0)) As always, this will all be easier to see if we plot the posterior predictions. There is no perfect way to plot the predictions of these log-cumulative-odds models. Why? Because each prediction is really a vector of probabilities, one for each possible outcome value. So as a predictor variable changes value, the entire vector changes. This kind of thing can be visualized in several different ways. (p. 388) Our approach to making the top panels of Figure 12.6 will start with fitted(). nd &lt;- d %&gt;% distinct(action, contact, intention) %&gt;% mutate(combination = str_c(action, contact, intention, sep = &quot;_&quot;)) f &lt;- fitted(b12.5, newdata = nd, summary = F) # what have we done? f %&gt;% str() ## num [1:4000, 1:6, 1:7] 0.1053 0.0843 0.0858 0.0903 0.0947 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : NULL ## ..$ : NULL ## ..$ : chr [1:7] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... That returned a three-dimensional array. The 4,000 rows correspond to the 4,000 post-warmup iterations. The six columns correspond to the six unique combinations of action, contact, and intention within the data (i.e., d %&gt;% distinct(action, contact, intention)). The three levels of the third dimension correspond to the seven levels of our response variable. This is all the information we need to plot our posterior in the triptych in the top row of Figure 12.6. It will take a bit of tricky wrangling to get it into a useful format. Our first several steps have to do with arranging the data into a long tibble format. f &lt;- rbind(f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5], f[, , 6], f[, , 7]) %&gt;% data.frame() %&gt;% set_names(pull(nd, combination)) %&gt;% mutate(response = rep(1:7, each = n() / 7), iter = rep(1:4000, times = 7)) %&gt;% pivot_longer(-c(iter, response), names_to = c(&quot;action&quot;, &quot;contact&quot;, &quot;intention&quot;), names_sep = &quot;_&quot;, values_to = &quot;pk&quot;) %&gt;% mutate(intention = intention %&gt;% as.integer()) # how do the data look, now? glimpse(f) ## Rows: 168,000 ## Columns: 6 ## $ response &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ iter &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, … ## $ action &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;1&quot;, &quot;0&quot;, &quot;1&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;1&quot;, &quot;0&quot;, &quot;1&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;1&quot;, &quot;0&quot;, &quot;1&quot;, &quot;0&quot;, … ## $ contact &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;1&quot;, &quot;1&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;1&quot;, &quot;1&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, … ## $ intention &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, … ## $ pk &lt;dbl&gt; 0.10528275, 0.32005882, 0.11230126, 0.06815842, 0.19022633, 0.09336426, 0.08428358, 0.3186… We’re moving pretty quickly, here. If it wasn’t apparent, the original values in the f data were in the probability metric. This is why we followed the convention from earlier in this section and named them pk. However, we need them to be in the cumulative-probability metric to make the top panels of the figure. # to order our factor levels for `facet` levels &lt;- c(&quot;action=0, contact=0&quot;, &quot;action=1, contact=0&quot;, &quot;action=0, contact=1&quot;) p1 &lt;- f %&gt;% # unnecessary for these plots filter(response &lt; 7) %&gt;% # this will help us define the three panels of the triptych mutate(facet = factor(str_c(&quot;action=&quot;, action, &quot;, contact=&quot;, contact), levels = levels)) %&gt;% # these next three lines allow us to compute the cumulative probabilities group_by(iter, facet, intention) %&gt;% arrange(iter, facet, intention, response) %&gt;% mutate(probability = cumsum(pk)) %&gt;% ungroup() %&gt;% # these next three lines are how we randomly selected 50 posterior draws nest(data = -iter) %&gt;% slice_sample(n = 50) %&gt;% unnest(data) %&gt;% # plot! ggplot(aes(x = intention, y = probability)) + geom_line(aes(group = interaction(iter, response), color = probability), alpha = 1/10) + geom_point(data = d %&gt;% # wrangle the original data to make the dots group_by(intention, contact, action) %&gt;% count(response) %&gt;% mutate(probability = cumsum(n / sum(n)), facet = factor(str_c(&quot;action=&quot;, action, &quot;, contact=&quot;, contact), levels = levels)) %&gt;% filter(response &lt; 7), color = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(breaks = c(0, .5, 1), limits = 0:1) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ facet) We will look at the results of our code in just a bit. For now, we’ll focus on the code for the triptych in the bottom panels of Figure 12.6. These plots will be based on predict(). p &lt;- predict(b12.5, newdata = nd, nsamples = 1000, scale = &quot;response&quot;, summary = F) p %&gt;% str() ## num [1:1000, 1:6] 3 1 5 7 1 4 1 7 2 2 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL ## - attr(*, &quot;levels&quot;)= chr [1:7] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... This time we have a simple two-dimensional array. There are only 1,000 rows because we set nsamples = 1000. Much like with fitted(), above, the six columns correspond to the six unique combinations of our three predictor variables. With the scale = \"response\" argument, we requested our results were in the metric of the original data, which were response values ranging from 1 to 7. Compared to the last plot, the post-predict() data wrangling for this triptych is low-key. We just need the data in a long tibble format that includes a variable with which we might facet. p2 &lt;- p %&gt;% data.frame() %&gt;% set_names(pull(nd, combination)) %&gt;% pivot_longer(everything(), names_to = c(&quot;action&quot;, &quot;contact&quot;, &quot;intention&quot;), names_sep = &quot;_&quot;, values_to = &quot;response&quot;) %&gt;% mutate(facet = factor(str_c(&quot;action=&quot;, action, &quot;, contact=&quot;, contact), levels = levels)) %&gt;% ggplot(aes(x = response, fill = intention)) + geom_bar(width = 1/3, position = position_dodge(width = .4)) + scale_fill_manual(values = canva_pal(&quot;Green fields&quot;)(4)[2:1]) + scale_x_continuous(&quot;response&quot;, breaks = 1:7) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ facet) Finally, we’re ready to combine our two triptychs into one glorious remake of Figure 12.6. (p1 / p2) &amp; theme(panel.background = element_rect(fill = &quot;grey94&quot;)) Just for kicks and giggles, I’d like to make an alternative version of Figure 12.6. The triptych on the top panels did a pretty good job depicting the model in terms of the thresholds, \\(\\alpha_k\\). It’s important that we, the data analysts, have a good sense of what those are. However, I suspect many of our substantively-oriented colleagues will find themselves confused by a plot like that. In my field, people generally just want to know What’s the mean for each group? At this point, you and I know that such a question is a bit impoverished compared the to the rich output from a model like this. But if we do want to boil these analyses down to comparisons of means, McElreath has already showed us how. Look back to R code 12.20 through 12.23 (pp. 386–387). In those blocks, we multiplied the vector of probability values (pk) by their respective response values and summed, which produced an average outcome value. We can use that same approach so that our top triptych might express the results of the model in terms of means rather than parameters. All it takes is a slightly amended wrangling workflow with respect to the f data. p1 &lt;- f %&gt;% mutate(facet = factor(str_c(&quot;action=&quot;, action, &quot;, contact=&quot;, contact), levels = levels)) %&gt;% group_by(iter, facet, intention) %&gt;% summarise(mean_response = sum(pk * response)) %&gt;% ungroup() %&gt;% nest(data = -iter) %&gt;% slice_sample(n = 50) %&gt;% unnest(data) %&gt;% ggplot(aes(x = intention, y = mean_response)) + geom_line(aes(group = iter, color = mean_response), alpha = 1/10) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[1]) + scale_x_continuous(&quot;intention&quot;, breaks = 0:1) + scale_y_continuous(&quot;resopnse&quot;, breaks = 1:7, limits = c(1, 7)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ facet) I really like how intuitive the histograms were in McElreath’s bottom triptych. A limitation of that approach, however, is there is no direct expression of uncertainty. To address this, we might recall that the bars in the histogram are basically just reparameterizations of the pk values already in our f data and, happily, our vector of pk values already contains the uncertainty in the posterior. One way we might express that is with the tidybayes::stat_ccdfinterval() function, which will return a bar plot where the top parts of the bars depict our uncertainty in terms of cumulative density curves. p2 &lt;- f %&gt;% mutate(facet = factor(str_c(&quot;action=&quot;, action, &quot;, contact=&quot;, contact), levels = levels)) %&gt;% ggplot(aes(x = response, y = pk, fill = factor(intention))) + stat_ccdfinterval(.width = .95, justification = 1, size = 1/4, shape = 21, point_fill = canva_pal(&quot;Green fields&quot;)(4)[3], point_size = 1/3, position = &quot;dodge&quot;, width = .75) + scale_fill_manual(values = canva_pal(&quot;Green fields&quot;)(4)[2:1]) + scale_x_continuous(&quot;resopnse&quot;, breaks = 1:7) + scale_y_continuous(&quot;count&quot;, breaks = 0:3 / 10, labels = 0:3 * 100, limits = c(0, NA)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ facet) Here’s our alternative plot. p1 / p2 I suspect our stat_ccdfinterval() approach would work better in plots with fewer bars. But hopefully it gives you some ideas. 12.3.3.1 Rethinking: Staring into the abyss. The plotting code for ordered logistic models is complicated, compared to that of models from previous chapters. But as models become more monstrous, so too does the code needed to compute predictions and display them. With power comes hardship. It’s better to see the guts of the machine than to live in awe or fear of it. (p. 391) 12.4 Ordered categorical predictors We can handle ordered outcome variables using a categorical model with a cumulative link. That was the previous section. What about ordered predictor variables? We could just include them as continuous predictors like in any linear model. But this isn’t ideal. Just like with ordered outcomes, we don’t really want to assume that the distance between each ordinal value is the same. Luckily, we don’t have to. (p. 391) Here are the eight levels of edu. distinct(d, edu) ## edu ## 1 Middle School ## 2 Bachelor&#39;s Degree ## 3 Some College ## 4 Master&#39;s Degree ## 5 High School Graduate ## 6 Graduate Degree ## 7 Some High School ## 8 Elementary School McElreath defined his edu_new variable with an impressively compact couple lines of code. I’m going to take a more explicit approach with the dplyr::recode() function. d &lt;- d %&gt;% mutate(edu_new = recode(edu, &quot;Elementary School&quot; = 1, &quot;Middle School&quot; = 2, &quot;Some High School&quot; = 3, &quot;High School Graduate&quot; = 4, &quot;Some College&quot; = 5, &quot;Bachelor&#39;s Degree&quot; = 6, &quot;Master&#39;s Degree&quot; = 7, &quot;Graduate Degree&quot; = 8) %&gt;% as.integer()) # what did we do? d %&gt;% distinct(edu, edu_new) %&gt;% arrange(edu_new) ## edu edu_new ## 1 Elementary School 1 ## 2 Middle School 2 ## 3 Some High School 3 ## 4 High School Graduate 4 ## 5 Some College 5 ## 6 Bachelor&#39;s Degree 6 ## 7 Master&#39;s Degree 7 ## 8 Graduate Degree 8 The prior often used to handle monotonic effects is the Dirichlet distribution. The Dirichlet distribution is the multivariate extension of the beta distribution, which we met back in Section 12.1.1. Here we follow McElreath’s R code 12.32 to simulate a few draws from the Dirichlet distribution. library(gtools) set.seed(1805) delta &lt;- rdirichlet(10, alpha = rep(2, 7)) str(delta) ## num [1:10, 1:7] 0.1053 0.2504 0.1917 0.1241 0.0877 ... Plot delta with ggplot2. delta %&gt;% data.frame() %&gt;% set_names(1:7) %&gt;% mutate(row = 1:n()) %&gt;% pivot_longer(-row, names_to = &quot;index&quot;) %&gt;% ggplot(aes(x = index, y = value, group = row, alpha = row == 3, color = row == 3)) + geom_line() + geom_point() + scale_alpha_manual(values = c(1/3, 1)) + scale_color_manual(values = canva_pal(&quot;Green fields&quot;)(4)[1:2]) + ylab(&quot;probability&quot;) + theme(legend.position = &quot;none&quot;) The brms package has a rdirichlet() function, too. Here we use that to make an alternative version of the plot, above. set.seed(12) brms::rdirichlet(n = 1e4, alpha = rep(2, 7)) %&gt;% data.frame() %&gt;% set_names(1:7) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = name %&gt;% as.double(), alpha = str_c(&quot;alpha[&quot;, name, &quot;]&quot;)) %&gt;% ggplot(aes(x = value, color = name, group = name, fill= name)) + geom_density(alpha = .8) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[2], high = canva_pal(&quot;Green fields&quot;)(4)[3]) + scale_color_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[2], high = canva_pal(&quot;Green fields&quot;)(4)[3]) + scale_x_continuous(&quot;probability&quot;, limits = c(0, 1), breaks = c(0, .5, 1), labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;), ) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(&quot;Dirichlet&quot;*(2*&quot;, &quot;*2*&quot;, &quot;*2*&quot;, &quot;*2*&quot;, &quot;*2*&quot;, &quot;*2*&quot;, &quot;*2))) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ alpha, labeller = label_parsed, nrow = 2) When using brms, the issue of treating a predictor in the way McElreath covered in this section is referred to as monotonic effects. Bürkner outlined the issue in his (2021c) vignette, Estimating monotonic effects with with brms, and in his (2020) article with Emmanuel Charpentier, Modelling monotonic effects of ordinal predictors in Bayesian regression models (click here for the freely-available preprint). From the introduction in Bürkner’s vignette, we read: For a single monotonic predictor, \\(x\\), the linear predictor term of observation \\(n\\) looks as follows: \\[\\eta_n = bD \\sum_{i = 1}^{x_n} \\zeta_i\\] The parameter \\(b\\) can take on any real value, while \\(\\zeta\\) is a simplex, which means that it satisfies \\(\\zeta_i \\in [0, 1]\\) and \\(\\sum_{i = 1}^D \\zeta_i = 1\\) with \\(D\\) being the number of elements of \\(\\zeta\\). Equivalently, \\(D\\) is the number of categories (or highest integer in the data) minus 1, since we start counting categories from zero to simplify the notation. In this context, \\(n\\) indexes the observations in the hypothetical data and \\(\\eta\\) denotes the linear model for some outcome \\(y\\). Unlike with rethinking, the brms syntax for fitting models with monotonic predictors is fairly simple. Just place your monotonic predictors within the mo() function and enter them into the formula. In the text, McElreath remarked it took about 20 minutes to fit this model in his computer. Using my 2019 MacBook Pro, it took just under 40 minutes. Mind you, our brms version of the model used twice the number of warmup and post-warmup iterations, so this isn’t a knock against the sampling speed of brms versus rethinking. But either way, get ready to wait a while when fitting a model like this. b12.6 &lt;- brm(data = d, family = cumulative, response ~ 1 + action + contact + intention + mo(edu_new), # note the `mo()` syntax prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = b), # note the new kinds of prior statements prior(normal(0, 0.143), class = b, coef = moedu_new), prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu_new1)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 12, file = &quot;fits/b12.06&quot;) Here’s the summary. print(b12.6) ## Family: cumulative ## Links: mu = logit; disc = identity ## Formula: response ~ 1 + action + contact + intention + mo(edu_new) ## Data: d (Number of observations: 9930) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept[1] -3.13 0.17 -3.52 -2.86 1.00 2128 2309 ## Intercept[2] -2.45 0.17 -2.84 -2.17 1.00 2108 2332 ## Intercept[3] -1.87 0.16 -2.25 -1.60 1.00 2126 2243 ## Intercept[4] -0.85 0.16 -1.22 -0.58 1.00 2141 2353 ## Intercept[5] -0.18 0.16 -0.56 0.09 1.00 2145 2330 ## Intercept[6] 0.73 0.16 0.36 1.00 1.00 2129 2352 ## action -0.71 0.04 -0.79 -0.63 1.00 4532 3254 ## contact -0.96 0.05 -1.06 -0.86 1.00 4424 3131 ## intention -0.72 0.04 -0.79 -0.65 1.00 5148 2810 ## moedu_new -0.05 0.03 -0.11 -0.01 1.00 2066 2294 ## ## Simplex Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## moedu_new1[1] 0.26 0.15 0.04 0.59 1.00 3118 3026 ## moedu_new1[2] 0.14 0.09 0.02 0.36 1.00 4727 2059 ## moedu_new1[3] 0.19 0.11 0.03 0.43 1.00 4412 2501 ## moedu_new1[4] 0.16 0.09 0.03 0.38 1.00 3816 2596 ## moedu_new1[5] 0.04 0.04 0.00 0.13 1.00 3154 2382 ## moedu_new1[6] 0.09 0.06 0.01 0.24 1.00 3963 2937 ## moedu_new1[7] 0.12 0.07 0.02 0.29 1.00 4514 2620 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## disc 1.00 0.00 1.00 1.00 1.00 4000 4000 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you compare our results to those in the text, you may be surprised by how small our summary values are for moedu_new. brms and rethinking have an important difference in how they parameterize \\(\\beta_\\text{Education}\\). From page 392 in the text, McElreath explained the sum of all the \\(\\delta\\) parameters is the maximum education effect. It will be very convenient for interpretation if we call this maximum sum an ordinary coefficient like \\(\\beta_\\text{E}\\) and then let the \\(\\delta\\) parameters be fractions of it. If we also make a dummy \\(\\delta_0 = 0\\) then we can write it all very compactly. Like this: \\[\\phi_i = \\beta_\\text{E} \\sum_{j = 0}^{\\text{E}_i - 1} \\delta_j + \\text{other stuff}\\] where \\(\\text{E}_i\\) is the completed education level of individual \\(i\\). Now the sum of every \\(\\delta_j\\) is 1, and we can interpret the maximum education effect by looking at \\(\\beta_\\text{E}\\). The current version of brms takes expresses \\(\\beta_\\text{E}\\) as an average effect. From Bürkner &amp; Charpentier (2020), we read: If the monotonic effect is used in a linear model, \\(b\\) can be interpreted as the expected average difference between two adjacent categories of \\(x\\), while \\(\\zeta_i\\) describes the expected difference between the categories \\(i\\) and \\(i - 1\\) in the form of a proportion of the overall difference between lowest and highest categories. Thus, this parameterization has an intuitive interpretation while guaranteeing the monotonicity of the effect (p. 6) To clarify, the \\(b\\) in this section is what we’re calling \\(\\beta_\\text{E}\\) in the current example and Bürkner and Charpentier used \\(\\zeta_i\\) in place of McElreath’s \\(\\delta_j\\). The upshot of all this is that if we’d like to compare the summary of our b12.6 to the results McElreath reported for his m12.6, we’ll need to multiply our moedu_new by 7. posterior_samples(b12.6) %&gt;% transmute(bE = bsp_moedu_new * 7) %&gt;% median_qi(.width = .89) %&gt;% mutate_if(is.double, round, digits = 2) ## bE .lower .upper .width .point .interval ## 1 -0.36 -0.69 -0.12 0.89 median qi This parameterization difference between brms and rethinking is also the reason why we set prior(normal(0, 0.143), class = b, coef = moedu_new) within b12.6 where as McElreath used a \\(\\operatorname{Normal}(0, 1)\\) prior for all his \\(\\beta\\) coefficients, including for his bE. Because our moedu_new (i.e., \\(\\beta_\\text{E}\\)) is parameterized as the average of seven \\(\\delta\\) parameters, it made sense to divide our hyperparameter for \\(\\sigma\\) by 7. That is, \\(1 / 7 \\approx 0.143\\). This might be a good place to practice expressing our model in formal statistical notation. Here we’ll complement McElreath’s formula from page 392 by mixing in a little notation from Bürkner &amp; Charpentier (2020): \\[\\begin{align*} \\text{response}_i &amp; \\sim \\operatorname{Categorical} (\\mathbf p) \\\\ \\operatorname{logit}(p_k) &amp; = \\alpha_k - \\phi_i \\\\ \\phi_i &amp; = \\beta_1 \\text{action}_i + \\beta_2 \\text{contact}_i + \\beta_3 \\text{intention}_i + \\color{#524a3a}{\\beta_4 \\operatorname{mo}(\\text{edu_new}_i, \\boldsymbol{\\delta})} \\\\ \\alpha_k &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\beta_1, \\dots, \\beta_3 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\color{#524a3a}{\\beta_4} &amp; \\color{#524a3a}\\sim \\color{#524a3a}{\\operatorname{Normal}(0, 0.143)} \\\\ \\color{#524a3a}{\\boldsymbol{\\delta}} &amp; \\color{#524a3a}\\sim \\color{#524a3a}{\\operatorname{Dirichlet}(2, 2, 2, 2, 2, 2, 2)}, \\end{align*}\\] where \\(\\operatorname{mo}(x, \\boldsymbol{\\delta})\\) is an operator indicating some predictor \\(x\\) is has undergone the monotonic transform and \\(\\boldsymbol{\\delta}\\) is our vector of simplex parameters \\(\\delta_1, \\dots, \\delta_7\\). That is, \\(\\beta_4 \\operatorname{mo}(\\text{edu_new}_i, \\boldsymbol{\\delta})\\) is our alternative brms-oriented way of expressing McElreath’s \\(\\beta_\\text{E} \\sum_{j = 0}^{\\text{E}_i - 1} \\delta_j\\). I don’t know that one is better. 🤷 At first contact, I found them both confusing. Since this will be our only pairs plot for this chapter, let’s use GGally::ggpairs() to make it fancy. First we customize the panels. my_lower &lt;- function(data, mapping, ...) { # get the x and y data to use the other code x &lt;- eval_data_col(data, mapping$x) y &lt;- eval_data_col(data, mapping$y) # compute the correlations corr &lt;- cor(x, y, method = &quot;p&quot;, use = &quot;pairwise&quot;) abs_corr &lt;- abs(corr) # plot the cor value ggally_text( label = formatC(corr, digits = 2, format = &quot;f&quot;) %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;), mapping = aes(), size = 4, color = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) } my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_density(fill = canva_pal(&quot;Green fields&quot;)(4)[1], size = 0) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) } my_upper &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_hex(bins = 18) + scale_fill_gradient(low = canva_pal(&quot;Green fields&quot;)(4)[4], high = canva_pal(&quot;Green fields&quot;)(4)[3]) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.background = element_rect(fill = canva_pal(&quot;Green fields&quot;)(4)[2])) } Now we are ready to make our custom version of the pairs plot in Figure 12.8. library(GGally) delta_labels &lt;- c(&quot;Elem&quot;, &quot;MidSch&quot;, &quot;SHS&quot;, &quot;HSG&quot;, &quot;SCol&quot;, &quot;Bach&quot;, &quot;Mast&quot;, &quot;Grad&quot;) posterior_samples(b12.6) %&gt;% select(contains(&quot;simo_moedu_new1&quot;)) %&gt;% set_names(str_c(delta_labels[2:8], &quot;~(delta[&quot;, 1:7, &quot;])&quot;)) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = label_parsed) + theme(strip.text = element_text(size = 8)) Here we add a normalized version of edu_new called edu_norm. d &lt;- d %&gt;% mutate(edu_norm = (edu_new - 1) / 7) # what does this look like? d %&gt;% distinct(edu, edu_new, edu_norm) %&gt;% arrange(edu_new) ## edu edu_new edu_norm ## 1 Elementary School 1 0.0000000 ## 2 Middle School 2 0.1428571 ## 3 Some High School 3 0.2857143 ## 4 High School Graduate 4 0.4285714 ## 5 Some College 5 0.5714286 ## 6 Bachelor&#39;s Degree 6 0.7142857 ## 7 Master&#39;s Degree 7 0.8571429 ## 8 Graduate Degree 8 1.0000000 Now fit the more conventional model in which we treat edu_norm as a simple continuous predictor. b12.7 &lt;- brm(data = d, family = cumulative, response ~ 1 + action + contact + intention + edu_norm, prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = b)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 12, file = &quot;fits/b12.07&quot;) fixef(b12.7)[7:10, ] ## Estimate Est.Error Q2.5 Q97.5 ## action -0.7073679 0.04021726 -0.7864181 -0.6289360 ## contact -0.9593262 0.04997124 -1.0545672 -0.8596630 ## intention -0.7187666 0.03718545 -0.7916682 -0.6474665 ## edu_norm -0.1144036 0.09039912 -0.2875478 0.0638620 It might be nice to get a better sense of the two models with a plot. For our approach, we’ll use fitted(). Start with b12.6. nd &lt;- tibble(edu_new = 1:8, action = 0, contact = 0, intention = 0) f &lt;- fitted(b12.6, newdata = nd) f %&gt;% str() ## num [1:8, 1:4, 1:7] 0.0423 0.0468 0.0491 0.0524 0.0553 ... ## - attr(*, &quot;dimnames&quot;)=List of 3 ## ..$ : NULL ## ..$ : chr [1:4] &quot;Estimate&quot; &quot;Est.Error&quot; &quot;Q2.5&quot; &quot;Q97.5&quot; ## ..$ : chr [1:7] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... The rows correspond to the eight educational levels. The columns are the typical summary columns. The seven levels of the third dimension are the seven levels of response. Before we plot, we’re going to need to wrangle that a little and then do the same thing all over for b12.7. # b12.6 f12.6 &lt;- rbind(f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5], f[, , 6], f[, , 7]) %&gt;% data.frame() %&gt;% mutate(edu = factor(rep(1:8, times = 7)), response = rep(1:7, each = 8)) # b12.7 nd &lt;- nd %&gt;% mutate(edu_norm = 1:8) f &lt;- fitted(b12.7, newdata = nd) f12.7 &lt;- rbind(f[, , 1], f[, , 2], f[, , 3], f[, , 4], f[, , 5], f[, , 6], f[, , 7]) %&gt;% data.frame() %&gt;% mutate(edu = factor(rep(1:8, times = 7)), response = rep(1:7, each = 8)) Now combine the two data objects and plot. # this will help with `scale_color_manual()` colors &lt;- scales::seq_gradient_pal(canva_pal(&quot;Green fields&quot;)(4)[4], canva_pal(&quot;Green fields&quot;)(4)[3])(seq(0, 1, length.out = 8)) bind_rows(f12.6, f12.7) %&gt;% mutate(fit = rep(c(&quot;b12.6 with `mo()` syntax&quot;, &quot;b12.7 with conventional syntax&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = response, y = Estimate, ymin = Q2.5, ymax = Q97.5, color = edu, group = edu)) + geom_pointrange(fatten = 3/2, position = position_dodge(width = 3/4)) + scale_color_manual(&quot;education&quot;, values = colors, labels = delta_labels) + scale_x_continuous(breaks = 1:7) + scale_y_continuous(&quot;probability&quot;, limits = c(0, .43)) + theme(legend.background = element_blank(), legend.position = &quot;right&quot;) + facet_wrap(~ fit) In case you were curious, the PSIS-LOO suggests the monotonic model (b12.6) made better sense of the data. b12.6 &lt;- add_criterion(b12.6, &quot;loo&quot;) b12.7 &lt;- add_criterion(b12.7, &quot;loo&quot;) loo_compare(b12.6, b12.7, criterion = &quot;loo&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic ## b12.6 0.0 0.0 -18540.6 38.1 11.1 0.1 37081.3 76.2 ## b12.7 -4.5 1.8 -18545.2 38.1 10.0 0.1 37090.3 76.1 model_weights(b12.6, b12.7, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## b12.6 b12.7 ## 0.99 0.01 We might explore the monotonic effects of b12.6 in one more way. If you were reading closely along in the text, you may have noticed that “the sum of every \\(\\delta_j\\) is 1” (p. 392). When using HMC, this is true for each posterior draw. We can exploit that information to visualize the \\(\\delta_j\\) parameters in a cumulative fashion. posterior_samples(b12.6) %&gt;% select(contains(&quot;new1&quot;)) %&gt;% set_names(1:7) %&gt;% mutate(iter = 1:n(), `0` = 0) %&gt;% pivot_longer(-iter, names_to = &quot;delta&quot;) %&gt;% arrange(delta) %&gt;% group_by(iter) %&gt;% mutate(cum_sum = cumsum(value)) %&gt;% ggplot(aes(x = delta, y = cum_sum)) + stat_pointinterval(.width = .95, size = 1, color = canva_pal(&quot;Green fields&quot;)(4)[1]) + stat_pointinterval(.width = .5, color = canva_pal(&quot;Green fields&quot;)(4)[4], point_color = canva_pal(&quot;Green fields&quot;)(4)[2]) + scale_x_discrete(NULL, labels = parse(text = str_c(&quot;delta[&quot;, 0:7 , &quot;]&quot;))) + ylab(&quot;cumulative sum&quot;) This is another way to show that the largest effects of education are when going from Elementary School to Middle School (\\(\\delta_0 \\rightarrow \\delta_1\\)) and when going from Some High School to High School Graduate (\\(\\delta_2 \\rightarrow \\delta_3\\)). d %&gt;% distinct(edu, edu_new) %&gt;% arrange(edu_new) %&gt;% mutate(`delta[j]` = edu_new - 1) ## edu edu_new delta[j] ## 1 Elementary School 1 0 ## 2 Middle School 2 1 ## 3 Some High School 3 2 ## 4 High School Graduate 4 3 ## 5 Some College 5 4 ## 6 Bachelor&#39;s Degree 6 5 ## 7 Master&#39;s Degree 7 6 ## 8 Graduate Degree 8 7 12.5 Summary “This chapter introduced several new types of regression, all of which are generalizations of generalized linear models (GLMs)” (p. 397). This chapter has been a ride. If you’d like more practice with the negative-binomial model and some of the others models for categorical data we covered in this chapter and in Chapter 11, Alan Agresti covered them extensively in his (2015) text, Foundations of linear and generalized linear models. For more on different kinds of zero-inflated count models, check out Atkins et al. (2013), A tutorial on count regression and zero-altered count models for longitudinal substance use data. If you’d like to learn more about using cumulative probabilities to model ordinal data in brms, check out Bürkner and Vuorre’s (2019) Ordinal regression models in psychology: A tutorial and its repository on the Open Science Framework (https://osf.io/cu8jv/). Also check out Chapter 23 of my (2020c) ebook Doing Bayesian Data Analysis in brms and the tidyverse were we model ordinal data with a series of cumulative probit models. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] GGally_2.1.1 gtools_3.8.2 patchwork_1.1.1 tidybayes_2.3.1 brms_2.15.0 Rcpp_1.0.6 ## [7] forcats_0.5.1 stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 ## [13] tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 ggthemes_4.2.4 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 RcppEigen_0.3.3.7.0 plyr_1.8.6 igraph_1.2.6 ## [6] splines_4.0.4 svUnit_1.0.3 crosstalk_1.1.0.1 TH.data_1.0-10 rstantools_2.1.1 ## [11] inline_0.3.17 digest_0.6.27 htmltools_0.5.1.1 rethinking_2.13 rsconnect_0.8.16 ## [16] fansi_0.4.2 BH_1.75.0-0 magrittr_2.0.1 modelr_0.1.8 RcppParallel_5.0.2 ## [21] matrixStats_0.57.0 xts_0.12.1 sandwich_3.0-0 prettyunits_1.1.1 colorspace_2.0-0 ## [26] rvest_0.3.6 ggdist_2.4.0.9000 haven_2.3.1 xfun_0.22 hexbin_1.28.1 ## [31] callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 lme4_1.1-25 survival_3.2-7 ## [36] zoo_1.8-8 glue_1.4.2 gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 ## [41] distributional_0.2.2 pkgbuild_1.2.0 rstan_2.21.2 shape_1.4.5 abind_1.4-5 ## [46] scales_1.1.1 mvtnorm_1.1-1 emo_0.0.0.9000 DBI_1.1.0 miniUI_0.1.1.1 ## [51] xtable_1.8-4 stats4_4.0.4 StanHeaders_2.21.0-7 DT_0.16 htmlwidgets_1.5.2 ## [56] httr_1.4.2 threejs_0.3.3 RColorBrewer_1.1-2 arrayhelpers_1.1-0 ellipsis_0.3.1 ## [61] reshape_0.8.8 pkgconfig_2.0.3 loo_2.4.1 farver_2.0.3 dbplyr_2.0.0 ## [66] utf8_1.1.4 tidyselect_1.1.0 labeling_0.4.2 rlang_0.4.10 reshape2_1.4.4 ## [71] later_1.1.0.1 munsell_0.5.0 cellranger_1.1.0 tools_4.0.4 cli_2.3.1 ## [76] generics_0.1.0 broom_0.7.5 ggridges_0.5.2 evaluate_0.14 fastmap_1.0.1 ## [81] processx_3.4.5 knitr_1.31 fs_1.5.0 nlme_3.1-152 mime_0.10 ## [86] projpred_2.0.2 xml2_1.3.2 compiler_4.0.4 bayesplot_1.8.0 shinythemes_1.1.2 ## [91] rstudioapi_0.13 curl_4.3 gamm4_0.2-6 reprex_0.3.0 statmod_1.4.35 ## [96] stringi_1.5.3 highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 lattice_0.20-41 ## [101] Matrix_1.3-2 nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 vctrs_0.3.6 ## [106] pillar_1.5.1 lifecycle_1.0.0 bridgesampling_1.0-0 estimability_1.3 httpuv_1.5.4 ## [111] R6_2.5.0 bookdown_0.21 promises_1.1.1 gridExtra_2.3 codetools_0.2-18 ## [116] boot_1.3-26 colourpicker_1.1.0 MASS_7.3-53 assertthat_0.2.1 withr_2.4.1 ## [121] shinystan_2.5.0 multcomp_1.4-16 mgcv_1.8-33 parallel_4.0.4 hms_0.5.3 ## [126] grid_4.0.4 coda_0.19-4 minqa_1.2.4 rmarkdown_2.7 shiny_1.5.0 ## [131] lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 "],["models-with-memory.html", "13 Models With Memory 13.1 Example: Multilevel tadpoles 13.2 Varying effects and the underfitting/overfitting trade-off 13.3 More than one type of cluster 13.4 Divergent transitions and non-centered priors 13.5 Multilevel posterior predictions 13.6 Summary Bonus: Post-stratification in an example Session info", " 13 Models With Memory Multilevel models… remember features of each cluster in the data as they learn about all of the clusters. Depending upon the variation among clusters, which is learned from the data as well, the model pools information across clusters. This pooling tends to improve estimates about each cluster. This improved estimation leads to several, more pragmatic sounding, benefits of the multilevel approach. (McElreath, 2020a, p. 400, emphasis in the original) These benefits include: better estimates for repeated sampling (i.e., in longitudinal data), better estimates when there are imbalances among subsamples, estimates of the variation across subsamples, and avoiding simplistic averaging by retaining variation across subsamples. All of these benefits flow out of the same strategy and model structure. You learn one basic design and you get all of this for free. When it comes to regression, multilevel regression deserves to be the default approach. There are certainly contexts in which it would be better to use an old-fashioned single-level model. But the contexts in which multilevel models are superior are much more numerous. It is better to begin to build a multilevel analysis, and then realize it’s unnecessary, than to overlook it. And once you grasp the basic multilevel strategy, it becomes much easier to incorporate related tricks such as allowing for measurement error in the data and even modeling missing data itself (Chapter 15). (p. 400) I’m totally on board with this. After learning about the multilevel model, I see it everywhere. For more on the sentiment it should be the default, check out McElreath’s blog post, Multilevel regression as default. 13.1 Example: Multilevel tadpoles Let’s load the reedfrogs data (see Vonesh &amp; Bolker, 2005) and fire up brms. library(brms) data(reedfrogs, package = &quot;rethinking&quot;) d &lt;- reedfrogs rm(reedfrogs) Go ahead and acquaint yourself with the reedfrogs. library(tidyverse) d %&gt;% glimpse() ## Rows: 48 ## Columns: 5 ## $ density &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 25, 25, 25, 25, 25, 25, 25,… ## $ pred &lt;fct&gt; no, no, no, no, no, no, no, no, pred, pred, pred, pred, pred, pred, pred, pred, no, no, no,… ## $ size &lt;fct&gt; big, big, big, big, small, small, small, small, big, big, big, big, small, small, small, sm… ## $ surv &lt;int&gt; 9, 10, 7, 10, 9, 9, 10, 9, 4, 9, 7, 6, 7, 5, 9, 9, 24, 23, 22, 25, 23, 23, 23, 21, 6, 13, 4… ## $ propsurv &lt;dbl&gt; 0.9000000, 1.0000000, 0.7000000, 1.0000000, 0.9000000, 0.9000000, 1.0000000, 0.9000000, 0.4… Making the tank cluster variable is easy. d &lt;- d %&gt;% mutate(tank = 1:nrow(d)) Here’s the formula for the un-pooled model in which each tank gets its own intercept: \\[\\begin{align*} \\text{surv}_i &amp; \\sim \\operatorname{Binomial}(n_i, p_i) \\\\ \\operatorname{logit}(p_i) &amp; = \\alpha_{\\text{tank}[i]} \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal} (0, 1.5) &amp; \\text{for } j = 1, \\dots, 48, \\end{align*}\\] where \\(n_i\\) is indexed by the density column. Its values are distributed like so. d %&gt;% count(density) ## density n ## 1 10 16 ## 2 25 16 ## 3 35 16 Now fit this simple aggregated binomial model much like we practiced in Chapter 11. b13.1 &lt;- brm(data = d, family = binomial, surv | trials(density) ~ 0 + factor(tank), prior(normal(0, 1.5), class = b), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, file = &quot;fits/b13.01&quot;) We don’t need a depth=2 argument to discover we have 48 different intercepts. The default print() behavior will do. print(b13.1) ## Family: binomial ## Links: mu = logit ## Formula: surv | trials(density) ~ 0 + factor(tank) ## Data: d (Number of observations: 48) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## factortank1 1.71 0.76 0.35 3.32 1.00 5968 2690 ## factortank2 2.41 0.90 0.77 4.38 1.00 4900 2558 ## factortank3 0.76 0.63 -0.40 2.04 1.00 6092 2689 ## factortank4 2.41 0.89 0.81 4.36 1.00 6258 3082 ## factortank5 1.71 0.77 0.34 3.36 1.00 4940 2499 ## factortank6 1.72 0.77 0.39 3.38 1.00 6044 3061 ## factortank7 2.41 0.91 0.88 4.40 1.00 5596 2867 ## factortank8 1.72 0.76 0.35 3.37 1.00 5477 2615 ## factortank9 -0.36 0.60 -1.56 0.82 1.00 6323 2826 ## factortank10 1.72 0.78 0.34 3.40 1.00 5799 2853 ## factortank11 0.76 0.62 -0.42 2.01 1.00 5467 2858 ## factortank12 0.35 0.63 -0.87 1.61 1.00 5133 3110 ## factortank13 0.76 0.65 -0.44 2.10 1.00 6180 2641 ## factortank14 0.01 0.59 -1.16 1.20 1.00 5359 3277 ## factortank15 1.72 0.79 0.31 3.41 1.00 6558 2823 ## factortank16 1.74 0.78 0.34 3.37 1.00 5035 2706 ## factortank17 2.55 0.69 1.36 4.06 1.00 4771 2989 ## factortank18 2.14 0.62 1.05 3.48 1.00 5673 2708 ## factortank19 1.82 0.55 0.84 3.00 1.00 6580 2789 ## factortank20 3.09 0.81 1.67 4.89 1.00 5512 2629 ## factortank21 2.16 0.63 1.06 3.56 1.00 4868 2438 ## factortank22 2.13 0.61 1.03 3.45 1.00 5752 2526 ## factortank23 2.14 0.58 1.11 3.36 1.00 5812 2941 ## factortank24 1.55 0.52 0.62 2.69 1.00 5606 2728 ## factortank25 -1.11 0.46 -2.05 -0.25 1.01 5548 2758 ## factortank26 0.07 0.41 -0.73 0.90 1.00 5360 2584 ## factortank27 -1.55 0.50 -2.60 -0.63 1.00 5138 2886 ## factortank28 -0.54 0.41 -1.35 0.24 1.00 5783 2878 ## factortank29 0.08 0.39 -0.67 0.84 1.00 5349 3120 ## factortank30 1.30 0.46 0.46 2.24 1.00 5681 2730 ## factortank31 -0.72 0.41 -1.58 0.04 1.00 5584 2848 ## factortank32 -0.39 0.41 -1.21 0.39 1.00 5755 3148 ## factortank33 2.84 0.65 1.70 4.18 1.00 4913 2760 ## factortank34 2.45 0.57 1.41 3.67 1.00 5085 2773 ## factortank35 2.48 0.59 1.41 3.73 1.00 6235 3034 ## factortank36 1.91 0.48 1.03 2.92 1.00 6056 3296 ## factortank37 1.91 0.48 1.03 2.91 1.00 5467 2841 ## factortank38 3.36 0.78 1.99 5.05 1.00 5907 2908 ## factortank39 2.44 0.59 1.40 3.73 1.00 6059 2850 ## factortank40 2.15 0.52 1.21 3.27 1.00 5156 2828 ## factortank41 -1.90 0.48 -2.93 -1.02 1.00 6242 2527 ## factortank42 -0.63 0.35 -1.34 0.03 1.00 6081 2929 ## factortank43 -0.52 0.34 -1.20 0.15 1.00 5480 2523 ## factortank44 -0.39 0.34 -1.08 0.25 1.00 6721 2985 ## factortank45 0.51 0.35 -0.16 1.20 1.00 6432 3238 ## factortank46 -0.63 0.35 -1.33 0.06 1.00 6303 2560 ## factortank47 1.91 0.49 1.04 2.93 1.00 5039 2543 ## factortank48 -0.06 0.34 -0.73 0.63 1.00 4892 3202 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This is much like the models we’ve fit in earlier chapters using McElreath’s index approach, but on steroids. It’ll be instructive to take a look at distribution of the \\(\\alpha_j\\) parameters in density plots. We’ll plot them in both their log-odds and probability metrics. For kicks and giggles, let’s use a FiveThirtyEight-like theme for this chapter’s plots. An easy way to do so is with help from the ggthemes package. library(ggthemes) library(tidybayes) # change the default theme_set(theme_gray() + theme_fivethirtyeight()) tibble(estimate = fixef(b13.1)[, 1]) %&gt;% mutate(p = inv_logit_scaled(estimate)) %&gt;% pivot_longer(estimate:p) %&gt;% mutate(name = if_else(name == &quot;p&quot;, &quot;expected survival probability&quot;, &quot;expected survival log-odds&quot;)) %&gt;% ggplot(aes(x = value, fill = name)) + stat_dots(size = 0) + scale_fill_manual(values = c(&quot;orange1&quot;, &quot;orange4&quot;)) + scale_y_continuous(breaks = NULL) + labs(title = &quot;Tank-level intercepts from the no-pooling model&quot;, subtitle = &quot;Notice now inspecting the distributions of the posterior means can offer insights you\\nmight not get if you looked at them one at a time.&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~ name, scales = &quot;free_x&quot;) Even though it seems like we can derive important insights from how the tank-level intercepts are distributed, that information is not explicitly encoded in the statistical model. Keep that in mind as we now consider the multilevel alternative. Its formula is \\[\\begin{align*} \\text{surv}_i &amp; \\sim \\operatorname{Binomial}(n_i, p_i) \\\\ \\operatorname{logit}(p_i) &amp; = \\alpha_{\\text{tank}[i]} \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal}(\\color{#CD8500}{\\bar \\alpha}, \\color{#CD8500} \\sigma) \\\\ \\color{#CD8500}{\\bar \\alpha} &amp; \\color{#CD8500} \\sim \\color{#CD8500}{\\operatorname{Normal}(0, 1.5)} \\\\ \\color{#CD8500} \\sigma &amp; \\color{#CD8500} \\sim \\color{#CD8500}{\\operatorname{Exponential}(1)}, \\end{align*}\\] where the prior for the tank intercepts is now a function of two parameters, \\(\\bar \\alpha\\) and \\(\\sigma\\). You can say \\(\\bar \\alpha\\) like “bar alpha.” The bar means average. These two parameters inside the prior is where the “multi” in multilevel arises. The Gaussian distribution with mean \\(\\bar \\alpha\\) standard deviation \\(\\sigma\\) is the prior for each tank’s intercept. But that prior itself has priors for \\(\\bar \\alpha\\) and \\(\\sigma\\). So there are two levels in the model, each resembling a simpler model. (p. 403, emphasis in the original) With brms, you might specify the corresponding multilevel model like this. b13.2 &lt;- brm(data = d, family = binomial, surv | trials(density) ~ 1 + (1 | tank), prior = c(prior(normal(0, 1.5), class = Intercept), # bar alpha prior(exponential(1), class = sd)), # sigma iter = 5000, warmup = 1000, chains = 4, cores = 4, sample_prior = &quot;yes&quot;, seed = 13, file = &quot;fits/b13.02&quot;) The syntax for the varying effects follows the lme4 style, ( &lt;varying parameter(s)&gt; | &lt;grouping variable(s)&gt; ). In this case (1 | tank) indicates only the intercept, 1, varies by tank. The extent to which parameters vary is controlled by the prior, prior(exponential(1), class = sd), which is parameterized in the standard deviation metric. Do note that last part. It’s common in multilevel software to model in the variance metric, instead. For technical reasons we won’t really get into until Chapter 14, Stan parameterizes this as a standard deviation. Let’s compute the WAIC comparisons. b13.1 &lt;- add_criterion(b13.1, &quot;waic&quot;) b13.2 &lt;- add_criterion(b13.2, &quot;waic&quot;) w &lt;- loo_compare(b13.1, b13.2, criterion = &quot;waic&quot;) print(w, simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13.2 0.0 0.0 -100.3 3.6 21.2 0.8 200.6 7.2 ## b13.1 -7.5 1.8 -107.7 2.4 26.0 1.3 215.5 4.7 The se_diff is small relative to the elpd_diff. If we convert the \\(\\text{elpd}\\) difference to the WAIC metric, the message stays the same. cbind(waic_diff = w[, 1] * -2, se = w[, 2] * 2) ## waic_diff se ## b13.2 0.00000 0.000000 ## b13.1 14.92095 3.663999 Here are the WAIC weights. model_weights(b13.1, b13.2, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b13.1 b13.2 ## 0 1 I’m not going to show it here, but if you’d like a challenge, try comparing the models with the PSIS-LOO. You’ll get some great practice with high pareto_k values and the moment matching for problematic observations (Paananen, Piironen, et al., 2020; see Paananen, Bürkner, et al., 2020). But back on track, McElreath commented on the number of effective parameters for the two models. This, recall, is listed in the column for \\(p_\\text{WAIC}\\). w[, &quot;p_waic&quot;] ## b13.2 b13.1 ## 21.15522 25.97493 And indeed, even though out multilevel model (b13.2) technically had two more parameters than the conventional single-level model (b13.1), its \\(p_\\text{WAIC}\\) is substantially smaller, due to the regularizing level-2 \\(\\sigma\\) parameter. Speaking of which, let’s examine the model summary. print(b13.2) ## Family: binomial ## Links: mu = logit ## Formula: surv | trials(density) ~ 1 + (1 | tank) ## Data: d (Number of observations: 48) ## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 16000 ## ## Group-Level Effects: ## ~tank (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.62 0.21 1.25 2.08 1.00 4165 7331 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.35 0.26 0.85 1.87 1.00 2713 5723 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This time we don’t get a list of 48 separate tank-level parameters. However, we do get a description of their distribution in terms of \\(\\bar \\alpha\\) (i.e., Intercept) and \\(\\sigma\\) (i.e., sd(Intercept)). If you’d like the actual tank-level parameters, don’t worry; they’re coming in Figure 13.1. We’ll need to do a little prep work, though. post &lt;- posterior_samples(b13.2) post_mdn &lt;- coef(b13.2, robust = T)$tank[, , ] %&gt;% data.frame() %&gt;% bind_cols(d) %&gt;% mutate(post_mdn = inv_logit_scaled(Estimate)) head(post_mdn) ## Estimate Est.Error Q2.5 Q97.5 density pred size surv propsurv tank post_mdn ## 1 2.0710149 0.8489633 0.6199451 4.004548 10 no big 9 0.9 1 0.8880539 ## 2 2.9601418 1.0788371 1.2017427 5.518909 10 no big 10 1.0 2 0.9507406 ## 3 0.9772195 0.6680379 -0.2359327 2.383979 10 no big 7 0.7 3 0.7265562 ## 4 2.9822519 1.0820401 1.1683675 5.623650 10 no big 10 1.0 4 0.9517659 ## 5 2.0677622 0.8479876 0.5778758 4.078550 10 no small 9 0.9 5 0.8877301 ## 6 2.0775342 0.8428334 0.6163349 4.021300 10 no small 9 0.9 6 0.8887004 Here’s the ggplot2 code to reproduce Figure 13.1. post_mdn %&gt;% ggplot(aes(x = tank)) + geom_hline(yintercept = inv_logit_scaled(median(post$b_Intercept)), linetype = 2, size = 1/4) + geom_vline(xintercept = c(16.5, 32.5), size = 1/4, color = &quot;grey25&quot;) + geom_point(aes(y = propsurv), color = &quot;orange2&quot;) + geom_point(aes(y = post_mdn), shape = 1) + annotate(geom = &quot;text&quot;, x = c(8, 16 + 8, 32 + 8), y = 0, label = c(&quot;small tanks&quot;, &quot;medium tanks&quot;, &quot;large tanks&quot;)) + scale_x_continuous(breaks = c(1, 16, 32, 48)) + scale_y_continuous(breaks = 0:5 / 5, limits = c(0, 1)) + labs(title = &quot;Multilevel shrinkage!&quot;, subtitle = &quot;The empirical proportions are in orange while the model-\\nimplied proportions are the black circles. The dashed line is\\nthe model-implied average survival proportion.&quot;) + theme(panel.grid.major = element_blank()) Here is the code for our version of Figure 13.2.a, where we visualize the model-implied population distribution of log-odds survival (i.e., the population distribution yielding all the tank-level intercepts). # this makes the output of `slice_sample()` reproducible set.seed(13) p1 &lt;- post %&gt;% mutate(iter = 1:n()) %&gt;% slice_sample(n = 100) %&gt;% expand(nesting(iter, b_Intercept, sd_tank__Intercept), x = seq(from = -4, to = 5, length.out = 100)) %&gt;% mutate(density = dnorm(x, mean = b_Intercept, sd = sd_tank__Intercept)) %&gt;% ggplot(aes(x = x, y = density, group = iter)) + geom_line(alpha = .2, color = &quot;orange2&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Population survival distribution&quot;, subtitle = &quot;log-odds scale&quot;) + coord_cartesian(xlim = c(-3, 4)) Now we make our Figure 13.2.b and then bind the two subplots with patchwork. set.seed(13) p2 &lt;- post %&gt;% slice_sample(n = 8000, replace = T) %&gt;% mutate(sim_tanks = rnorm(n(), mean = b_Intercept, sd = sd_tank__Intercept)) %&gt;% ggplot(aes(x = inv_logit_scaled(sim_tanks))) + geom_density(size = 0, fill = &quot;orange2&quot;, adjust = 0.1) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Probability of survival&quot;, subtitle = &quot;transformed by the inverse-logit function&quot;) library(patchwork) (p1 + p2) &amp; theme(plot.title = element_text(size = 12), plot.subtitle = element_text(size = 10)) Both plots show different ways in expressing the model uncertainty in terms of both location \\(\\alpha\\) and scale \\(\\sigma\\). 13.1.0.1 Rethinking: Varying intercepts as over-dispersion. In the previous chapter (page 369), the beta-binomial and gamma-Poisson models were presented as ways for coping with over-dispersion of count data. Varying intercepts accomplish the same thing, allowing count outcomes to be over-dispersed. They accomplish this, because when each observed count gets its own unique intercept, but these intercepts are pooled through a common distribution, the predictions expect over-dispersion just like a beta-binomial or gamma-Poisson model would. Multilevel models are also mixtures. Compared to a beta-binomial or gamma-Poisson model, a binomial or Poisson model with a varying intercept on every observed outcome will often be easier to estimate and easier to extend. (p. 407, emphasis in the original) 13.1.0.2 Overthinking: Prior for variance components. Yep, you can use the half-Normal distribution for your priors in brms, too. Here it is for model b13.2. b13.2b &lt;- update(b13.2, prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 5000, warmup = 1000, chains = 4, cores = 4, sample_prior = &quot;yes&quot;, seed = 13, file = &quot;fits/b13.02b&quot;) McElreath mentioned how one might set a lower bound at zero for the half-Normal prior when using rethinking::ulam(). There’s no need to do so when using brms::brm(). The lower bounds for priors of class = sd are already set to zero by default. Check the model summary. print(b13.2b) ## Family: binomial ## Links: mu = logit ## Formula: surv | trials(density) ~ 1 + (1 | tank) ## Data: d (Number of observations: 48) ## Samples: 4 chains, each with iter = 5000; warmup = 1000; thin = 1; ## total post-warmup samples = 16000 ## ## Group-Level Effects: ## ~tank (Number of levels: 48) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.59 0.20 1.24 2.02 1.00 4700 9047 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.33 0.25 0.84 1.84 1.00 3514 6311 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you’re curious how the exponential and half-Normal priors compare to one another and to their posteriors, you might just plot. # for annotation text &lt;- tibble(value = c(0.5, 2.4), density = c(1, 1.85), distribution = factor(c(&quot;prior&quot;, &quot;posterior&quot;), levels = c(&quot;prior&quot;, &quot;posterior&quot;)), prior = &quot;Exponential(1)&quot;) # gather and wrangle the prior and posterior draws tibble(`prior_Exponential(1)` = prior_samples(b13.2) %&gt;% pull(sd_tank), `posterior_Exponential(1)` = posterior_samples(b13.2) %&gt;% pull(sd_tank__Intercept), `prior_Half-Normal(0, 1)` = prior_samples(b13.2b) %&gt;% pull(sd_tank), `posterior_Half-Normal(0, 1)` = posterior_samples(b13.2b) %&gt;% pull(sd_tank__Intercept)) %&gt;% pivot_longer(everything(), names_sep = &quot;_&quot;, names_to = c(&quot;distribution&quot;, &quot;prior&quot;)) %&gt;% mutate(distribution = factor(distribution, levels = c(&quot;prior&quot;, &quot;posterior&quot;))) %&gt;% # plot! ggplot(aes(x = value, fill = distribution)) + geom_density(size = 0, alpha = 2/3, adjust = 0.25) + geom_text(data = text, aes(y = density, label = distribution, color = distribution)) + scale_fill_manual(NULL, values = c(&quot;orange4&quot;, &quot;orange2&quot;)) + scale_color_manual(NULL, values = c(&quot;orange4&quot;, &quot;orange2&quot;)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(Hierarchical~sigma~parameter)) + coord_cartesian(xlim = c(0, 4)) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ prior) By the way, this is why we set iter = 5000 and sample_prior = \"yes\" for the last two models. Neither were necessary to fit the models, but both helped us out with this plot. 13.2 Varying effects and the underfitting/overfitting trade-off Varying intercepts are just regularized estimates, but adaptively regularized by estimating how diverse the clusters are while estimating the features of each cluster. This fact is not easy to grasp…. A major benefit of using varying effects estimates, instead of the empirical raw estimates, is that they provide more accurate estimates of the individual cluster (tank) intercepts. On average, the varying effects actually provide a better estimate of the individual tank (cluster) means. The reason that the varying intercepts provide better estimates is that they do a better job of trading off underfitting and overfitting. (p. 408) In this section, we explicate this by contrasting three perspectives: complete pooling (i.e., a single-\\(\\alpha\\) model), no pooling (i.e., the single-level \\(\\alpha_{\\text{tank}[i]}\\) model), and partial pooling [i.e., the multilevel model for which \\(\\alpha_j \\sim \\operatorname{Normal} (\\bar \\alpha, \\sigma)\\)]. To demonstrate [the magic of the multilevel model], we’ll simulate some tadpole data. That way, we’ll know the true per-pond survival probabilities. Then we can compare the no-pooling estimates to the partial pooling estimates, by computing how close each gets to the true values they are trying to estimate. The rest of this section shows how to do such a simulation. (p. 409) 13.2.1 The model. The simulation formula should look familiar. \\[\\begin{align*} \\text{surv}_i &amp; \\sim \\operatorname{Binomial}(n_i, p_i) \\\\ \\operatorname{logit}(p_i) &amp; = \\alpha_{\\text{pond}[i]} \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal}(\\bar \\alpha, \\sigma) \\\\ \\bar \\alpha &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1) \\end{align*}\\] 13.2.2 Assign values to the parameters. Here we follow along with McElreath and “assign specific values representative of the actual tadpole data” (p. 409). Because he included a set.seed() line in his R code 13.8, our results should match his exactly. a_bar &lt;- 1.5 sigma &lt;- 1.5 n_ponds &lt;- 60 set.seed(5005) dsim &lt;- tibble(pond = 1:n_ponds, ni = rep(c(5, 10, 25, 35), each = n_ponds / 4) %&gt;% as.integer(), true_a = rnorm(n = n_ponds, mean = a_bar, sd = sigma)) head(dsim) ## # A tibble: 6 x 3 ## pond ni true_a ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 0.567 ## 2 2 5 1.99 ## 3 3 5 -0.138 ## 4 4 5 1.86 ## 5 5 5 3.91 ## 6 6 5 1.95 McElreath twice urged us to inspect the contents of this simulation. In addition to looking at the data with head(), we might well plot. dsim %&gt;% mutate(ni = factor(ni)) %&gt;% ggplot(aes(x = true_a, y = ni)) + stat_dotsinterval(fill = &quot;orange2&quot;, slab_size = 0, .width = .5) + ggtitle(&quot;Log-odds varying by # tadpoles per pond&quot;) + theme(plot.title = element_text(size = 14)) 13.2.3 Sumulate survivors. Each pond \\(i\\) has \\(n_i\\) potential survivors, and nature flips each tadpole’s coin, so to speak, with probability of survival \\(p_i\\). This probability \\(p_i\\) is implied by the model definition, and is equal to: \\[p_i = \\frac{\\exp (\\alpha_i)}{1 + \\exp (\\alpha_i)}\\] The model uses a logit link, and so the probability is defined by the [inv_logit_scaled()] function. (p. 411) Although McElreath shared his set.seed() number in the last section, he didn’t share it for this bit. We’ll go ahead and carry over the one from last time. However, in a moment we’ll see this clearly wasn’t the one he used here. As a consequence, our results will deviate a bit from his. set.seed(5005) ( dsim &lt;- dsim %&gt;% mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni)) ) ## # A tibble: 60 x 4 ## pond ni true_a si ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 5 0.567 4 ## 2 2 5 1.99 4 ## 3 3 5 -0.138 3 ## 4 4 5 1.86 5 ## 5 5 5 3.91 5 ## 6 6 5 1.95 4 ## 7 7 5 1.49 4 ## 8 8 5 2.52 4 ## 9 9 5 2.18 3 ## 10 10 5 2.05 4 ## # … with 50 more rows 13.2.4 Compute the no-pooling estimates. The no-pooling estimates (i.e., \\(\\alpha_{\\text{tank}[i]}\\)) are the results of simple algebra. ( dsim &lt;- dsim %&gt;% mutate(p_nopool = si / ni) ) ## # A tibble: 60 x 5 ## pond ni true_a si p_nopool ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 5 0.567 4 0.8 ## 2 2 5 1.99 4 0.8 ## 3 3 5 -0.138 3 0.6 ## 4 4 5 1.86 5 1 ## 5 5 5 3.91 5 1 ## 6 6 5 1.95 4 0.8 ## 7 7 5 1.49 4 0.8 ## 8 8 5 2.52 4 0.8 ## 9 9 5 2.18 3 0.6 ## 10 10 5 2.05 4 0.8 ## # … with 50 more rows “These are the same no-pooling estimates you’d get by fitting a model with a dummy variable for each pond and flat priors that induce no regularization” (p. 411). That is, these are the same kinds of estimates we got back when we fit b13.1. 13.2.5 Compute the partial-pooling estimates. Fit the multilevel (partial-pooling) model. b13.3 &lt;- brm(data = dsim, family = binomial, si | trials(ni) ~ 1 + (1 | pond), prior = c(prior(normal(0, 1.5), class = Intercept), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, file = &quot;fits/b13.03&quot;) Here’s our standard brms summary. print(b13.3) ## Family: binomial ## Links: mu = logit ## Formula: si | trials(ni) ~ 1 + (1 | pond) ## Data: dsim (Number of observations: 60) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~pond (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.49 0.20 1.14 1.92 1.00 1506 2368 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.46 0.23 1.03 1.94 1.00 961 1663 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’m not aware that you can use McElreath’s depth=2 trick in brms for summary() or print(). However, you can get most of that information and more with the Stan-like summary using the $fit syntax. b13.3$fit ## Inference for Stan model: 50bbea81c6f51c3bd01edfc7641dfc55. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 1.46 0.01 0.23 1.03 1.31 1.46 1.62 1.94 948 1 ## sd_pond__Intercept 1.49 0.01 0.20 1.14 1.35 1.48 1.62 1.92 1480 1 ## r_pond[1,Intercept] 0.09 0.01 0.97 -1.71 -0.57 0.05 0.69 2.17 7322 1 ## r_pond[2,Intercept] 0.10 0.01 0.96 -1.66 -0.56 0.04 0.71 2.12 6248 1 ## r_pond[3,Intercept] -0.68 0.01 0.84 -2.24 -1.27 -0.70 -0.13 1.00 5687 1 ## r_pond[4,Intercept] 1.14 0.01 1.12 -0.86 0.36 1.06 1.86 3.53 6376 1 ## r_pond[5,Intercept] 1.14 0.01 1.14 -0.85 0.33 1.06 1.84 3.62 6681 1 ## r_pond[6,Intercept] 0.09 0.01 0.96 -1.65 -0.57 0.04 0.71 2.09 5908 1 ## r_pond[7,Intercept] 0.09 0.01 0.98 -1.74 -0.59 0.04 0.72 2.11 6581 1 ## r_pond[8,Intercept] 0.10 0.01 0.95 -1.62 -0.54 0.06 0.68 2.12 6627 1 ## r_pond[9,Intercept] -0.70 0.01 0.85 -2.35 -1.26 -0.71 -0.13 1.01 6139 1 ## r_pond[10,Intercept] 0.10 0.01 0.94 -1.65 -0.53 0.04 0.70 2.11 6207 1 ## r_pond[11,Intercept] 1.13 0.01 1.12 -0.89 0.35 1.06 1.81 3.54 6635 1 ## r_pond[12,Intercept] -1.36 0.01 0.83 -2.97 -1.93 -1.36 -0.81 0.22 6388 1 ## r_pond[13,Intercept] 1.14 0.01 1.13 -0.86 0.34 1.08 1.86 3.51 6325 1 ## r_pond[14,Intercept] 0.08 0.01 0.95 -1.65 -0.57 0.05 0.70 2.08 6733 1 ## r_pond[15,Intercept] 1.12 0.01 1.12 -0.86 0.36 1.04 1.83 3.54 6781 1 ## r_pond[16,Intercept] -0.85 0.01 0.63 -2.06 -1.28 -0.85 -0.42 0.41 5022 1 ## r_pond[17,Intercept] -1.95 0.01 0.65 -3.26 -2.38 -1.93 -1.52 -0.77 4933 1 ## r_pond[18,Intercept] -1.21 0.01 0.64 -2.46 -1.65 -1.21 -0.80 0.06 5232 1 ## r_pond[19,Intercept] -1.22 0.01 0.65 -2.46 -1.65 -1.22 -0.80 0.13 4799 1 ## r_pond[20,Intercept] -0.43 0.01 0.70 -1.74 -0.91 -0.46 0.02 0.99 5399 1 ## r_pond[21,Intercept] -1.57 0.01 0.65 -2.85 -1.99 -1.56 -1.12 -0.34 4858 1 ## r_pond[22,Intercept] 0.66 0.01 0.86 -0.86 0.05 0.60 1.20 2.51 5055 1 ## r_pond[23,Intercept] 1.55 0.02 1.00 -0.19 0.83 1.48 2.16 3.75 4406 1 ## r_pond[24,Intercept] -0.84 0.01 0.68 -2.13 -1.31 -0.85 -0.40 0.53 4207 1 ## r_pond[25,Intercept] 0.06 0.01 0.75 -1.32 -0.47 0.02 0.55 1.58 4978 1 ## r_pond[26,Intercept] 0.68 0.01 0.87 -0.86 0.06 0.61 1.22 2.56 5753 1 ## r_pond[27,Intercept] 0.06 0.01 0.74 -1.29 -0.46 0.03 0.56 1.63 5016 1 ## r_pond[28,Intercept] -0.44 0.01 0.69 -1.77 -0.90 -0.46 0.01 0.97 5209 1 ## r_pond[29,Intercept] -0.85 0.01 0.65 -2.07 -1.28 -0.86 -0.43 0.43 4208 1 ## r_pond[30,Intercept] -0.45 0.01 0.69 -1.74 -0.93 -0.47 0.00 0.97 5670 1 ## r_pond[31,Intercept] 1.41 0.01 0.78 0.03 0.87 1.34 1.90 3.11 5320 1 ## r_pond[32,Intercept] 0.55 0.01 0.63 -0.61 0.11 0.53 0.95 1.86 3754 1 ## r_pond[33,Intercept] 1.41 0.01 0.78 0.03 0.86 1.37 1.90 3.06 5328 1 ## r_pond[34,Intercept] -0.43 0.01 0.49 -1.37 -0.77 -0.43 -0.11 0.57 3148 1 ## r_pond[35,Intercept] -0.96 0.01 0.46 -1.83 -1.27 -0.97 -0.65 -0.04 2441 1 ## r_pond[36,Intercept] 2.10 0.01 0.95 0.46 1.44 2.00 2.66 4.25 4792 1 ## r_pond[37,Intercept] -3.37 0.01 0.61 -4.65 -3.76 -3.33 -2.95 -2.24 3989 1 ## r_pond[38,Intercept] -2.06 0.01 0.46 -2.99 -2.36 -2.05 -1.75 -1.18 3079 1 ## r_pond[39,Intercept] -0.97 0.01 0.47 -1.90 -1.27 -0.97 -0.65 -0.05 3260 1 ## r_pond[40,Intercept] 2.10 0.01 0.95 0.47 1.43 2.02 2.66 4.19 4059 1 ## r_pond[41,Intercept] 2.11 0.01 0.94 0.55 1.44 2.02 2.67 4.22 4566 1 ## r_pond[42,Intercept] 0.55 0.01 0.60 -0.52 0.13 0.52 0.92 1.83 3807 1 ## r_pond[43,Intercept] -1.74 0.01 0.46 -2.66 -2.04 -1.73 -1.43 -0.85 2776 1 ## r_pond[44,Intercept] -0.63 0.01 0.47 -1.49 -0.94 -0.65 -0.32 0.34 2879 1 ## r_pond[45,Intercept] -2.63 0.01 0.50 -3.63 -2.95 -2.62 -2.29 -1.71 3504 1 ## r_pond[46,Intercept] -1.44 0.01 0.40 -2.22 -1.71 -1.44 -1.18 -0.66 2260 1 ## r_pond[47,Intercept] 2.34 0.01 0.92 0.74 1.69 2.26 2.90 4.35 5553 1 ## r_pond[48,Intercept] 2.33 0.01 0.93 0.78 1.67 2.25 2.88 4.38 4558 1 ## r_pond[49,Intercept] 0.16 0.01 0.48 -0.75 -0.16 0.14 0.47 1.13 3172 1 ## r_pond[50,Intercept] 0.16 0.01 0.49 -0.78 -0.17 0.15 0.48 1.15 2688 1 ## r_pond[51,Intercept] 0.15 0.01 0.48 -0.77 -0.17 0.15 0.47 1.14 3165 1 ## r_pond[52,Intercept] -1.44 0.01 0.39 -2.24 -1.71 -1.44 -1.17 -0.68 2353 1 ## r_pond[53,Intercept] 0.16 0.01 0.48 -0.73 -0.17 0.15 0.49 1.12 2764 1 ## r_pond[54,Intercept] 2.32 0.01 0.91 0.77 1.67 2.25 2.89 4.38 4731 1 ## r_pond[55,Intercept] -0.88 0.01 0.42 -1.70 -1.15 -0.87 -0.60 -0.03 2715 1 ## r_pond[56,Intercept] 1.70 0.01 0.76 0.36 1.17 1.64 2.16 3.41 5144 1 ## r_pond[57,Intercept] -0.76 0.01 0.41 -1.53 -1.03 -0.76 -0.49 0.06 2612 1 ## r_pond[58,Intercept] 1.23 0.01 0.66 0.02 0.78 1.17 1.65 2.64 4193 1 ## r_pond[59,Intercept] 0.37 0.01 0.52 -0.58 0.01 0.34 0.71 1.40 3089 1 ## r_pond[60,Intercept] 1.23 0.01 0.65 0.10 0.77 1.19 1.64 2.60 3791 1 ## lp__ -185.86 0.25 7.57 -201.09 -190.96 -185.44 -180.61 -171.64 889 1 ## ## Samples were drawn using NUTS(diag_e) at Sun Mar 14 22:15:10 2021. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). As an aside, notice how this summary still reports the old-style n_eff values, rather than the updated Bulk_ESS and Tail_ESS values. I suspect this will change sometime soon. In the meantime, here’s a thread on the Stan Forums featuring members of the Stan team discussing how. Let’s get ready for the diagnostic plot of Figure 13.3. First we add the partially-pooled estimates, as summarized by their posterior means, to the dsim data. Then we compute error values. # we could have included this step in the block of code below, if we wanted to p_partpool &lt;- coef(b13.3)$pond[, , ] %&gt;% data.frame() %&gt;% transmute(p_partpool = inv_logit_scaled(Estimate)) dsim &lt;- dsim %&gt;% bind_cols(p_partpool) %&gt;% mutate(p_true = inv_logit_scaled(true_a)) %&gt;% mutate(nopool_error = abs(p_nopool - p_true), partpool_error = abs(p_partpool - p_true)) dsim %&gt;% glimpse() ## Rows: 60 ## Columns: 9 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24… ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, … ## $ true_a &lt;dbl&gt; 0.56673123, 1.99002317, -0.13775688, 1.85676651, 3.91208800, 1.95414869, 1.48963805, … ## $ si &lt;int&gt; 4, 4, 3, 5, 5, 4, 4, 4, 3, 4, 5, 2, 5, 4, 5, 6, 3, 5, 5, 7, 4, 9, 10, 6, 8, 9, 8, 7, … ## $ p_nopool &lt;dbl&gt; 0.80, 0.80, 0.60, 1.00, 1.00, 0.80, 0.80, 0.80, 0.60, 0.80, 1.00, 0.40, 1.00, 0.80, 1… ## $ p_partpool &lt;dbl&gt; 0.8256632, 0.8267518, 0.6859881, 0.9313513, 0.9308751, 0.8251188, 0.8253531, 0.826688… ## $ p_true &lt;dbl&gt; 0.6380086, 0.8797456, 0.4656151, 0.8649196, 0.9803934, 0.8758983, 0.8160239, 0.925812… ## $ nopool_error &lt;dbl&gt; 0.161991419, 0.079745589, 0.134384860, 0.135080387, 0.019606594, 0.075898310, 0.01602… ## $ partpool_error &lt;dbl&gt; 0.1876546251, 0.0529937565, 0.2203729151, 0.0664317322, 0.0495182966, 0.0507795368, 0… Here is our code for Figure 13.3. The extra data processing for dfline is how we get the values necessary for the horizontal summary lines. dfline &lt;- dsim %&gt;% select(ni, nopool_error:partpool_error) %&gt;% pivot_longer(-ni) %&gt;% group_by(name, ni) %&gt;% summarise(mean_error = mean(value)) %&gt;% mutate(x = c( 1, 16, 31, 46), xend = c(15, 30, 45, 60)) dsim %&gt;% ggplot(aes(x = pond)) + geom_vline(xintercept = c(15.5, 30.5, 45.4), color = &quot;white&quot;, size = 2/3) + geom_point(aes(y = nopool_error), color = &quot;orange2&quot;) + geom_point(aes(y = partpool_error), shape = 1) + geom_segment(data = dfline, aes(x = x, xend = xend, y = mean_error, yend = mean_error), color = rep(c(&quot;orange2&quot;, &quot;black&quot;), each = 4), linetype = rep(1:2, each = 4)) + annotate(geom = &quot;text&quot;, x = c(15 - 7.5, 30 - 7.5, 45 - 7.5, 60 - 7.5), y = .45, label = c(&quot;tiny (5)&quot;, &quot;small (10)&quot;, &quot;medium (25)&quot;, &quot;large (35)&quot;)) + scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60)) + labs(title = &quot;Estimate error by model type&quot;, subtitle = &quot;The horizontal axis displays pond number. The vertical axis measures\\nthe absolute error in the predicted proportion of survivors, compared to\\nthe true value used in the simulation. The higher the point, the worse\\nthe estimate. No-pooling shown in orange. Partial pooling shown in black.\\nThe orange and dashed black lines show the average error for each kind\\nof estimate, across each initial density of tadpoles (pond size).&quot;, y = &quot;absolute error&quot;) + theme(panel.grid.major = element_blank(), plot.subtitle = element_text(size = 10)) If you wanted to quantify the difference in simple summaries, you might execute something like this. dsim %&gt;% select(ni, nopool_error:partpool_error) %&gt;% pivot_longer(-ni) %&gt;% group_by(name) %&gt;% summarise(mean_error = mean(value) %&gt;% round(digits = 3), median_error = median(value) %&gt;% round(digits = 3)) ## # A tibble: 2 x 3 ## name mean_error median_error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nopool_error 0.059 0.042 ## 2 partpool_error 0.054 0.034 Although many years of work in statistics have shown that partially pooled estimates are better, on average, this is not always the case. Our results are an example of this. McElreath addressed this directly: But there are some cases in which the no-pooling estimates are better. These exceptions often result from ponds with extreme probabilities of survival. The partial pooling estimates shrink such extreme ponds towards the mean, because few ponds exhibit such extreme behavior. But sometimes outliers really are outliers. (p. 414) I originally learned about the multilevel in order to work with longitudinal data. In that context, I found the basic principles of a multilevel structure quite intuitive. The concept of partial pooling, however, took me some time to wrap my head around. If you’re struggling with this, be patient and keep chipping away. When McElreath lectured on this topic in 2015, he traced partial pooling to statistician Charles M. Stein. Efron and Morris (1977) wrote the now classic paper, Stein’s paradox in statistics, which does a nice job breaking down why partial pooling can be so powerful. One of the primary examples they used in the paper was of 1970 batting average data. If you’d like more practice seeing how partial pooling works–or if you just like baseball–, check out my blog post, Stein’s paradox and what partial pooling can do for you. 13.2.5.1 Overthinking: Repeating the pond simulation. Within the brms workflow, we can reuse a compiled model with update(). But first, we’ll simulate new data. a_bar &lt;- 1.5 sigma &lt;- 1.5 n_ponds &lt;- 60 set.seed(1999) # for new data, set a new seed new_dsim &lt;- tibble(pond = 1:n_ponds, ni = rep(c(5, 10, 25, 35), each = n_ponds / 4) %&gt;% as.integer(), true_a = rnorm(n = n_ponds, mean = a_bar, sd = sigma)) %&gt;% mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni)) %&gt;% mutate(p_nopool = si / ni) glimpse(new_dsim) ## Rows: 60 ## Columns: 5 ## $ pond &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, … ## $ ni &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10… ## $ true_a &lt;dbl&gt; 2.5990087, 1.4432554, 3.3045137, 3.7047030, 1.7005354, 2.2797409, 0.6759270, -0.2784119, -0… ## $ si &lt;int&gt; 4, 4, 5, 4, 4, 4, 2, 4, 3, 5, 4, 5, 2, 2, 5, 10, 8, 10, 10, 9, 10, 9, 5, 10, 10, 6, 7, 7, 8… ## $ p_nopool &lt;dbl&gt; 0.80, 0.80, 1.00, 0.80, 0.80, 0.80, 0.40, 0.80, 0.60, 1.00, 0.80, 1.00, 0.40, 0.40, 1.00, 1… Fit the new model. b13.3_new &lt;- update(b13.3, newdata = new_dsim, chains = 4, cores = 4, seed = 13, file = &quot;fits/b13.03_new&quot;) print(b13.3_new) ## Family: binomial ## Links: mu = logit ## Formula: si | trials(ni) ~ 1 + (1 | pond) ## Data: new_dsim (Number of observations: 60) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~pond (Number of levels: 60) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.33 0.19 1.01 1.74 1.00 1469 2085 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.64 0.21 1.24 2.08 1.00 1431 2406 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Why not plot the first simulation versus the second one? bind_rows(posterior_samples(b13.3), posterior_samples(b13.3_new)) %&gt;% mutate(model = rep(c(&quot;b13.3&quot;, &quot;b13.3_new&quot;), each = n() / 2)) %&gt;% ggplot(aes(x = b_Intercept, y = sd_pond__Intercept)) + stat_density_2d(geom = &quot;raster&quot;, aes(fill = stat(density)), contour = F, n = 200) + geom_vline(xintercept = a_bar, color = &quot;orange3&quot;, linetype = 3) + geom_hline(yintercept = sigma, color = &quot;orange3&quot;, linetype = 3) + scale_fill_gradient(low = &quot;grey25&quot;, high = &quot;orange3&quot;) + ggtitle(&quot;Our simulation posteriors contrast a bit&quot;, subtitle = expression(alpha*&quot; is on the x and &quot;*sigma*&quot; is on the y, both in log-odds. The dotted lines intersect at the true values.&quot;)) + coord_cartesian(xlim = c(.7, 2), ylim = c(.8, 1.9)) + theme(legend.position = &quot;none&quot;, panel.grid.major = element_blank()) + facet_wrap(~ model, ncol = 2) If you’d like the stanfit portion of your brm() object, subset with $fit. Take b13.3, for example. You might check out its structure via b13.3$fit %&gt;% str(). Here’s the actual Stan code. b13.3$fit@stanmodel ## S4 class stanmodel &#39;50bbea81c6f51c3bd01edfc7641dfc55&#39; coded as follows: ## // generated with brms 2.15.0 ## functions { ## } ## data { ## int&lt;lower=1&gt; N; // total number of observations ## int Y[N]; // response variable ## int trials[N]; // number of trials ## // data for group-level effects of ID 1 ## int&lt;lower=1&gt; N_1; // number of grouping levels ## int&lt;lower=1&gt; M_1; // number of coefficients per level ## int&lt;lower=1&gt; J_1[N]; // grouping indicator per observation ## // group-level predictor values ## vector[N] Z_1_1; ## int prior_only; // should the likelihood be ignored? ## } ## transformed data { ## } ## parameters { ## real Intercept; // temporary intercept for centered predictors ## vector&lt;lower=0&gt;[M_1] sd_1; // group-level standard deviations ## vector[N_1] z_1[M_1]; // standardized group-level effects ## } ## transformed parameters { ## vector[N_1] r_1_1; // actual group-level effects ## r_1_1 = (sd_1[1] * (z_1[1])); ## } ## model { ## // likelihood including constants ## if (!prior_only) { ## // initialize linear predictor term ## vector[N] mu = Intercept + rep_vector(0.0, N); ## for (n in 1:N) { ## // add more terms to the linear predictor ## mu[n] += r_1_1[J_1[n]] * Z_1_1[n]; ## } ## target += binomial_logit_lpmf(Y | trials, mu); ## } ## // priors including constants ## target += normal_lpdf(Intercept | 0, 1.5); ## target += exponential_lpdf(sd_1 | 1); ## target += std_normal_lpdf(z_1[1]); ## } ## generated quantities { ## // actual population-level intercept ## real b_Intercept = Intercept; ## } ## 13.3 More than one type of cluster “We can use and often should use more than one type of cluster in the same model” (p. 415). 13.3.0.1 Rethinking: Cross-classification and hierarchy. The kind of data structure in data(chimpanzees) is usually called a cross-classified multilevel model. It is cross-classified, because actors are not nested within unique blocks. If each chimpanzee had instead done all of his or her pulls on a single day, within a single block, then the data structure would instead be hierarchical. However, the model specification would typically be the same. So the model structure and code you’ll see below will apply both to cross-classified designs and hierarchical designs. (p. 415, emphasis in the original) 13.3.1 Multilevel chimpanzees. The initial multilevel update from model b11.4 from Section 11.1.1 follows the statistical formula \\[\\begin{align*} \\text{left_pull}_i &amp; \\sim \\operatorname{Binomial}(n_i = 1, p_i) \\\\ \\operatorname{logit} (p_i) &amp; = \\alpha_{\\text{actor}[i]} + \\color{#CD8500}{\\gamma_{\\text{block}[i]}} + \\beta_{\\text{treatment}[i]} \\\\ \\beta_j &amp; \\sim \\operatorname{Normal}(0, 0.5) \\;\\;\\; , \\text{for } j = 1, \\dots, 4 \\\\ \\alpha_j &amp; \\sim \\operatorname{Normal}(\\bar \\alpha, \\sigma_\\alpha) \\;\\;\\; , \\text{for } j = 1, \\dots, 7 \\\\ \\color{#CD8500}{\\gamma_j} &amp; \\color{#CD8500} \\sim \\color{#CD8500}{\\operatorname{Normal}(0, \\sigma_\\gamma) \\;\\;\\; , \\text{for } j = 1, \\dots, 6} \\\\ \\bar \\alpha &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\sigma_\\alpha &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\color{#CD8500}{\\sigma_\\gamma} &amp; \\color{#CD8500} \\sim \\color{#CD8500}{\\operatorname{Exponential}(1)}. \\end{align*}\\] ️ WARNING ️ I am so sorry, but we are about to head straight into a load of confusion. If you follow along linearly in the text, we won’t have the language to parse this all out until Section 13.4. In short, our difficulties will have to do with what are called the centered and the non-centered parameterizations for multilevel models. For the next several models in the text, McElreath used the centered parameterization. As we’ll learn in Section 13.4, this often causes problems when you use Stan to fit your multilevel models. Happily, the solution to those problems is often the non-centered parameterization, which is well known among the Stan team. This issue is so well known, in fact, that Bürkner only supports the non-centered parameterization with brms (see here). To my knowledge, there is no easy way around this. In the long run, this is a good thing. Your brms models will likely avoid some of the problems McElreath highlighted in this part of the text. In the short term, this also means that our results will not completely match up with those in the text. If you really want to reproduce McElreath’s models m13.4 through m13.6, you’ll have to fit them with the rethinking package or directly in Stan. Our models b13.4 through b13.6 will be the non-centered brms alternatives. Either way, the models make the same predictions, but the nuts and bolts and gears we’ll use to construct our multilevel golems will look a little different. With all that in mind, here’s how we might express our statistical model using the non-centered parameterization more faithful to the way it will be expressed with brms::brm(): \\[\\begin{align*} \\text{left_pull}_i &amp; \\sim \\operatorname{Binomial}(n_i = 1, p_i) \\\\ \\operatorname{logit} (p_i) &amp; = \\bar \\alpha + \\beta_{\\text{treatment}[i]} + \\color{#CD8500}{z_{\\text{actor}[i]} \\sigma_\\alpha + x_{\\text{block}[i]} \\sigma_\\gamma} \\\\ \\bar \\alpha &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\beta_j &amp; \\sim \\operatorname{Normal}(0, 0.5) \\;\\;\\; , \\text{for } j = 1, \\dots, 4 \\\\ \\color{#CD8500}{z_j} &amp; \\color{#CD8500}\\sim \\color{#CD8500}{\\operatorname{Normal}(0, 1)} \\\\ \\color{#CD8500}{x_j} &amp; \\color{#CD8500}\\sim \\color{#CD8500}{\\operatorname{Normal}(0, 1)} \\\\ \\sigma_\\alpha &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_\\gamma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] If you jump ahead to Section 13.4.2, you’ll see this is just re-write of the formula on the top of page 424. For now, let’s load the data. data(chimpanzees, package = &quot;rethinking&quot;) d &lt;- chimpanzees rm(chimpanzees) Wrangle and view. d &lt;- d %&gt;% mutate(actor = factor(actor), block = factor(block), treatment = factor(1 + prosoc_left + 2 * condition)) glimpse(d) ## Rows: 504 ## Columns: 9 ## $ actor &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ recipient &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ condition &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ block &lt;fct&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, … ## $ trial &lt;int&gt; 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46,… ## $ prosoc_left &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, … ## $ chose_prosoc &lt;int&gt; 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, … ## $ pulled_left &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, … ## $ treatment &lt;fct&gt; 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, … Even when using the non-centered parameterization, McElreath’s m13.4 is a bit of an odd model to translate into brms syntax. To my knowledge, it can’t be done with conventional syntax. But we can fit the model with careful use of the non-linear syntax, which might look like this. b13.4 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 1 + (1 | actor) + (1 | block), b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = b), prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = a), prior(exponential(1), class = sd, group = actor, nlpar = a), prior(exponential(1), class = sd, group = block, nlpar = a)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, file = &quot;fits/b13.04&quot;) The b ~ 0 + treatment part of the formula is our expression of what we wrote above as \\(\\beta_{\\text{treatment}[i]}\\). There’s a lot going on with the a ~ 1 + (1 | actor) + (1 | block) part of the formula. The initial 1 outside of the parenthesis is \\(\\bar \\alpha\\). The (1 | actor) and (1 | block) parts correspond to \\(z_{\\text{actor}[i]} \\sigma_\\alpha\\) and \\(x_{\\text{block}[i]} \\sigma_\\gamma\\), respectively. Check the trace plots. library(bayesplot) color_scheme_set(&quot;orange&quot;) post &lt;- posterior_samples(b13.4, add_chain = T) post %&gt;% mcmc_trace(pars = vars(-iter, -lp__), facet_args = list(ncol = 4), size = .15) + theme(legend.position = &quot;none&quot;) They all look fine. In the text (e.g., page 416), McElreath briefly mentioned warnings about divergent transitions. We didn’t get any warnings like that. Keep following along and you’ll soon learn why. Here’s a look at the summary when using print(). print(b13.4) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ a + b ## a ~ 1 + (1 | actor) + (1 | block) ## b ~ 0 + treatment ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~actor (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(a_Intercept) 1.98 0.63 1.08 3.52 1.00 1612 2195 ## ## ~block (Number of levels: 6) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(a_Intercept) 0.20 0.17 0.01 0.62 1.00 1450 1567 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_Intercept 0.60 0.71 -0.83 2.07 1.01 999 1494 ## b_treatment1 -0.14 0.30 -0.74 0.43 1.00 2000 2559 ## b_treatment2 0.39 0.30 -0.20 0.97 1.00 2189 3051 ## b_treatment3 -0.48 0.30 -1.06 0.09 1.00 2094 2688 ## b_treatment4 0.28 0.30 -0.29 0.88 1.00 2113 2765 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). When you use the (1 | &lt;group&gt;) syntax within brm(), the group-specific parameters are not shown with print(). You only get the hierarchical \\(\\sigma_\\text{&lt;group&gt;}\\) summaries, shown here as the two rows for sd(a_Intercept). However, you can get a summary of all the parameters with the posterior_summary() function. posterior_summary(b13.4) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_a_Intercept 0.60 0.71 -0.83 2.07 ## b_b_treatment1 -0.14 0.30 -0.74 0.43 ## b_b_treatment2 0.39 0.30 -0.20 0.97 ## b_b_treatment3 -0.48 0.30 -1.06 0.09 ## b_b_treatment4 0.28 0.30 -0.29 0.88 ## sd_actor__a_Intercept 1.98 0.63 1.08 3.52 ## sd_block__a_Intercept 0.20 0.17 0.01 0.62 ## r_actor__a[1,Intercept] -0.96 0.72 -2.42 0.47 ## r_actor__a[2,Intercept] 4.08 1.35 2.01 7.29 ## r_actor__a[3,Intercept] -1.26 0.73 -2.75 0.17 ## r_actor__a[4,Intercept] -1.26 0.73 -2.71 0.19 ## r_actor__a[5,Intercept] -0.96 0.72 -2.38 0.49 ## r_actor__a[6,Intercept] -0.01 0.72 -1.46 1.46 ## r_actor__a[7,Intercept] 1.51 0.77 0.03 3.09 ## r_block__a[1,Intercept] -0.16 0.22 -0.72 0.14 ## r_block__a[2,Intercept] 0.04 0.18 -0.29 0.45 ## r_block__a[3,Intercept] 0.05 0.18 -0.27 0.47 ## r_block__a[4,Intercept] 0.01 0.18 -0.35 0.41 ## r_block__a[5,Intercept] -0.03 0.17 -0.40 0.32 ## r_block__a[6,Intercept] 0.11 0.20 -0.19 0.61 ## lp__ -286.98 3.87 -295.39 -280.38 We might make the coefficient plot of Figure 13.4.a with bayesplot::mcmc_plot(). mcmc_plot(b13.4, pars = c(&quot;^r_&quot;, &quot;^b_&quot;, &quot;^sd_&quot;)) + theme(axis.text.y = element_text(hjust = 0)) For a little more control, we might switch to a tidybayes-oriented approach. # this is all stylistic fluff levels &lt;- c(&quot;sd_block__a_Intercept&quot;, &quot;sd_actor__a_Intercept&quot;, &quot;b_a_Intercept&quot;, str_c(&quot;r_block__a[&quot;, 6:1, &quot;,Intercept]&quot;), str_c(&quot;r_actor__a[&quot;, 7:1, &quot;,Intercept]&quot;), str_c(&quot;b_b_treatment&quot;, 4:1)) text &lt;- tibble(x = posterior_summary(b13.4, probs = c(0.055, 0.955),)[&quot;r_actor__a[2,Intercept]&quot;, c(3, 1)], y = c(13.5, 16.5), label = c(&quot;89% CI&quot;, &quot;mean&quot;), hjust = c(.5, 0)) arrow &lt;- tibble(x = posterior_summary(b13.4, probs = c(0.055, 0.955),)[&quot;r_actor__a[2,Intercept]&quot;, c(3, 1)] + c(- 0.3, 0.2), xend = posterior_summary(b13.4, probs = c(0.055, 0.955),)[&quot;r_actor__a[2,Intercept]&quot;, c(3, 1)], y = c(14, 16), yend = c(14.8, 15.35)) # here&#39;s the main event post %&gt;% pivot_longer(-(iter:lp__)) %&gt;% mutate(name = factor(name, levels = levels)) %&gt;% ggplot(aes(x = value, y = name)) + stat_pointinterval(point_interval = mean_qi, .width = .89, shape = 21, size = 1, point_size = 2, point_fill = &quot;blue&quot;) + geom_text(data = text, aes(x = x, y = y, label = label, hjust = hjust)) + geom_segment(data = arrow, aes(x = x, xend = xend, y = y, yend = yend), arrow = arrow(length = unit(0.15, &quot;cm&quot;))) + theme(axis.text.y = element_text(hjust = 0), panel.grid.major.y = element_line(linetype = 3)) Regardless of whether we use a bayesplot- or tidybayes-oriented workflow, a careful look at our coefficient plots will show the parameters are a little different from those McElreath reported. Again, this is because of the subtle differences between our non-centered parameterization and McElreath’s centered parameterization. This will all make more sense in Section 13.4. Now use post to compare the group-level \\(\\sigma\\) parameters as in Figure 13.4.b. post %&gt;% pivot_longer(starts_with(&quot;sd&quot;)) %&gt;% ggplot(aes(x = value, fill = name)) + geom_density(size = 0, alpha = 3/4, adjust = 2/3, show.legend = F) + annotate(geom = &quot;text&quot;, x = 0.67, y = 2, label = &quot;block&quot;, color = &quot;orange4&quot;) + annotate(geom = &quot;text&quot;, x = 2.725, y = 0.5, label = &quot;actor&quot;, color = &quot;orange1&quot;) + scale_fill_manual(values = str_c(&quot;orange&quot;, c(1, 4))) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(sigma[&quot;&lt;group&gt;&quot;])) + coord_cartesian(xlim = c(0, 4)) Since both the coefficient plots and the density plots indicate there is much more variability among the actor parameters than in the block parameters, we might fit a model that ignores the variation among the levels of block. b13.5 &lt;- brm(data = d, family = binomial, bf(pulled_left | trials(1) ~ a + b, a ~ 1 + (1 | actor), b ~ 0 + treatment, nl = TRUE), prior = c(prior(normal(0, 0.5), nlpar = b), prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = a), prior(exponential(1), class = sd, group = actor, nlpar = a)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, file = &quot;fits/b13.05&quot;) We might compare our models by their WAIC estimates. b13.4 &lt;- add_criterion(b13.4, &quot;waic&quot;) b13.5 &lt;- add_criterion(b13.5, &quot;waic&quot;) loo_compare(b13.4, b13.5, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13.5 0.0 0.0 -265.6 9.6 8.6 0.4 531.2 19.2 ## b13.4 -0.4 0.8 -266.0 9.7 10.5 0.5 532.1 19.4 model_weights(b13.4, b13.5, weights = &quot;waic&quot;) %&gt;% round(digits = 2) ## b13.4 b13.5 ## 0.4 0.6 The two models yield nearly-equivalent WAIC estimates. Just as in the text, our p_waic column shows the models differ by about 2 effective parameters due to the shrinkage from the multilevel partial pooling. Yet recall what McElreath wrote: There is nothing to gain here by selecting either model. The comparison of the two models tells a richer story… Since this is an experiment, there is nothing to really select. The experimental design tells us the relevant causal model to inspect. (pp. 418–419) 13.3.2 Even more clusters. We can extend partial pooling to the treatment conditions, too. With brms, it will be more natural to revert to the conventional formula syntax. b13.6 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | block) + (1 | treatment), prior = c(prior(normal(0, 1.5), class = Intercept), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 13, file = &quot;fits/b13.06&quot;) Recall that with brms, we don’t have a coeftab() like with McElreath’s rethinking. For us, one approach would be to compare the relevent rows from fixef(b13.4) to the relevant elements from ranef(b13.6). tibble(parameter = str_c(&quot;b[&quot;, 1:4, &quot;]&quot;), `b13.4` = fixef(b13.4)[2:5, 1], `b13.6` = ranef(b13.6)$treatment[, 1, &quot;Intercept&quot;]) %&gt;% mutate_if(is.double, round, digits = 2) ## # A tibble: 4 x 3 ## parameter b13.4 b13.6 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b[1] -0.14 -0.1 ## 2 b[2] 0.39 0.4 ## 3 b[3] -0.48 -0.43 ## 4 b[4] 0.28 0.290 Like in the text, “these are not identical, but they are very close” (p. 419). We might compare the group-level \\(\\sigma\\) parameters with a plot. posterior_samples(b13.6) %&gt;% pivot_longer(starts_with(&quot;sd&quot;)) %&gt;% mutate(group = str_remove(name, &quot;sd_&quot;) %&gt;% str_remove(., &quot;__Intercept&quot;)) %&gt;% mutate(parameter = str_c(&quot;sigma[&quot;, group,&quot;]&quot;)) %&gt;% ggplot(aes(x = value, y = parameter)) + stat_halfeye(.width = .95, size = 1, fill = &quot;orange&quot;, adjust = 0.1) + scale_y_discrete(labels = ggplot2:::parse_safe) + labs(subtitle = &quot;The variation among treatment levels is small, but the\\nvariation among the levels of block is still the smallest.&quot;) + coord_cartesian(ylim = c(1.5, 3)) + theme(axis.text.y = element_text(hjust = 0)) Among the three \\(\\sigma_\\text{&lt;group&gt;}\\) parameters, \\(\\sigma_\\text{block}\\) is the smallest. Now we’ll compare b13.6 to the last two models with the WAIC. b13.6 &lt;- add_criterion(b13.6, &quot;waic&quot;) loo_compare(b13.4, b13.5, b13.6, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b13.5 0.0 0.0 -265.6 9.6 8.6 0.4 531.2 19.2 ## b13.4 -0.4 0.8 -266.0 9.7 10.5 0.5 532.1 19.4 ## b13.6 -1.0 0.8 -266.6 9.6 10.9 0.5 533.3 19.2 model_weights(b13.4, b13.5, b13.6, weights = &quot;loo&quot;) %&gt;% round(digits = 2) ## b13.4 b13.5 b13.6 ## 0.33 0.50 0.18 The models show little difference “on purely predictive criteria. This is the typical result, when each cluster (each treatment here) has a lot of data to inform its parameters” (p. 419). Unlike in the text, we didn’t have a problem with divergent transitions. We’ll see why in the next section. Before we move on, this section just hints at a historical software difficulty. In short, it’s not uncommon to have a theory-based model that includes multiple sources of clustering (i.e., requiring many ( &lt;varying parameter(s)&gt; | &lt;grouping variable(s)&gt; ) parts in the model formula). This can make for all kinds of computational difficulties and result in software error messages, inadmissible solutions, and so on. One of the practical solutions to difficulties like these has been to simplify the statistical models by removing some of the clustering terms. Even though such simpler models were not the theory-based ones, at least they yielded solutions. Nowadays, Stan (via brms or otherwise) is making it easier to fit the full theoretically-based model. To learn more about this topic, check out this nice blog post by Michael Frank, Mixed effects models: Is it time to go Bayesian by default?. Make sure to check out the discussion in the comments section, which includes all-stars like Bürkner and Douglas Bates. You can get more context for the issue from Barr et al. (2013), Random effects structure for confirmatory hypothesis testing: Keep it maximal. 13.4 Divergent transitions and non-centered priors Although we did not get divergent transitions warnings in from our last few models the way McElreath did with his, the issues is still relevant for brms. One of the best things about Hamiltonian Monte Carlo is that it provides internal checks of efficiency and accuracy. One of these checks comes free, arising from the constraints on the physics simulation. Recall that HMC simulates the frictionless flow of a particle on a surface. In any given transition, which is just a single flick of the particle, the total energy at the start should be equal to the total energy at the end. That’s how energy in a closed system works. And in a purely mathematical system, the energy is always conserved correctly. It’s just a fact about the physics. But in a numerical system, it might not be. Sometimes the total energy is not the same at the end as it was at the start. In these cases, the energy is divergent. How can this happen? It tends to happen when the posterior distribution is very steep in some region of parameter space. Steep changes in probability are hard for a discrete physics simulation to follow. When that happens, the algorithm notices by comparing the energy at the start to the energy at the end. When they don’t match, it indicates numerical problems exploring that part of the posterior distribution. Divergent transitions are rejected. They don’t directly damage your approximation of the posterior distribution. But they do hurt it indirectly, because the region where divergent transitions happen is hard to explore correctly. (p. 420, emphasis in the original) Two primary ways to handle divergent transitions are by increasing the adapt_delta parameter, which we’ve already done a few times in previous chapters, or reparameterizing the model. As McElreath will cover in a bit, switching from the centered to the non-centered parameterization will often work when using multilevel models. 13.4.1 The Devil’s Funnel. McElreath posed a joint distribution \\[\\begin{align*} v &amp; \\sim \\operatorname{Normal}(0, 3) \\\\ x &amp; \\sim \\operatorname{Normal}(0, \\exp(v)), \\end{align*}\\] where the scale of \\(x\\) depends on another variable, \\(v\\). In R code 13.26, McElreath then proposed fitting the following model with rethinking::ulam(). m13.7 &lt;- ulam( data = list(N = 1), alist( v ~ normal(0, 3), x ~ normal(0, exp(v)) ), chains = 4 ) I’m not aware that you can do something like this with brms. If you think I’m in error, please share your solution. We can at least get a sense of the model by simulating from the joint distribution and plotting. set.seed(13) tibble(v = rnorm(1e3, mean = 0, sd = 3)) %&gt;% mutate(x = rnorm(1e3, mean = 0, sd = exp(v))) %&gt;% ggplot(aes(x = x)) + geom_histogram(binwidth = 1, fill = &quot;orange2&quot;) + annotate(geom = &quot;text&quot;, x = -100, y = 490, hjust = 0, label = expression(italic(v)%~%Normal(0, 3))) + annotate(geom = &quot;text&quot;, x = -100, y = 440, hjust = 0, label = expression(italic(x)%~%Normal(0, exp(italic(v))))) + coord_cartesian(xlim = c(-100, 100)) + scale_y_continuous(breaks = NULL) The distribution looks something like a Student-\\(t\\) with a very low \\(\\nu\\) parameter. We can express the joint likelihood of \\(p(v, x)\\) as \\[p(v, x) = p(x | v)\\ p(v).\\] Here that is in a plot. # define the parameter space parameter_space &lt;- seq(from = -4, to = 4, length.out = 200) # simulate crossing(v = parameter_space, x = parameter_space) %&gt;% mutate(likelihood_v = dnorm(v, mean = 0, sd = 3), likelihood_x = dnorm(x, mean = 0, sd = exp(v))) %&gt;% mutate(joint_likelihood = likelihood_v * likelihood_x) %&gt;% # plot! ggplot(aes(x = x, y = v, fill = joint_likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(subtitle = &quot;Centered parameterization&quot;) + theme(legend.position = &quot;none&quot;) This ends up as a version of McElreath’s Figure 13.5.a. At low values of \\(v\\), the distribution of \\(x\\) contracts around zero. This forms a very steep valley that the Hamiltonian particle needs to explore. Steep surfaces are hard to simulate, because the simulation is not actually continuous. It happens in discrete steps. If the steps are too big, the simulation will overshoot. (p. 421) To avoid the divergent transitions than can arise from steep valleys like this, we can switch from our original formula to a non-centered parameterization, such as: \\[\\begin{align*} v &amp; \\sim \\operatorname{Normal}(0, 3) \\\\ z &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ x &amp; = z \\exp(v), \\end{align*}\\] where \\(x\\) is now the product of two independent distributions, \\(v\\) and \\(z\\). With this parameterization, we can express the joint likelihood \\(p(v, z)\\) as \\[p(v, z) = p(z) \\ p(v),\\] where \\(p(z)\\) is not conditional on \\(v\\) and \\(p(v)\\) is not conditional on \\(z\\). Here’s what that looks like in a plot. # simulate crossing(v = parameter_space, z = parameter_space / 2) %&gt;% mutate(likelihood_v = dnorm(v, mean = 0, sd = 3), likelihood_z = dnorm(z, mean = 0, sd = 1)) %&gt;% mutate(joint_likelihood = likelihood_v * likelihood_z) %&gt;% # plot! ggplot(aes(x = z, y = v, fill = joint_likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(subtitle = &quot;Non-centered parameterization&quot;) + theme(legend.position = &quot;none&quot;) This is our version of the right-hand panel of McElreath’s Figure 13.5. No nasty funnel–just a friendly glowing likelihood orb. 13.4.2 Non-centered chimpanzees. At the top of the section, McElreath reported the rethinking::ulam() default is to set adapt_delta = 0.95. Readers should be aware that the brms::brm() default is adapt_delta = 0.80. A consequence of this difference is rethinking::ulam() will tend to take smaller step sizes than brms::brm(), at the cost of slower exploration of the posterior. I don’t know that one is inherently better than the other. They’re just defaults. Recall that due to how brms only supports the non-centered parameterization, we have already fit our version of McElreath’s m13.4nc. We called it b13.4. Here is the model summary, again. print(b13.4) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ a + b ## a ~ 1 + (1 | actor) + (1 | block) ## b ~ 0 + treatment ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~actor (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(a_Intercept) 1.98 0.63 1.08 3.52 1.00 1612 2195 ## ## ~block (Number of levels: 6) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(a_Intercept) 0.20 0.17 0.01 0.62 1.00 1450 1567 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_Intercept 0.60 0.71 -0.83 2.07 1.01 999 1494 ## b_treatment1 -0.14 0.30 -0.74 0.43 1.00 2000 2559 ## b_treatment2 0.39 0.30 -0.20 0.97 1.00 2189 3051 ## b_treatment3 -0.48 0.30 -1.06 0.09 1.00 2094 2688 ## b_treatment4 0.28 0.30 -0.29 0.88 1.00 2113 2765 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Because we only fit this model using the non-centered parameterization, we won’t be able to fully reproduce McElreath’s Figure 13.6. But we can still plot our effective sample sizes. Recall that unlike the way rethinking only reports n_eff, brms now reports both Bulk_ESS and Tail_ESS (see Vehtari, Gelman, et al., 2019). At the moment, brms does not offer a convenience function that allows users to collect those values in a data frame. However you can do so with help from the posterior package (Bürkner et al., 2020), which has not made its way to CRAN, yet, but can be downloaded directly from GitHub. # install the beta release with this install.packages(&quot;posterior&quot;, repos = c(&quot;https://mc-stan.org/r-packages/&quot;, getOption(&quot;repos&quot;))) # install the latest development version with this instead install.packages(&quot;remotes&quot;) remotes::install_github(&quot;stan-dev/posterior&quot;) For our purposes, the function of interest is summarise_draws(), which will take the output from posterior_samples() as input. library(posterior) posterior_samples(b13.4) %&gt;% summarise_draws() ## # A tibble: 21 x 10 ## variable mean median sd mad q5 q95 rhat ess_bulk ess_tail ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 b_a_Intercept 0.605 0.602 0.714 0.654 -0.556 1.78 1.00 993. 1482. ## 2 b_b_treatment1 -0.139 -0.138 0.296 0.295 -0.627 0.339 1.00 1953. 2530. ## 3 b_b_treatment2 0.390 0.393 0.299 0.298 -0.105 0.878 1.00 2159. 2979. ## 4 b_b_treatment3 -0.480 -0.475 0.295 0.298 -0.978 0.00308 1.00 2054. 2616. ## 5 b_b_treatment4 0.279 0.273 0.301 0.308 -0.213 0.787 1.00 2066. 2750. ## 6 sd_actor__a_Intercept 1.98 1.87 0.632 0.553 1.17 3.16 1.00 1576. 2090. ## 7 sd_block__a_Intercept 0.203 0.166 0.173 0.146 0.0144 0.524 1.00 1430. 1548. ## 8 r_actor__a[1,Intercept] -0.961 -0.954 0.719 0.669 -2.15 0.205 1.00 998. 1642. ## 9 r_actor__a[2,Intercept] 4.08 3.88 1.35 1.20 2.28 6.50 1.00 1972. 2408. ## 10 r_actor__a[3,Intercept] -1.26 -1.26 0.729 0.676 -2.44 -0.0535 1.00 973. 1584. ## # … with 11 more rows Note how the last three columns are the rhat, the ess_bulk, and the ess_tail. Here we summarize those two effective sample size columns in a scatter plot similar to Figure 13.6, but based only on our b13.4, which used the non-centered parameterization. posterior_samples(b13.4) %&gt;% summarise_draws() %&gt;% ggplot(aes(x = ess_bulk, y = ess_tail)) + geom_abline(linetype = 2) + geom_point(color = &quot;blue&quot;) + xlim(0, 4700) + ylim(0, 4700) + ggtitle(&quot;Effective sample size summaries for b13.4&quot;, subtitle = &quot;ess_bulk is on the x and ess_tail is on the y&quot;) + theme(plot.subtitle = element_text(size = 10), plot.title = element_text(size = 11.5), plot.title.position = &quot;plot&quot;) Both measures of effective sample size are fine. So should we always use the non-centered parameterization? No. Sometimes the centered form is better. It could even be true that the centered form is better for one cluster in a model while the non-centered form is better for another cluster in the same model. It all depends upon the details. Typically, a cluster with low variation, like the blocks in m13.4, will sample better with a non-centered prior. And if you have a large number of units inside a cluster, but not much data for each unit, then the non-centered is also usually better. But being able to switch back and forth as needed is very useful. (p. 425) I won’t argue with McElreath, here. But if you run into a situation where you’d like to use the centered parameterization, you will have to use rethinking or fit your model directly in Stan. brms won’t support you, there. 13.5 Multilevel posterior predictions Every model is a merger of sense and nonsense. When we understand a model, we can find its sense and control its nonsense. But as models get more complex, it is very difficult to impossible to understand them just by inspecting tables of posterior means and intervals. Exploring implied posterior predictions helps much more…. The introduction of varying effects does introduce nuance, however. First, we should no longer expect the model to exactly retrodict the sample, because adaptive regularization has as its goal to trade off poorer fit in sample for better inference and hopefully better fit out of sample. That is what shrinkage does for us. Of course, we should never be trying to really retrodict the sample. But now you have to expect that even a perfectly good model fit will differ from the raw data in a systematic way. Second, “prediction” in the context of a multilevel model requires additional choices. If we wish to validate a model against the specific clusters used to fit the model, that is one thing. But if we instead wish to compute predictions for new clusters, other than the ones observed in the sample, that is quite another. We’ll consider each of these in turn, continuing to use the chimpanzees model from the previous section. (p. 426) 13.5.1 Posterior prediction for same clusters. Like McElreath did in the text, we’ll do this two ways. Recall we use brms::fitted() in place of rethinking::link(). chimp &lt;- 2 nd &lt;- d %&gt;% distinct(treatment) %&gt;% mutate(actor = chimp, block = 1) labels &lt;- c(&quot;R/N&quot;, &quot;L/N&quot;, &quot;R/P&quot;, &quot;L/P&quot;) f &lt;- fitted(b13.4, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(treatment = factor(treatment, labels = labels)) f ## Estimate Est.Error Q2.5 Q97.5 treatment actor block ## 1 0.9790908 0.01961220 0.9277346 0.9994592 R/N 2 1 ## 2 0.9873767 0.01242438 0.9547778 0.9996282 L/N 2 1 ## 3 0.9711403 0.02662191 0.9008207 0.9991563 R/P 2 1 ## 4 0.9860410 0.01340503 0.9501147 0.9996183 L/P 2 1 Here are the empirical probabilities computed directly from the data (i.e., the no-pooling model). ( chimp_2_d &lt;- d %&gt;% filter(actor == chimp) %&gt;% group_by(treatment) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() %&gt;% mutate(treatment = factor(treatment, labels = labels)) ) ## # A tibble: 4 x 2 ## treatment prob ## &lt;fct&gt; &lt;dbl&gt; ## 1 R/N 1 ## 2 L/N 1 ## 3 R/P 1 ## 4 L/P 1 McElreath didn’t show the corresponding plot in the text. It might look like this. f %&gt;% # if you want to use `geom_line()` or `geom_ribbon()` with a factor on the x-axis, # you need to code something like `group = 1` in `aes()` ggplot(aes(x = treatment, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + geom_point(data = chimp_2_d, aes(y = prob), color = &quot;grey25&quot;) + ggtitle(&quot;Chimp #2&quot;, subtitle = &quot;The posterior mean and 95%\\nintervals are the blue line\\nand orange band, respectively.\\nThe empirical means are\\nthe charcoal dots.&quot;) + coord_cartesian(ylim = c(.75, 1)) + theme(plot.subtitle = element_text(size = 10)) Do note how severely we’ve restricted the \\(y\\)-axis range. But okay, now let’s do things by hand. We’ll need to extract the posterior samples and look at the structure of the data. post &lt;- posterior_samples(b13.4) glimpse(post) ## Rows: 4,000 ## Columns: 21 ## $ b_a_Intercept &lt;dbl&gt; 1.3021558, 1.2230044, 1.4514463, 1.2493757, 1.4410301, 0.8879041, 0.340566… ## $ b_b_treatment1 &lt;dbl&gt; 0.045943207, 0.193764903, -0.144046413, 0.113641650, 0.050649844, 0.050379… ## $ b_b_treatment2 &lt;dbl&gt; 0.527943052, 0.607728359, 0.664090709, 0.427799420, 0.584119370, 0.8932382… ## $ b_b_treatment3 &lt;dbl&gt; -0.65337805, -0.60835264, -0.33002358, -0.30180977, -0.27854154, 0.0605969… ## $ b_b_treatment4 &lt;dbl&gt; 0.62706385, 0.62201381, 0.29950707, 0.55240433, 0.42444655, 0.32087526, 0.… ## $ sd_actor__a_Intercept &lt;dbl&gt; 1.703332, 1.882084, 1.649978, 1.752502, 1.801833, 1.506574, 1.510735, 1.82… ## $ sd_block__a_Intercept &lt;dbl&gt; 0.22532974, 0.13789393, 0.44017726, 0.11146800, 0.15788464, 0.21062242, 0.… ## $ `r_actor__a[1,Intercept]` &lt;dbl&gt; -2.0639022, -2.0221336, -1.7018977, -1.7352499, -1.8256881, -1.7212934, -0… ## $ `r_actor__a[2,Intercept]` &lt;dbl&gt; 3.445438, 3.504269, 3.045323, 3.254185, 2.974482, 4.194500, 2.500664, 3.55… ## $ `r_actor__a[3,Intercept]` &lt;dbl&gt; -2.109574, -2.076856, -1.995991, -1.886381, -2.162284, -1.955175, -1.16851… ## $ `r_actor__a[4,Intercept]` &lt;dbl&gt; -2.2207650, -1.8723443, -2.2369024, -2.0233436, -2.3830720, -1.7420705, -1… ## $ `r_actor__a[5,Intercept]` &lt;dbl&gt; -2.0945013, -2.1226541, -1.2873641, -2.0951884, -2.0873063, -1.4032776, -1… ## $ `r_actor__a[6,Intercept]` &lt;dbl&gt; -1.0146662, -1.1288649, -0.6976608, -0.8773965, -0.6263590, -1.2479195, 0.… ## $ `r_actor__a[7,Intercept]` &lt;dbl&gt; 1.25759931, 0.93388493, 0.74569028, 0.72711619, 0.72208450, 0.44192249, 1.… ## $ `r_block__a[1,Intercept]` &lt;dbl&gt; 0.073141572, -0.216067313, -0.506808535, -0.030269403, -0.061018504, 0.035… ## $ `r_block__a[2,Intercept]` &lt;dbl&gt; 0.2421557109, -0.0631763240, -0.1262601137, 0.0867576875, 0.2595894945, 0.… ## $ `r_block__a[3,Intercept]` &lt;dbl&gt; 0.14889112, 0.12668037, 0.24547520, -0.08049143, -0.15796409, -0.07350171,… ## $ `r_block__a[4,Intercept]` &lt;dbl&gt; 0.0102698121, -0.1197069569, -0.2647210412, 0.0695288459, 0.0594889707, 0.… ## $ `r_block__a[5,Intercept]` &lt;dbl&gt; -0.067246997, 0.043689655, -0.239099466, 0.085397146, 0.166067563, -0.0231… ## $ `r_block__a[6,Intercept]` &lt;dbl&gt; -0.0192112634, 0.0955534719, 0.1932751562, 0.0690833532, 0.1700483555, 0.4… ## $ lp__ &lt;dbl&gt; -285.4452, -285.5290, -281.8289, -282.9040, -285.8187, -290.3204, -288.476… McElreath didn’t show what his R code 13.33 dens( post$a[,5] ) would look like. But here’s our analogue. post %&gt;% transmute(actor_5 = `r_actor__a[5,Intercept]`) %&gt;% ggplot(aes(x = actor_5)) + geom_density(size = 0, fill = &quot;blue&quot;) + scale_y_continuous(breaks = NULL) + ggtitle(&quot;Chimp #5&#39;s density&quot;) And because we made the density only using the r_actor__a[5,Intercept] values (i.e., we didn’t add b_Intercept to them), the density is in a deviance-score metric. McElreath built his own link() function in R code 13.34. With this particular model, it will be easiest for us to just work directly with post. f &lt;- post %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(b_a_Intercept + value + `r_actor__a[1,Intercept]` + `r_block__a[1,Intercept]`)) %&gt;% mutate(treatment = factor(str_remove(name, &quot;b_b_treatment&quot;), labels = labels)) %&gt;% select(name:treatment) f ## # A tibble: 16,000 x 4 ## name value fitted treatment ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 b_b_treatment1 0.0459 0.345 R/N ## 2 b_b_treatment2 0.528 0.460 L/N ## 3 b_b_treatment3 -0.653 0.207 R/P ## 4 b_b_treatment4 0.627 0.485 L/P ## 5 b_b_treatment1 0.194 0.305 R/N ## 6 b_b_treatment2 0.608 0.400 L/N ## 7 b_b_treatment3 -0.608 0.165 R/P ## 8 b_b_treatment4 0.622 0.403 L/P ## 9 b_b_treatment1 -0.144 0.289 R/N ## 10 b_b_treatment2 0.664 0.477 L/N ## # … with 15,990 more rows Now we’ll summarize those values and compute their empirical analogues directly from the data. # the posterior summaries ( f &lt;- f %&gt;% group_by(treatment) %&gt;% tidybayes::mean_qi(fitted) ) ## # A tibble: 4 x 7 ## treatment fitted .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 R/N 0.346 0.200 0.503 0.95 mean qi ## 2 L/N 0.470 0.303 0.631 0.95 mean qi ## 3 R/P 0.275 0.153 0.420 0.95 mean qi ## 4 L/P 0.443 0.273 0.604 0.95 mean qi # the empirical summaries chimp &lt;- 5 ( chimp_5_d &lt;- d %&gt;% filter(actor == chimp) %&gt;% group_by(treatment) %&gt;% summarise(prob = mean(pulled_left)) %&gt;% ungroup() %&gt;% mutate(treatment = factor(treatment, labels = labels)) ) ## # A tibble: 4 x 2 ## treatment prob ## &lt;fct&gt; &lt;dbl&gt; ## 1 R/N 0.333 ## 2 L/N 0.556 ## 3 R/P 0.278 ## 4 L/P 0.5 Okay, let’s see how good we are at retrodicting the pulled_left probabilities for actor == 5. f %&gt;% ggplot(aes(x = treatment, y = fitted, group = 1)) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + geom_point(data = chimp_5_d, aes(y = prob), color = &quot;grey25&quot;) + ggtitle(&quot;Chimp #5&quot;, subtitle = &quot;This plot is like the last except\\nwe did more by hand.&quot;) + coord_cartesian(ylim = 0:1) + theme(plot.subtitle = element_text(size = 10)) Not bad. 13.5.2 Posterior prediction for new clusters. By average actor, McElreath referred to a chimp with an intercept exactly at the population mean \\(\\bar \\alpha\\). Given our non-centered parameterization for b13.4, this means we’ll leave out the random effects for actor. Since we’re predicting what might happen in new experimental blocks, we’ll leave out the random effects for block, too. When doing this by hand, the workflow is much like is was before, just with fewer columns added together within the first mutate() line. f &lt;- post %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(b_a_Intercept + value)) %&gt;% mutate(treatment = factor(str_remove(name, &quot;b_b_treatment&quot;), labels = labels)) %&gt;% select(name:treatment) %&gt;% group_by(treatment) %&gt;% # note we&#39;re using 80% intervals mean_qi(fitted, .width = .8) f ## # A tibble: 4 x 7 ## treatment fitted .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 R/N 0.604 0.400 0.795 0.8 mean qi ## 2 L/N 0.711 0.529 0.868 0.8 mean qi ## 3 R/P 0.528 0.321 0.734 0.8 mean qi ## 4 L/P 0.690 0.502 0.856 0.8 mean qi Make Figure 13.7.a. p1 &lt;- f %&gt;% ggplot(aes(x = treatment, y = fitted, group = 1)) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + ggtitle(&quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) + theme(plot.title = element_text(size = 14, hjust = .5)) p1 If we want to depict the variability across the chimps, we need to include sd_actor__a_Intercept into the calculations. In the first block of code, below, we simulate a bundle of new intercepts defined by \\[\\text{simulated chimpanzees} \\sim \\operatorname{Normal}(\\bar \\alpha, \\sigma_\\alpha).\\] As before, we are also averaging over block. set.seed(13) f &lt;- post %&gt;% # simulated chimpanzees mutate(a_sim = rnorm(n(), mean = b_a_Intercept, sd = sd_actor__a_Intercept)) %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(a_sim + value)) %&gt;% mutate(treatment = factor(str_remove(name, &quot;b_b_treatment&quot;), labels = labels)) %&gt;% group_by(treatment) %&gt;% # note we&#39;re using 80% intervals mean_qi(fitted, .width = .8) f ## # A tibble: 4 x 7 ## treatment fitted .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 R/N 0.569 0.102 0.956 0.8 mean qi ## 2 L/N 0.645 0.163 0.974 0.8 mean qi ## 3 R/P 0.518 0.0754 0.939 0.8 mean qi ## 4 L/P 0.630 0.148 0.969 0.8 mean qi Behold Figure 13.7.b. p2 &lt;- f %&gt;% ggplot(aes(x = treatment, y = fitted, group = 1)) + geom_ribbon(aes(ymin = .lower, ymax = .upper), fill = &quot;orange1&quot;) + geom_line(color = &quot;blue&quot;) + ggtitle(&quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) + theme(plot.title = element_text(size = 14, hjust = .5)) p2 The big difference between this workflow and the last is now we start of by marking off the rows in post with an iter index and then use slice_sample() to randomly sample 100 posterior rows. We also omit the group_by() and mean_qi() lines at the end. # how many simulated chimps would you like? n_chimps &lt;- 100 set.seed(13) f &lt;- post %&gt;% mutate(iter = 1:n()) %&gt;% slice_sample(n = n_chimps) %&gt;% # simulated chimpanzees mutate(a_sim = rnorm(n(), mean = b_a_Intercept, sd = sd_actor__a_Intercept)) %&gt;% pivot_longer(b_b_treatment1:b_b_treatment4) %&gt;% mutate(fitted = inv_logit_scaled(a_sim + value)) %&gt;% mutate(treatment = factor(str_remove(name, &quot;b_b_treatment&quot;), labels = labels)) %&gt;% select(iter:treatment) f ## # A tibble: 400 x 6 ## iter a_sim name value fitted treatment ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 1496 -6.53 b_b_treatment1 -1.03 0.000523 R/N ## 2 1496 -6.53 b_b_treatment2 -0.214 0.00118 L/N ## 3 1496 -6.53 b_b_treatment3 -1.80 0.000241 R/P ## 4 1496 -6.53 b_b_treatment4 -0.561 0.000832 L/P ## 5 3843 -0.978 b_b_treatment1 -0.100 0.254 R/N ## 6 3843 -0.978 b_b_treatment2 0.444 0.370 L/N ## 7 3843 -0.978 b_b_treatment3 -0.571 0.175 R/P ## 8 3843 -0.978 b_b_treatment4 0.329 0.343 L/P ## 9 960 1.39 b_b_treatment1 -0.114 0.782 R/N ## 10 960 1.39 b_b_treatment2 0.522 0.871 L/N ## # … with 390 more rows Make Figure 13.7.c. p3 &lt;- f %&gt;% ggplot(aes(x = treatment, y = fitted, group = iter)) + geom_line(alpha = 1/2, color = &quot;orange3&quot;) + ggtitle(&quot;100 simulated actors&quot;) + coord_cartesian(ylim = 0:1) + theme(plot.title = element_text(size = 14, hjust = .5)) p3 For the finale, we’ll combine the three plots with patchwork. library(patchwork) p1 | p2 | p3 13.5.2.1 Bonus: Let’s use fitted() this time. We just made those plots using various wrangled versions of post, the data frame returned by posterior_samples(b.13.4). If you followed along closely, part of what made that a great exercise is that it forced you to consider what the various vectors in post meant with respect to the model formula. But it’s also handy to see how to do that from a different perspective. So in this section, we’ll repeat that process by relying on the fitted() function, instead. We’ll go in the same order, starting with the average actor. nd &lt;- distinct(d, treatment) ( f &lt;- fitted(b13.4, newdata = nd, re_formula = NA, probs = c(.1, .9)) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(treatment = factor(treatment, labels = labels)) ) ## Estimate Est.Error Q10 Q90 treatment ## 1 0.6039864 0.1519271 0.3997364 0.7952008 R/N ## 2 0.7111121 0.1355745 0.5293721 0.8676832 L/N ## 3 0.5284151 0.1584498 0.3213219 0.7336165 R/P ## 4 0.6899462 0.1396600 0.5018703 0.8558593 L/P You should notice a few things. Since b13.4 is a cross-classified multilevel model, it had three predictors: treatment, block, and actor. However, our nd data only included the first of those three. The reason fitted() permitted that was because we set re_formula = NA. When you do that, you tell fitted() to ignore group-level effects (i.e., focus only on the fixed effects). This was our fitted() version of ignoring the r_ vectors returned by posterior_samples(). Here’s the plot. p4 &lt;- f %&gt;% ggplot(aes(x = treatment, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q10, ymax = Q90), fill = &quot;blue&quot;) + geom_line(color = &quot;orange1&quot;) + ggtitle(&quot;Average actor&quot;) + coord_cartesian(ylim = 0:1) + theme(plot.title = element_text(size = 14, hjust = .5)) p4 For marginal of actor, we can continue using the same nd data. This time we’ll be sticking with the default re_formula setting, which will accommodate the multilevel nature of the model. However, we’ll also be adding allow_new_levels = T and sample_new_levels = \"gaussian\". The former will allow us to marginalize across the specific actors and blocks in our data and the latter will instruct fitted() to use the multivariate normal distribution implied by the random effects. It’ll make more sense why I say multivariate normal by the end of the next chapter. For now, just go with it. ( f &lt;- fitted(b13.4, newdata = nd, probs = c(.1, .9), allow_new_levels = T, sample_new_levels = &quot;gaussian&quot;) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(treatment = factor(treatment, labels = labels)) ) ## Estimate Est.Error Q10 Q90 treatment ## 1 0.5760368 0.3180720 0.09308039 0.9623062 R/N ## 2 0.6496611 0.3060498 0.14301753 0.9772939 L/N ## 3 0.5269124 0.3220368 0.06909875 0.9474094 R/P ## 4 0.6346053 0.3095338 0.13767220 0.9744638 L/P Here’s our fitted()-based marginal of actor plot. p5 &lt;- f %&gt;% ggplot(aes(x = treatment, y = Estimate, group = 1)) + geom_ribbon(aes(ymin = Q10, ymax = Q90), fill = &quot;blue&quot;) + geom_line(color = &quot;orange1&quot;) + ggtitle(&quot;Marginal of actor&quot;) + coord_cartesian(ylim = 0:1) + theme(plot.title = element_text(size = 14, hjust = .5)) p5 We’ll have to amend our workflow a bit to make a fitted() version of the third panel. First we redefine our nd data and execute the fitted() code. # how many simulated chimps would you like? n_chimps &lt;- 100 nd &lt;- distinct(d, treatment) %&gt;% # define 100 new actors expand(actor = str_c(&quot;new&quot;, 1:n_chimps), treatment) %&gt;% # this adds a row number, which will come in handy, later mutate(row = 1:n()) # fitted set.seed(13) f &lt;- fitted(b13.4, newdata = nd, allow_new_levels = T, sample_new_levels = &quot;gaussian&quot;, summary = F, nsamples = n_chimps) Our f object will need a lot of wrangling. Before I walk out the wrangling steps, we should reiterate what McElreath originally did in the text (pp. 429–430). He based the new actors on the deviation scores from post$sigma_a. That was the first working line in his R code 13.38. In the remaining lines in that code block, he used the model formula to compute the actor-level trajectories. Then in his plot code in R code 13.39, he just used the first 100 rows from that output. In our fitted() code, above, we saved a little time and computer memory by setting nsamples = n_chimps, which equaled 100. That’s functionally the same as when McElreath used the first 100 posterior draws in the plot. A difficulty for us is the way brms::fitted() returns the output, the 100 new levels of actor and the four levels of treatment are confounded in the 400 columns. In the code block, below, the data.frame() through left_join() lines are meant to disentangle those two. After that, we’ll make an actor_number variable, which which we’ll filter the data such that the first row returned by fitted() is only assigned to the new actor #1, the second row is only assigned to the new actor #2, and so on. The result is that we have 100 new simulated actors, each of which corresponds to a different iteration of the posterior draws from the fixed effects4. p6 &lt;- f %&gt;% data.frame() %&gt;% # name the columns by the `row` values in `nd` set_names(pull(nd, row)) %&gt;% # add an iteration index mutate(iter = 1:n()) %&gt;% # make it long pivot_longer(-iter, names_to = &quot;row&quot;) %&gt;% mutate(row = as.double(row)) %&gt;% # add the new data left_join(nd, by = &quot;row&quot;) %&gt;% # extract the numbers from the names of the new actors mutate(actor_number = str_extract(actor, &quot;\\\\d+&quot;) %&gt;% as.double()) %&gt;% # only keep the posterior iterations that match the `actor_number` values filter(actor_number == iter) %&gt;% # add the `treatment` labels mutate(treatment = factor(treatment, labels = labels)) %&gt;% # plot! ggplot(aes(x = treatment, y = value, group = actor)) + geom_line(alpha = 1/2, color = &quot;blue&quot;) + ggtitle(&quot;100 simulated actors&quot;) + theme(plot.title = element_text(size = 14, hjust = .5)) p6 Here they are altogether. p4 | p5 | p6 13.5.3 Post-stratification. If you have estimates \\(p_i\\) for each relevant demographic category \\(i\\), the post-stratified prediction for the whole population just re-weights these estimates using the number of individuals \\(N_i\\) in each category with the formula \\[\\frac{\\sum_i N_i p_i}{\\sum_i N_i}.\\] Within the multilevel context, this is called multilevel regression and post-stratification (MRP, pronounced “Mister P”). Gelman is a long-time advocate for MRP (e.g., Gelman &amp; Little, 1997; Park et al., 2004). He mentions MRP a lot in his blog (e.g., here, here, here, here, here, here, here, here). 13.6 Summary Bonus: Post-stratification in an example Though I was excited to see McElreath introduce MRP, I was disappointed he did not work through an example. Happily, MRP tutorials have been popping up all over the place online. In this bonus section, we’ll draw heavily from the great blog post from demographer Monica Alexander, Analyzing name changes after marriage using a non-representative survey. From the introduction of her post, we read: Recently on Twitter, sociologist Phil Cohen put out a survey asking people about their decisions to change their name (or not) after marriage. The response was impressive - there are currently over 5,000 responses. Thanks to Phil, the data from the survey are publicly available and downloadable here for anyone to do their own analysis. However, there’s an issue with using the raw data without lots of caveats: the respondents are not very representative of the broader population, and in particular tend to have a higher education level and are younger than average…. This is a very common problem for social scientists: trying to come up with representative estimates using non-representative data. In this post I’ll introduce one particular technique of trying to do this: multilevel regression and post-stratification (MRP). In particular, I’ll use data from the marital name change survey to estimate the proportion of women in the US who kept their maiden name after marriage. 13.6.1 Meet the data. Alexander used two data sources in her example. As alluded to in the block quote, above, she used a subset of the data from Cohen’s Twitter poll. She derived her post-stratification weights from the 2017 5-year ACS data from IPUMS-USA, which provides U.S. census data for research use. Alexander provided some of her data wrangling code in her post and her full R code is available on her GitHub repo, marriage-name-change. For the sake of space, I downloaded the data, wrangled them similarly to how they were used in her blog, and saved the tidied data as external files in my data folder on GitHub. You can download them from there. Load the data. load(&quot;data/mrp_data_ch13.rds&quot;) glimpse(d) ## Rows: 4,413 ## Columns: 5 ## $ kept_name &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1… ## $ state_name &lt;chr&gt; &quot;ohio&quot;, &quot;virginia&quot;, &quot;new york&quot;, &quot;rhode island&quot;, &quot;illinois&quot;, &quot;north carolina&quot;, &quot;iowa&quot;,… ## $ age_group &lt;chr&gt; &quot;50&quot;, &quot;35&quot;, &quot;35&quot;, &quot;55&quot;, &quot;35&quot;, &quot;25&quot;, &quot;35&quot;, &quot;35&quot;, &quot;35&quot;, &quot;35&quot;, &quot;40&quot;, &quot;35&quot;, &quot;30&quot;, &quot;30&quot;, &quot;… ## $ decade_married &lt;chr&gt; &quot;1979&quot;, &quot;1999&quot;, &quot;2009&quot;, &quot;1999&quot;, &quot;2009&quot;, &quot;2009&quot;, &quot;1999&quot;, &quot;2009&quot;, &quot;1999&quot;, &quot;2009&quot;, &quot;2009… ## $ educ_group &lt;chr&gt; &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;… glimpse(cell_counts) ## Rows: 6,058 ## Columns: 5 ## $ state_name &lt;chr&gt; &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama&quot;, &quot;alaska&quot;, &quot;alaska&quot;,… ## $ age_group &lt;chr&gt; &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;… ## $ decade_married &lt;chr&gt; &quot;1999&quot;, &quot;2009&quot;, &quot;1999&quot;, &quot;2009&quot;, &quot;1999&quot;, &quot;2009&quot;, &quot;1999&quot;, &quot;2009&quot;, &quot;1999&quot;, &quot;2009&quot;, &quot;1999… ## $ educ_group &lt;chr&gt; &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;BA&quot;, &quot;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&gt;BA&quot;, &quot;&gt;BA&quot;, &quot;BA&quot;, &quot;BA&quot;, &quot;&lt;BA&quot;… ## $ n &lt;dbl&gt; 19012, 37488, 959, 5319, 2986, 14261, 3320, 7001, 159, 435, 341, 2660, 23279, 45477, … Our primary data file, which contains the survey responses to whether women changed their names after marriage, is d. Our criterion variable will be kept_name, which is dummy coded 0 = “no” 1 = “yes.” We have four grouping variables: age_group, which ranges from 25 to 75 and is discretized such that 25 = [25, 30), 30 = [30, 35), and so on; decade_married, which ranges from 1979 to 2019 and is discretized such that 1979 = [1979, 1989), 1989 = [1989, 1999), and so on; educ_group, which is coded as &lt;BA = no bachelor’s degree, BA = bachelor’s degree, and &gt;BA = above a bachelor’s degree; and state_name, which includes the names of the 50 US states, the District of Columbia, and Puerto Rico. The cell_counts data contains the relevant information from the US census. The first four columns, state_name, age_group, decade_married, and educ_group are the same demographic categories from the survey data. The fifth column, n, has the counts of women falling within those categories from the US census. There were 6,058 unique combinations of the demographic categories represented in the census data. cell_counts %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 6058 We can use a histogram to get a sense of how those counts vary. cell_counts %&gt;% ggplot(aes(x = n)) + geom_histogram(binwidth = 2000, fill = &quot;blue&quot;) + scale_x_continuous(breaks = 0:3 * 100000, labels = c(0, &quot;100K&quot;, &quot;200K&quot;, &quot;300K&quot;)) Though some of the categories are large with an excess of 100,000 persons in them, many are fairly small. It seems unlikely that the women who participated in Cohen’s Twitter poll fell into these categories in the same proportions. This is where post-stratification will help. 13.6.2 Settle the MR part of MRP. Like in the earlier examples in this chapter, we will model the data with multilevel logistic regression. Alexander fit her model with brms and kept things simple by using default priors. Here we’ll continue on with McElreath’s recommendations and use weakly regularizing priors. Though I am no expert on the topic of women’s name-changing practices following marriage, my layperson’s sense is that most do not keep their maiden name after they marry. I’m not quite sure what the proportion might be, but I’d like my \\(\\bar \\alpha\\) prior to tend closer to 0 than to 1. Recall that the \\(\\bar \\alpha\\) for a multilevel logistic model is typically a Gaussian set on the log-odds scale. If we were to use \\(\\operatorname{Normal}(-1, 1)\\), here’s what that would look like when converted back to the probability metric. set.seed(13) tibble(n = rnorm(1e6, -1, 1)) %&gt;% mutate(p = inv_logit_scaled(n)) %&gt;% ggplot(aes(x = p)) + geom_histogram(fill = &quot;blue&quot;, binwidth = .02) + scale_y_continuous(breaks = NULL) To my eye, this looks like a good place to start. Feel free to experiment with different priors on your end. As to the hierarchical \\(\\sigma_\\text{&lt;group&gt;}\\) priors, we will continue our practice of setting them to \\(\\operatorname{Exponential}(1)\\). Here’s how to fit the model. b13.7 &lt;- brm(data = d, family = binomial, kept_name | trials(1) ~ 1 + (1 | age_group) + (1 | decade_married) + (1 | educ_group) + (1 | state_name), prior = c(prior(normal(-1, 1), class = Intercept), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, chains = 4, cores = 4, control = list(adapt_delta = .98), seed = 13, file = &quot;fits/b13.07&quot;) Note how, like Alexander did in the blog, we had to set adept_delta = .98 to stave off a few divergent transitions. In my experience, this is common when your hierarchical grouping variables have few levels. Our decade_married has five levels and educ_group has only four. Happily, brms::brm() came through in the end. You can see by checking the summary. print(b13.7) ## Family: binomial ## Links: mu = logit ## Formula: kept_name | trials(1) ~ 1 + (1 | age_group) + (1 | decade_married) + (1 | educ_group) + (1 | state_name) ## Data: d (Number of observations: 4373) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~age_group (Number of levels: 11) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.15 0.30 0.71 1.85 1.00 1111 2061 ## ## ~decade_married (Number of levels: 5) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 1.00 0.40 0.50 2.03 1.00 2022 2540 ## ## ~educ_group (Number of levels: 4) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.94 0.49 0.36 2.22 1.00 1875 2573 ## ## ~state_name (Number of levels: 52) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.25 0.06 0.15 0.38 1.00 1525 2282 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.73 0.61 -2.02 0.40 1.00 1198 2191 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Even with 4,373 cases in the data, the uncertainty around \\(\\bar \\alpha\\) is massive, -0.73 [-2.02, 0.4], suggesting a lot of the action is lurking in the \\(\\sigma_\\text{&lt;group&gt;}\\) parameters. It might be easier to compare the \\(\\sigma_\\text{&lt;group&gt;}\\) parameters with an interval plot. posterior_samples(b13.7) %&gt;% select(starts_with(&quot;sd_&quot;)) %&gt;% set_names(str_c(&quot;sigma[&quot;, c(&quot;age&quot;, &quot;decade~married&quot;, &quot;education&quot;, &quot;state&quot;), &quot;]&quot;)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% median_qi(.width = seq(from = .5, to = .9, by = .1)) %&gt;% ggplot(aes(x = value, xmin = .lower, xmax = .upper, y = reorder(name, value))) + geom_interval(aes(alpha = .width), color = &quot;orange3&quot;) + scale_alpha_continuous(&quot;CI width&quot;, range = c(.7, .15)) + scale_y_discrete(labels = ggplot2:::parse_safe) + xlim(0, NA) + theme(axis.text.y = element_text(hjust = 0), panel.grid.major.y = element_blank()) It seems the largest share of the variation is to be found among the age groups. Since there was relatively less variation across states, we can expect more aggressive regularization along those lines. 13.6.3 Post-stratify to put the P in MRP. In her post, Alexander contrasted the MRP results with the empirical proportions from the Twitter survey in a series of four plots, one for each of the four grouping variables. We will take a slightly different approach. For simplicity, we will only focus on the results for age_group and state. However, we will examine the results for each using three estimation methods: the empirical proportions, the naïve results from the multilevel model, and the MRP estimates. 13.6.3.1 Estimates by age group. To warm up, here is the plot of the empirical proportions for kept_name, by age_group. levels &lt;- c(&quot;raw data&quot;, &quot;multilevel&quot;, &quot;MRP&quot;) p1 &lt;- # compute the proportions from the data d %&gt;% group_by(age_group, kept_name) %&gt;% summarise(n = n()) %&gt;% group_by(age_group) %&gt;% mutate(prop = n/sum(n), type = factor(&quot;raw data&quot;, levels = levels)) %&gt;% filter(kept_name == 1, age_group &lt; 80, age_group &gt; 20) %&gt;% # plot! ggplot(aes(x = prop, y = age_group)) + geom_point() + scale_x_continuous(breaks = c(0, .5, 1), limits = 0:1) + facet_wrap(~ type) p1 We’ll combine that plot with the next two, in a bit. I just wanted to give a preview of what we’re doing. The second plot will showcase the typical multilevel estimates for the same. The most straightforward way to do this with brms is with the fitted() function. We’ll use the re_formula argument to average over the levels of all grouping variables other than age_group. Relatedly, we’ll feed in the unique levels of age_group into the newdata argument. Then we just wrangle and plot. nd &lt;- distinct(d, age_group) %&gt;% arrange(age_group) p2 &lt;- fitted(b13.7, re_formula = ~ (1 | age_group), newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(prop = Estimate, type = factor(&quot;multilevel&quot;, levels = levels)) %&gt;% ggplot(aes(x = prop, xmin = Q2.5, xmax = Q97.5, y = age_group)) + geom_pointrange(color = &quot;blue2&quot;, size = 0.8, fatten = 2) + scale_x_continuous(breaks = c(0, .5, 1), limits = c(0, 1)) + scale_y_discrete(labels = NULL) + facet_wrap(~ type) We will take a look at the multilevel coefficient plot in just a bit. Now we turn our focus to computing the MRP estimates. As a first step, we’ll follow Alexander’s lead and add a prop column to the cell_counts data, which will give us the proportions of the combinations of the other three demographic categories, within each level of age_group. We’ll save the results as age_prop. age_prop &lt;- cell_counts %&gt;% group_by(age_group) %&gt;% mutate(prop = n / sum(n)) %&gt;% ungroup() age_prop ## # A tibble: 6,058 x 6 ## state_name age_group decade_married educ_group n prop ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alabama 25 1999 &lt;BA 19012 0.00414 ## 2 alabama 25 2009 &lt;BA 37488 0.00816 ## 3 alabama 25 1999 &gt;BA 959 0.000209 ## 4 alabama 25 2009 &gt;BA 5319 0.00116 ## 5 alabama 25 1999 BA 2986 0.000650 ## 6 alabama 25 2009 BA 14261 0.00310 ## 7 alaska 25 1999 &lt;BA 3320 0.000723 ## 8 alaska 25 2009 &lt;BA 7001 0.00152 ## 9 alaska 25 1999 &gt;BA 159 0.0000346 ## 10 alaska 25 2009 &gt;BA 435 0.0000947 ## # … with 6,048 more rows These results are then fed into the newdata argument within the add_predicted_draws() function, which we’ll save as p. p &lt;- add_predicted_draws(b13.7, newdata = age_prop %&gt;% filter(age_group &gt; 20, age_group &lt; 80, decade_married &gt; 1969), allow_new_levels = T) glimpse(p) ## Rows: 24,232,000 ## Columns: 11 ## Groups: state_name, age_group, decade_married, educ_group, n, prop, .row [6,058] ## $ state_name &lt;chr&gt; &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama&quot;, &quot;alabama… ## $ age_group &lt;chr&gt; &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;25&quot;, &quot;… ## $ decade_married &lt;chr&gt; &quot;1999&quot;, &quot;1999&quot;, &quot;1999&quot;, &quot;1999&quot;, &quot;1999&quot;, &quot;1999&quot;, &quot;1999&quot;, &quot;1999&quot;, &quot;1999&quot;, &quot;1999&quot;, &quot;1999… ## $ educ_group &lt;chr&gt; &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;&lt;BA&quot;, &quot;… ## $ n &lt;dbl&gt; 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 1… ## $ prop &lt;dbl&gt; 0.004137905, 0.004137905, 0.004137905, 0.004137905, 0.004137905, 0.004137905, 0.00413… ## $ .row &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ .chain &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .iteration &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ .draw &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24… ## $ .prediction &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… The tidybayes::add_predicted_draws() function is somewhat analogous to brms::predict(). It allowed us to compute the posterior predictions from our model, given the levels of the predictors we fed into newdata. The results were returned in a tidy data format, including the levels of the predictors from the newdata argument. Because there were 6,058 unique predictor values and 4,000 posterior draws, this produced a 24,232,000-row data frame. The posterior predictions are in the .prediction column on the end. Since we used a binomial regression model, we got a series of 0’s and 1’s. Next comes the MRP magic. If we group the results by age_group and .draw, we can sum the product of the posterior predictions and the weights, which will leave us with 4,000 stratified posterior draws for each of the 11 levels of age_group. This is the essence of the post-stratification equation McElreath presented in Section 13.5.3, \\[\\frac{\\sum_i N_i p_i}{\\sum_i N_i}.\\] We will follow Alexander and call these summary values kept_name_predict. We then complete the project by grouping by age_group and summarizing each stratified posterior predictive distribution by its mean and 95% interval. p &lt;- p %&gt;% group_by(age_group, .draw) %&gt;% summarise(kept_name_predict = sum(.prediction * prop)) %&gt;% group_by(age_group) %&gt;% mean_qi(kept_name_predict) p ## # A tibble: 11 x 7 ## age_group kept_name_predict .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 25 0.175 0.0926 0.278 0.95 mean qi ## 2 30 0.181 0.115 0.262 0.95 mean qi ## 3 35 0.217 0.148 0.298 0.95 mean qi ## 4 40 0.245 0.174 0.328 0.95 mean qi ## 5 45 0.278 0.200 0.366 0.95 mean qi ## 6 50 0.301 0.217 0.395 0.95 mean qi ## 7 55 0.325 0.235 0.429 0.95 mean qi ## 8 60 0.436 0.327 0.551 0.95 mean qi ## 9 65 0.562 0.421 0.702 0.95 mean qi ## 10 70 0.514 0.292 0.754 0.95 mean qi ## 11 75 0.227 0.0502 0.510 0.95 mean qi Now we are finally ready to plot our MRP estimates and combine the three subplots into a coherent whole with patchwork syntax. # MRP plot p3 &lt;- p %&gt;% mutate(type = factor(&quot;MRP&quot;, levels = levels)) %&gt;% ggplot(aes(x = kept_name_predict, xmin = .lower, xmax = .upper, y = age_group)) + geom_pointrange(color = &quot;orange2&quot;, size = 0.8, fatten = 2) + scale_x_continuous(breaks = c(0, .5, 1), limits = 0:1) + scale_y_discrete(labels = NULL) + facet_wrap(~ type) # combine! (p1 | p2 | p3) + plot_annotation(title = &quot;Proportion of women keeping name after marriage, by age&quot;, subtitle = &quot;Proportions are on the x-axis and age groups are on the y-axis.&quot;) Both multilevel and MRP estimates tended to be a little lower than the raw proportions, particularly for women in the younger age groups. Alexander mused this was “likely due to the fact that the survey has an over-sample of highly educated women, who are more likely to keep their name.” The MRP estimates were more precise than the multilevel predictions, which averaged across the grouping variables other than age. All three estimates show something of an inverted U-shape curve across age, which Alexander noted “is consistent with past observations that there was a peak in name retention in the 80s and 90s.” 13.6.3.2 Estimates by US state. Now we turn out attention to variation across states. The workflow, here, will only deviate slightly from what we just did. This time, of course, we will be grouping the estimates by state_name instead of by age_group. The other notable difference is since we’re plotting US state, it might be fun to show the results in a map format. Alexander used the geom_statebins() function from the statebins package (Rudis, 2020). I thought the results were pretty cool, we will do the same. To give you a sense of what we’re building, here’s the plot of the empirical proportions. library(statebins) p1 &lt;- d %&gt;% group_by(state_name, kept_name) %&gt;% summarise(n = n()) %&gt;% group_by(state_name) %&gt;% mutate(prop = n/sum(n)) %&gt;% filter(kept_name == 1, state_name != &quot;puerto rico&quot;) %&gt;% mutate(type = factor(&quot;raw data&quot;, levels = levels), statename = str_to_title(state_name)) %&gt;% ggplot(aes(fill = prop, state = statename)) + geom_statebins(lbl_size = 2.5, border_size = 1/4, radius = grid::unit(2, &quot;pt&quot;)) + scale_fill_viridis_c(&quot;proportion\\nkeeping\\nname&quot;, option = &quot;B&quot;, limits = c(0, 0.8)) + scale_x_continuous(breaks = NULL) + scale_y_continuous(breaks = NULL) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ type) p1 For the naïve multilevel estimates, we’ll continue using fitted(). nd &lt;- distinct(d, state_name) p2 &lt;- fitted(b13.7, re_formula = ~ (1 | state_name), newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% filter(state_name != &quot;puerto rico&quot;) %&gt;% mutate(prop = Estimate, type = factor(&quot;multilevel&quot;, levels = levels), statename = str_to_title(state_name)) %&gt;% ggplot(aes(fill = prop, state = statename)) + geom_statebins(lbl_size = 2.5, border_size = 1/4, radius = grid::unit(2, &quot;pt&quot;)) + scale_fill_viridis_c(&quot;proportion\\nkeeping\\nname&quot;, option = &quot;B&quot;, limits = c(0, 0.8)) + scale_x_continuous(breaks = NULL) + scale_y_continuous(breaks = NULL) + facet_wrap(~ type) In preparation for the MRP estimates, we’ll first wrangle cell_counts, this time grouping by state_name before computing the weights. state_prop &lt;- cell_counts %&gt;% group_by(state_name) %&gt;% mutate(prop = n/sum(n)) %&gt;% ungroup() state_prop ## # A tibble: 6,058 x 6 ## state_name age_group decade_married educ_group n prop ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alabama 25 1999 &lt;BA 19012 0.0187 ## 2 alabama 25 2009 &lt;BA 37488 0.0369 ## 3 alabama 25 1999 &gt;BA 959 0.000945 ## 4 alabama 25 2009 &gt;BA 5319 0.00524 ## 5 alabama 25 1999 BA 2986 0.00294 ## 6 alabama 25 2009 BA 14261 0.0141 ## 7 alaska 25 1999 &lt;BA 3320 0.0225 ## 8 alaska 25 2009 &lt;BA 7001 0.0474 ## 9 alaska 25 1999 &gt;BA 159 0.00108 ## 10 alaska 25 2009 &gt;BA 435 0.00295 ## # … with 6,048 more rows Now we’ll feed those state_prop values into add_predicted_draws(), wrangle, and plot the MRP plot in one step. p3 &lt;- add_predicted_draws(b13.7, newdata = state_prop %&gt;% filter(age_group &gt; 20, age_group &lt; 80, decade_married &gt; 1969), allow_new_levels = T) %&gt;% group_by(state_name, .draw) %&gt;% summarise(kept_name_predict = sum(.prediction * prop)) %&gt;% group_by(state_name) %&gt;% mean_qi(kept_name_predict) %&gt;% mutate(prop = kept_name_predict, type = factor(&quot;MRP&quot;, levels = levels), statename = str_to_title(state_name)) %&gt;% ggplot(aes(fill = kept_name_predict, state = statename)) + geom_statebins(lbl_size = 2.5, border_size = 1/4, radius = grid::unit(2, &quot;pt&quot;)) + scale_fill_viridis_c(&quot;proportion\\nkeeping\\nname&quot;, option = &quot;B&quot;, limits = c(0, 0.8)) + scale_x_continuous(breaks = NULL) + scale_y_continuous(breaks = NULL) + theme(legend.position = &quot;none&quot;) + facet_wrap(~ type) We’re finally ready to combine our three panels into one grand plot. (p1 | p2 | p3) + plot_annotation(title = &quot;Proportion off women keeping name after marriage, by state&quot;, theme = theme(plot.margin = margin(0.2, 0, 0.01, 0, &quot;cm&quot;))) Remember how small the posterior for \\(\\sigma_\\text{state}\\) was relative to the other \\(\\sigma_\\text{&lt;group&gt;}\\) posteriors? We said that would imply more aggressive regularization across states. You can really see that regularization in the panels showing the multilevel and MRP estimates. They are much more uniform than the proportions from the raw data, which are all over the place. This is why you use multilevel models and/or stratify. When you divide the responses up at the state level, the proportions get jerked all around due to small and unrepresentative samples. Even with the regularization from the multilevel partial pooling, you can still see some interesting differences in the multilevel and MRP panels. Both suggest women keep their maiden names in relatively low proportions in Utah and relatively high proportions in New York. For those acquainted with American culture, this shouldn’t be a great surprise. 13.6.4 Wrap this MRP up. Interested readers should practice exploring the MRP estimates by the other two grouping variables, educ_group and decate_married. Both contain interesting results. Also, there are many other great free resources for learning about MRP. Tim Mastny showed how to use MRP via brms with data of US state level opinions for gay marriage in his blog post, MRP Using brms and tidybayes. Rohan Alexander showed how to fit political poling data with both lme4 and brms in his post, Getting started with MRP. Lauren Kennedy and Andrew Gelman have a (2020) preprint called Know your population and know your model: Using model-based regression and post-stratification to generalize findings beyond the observed sample, which shows how to use brms to apply MRP to Big Five personality data. For a more advanced application, check out the paper by Kolczynska, Bürkner, Kennedy, and Vehtari (2020), which combines MRP with a model with ordinal outcomes (recall Section 12.3). Their supplemental material, which includes their R code, lives at https://osf.io/dz4y7/. With all this good stuff, it seems we have an embarrassment of riches when it comes to brms and MRP! To wrap this section up, we’ll give Monica Alexander the last words: MRP is probably most commonly used in political analysis to reweight polling data, but it is a useful technique for many different survey responses. Many modeling extensions are possible. For example, the multilevel regression need not be limited to just using random effects, as was used here, and other model set ups could be investigated. MRP is a relatively easy and quick way of trying to get more representative estimates out of non-representative data, while giving you a sense of the uncertainty around the estimates (unlike traditional post-stratification). Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] statebins_1.4.0 posterior_0.1.3 bayesplot_1.8.0 patchwork_1.1.1 tidybayes_2.3.1 ggthemes_4.2.4 ## [7] forcats_0.5.1 stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 ## [13] tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 brms_2.15.0 Rcpp_1.0.6 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 igraph_1.2.6 svUnit_1.0.3 ## [6] splines_4.0.4 crosstalk_1.1.0.1 TH.data_1.0-10 rstantools_2.1.1 inline_0.3.17 ## [11] digest_0.6.27 htmltools_0.5.1.1 rsconnect_0.8.16 fansi_0.4.2 checkmate_2.0.0 ## [16] magrittr_2.0.1 modelr_0.1.8 RcppParallel_5.0.2 matrixStats_0.57.0 xts_0.12.1 ## [21] sandwich_3.0-0 prettyunits_1.1.1 colorspace_2.0-0 rvest_0.3.6 ggdist_2.4.0.9000 ## [26] haven_2.3.1 xfun_0.22 callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 ## [31] lme4_1.1-25 survival_3.2-7 zoo_1.8-8 glue_1.4.2 gtable_0.3.0 ## [36] emmeans_1.5.2-1 V8_3.4.0 distributional_0.2.2 pkgbuild_1.2.0 rstan_2.21.2 ## [41] abind_1.4-5 scales_1.1.1 mvtnorm_1.1-1 emo_0.0.0.9000 DBI_1.1.0 ## [46] miniUI_0.1.1.1 viridisLite_0.3.0 xtable_1.8-4 stats4_4.0.4 StanHeaders_2.21.0-7 ## [51] DT_0.16 htmlwidgets_1.5.2 httr_1.4.2 threejs_0.3.3 arrayhelpers_1.1-0 ## [56] ellipsis_0.3.1 farver_2.0.3 pkgconfig_2.0.3 loo_2.4.1 dbplyr_2.0.0 ## [61] utf8_1.1.4 labeling_0.4.2 tidyselect_1.1.0 rlang_0.4.10 reshape2_1.4.4 ## [66] later_1.1.0.1 munsell_0.5.0 cellranger_1.1.0 tools_4.0.4 cli_2.3.1 ## [71] generics_0.1.0 broom_0.7.5 ggridges_0.5.2 evaluate_0.14 fastmap_1.0.1 ## [76] processx_3.4.5 knitr_1.31 fs_1.5.0 nlme_3.1-152 mime_0.10 ## [81] projpred_2.0.2 xml2_1.3.2 compiler_4.0.4 shinythemes_1.1.2 rstudioapi_0.13 ## [86] curl_4.3 gamm4_0.2-6 reprex_0.3.0 statmod_1.4.35 stringi_1.5.3 ## [91] highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 lattice_0.20-41 Matrix_1.3-2 ## [96] nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 vctrs_0.3.6 pillar_1.5.1 ## [101] lifecycle_1.0.0 bridgesampling_1.0-0 estimability_1.3 httpuv_1.5.4 R6_2.5.0 ## [106] bookdown_0.21 promises_1.1.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-26 ## [111] colourpicker_1.1.0 MASS_7.3-53 gtools_3.8.2 assertthat_0.2.1 withr_2.4.1 ## [116] shinystan_2.5.0 multcomp_1.4-16 mgcv_1.8-33 parallel_4.0.4 hms_0.5.3 ## [121] grid_4.0.4 coda_0.19-4 minqa_1.2.4 rmarkdown_2.7 shiny_1.5.0 ## [126] lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 The fitted() version of the code for the third panel is cumbersome. Indeed, this in one of those cases where it seems more straightforward to work directly with the posterior_samples() output, rather than with fitted(). The workflow in this section from previous editions of this ebook was more streamlined and superficially seemed to work. However, fellow researcher Ladislas Nalborczyk kindly pointed out I was taking 100 draws from one new simulated actor, rather than one simulated draw from 100 new levels of actor. To my knowledge, if you want 100 new levels of actor AND want each one to be from a different posterior iteration, you’ll need a lot of post-processing code when working with fitted().↩︎ "],["adventures-in-covariance.html", "14 Adventures in Covariance 14.1 Varying slopes by construction 14.2 Advanced varying slopes 14.3 Instruments and causal designs 14.4 Social relations as correlated varying effects 14.5 Continuous categories and the Gaussian process 14.6 Summary Bonus: Multilevel growth models and the MELSM Session info", " 14 Adventures in Covariance In this chapter, you’ll see how to… specify varying slopes in combination with the varying intercepts of the previous chapter. This will enable pooling that will improve estimates of how different units respond to or are influenced by predictor variables. It will also improve estimates of intercepts, by borrowing information across parameter types. Essentially, varying slopes models are massive interaction machines. They allow every unit in the data to have its own response to any treatment or exposure or event, while also improving estimates via pooling. When the variation in slopes is large, the average slope is of less interest. Sometimes, the pattern of variation in slopes provides hints about omitted variables that explain why some units respond more or less. We’ll see an example in this chapter. The machinery that makes such complex varying effects possible will be used later in the chapter to extend the varying effects strategy to more subtle model types, including the use of continuous categories, using Gaussian process. Ordinary varying effects work only with discrete, unordered categories, such as individuals, countries, or ponds. In these cases, each category is equally different from all of the others. But it is possible to use pooling with categories such as age or location. In these cases, some ages and some locations are more similar than others. You’ll see how to model covariation among continuous categories of this kind, as well as how to generalize the strategy to seemingly unrelated types of models such as phylogenetic and network regressions. Finally, we’ll circle back to causal inference and use our new powers over covariance to go beyond the tools of Chapter 6, introducing instrumental variables. Instruments are ways of inferring cause without closing backdoor paths. However they are very tricky both in design and estimation. (McElreath, 2020a, pp. 436–437, emphasis in the original) 14.1 Varying slopes by construction How should the robot pool information across intercepts and slopes? By modeling the joint population of intercepts and slopes, which means by modeling their covariance. In conventional multilevel models, the device that makes this possible is a joint multivariate Gaussian distribution for all of the varying effects, both intercepts and slopes. So instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two-dimensional Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension). (p. 437) 14.1.0.1 Rethinking: Why Gaussian? There is no reason the multivariate distribution of intercepts and slopes must be Gaussian. But there are both practical and epistemological justifications. On the practical side, there aren’t many multivariate distributions that are easy to work with. The only common ones are multivariate Gaussian and multivariate Student-t distributions. On the epistemological side, if all we want to say about these intercepts and slopes is their means, variances, and covariances, then the maximum entropy distribution is multivariate Gaussian. (p. 437) As it turns out, brms does currently allow users to use the multivariate Student-\\(t\\) distribution in this way. For details, check out this discussion from the brms GitHub repository. Bürkner’s exemplar syntax from his comment on May 13, 2018, was y ~ x + (x | gr(g, dist = \"student\")). I haven’t experimented with this, but if you do, do consider sharing how it went. 14.1.1 Simulate the population. If you follow this section closely, it’s a great template for simulating multilevel code for any of your future projects. You might think of this as an alternative to a frequentist power analysis. Vourre has done some nice work along these lines, I have a blog series on Bayesian power analysis, and Kruschke covered the topic in Chapter 13 of his (2015) text. a &lt;- 3.5 # average morning wait time b &lt;- -1 # average difference afternoon wait time sigma_a &lt;- 1 # std dev in intercepts sigma_b &lt;- 0.5 # std dev in slopes rho &lt;- -.7 # correlation between intercepts and slopes # the next three lines of code simply combine the terms, above mu &lt;- c(a, b) cov_ab &lt;- sigma_a * sigma_b * rho sigma &lt;- matrix(c(sigma_a^2, cov_ab, cov_ab, sigma_b^2), ncol = 2) It’s common to refer to a covariance matrix as \\(\\mathbf \\Sigma\\). The mathematical notation for those last couple lines of code is \\[ \\mathbf \\Sigma = \\begin{bmatrix} \\sigma_\\alpha^2 &amp; \\sigma_\\alpha \\sigma_\\beta \\rho \\\\ \\sigma_\\alpha \\sigma_\\beta \\rho &amp; \\sigma_\\beta^2 \\end{bmatrix}. \\] Anyway, if you haven’t used the matirx() function before, you might get a sense of the elements like so. matrix(1:4, nrow = 2, ncol = 2) ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 This next block of code will finally yield our café data. library(tidyverse) sigmas &lt;- c(sigma_a, sigma_b) # standard deviations rho &lt;- matrix(c(1, rho, # correlation matrix rho, 1), nrow = 2) # now matrix multiply to get covariance matrix sigma &lt;- diag(sigmas) %*% rho %*% diag(sigmas) # how many cafes would you like? n_cafes &lt;- 20 set.seed(5) # used to replicate example vary_effects &lt;- MASS::mvrnorm(n_cafes, mu, sigma) %&gt;% data.frame() %&gt;% set_names(&quot;a_cafe&quot;, &quot;b_cafe&quot;) head(vary_effects) ## a_cafe b_cafe ## 1 4.223962 -1.6093565 ## 2 2.010498 -0.7517704 ## 3 4.565811 -1.9482646 ## 4 3.343635 -1.1926539 ## 5 1.700971 -0.5855618 ## 6 4.134373 -1.1444539 Let’s make sure we’re keeping this all straight. a_cafe = our café-specific intercepts; b_cafe = our café-specific slopes. These aren’t the actual data, yet. But at this stage, it might make sense to ask What’s the distribution of a_cafe and b_cafe? Our variant of Figure 14.2 contains the answer. For our plots in this chapter, we’ll make our own custom ggplot2 theme. The color palette will come from the “pearl_earring” palette of the dutchmasters package (Thoen, 2019). You can learn more about the original painting, Vermeer’s (1665) Girl with a Pearl Earring, here. # devtools::install_github(&quot;EdwinTh/dutchmasters&quot;) library(dutchmasters) dutchmasters$pearl_earring ## red(lips) skin blue(scarf1) blue(scarf2) white(colar) gold(dress) ## &quot;#A65141&quot; &quot;#E7CDC2&quot; &quot;#80A0C7&quot; &quot;#394165&quot; &quot;#FCF9F0&quot; &quot;#B1934A&quot; ## gold(dress2) black(background) grey(scarf3) yellow(scarf4) ## &quot;#DCA258&quot; &quot;#100F14&quot; &quot;#8B9DAF&quot; &quot;#EEDA9D&quot; &quot;#E8DCCF&quot; scales::show_col(dutchmasters$pearl_earring) We’ll name our custom theme theme_pearl_earring(). I cobbled together this approach to defining a custom ggplot2 theme with help from Chapter 17 of Wichkam’s (2016) ggplot2: Elegant graphics for data analysis; Section 4.6 of Peng, Kross, and Anderson’s (2017) Mastering Software Development in R; Lea Waniek’s blog post, Custom themes in ggplot2, and Joey Stanley’s blog post of the same name, Custom themes in ggplot2. theme_pearl_earring &lt;- function(light_color = &quot;#E8DCCF&quot;, dark_color = &quot;#100F14&quot;, my_family = &quot;Courier&quot;, ...) { theme(line = element_line(color = light_color), text = element_text(color = light_color, family = my_family), axis.line = element_blank(), axis.text = element_text(color = light_color), axis.ticks = element_line(color = light_color), legend.background = element_rect(fill = dark_color, color = &quot;transparent&quot;), legend.key = element_rect(fill = dark_color, color = &quot;transparent&quot;), panel.background = element_rect(fill = dark_color, color = light_color), panel.grid = element_blank(), plot.background = element_rect(fill = dark_color, color = dark_color), strip.background = element_rect(fill = dark_color, color = &quot;transparent&quot;), strip.text = element_text(color = light_color, family = my_family), ...) } # now set `theme_pearl_earring()` as the default theme theme_set(theme_pearl_earring()) Note how our custom theme_pearl_earing() function has a few adjustable parameters. Feel free to play around with alternative settings to see how they work. If we just use the defaults as we have defined them, here is our Figure 14.2. vary_effects %&gt;% ggplot(aes(x = a_cafe, y = b_cafe)) + geom_point(color = &quot;#80A0C7&quot;) + geom_rug(color = &quot;#8B9DAF&quot;, size = 1/7) Again, these are not “data.” Figure 14.2 shows a distribution of parameters. Here’s their Pearson’s correlation coefficient. cor(vary_effects$a_cafe, vary_effects$b_cafe) ## [1] -0.5721537 Since there are only 20 rows in our vary_effects simulation, it shouldn’t be a surprise that the Pearson’s correlation is a bit off from the population value of \\(\\rho = -.7\\). If you rerun the simulation with n_cafes &lt;- 200, the Pearson’s correlation is much closer to the data generating value. 14.1.2 Simulate observations. Here we put those simulated parameters to use and simulate actual data from them. n_visits &lt;- 10 sigma &lt;- 0.5 # std dev within cafes set.seed(22) # used to replicate example d &lt;- vary_effects %&gt;% mutate(cafe = 1:n_cafes) %&gt;% expand(nesting(cafe, a_cafe, b_cafe), visit = 1:n_visits) %&gt;% mutate(afternoon = rep(0:1, times = n() / 2)) %&gt;% mutate(mu = a_cafe + b_cafe * afternoon) %&gt;% mutate(wait = rnorm(n = n(), mean = mu, sd = sigma)) We might peek at the data. d %&gt;% glimpse() ## Rows: 200 ## Columns: 7 ## $ cafe &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, … ## $ a_cafe &lt;dbl&gt; 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, … ## $ b_cafe &lt;dbl&gt; -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1.609… ## $ visit &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1… ## $ afternoon &lt;int&gt; 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, … ## $ mu &lt;dbl&gt; 4.223962, 2.614606, 4.223962, 2.614606, 4.223962, 2.614606, 4.223962, 2.614606, 4.223962, … ## $ wait &lt;dbl&gt; 3.9678929, 3.8571978, 4.7278755, 2.7610133, 4.1194827, 3.5436522, 4.1909492, 2.5332235, 4.… Now we’ve finally simulated our data, we are ready to make our version of Figure 14.1, from way back on page 436. d %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;M&quot;, &quot;A&quot;), day = rep(rep(1:5, each = 2), times = n_cafes)) %&gt;% filter(cafe %in% c(3, 5)) %&gt;% mutate(cafe = str_c(&quot;café #&quot;, cafe)) %&gt;% ggplot(aes(x = visit, y = wait, group = day)) + geom_point(aes(color = afternoon), size = 2) + geom_line(color = &quot;#8B9DAF&quot;) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#EEDA9D&quot;)) + scale_x_continuous(NULL, breaks = 1:10, labels = rep(c(&quot;M&quot;, &quot;A&quot;), times = 5)) + scale_y_continuous(&quot;wait time in minutes&quot;, limits = c(0, NA)) + theme_pearl_earring(axis.ticks.x = element_blank(), legend.position = &quot;none&quot;) + facet_wrap(~ cafe, ncol = 1) 14.1.2.1 Rethinking: Simulation and misspecification. In this exercise, we are simulating data from a generative process and then analyzing that data with a model that reflects exactly the correct structure of that process. But in the real world, we’re never so lucky. Instead we are always forced to analyze data with a model that is misspecified: The true data-generating process is different than the model. Simulation can be used however to explore misspecification. Just simulate data from a process and then see how a number of models, none of which match exactly the data-generating process, perform. And always remember that Bayesian inference does not depend upon data-generating assumptions, such as the likelihood, being true. (p. 441) 14.1.3 The varying slopes model. The statistical formula for our varying intercepts and slopes café model follows the form \\[\\begin{align*} \\text{wait}_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha_{\\text{café}[i]} + \\beta_{\\text{café}[i]} \\text{afternoon}_i \\\\ \\begin{bmatrix} \\alpha_\\text{café} \\\\ \\beta_\\text{café} \\end{bmatrix} &amp; \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf \\Sigma \\end{pmatrix} \\\\ \\mathbf \\Sigma &amp; = \\begin{bmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{bmatrix} \\mathbf R \\begin{bmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{bmatrix} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(5, 2) \\\\ \\beta &amp; \\sim \\operatorname{Normal}(-1, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_\\alpha &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_\\beta &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf R &amp; \\sim \\operatorname{LKJcorr}(2), \\end{align*}\\] where \\(\\mathbf \\Sigma\\) is the covariance matrix and \\(\\mathbf R\\) is the corresponding correlation matrix, which we might more fully express as \\[\\mathbf R = \\begin{bmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{bmatrix}.\\] And according to our prior, \\(\\mathbf R\\) is distributed as \\(\\operatorname{LKJcorr}(2)\\). We’ll use rethinking::rlkjcorr() to get a better sense of what that even is. library(rethinking) n_sim &lt;- 1e4 set.seed(14) r_1 &lt;- rlkjcorr(n_sim, K = 2, eta = 1) %&gt;% data.frame() set.seed(14) r_2 &lt;- rlkjcorr(n_sim, K = 2, eta = 2) %&gt;% data.frame() set.seed(14) r_4 &lt;- rlkjcorr(n_sim, K = 2, eta = 4) %&gt;% data.frame() Here are the \\(\\text{LKJcorr}\\) distributions of Figure 14.3. # for annotation text &lt;- tibble(x = c(.83, .625, .45), y = c(.56, .75, 1.07), label = c(&quot;eta = 1&quot;, &quot;eta = 2&quot;, &quot;eta = 4&quot;)) # plot r_1 %&gt;% ggplot(aes(x = X2)) + geom_density(color = &quot;transparent&quot;, fill = &quot;#394165&quot;, alpha = 2/3, adjust = 1/2) + geom_density(data = r_2, color = &quot;transparent&quot;, fill = &quot;#DCA258&quot;, alpha = 2/3, adjust = 1/2) + geom_density(data = r_4, color = &quot;transparent&quot;, fill = &quot;#FCF9F0&quot;, alpha = 2/3, adjust = 1/2) + geom_text(data = text, aes(x = x, y = y, label = label), color = &quot;#A65141&quot;, family = &quot;Courier&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = expression(LKJcorr(eta)), x = &quot;correlation&quot;) As it turns out, the shape of the LKJ is sensitive to both \\(\\eta\\) and the \\(K\\) dimensions of the correlation matrix. Our simulations only considered the shapes for when \\(K = 2\\). We can use a combination of the parse_dist() and stat_dist_halfeye() functions from the tidybayes package to derive analytic solutions for different combinations of \\(\\eta\\) and \\(K\\). library(tidybayes) crossing(k = 2:5, eta = 1:4) %&gt;% mutate(prior = str_c(&quot;lkjcorr_marginal(&quot;, k, &quot;, &quot;, eta, &quot;)&quot;), strip = str_c(&quot;K==&quot;, k)) %&gt;% parse_dist(prior) %&gt;% ggplot(aes(y = eta, dist = .dist, args = .args)) + stat_dist_halfeye(.width = c(.5, .95), color = &quot;#FCF9F0&quot;, fill = &quot;#A65141&quot;) + scale_x_continuous(expression(rho), limits = c(-1, 1), breaks = c(-1, -.5, 0, .5, 1), labels = c(&quot;-1&quot;, &quot;-.5&quot;, &quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + scale_y_continuous(expression(eta), breaks = 1:4) + ggtitle(expression(&quot;Marginal correlation for the LKJ prior relative to K and &quot;*eta)) + facet_wrap(~ strip, labeller = label_parsed, ncol = 4) To learn more about this plotting method, check out Kay’s (2020b) Marginal distribution of a single correlation from an LKJ distribution. To get a better intuition what that plot means, check out the illuminating blog post by Stephen Martin, Is the LKJ(1) prior uniform? “Yes”. Okay, let’s get ready to model and switch out rethinking for brms. detach(package:rethinking, unload = T) library(brms) As defined above, our first model has both varying intercepts and afternoon slopes. I should point out that the (1 + afternoon | cafe) syntax specifies that we’d like brm() to fit the random effects for 1 (i.e., the intercept) and the afternoon slope as correlated. Had we wanted to fit a model in which they were orthogonal, we’d have coded (1 + afternoon || cafe). b14.1 &lt;- brm(data = d, family = gaussian, wait ~ 1 + afternoon + (1 + afternoon | cafe), prior = c(prior(normal(5, 2), class = Intercept), prior(normal(-1, 0.5), class = b), prior(exponential(1), class = sd), prior(exponential(1), class = sigma), prior(lkj(2), class = cor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 867530, file = &quot;fits/b14.01&quot;) With Figure 14.4, we assess how the posterior for the correlation of the random effects compares to its prior. post &lt;- posterior_samples(b14.1) post %&gt;% ggplot() + geom_density(data = r_2, aes(x = X2), color = &quot;transparent&quot;, fill = &quot;#EEDA9D&quot;, alpha = 3/4) + geom_density(aes(x = cor_cafe__Intercept__afternoon), color = &quot;transparent&quot;, fill = &quot;#A65141&quot;, alpha = 9/10) + annotate(geom = &quot;text&quot;, x = c(-0.15, 0), y = c(2.21, 0.85), label = c(&quot;posterior&quot;, &quot;prior&quot;), color = c(&quot;#A65141&quot;, &quot;#EEDA9D&quot;), family = &quot;Courier&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Correlation between intercepts\\nand slopes, prior and posterior&quot;, x = &quot;correlation&quot;) McElreath then depicted multidimensional shrinkage by plotting the posterior mean of the varying effects compared to their raw, unpooled estimated. With brms, we can get the cafe-specific intercepts and afternoon slopes with coef(), which returns a three-dimensional list. # coef(b14.1) %&gt;% glimpse() coef(b14.1) ## $cafe ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 4.216696 0.1993313 3.826785 4.606377 ## 2 2.157785 0.2042999 1.751544 2.565549 ## 3 4.376312 0.2052879 3.961862 4.778193 ## 4 3.241655 0.1997675 2.853799 3.629085 ## 5 1.875707 0.2032712 1.475846 2.272343 ## 6 4.259519 0.2040913 3.870432 4.659490 ## 7 3.609445 0.1932584 3.225994 3.997230 ## 8 3.946772 0.2060604 3.552232 4.352194 ## 9 3.981665 0.2002013 3.583096 4.377192 ## 10 3.559604 0.2009870 3.174088 3.948352 ## 11 1.931988 0.2060002 1.533306 2.333540 ## 12 3.838585 0.1972990 3.439314 4.223135 ## 13 3.883919 0.2067283 3.473713 4.284497 ## 14 3.174623 0.2008026 2.778669 3.560395 ## 15 4.453381 0.2056054 4.053133 4.848117 ## 16 3.390474 0.2002557 3.010207 3.790014 ## 17 4.222700 0.1938111 3.846177 4.611957 ## 18 5.745584 0.2014613 5.348419 6.146005 ## 19 3.244284 0.2032538 2.845847 3.644350 ## 20 3.736465 0.1983782 3.350465 4.119243 ## ## , , afternoon ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 -1.1539479 0.2683787 -1.6784707 -0.6347978 ## 2 -0.9132497 0.2660157 -1.4386466 -0.3765573 ## 3 -1.9394732 0.2721773 -2.4915366 -1.4146809 ## 4 -1.2347574 0.2673954 -1.7641862 -0.7055286 ## 5 -0.1319460 0.2771434 -0.6642467 0.4219599 ## 6 -1.2984953 0.2694507 -1.8251391 -0.7799761 ## 7 -1.0157580 0.2566564 -1.5246254 -0.5173951 ## 8 -1.6374434 0.2709313 -2.1798554 -1.1128193 ## 9 -1.3079207 0.2663182 -1.8157938 -0.7835424 ## 10 -0.9462419 0.2619188 -1.4567145 -0.4321138 ## 11 -0.4319153 0.2737102 -0.9616812 0.1106692 ## 12 -1.1807311 0.2669675 -1.7187254 -0.6584106 ## 13 -1.8196329 0.2732656 -2.3606439 -1.2880028 ## 14 -0.9436111 0.2597548 -1.4502848 -0.4451183 ## 15 -2.1977218 0.2805511 -2.7615255 -1.6357641 ## 16 -1.0487636 0.2652781 -1.5677353 -0.5353321 ## 17 -1.2249797 0.2580986 -1.7306851 -0.7230910 ## 18 -1.0204906 0.2819026 -1.5696446 -0.4603278 ## 19 -0.2559100 0.2750050 -0.7939099 0.2664067 ## 20 -1.0663943 0.2583372 -1.5802572 -0.5629665 Here’s the code to extract the relevant elements from the coef() list, convert them to a tibble, and add the cafe index. partially_pooled_params &lt;- # with this line we select each of the 20 cafe&#39;s posterior mean (i.e., Estimate) # for both `Intercept` and `afternoon` coef(b14.1)$cafe[ , 1, 1:2] %&gt;% data.frame() %&gt;% # convert the two vectors to a data frame rename(Slope = afternoon) %&gt;% mutate(cafe = 1:nrow(.)) %&gt;% # add the `cafe` index select(cafe, everything()) # simply moving `cafe` to the leftmost position Like McElreath, we’ll compute the unpooled estimates directly from the data. # compute unpooled estimates directly from data un_pooled_params &lt;- d %&gt;% # with these two lines, we compute the mean value for each cafe&#39;s wait time # in the morning and then the afternoon group_by(afternoon, cafe) %&gt;% summarise(mean = mean(wait)) %&gt;% ungroup() %&gt;% # ungrouping allows us to alter afternoon, one of the grouping variables mutate(afternoon = ifelse(afternoon == 0, &quot;Intercept&quot;, &quot;Slope&quot;)) %&gt;% spread(key = afternoon, value = mean) %&gt;% # use `spread()` just as in the previous block mutate(Slope = Slope - Intercept) # finally, here&#39;s our slope! # here we combine the partially-pooled and unpooled means into a single data object, # which will make plotting easier. params &lt;- # `bind_rows()` will stack the second tibble below the first bind_rows(partially_pooled_params, un_pooled_params) %&gt;% # index whether the estimates are pooled mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = nrow(.)/2)) # here&#39;s a glimpse at what we&#39;ve been working for params %&gt;% slice(c(1:5, 36:40)) ## cafe Intercept Slope pooled ## 1 1 4.216696 -1.15394786 partially ## 2 2 2.157785 -0.91324975 partially ## 3 3 4.376312 -1.93947320 partially ## 4 4 3.241655 -1.23475741 partially ## 5 5 1.875707 -0.13194596 partially ## ...6 16 3.373496 -1.02563866 not ## ...7 17 4.236192 -1.22236910 not ## ...8 18 5.755987 -0.87660383 not ## ...9 19 3.121060 0.01441784 not ## ...10 20 3.728481 -1.03811567 not Finally, here’s our code for Figure 14.5.a, showing shrinkage in two dimensions. p1 &lt;- ggplot(data = params, aes(x = Intercept, y = Slope)) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 1/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 2/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 3/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 4/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 5/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 6/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 7/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 8/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = 9/10, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, level = .99, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;) + geom_point(aes(group = cafe, color = pooled)) + geom_line(aes(group = cafe), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + coord_cartesian(xlim = range(params$Intercept), ylim = range(params$Slope)) p1 Learn more about stat_ellipse(), here. Let’s prep for Figure 14.5.b. # retrieve the partially-pooled estimates with `coef()` partially_pooled_estimates &lt;- coef(b14.1)$cafe[ , 1, 1:2] %&gt;% # convert the two vectors to a data frame data.frame() %&gt;% # the Intercept is the wait time for morning (i.e., `afternoon == 0`) rename(morning = Intercept) %&gt;% # `afternoon` wait time is the `morning` wait time plus the afternoon slope mutate(afternoon = morning + afternoon, cafe = 1:n()) %&gt;% # add the `cafe` index select(cafe, everything()) # compute unpooled estimates directly from data un_pooled_estimates &lt;- d %&gt;% # as above, with these two lines, we compute each cafe&#39;s mean wait value by time of day group_by(afternoon, cafe) %&gt;% summarise(mean = mean(wait)) %&gt;% # ungrouping allows us to alter the grouping variable, afternoon ungroup() %&gt;% mutate(afternoon = ifelse(afternoon == 0, &quot;morning&quot;, &quot;afternoon&quot;)) %&gt;% # this separates out the values into morning and afternoon columns spread(key = afternoon, value = mean) estimates &lt;- bind_rows(partially_pooled_estimates, un_pooled_estimates) %&gt;% mutate(pooled = rep(c(&quot;partially&quot;, &quot;not&quot;), each = n() / 2)) The code for Figure 14.5.b. p2 &lt;- ggplot(data = estimates, aes(x = morning, y = afternoon)) + # nesting `stat_ellipse()` within `mapply()` is a less redundant way to produce the # ten-layered semitransparent ellipses we did with ten lines of `stat_ellipse()` # functions in the previous plot mapply(function(level) { stat_ellipse(geom = &quot;polygon&quot;, type = &quot;norm&quot;, size = 0, alpha = 1/20, fill = &quot;#E7CDC2&quot;, level = level) }, # enter the levels here level = c(1:9 / 10, .99)) + geom_point(aes(group = cafe, color = pooled)) + geom_line(aes(group = cafe), size = 1/4) + scale_color_manual(&quot;Pooled?&quot;, values = c(&quot;#80A0C7&quot;, &quot;#A65141&quot;)) + labs(x = &quot;morning wait (mins)&quot;, y = &quot;afternoon wait (mins)&quot;) + coord_cartesian(xlim = range(estimates$morning), ylim = range(estimates$afternoon)) Here we combine the two subplots together with patchwork syntax. library(patchwork) (p1 + theme(legend.position = &quot;none&quot;)) + p2 + plot_annotation(title = &quot;Shrinkage in two dimensions&quot;) What I want you to appreciate in this plot is that shrinkage on the parameter scale naturally produces shrinkage where we actually care about it: on the outcome scale. And it also implies a population of wait times, shown by the [semitransparent ellipses]. That population is now positively correlated–cafés with longer morning waits also tend to have longer afternoon waits. They are popular, after all. But the population lies mostly below the dashed line where the waits are equal. You’ll wait less in the afternoon, on average. (p. 446) 14.2 Advanced varying slopes In Section 13.3 we saw that data can be considered cross-classified if they have multiple grouping factors. We used the chipanzees data in that section and we only considered cross-classification by single intercepts. Turns out cross-classified models can be extended further. Let’s load and wrangle those data. data(chimpanzees, package = &quot;rethinking&quot;) d &lt;- chimpanzees rm(chimpanzees) # wrangle d &lt;- d %&gt;% mutate(actor = factor(actor), block = factor(block), treatment = factor(1 + prosoc_left + 2 * condition), # this will come in handy, later labels = factor(treatment, levels = 1:4, labels = c(&quot;r/n&quot;, &quot;l/n&quot;, &quot;r/p&quot;, &quot;l/p&quot;))) glimpse(d) ## Rows: 504 ## Columns: 10 ## $ actor &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ recipient &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ condition &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ block &lt;fct&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, … ## $ trial &lt;int&gt; 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46,… ## $ prosoc_left &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, … ## $ chose_prosoc &lt;int&gt; 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, … ## $ pulled_left &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, … ## $ treatment &lt;fct&gt; 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, … ## $ labels &lt;fct&gt; r/n, r/n, l/n, r/n, l/n, l/n, l/n, l/n, r/n, r/n, r/n, l/n, r/n, l/n, r/n, l/n, l/n, r/… If I’m following along correctly with the text, McElreath’s m14.2 uses the centered parameterization. Recall from the last chapter that brms only supports the non-centered parameterization. Happily, McElreath’s m14.3 appears to use the non-centered parameterization. Thus, we’ll skip making a b14.2 and jump directly into making a b14.3. I believe one could describe the statistical model as \\[\\begin{align*} \\text{left_pull}_i &amp; \\sim \\operatorname{Binomial}(n_i = 1, p_i) \\\\ \\operatorname{logit} (p_i) &amp; = \\gamma_{\\text{treatment}[i]} + \\alpha_{\\text{actor}[i], \\text{treatment}[i]} + \\beta_{\\text{block}[i], \\text{treatment}[i]} \\\\ \\gamma_j &amp; \\sim \\operatorname{Normal}(0, 1), \\;\\;\\; \\text{for } j = 1, \\dots, 4 \\\\ \\begin{bmatrix} \\alpha_{j, 1} \\\\ \\alpha_{j, 2} \\\\ \\alpha_{j, 3} \\\\ \\alpha_{j, 4} \\end{bmatrix} &amp; \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma_\\text{actor} \\end{pmatrix} \\\\ \\begin{bmatrix} \\beta_{j, 1} \\\\ \\beta_{j, 2} \\\\ \\beta_{j, 3} \\\\ \\beta_{j, 4} \\end{bmatrix} &amp; \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma_\\text{block} \\end{pmatrix} \\\\ \\mathbf \\Sigma_\\text{actor} &amp; = \\mathbf{S_\\alpha R_\\alpha S_\\alpha} \\\\ \\mathbf \\Sigma_\\text{block} &amp; = \\mathbf{S_\\beta R_\\beta S_\\beta} \\\\ \\sigma_{\\alpha, [1]}, \\dots, \\sigma_{\\alpha, [4]} &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_{\\beta, [1]}, \\dots, \\sigma_{\\beta, [4]} &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf R_\\alpha &amp; \\sim \\operatorname{LKJ}(2) \\\\ \\mathbf R_\\beta &amp; \\sim \\operatorname{LKJ}(2). \\end{align*}\\] In this model, we have four population-level intercepts, \\(\\gamma_1, \\dots, \\gamma_4\\), one for each of the four levels of treatment. There are two higher-level grouping variables, actor and block, making this a cross-classified model. The term \\(\\alpha_{\\text{actor}[i], \\text{treatment}[i]}\\) is meant to convey that each of the treatment effects can vary by actor. The first line containing the \\(\\operatorname{MVNormal}(\\cdot)\\) operator indicates the actor-level deviations from the population-level estimates for \\(\\gamma_j\\) follow the multivariate normal distribution where the four means are set to zero (i.e., they are deviations) and their spread around those zeros are controlled by \\(\\Sigma_\\text{actor}\\). In the first line below the last line containing \\(\\operatorname{MVNormal}(\\cdot)\\), we learn that \\(\\Sigma_\\text{actor}\\) can be decomposed into two terms, \\(\\mathbf S_\\alpha\\) and \\(\\mathbf R_\\alpha\\). It may not yet be clear by the notation, but \\(\\mathbf S_\\alpha\\) is a \\(4 \\times 4\\) matrix, \\[ \\mathbf S_\\alpha = \\begin{bmatrix} \\sigma_{\\alpha, [1]} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_{\\alpha, [2]} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_{\\alpha, [3]} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_{\\alpha, [4]} \\end{bmatrix}. \\] In a similar way, \\(\\mathbf R_\\alpha\\) is a \\(4 \\times 4\\) matrix, \\[ \\mathbf R_\\alpha = \\begin{bmatrix} 1 &amp; \\rho_{\\alpha, [1, 2]} &amp; \\rho_{\\alpha, [1, 3]} &amp; \\rho_{\\alpha, [1, 4]} \\\\ \\rho_{\\alpha, [2, 1]} &amp; 1 &amp; \\rho_{\\alpha, [2, 3]} &amp; \\rho_{\\alpha, [2, 4]} \\\\ \\rho_{\\alpha, [3, 1]} &amp; \\rho_{\\alpha, [3, 2]} &amp; 1 &amp; \\rho_{\\alpha, [3, 4]} \\\\ \\rho_{\\alpha, [4, 1]} &amp; \\rho_{\\alpha, [4, 2]} &amp; \\rho_{\\alpha, [4, 3]} &amp; 1 \\end{bmatrix}. \\] The same overall pattern holds true for \\(\\beta_{\\text{block}[i], \\text{treatment}[i]}\\) and the associated \\(\\beta\\) parameters connected to the block grouping variable. All the population parameters \\(\\sigma_{\\alpha, [1]}, \\dots, \\sigma_{\\alpha, [4]}\\) and \\(\\sigma_{\\beta, [1]}, \\dots, \\sigma_{\\beta, [4]}\\) have individual \\(\\operatorname{Exponential}(1)\\) priors. The two \\(\\mathbf R_{&lt; \\cdot &gt;}\\) matrices have the priors \\(\\operatorname{LKJ}(2)\\). I know; this is a lot. This all takes time to grapple with. Here’s how to fit such a model with brms. b14.3 &lt;- brm(data = d, family = binomial, pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor) + (0 + treatment | block), prior = c(prior(normal(0, 1), class = b), prior(exponential(1), class = sd, group = actor), prior(exponential(1), class = sd, group = block), prior(lkj(2), class = cor, group = actor), prior(lkj(2), class = cor, group = block)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 4387510, file = &quot;fits/b14.03&quot;) Happily, we got no warnings about divergent transitions. Since it’s been a while, we’ll use bayesplotmcmc_rank_overlay() to examine the primary parameters with a trank plot. library(bayesplot) # give the parameters fancy names names &lt;- c(str_c(&quot;treatment[&quot;, 1:4, &quot;]&quot;), str_c(&quot;sigma[&#39;actor[&quot;, 1:4, &quot;]&#39;]&quot;), str_c(&quot;sigma[&#39;block[&quot;, 1:4, &quot;]&#39;]&quot;), str_c(&quot;rho[&#39;actor:treatment[&quot;, c(1, 1:2, 1:3), &quot;,&quot;, rep(2:4, times = 1:3), &quot;]&#39;]&quot;), str_c(&quot;rho[&#39;block:treatment[&quot;, c(1, 1:2, 1:3), &quot;,&quot;, rep(2:4, times = 1:3), &quot;]&#39;]&quot;), &quot;chain&quot;) # wrangle posterior_samples(b14.3, add_chain = T) %&gt;% select(b_treatment1:`cor_block__treatment3__treatment4`, chain) %&gt;% set_names(names) %&gt;% # plot mcmc_rank_overlay() + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#B1934A&quot;, &quot;#A65141&quot;, &quot;#EEDA9D&quot;)) + scale_x_continuous(breaks = 0:4 * 1e3, labels = c(0, str_c(1:4, &quot;K&quot;))) + coord_cartesian(ylim = c(30, NA)) + theme(legend.position = &quot;bottom&quot;) + facet_wrap(~ parameter, labeller = label_parsed, ncol = 4) Because we only fit a non-centered version of the model, we aren’t able to make a faithful version of McElreath’s Figure 14.6. However, we can still use posterior::summarise_draws() to help make histograms of the two kinds of effective sample sizes for our b14.3. library(posterior) posterior_samples(b14.3) %&gt;% summarise_draws() %&gt;% pivot_longer(starts_with(&quot;ess&quot;)) %&gt;% ggplot(aes(x = value)) + geom_histogram(binwidth = 250, fill = &quot;#EEDA9D&quot;, color = &quot;#DCA258&quot;) + xlim(0, NA) + facet_wrap(~ name) Here is a summary of the model parameters. print(b14.3) ## Family: binomial ## Links: mu = logit ## Formula: pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor) + (0 + treatment | block) ## Data: d (Number of observations: 504) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~actor (Number of levels: 7) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(treatment1) 1.40 0.50 0.69 2.64 1.00 2180 2417 ## sd(treatment2) 0.91 0.41 0.33 1.91 1.00 2810 2411 ## sd(treatment3) 1.84 0.55 1.03 3.15 1.00 3534 3178 ## sd(treatment4) 1.56 0.59 0.73 3.01 1.00 3092 2254 ## cor(treatment1,treatment2) 0.42 0.28 -0.22 0.87 1.00 3004 3131 ## cor(treatment1,treatment3) 0.52 0.25 -0.06 0.89 1.00 2874 2709 ## cor(treatment2,treatment3) 0.49 0.26 -0.10 0.89 1.00 3276 3535 ## cor(treatment1,treatment4) 0.44 0.26 -0.14 0.87 1.00 3030 2697 ## cor(treatment2,treatment4) 0.44 0.28 -0.19 0.88 1.00 3560 3247 ## cor(treatment3,treatment4) 0.58 0.24 0.01 0.92 1.00 3481 2826 ## ## ~block (Number of levels: 6) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(treatment1) 0.42 0.34 0.01 1.26 1.00 2235 2066 ## sd(treatment2) 0.43 0.35 0.02 1.29 1.00 1901 2460 ## sd(treatment3) 0.30 0.27 0.01 0.99 1.00 2864 1930 ## sd(treatment4) 0.47 0.38 0.02 1.41 1.00 2116 2359 ## cor(treatment1,treatment2) -0.07 0.37 -0.74 0.64 1.00 5342 3066 ## cor(treatment1,treatment3) -0.02 0.38 -0.72 0.71 1.00 8111 2730 ## cor(treatment2,treatment3) -0.02 0.38 -0.72 0.69 1.00 5309 2914 ## cor(treatment1,treatment4) 0.05 0.37 -0.68 0.72 1.00 4579 2891 ## cor(treatment2,treatment4) 0.05 0.38 -0.66 0.73 1.00 4518 3276 ## cor(treatment3,treatment4) 0.02 0.38 -0.67 0.72 1.00 3951 3405 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## treatment1 0.22 0.52 -0.80 1.27 1.00 2221 2356 ## treatment2 0.64 0.40 -0.17 1.43 1.00 2906 2486 ## treatment3 -0.02 0.57 -1.16 1.09 1.00 3243 3001 ## treatment4 0.66 0.54 -0.40 1.73 1.00 3365 2678 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Like McElreath explained on page 450, our b14.3 has 76 parameters: 4 average treatment effects, as listed in the ‘Population-Level Effects’ section; 7 \\(\\times\\) 4 = 28 varying effects on actor, as indicated in the ‘~actor:treatment (Number of levels: 7)’ header multiplied by the four levels of treatment; 6 \\(\\times\\) 4 = 24 varying effects on block, as indicated in the ‘~block:treatment (Number of levels: 6)’ header multiplied by the four levels of treatment; 8 standard deviations listed in the eight rows beginning with sd(; and 12 free correlation parameters listed in the eight rows beginning with cor(. Compute the WAIC estimate. b14.3 &lt;- add_criterion(b14.3, &quot;waic&quot;) waic(b14.3) ## ## Computed from 4000 by 504 log-likelihood matrix ## ## Estimate SE ## elpd_waic -272.5 9.8 ## p_waic 27.0 1.4 ## waic 545.1 19.7 ## ## 1 (0.2%) p_waic estimates greater than 0.4. We recommend trying loo instead. Like the \\(p_\\text{WAIC}\\), our brms version of the model has about 27 effective parameters. Now we’ll get a better sense of the model with a posterior predictive check in the form of our version of Figure 14.7. McElreath described his R code 14.22 as “a big chunk of code” (p. 451). I’ll leave up to the reader to decide whether our big code chunk is any better. # for annotation text &lt;- distinct(d, labels) %&gt;% mutate(actor = 1, prop = c(.07, .8, .08, .795)) nd &lt;- d %&gt;% distinct(actor, condition, labels, prosoc_left, treatment) %&gt;% mutate(block = 5) # compute and wrangle the posterior predictions fitted(b14.3, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% # add the empirical proportions left_join( d %&gt;% group_by(actor, treatment) %&gt;% mutate(proportion = mean(pulled_left)) %&gt;% distinct(actor, treatment, proportion), by = c(&quot;actor&quot;, &quot;treatment&quot;) ) %&gt;% mutate(condition = factor(condition)) %&gt;% # plot! ggplot(aes(x = labels)) + geom_hline(yintercept = .5, color = &quot;#E8DCCF&quot;, alpha = 1/2, linetype = 2) + # empirical proportions geom_line(aes(y = proportion, group = prosoc_left), size = 1/4, color = &quot;#394165&quot;) + geom_point(aes(y = proportion, shape = condition), color = &quot;#394165&quot;, fill = &quot;#100F14&quot;, size = 2.5, show.legend = F) + # posterior predictions geom_line(aes(y = Estimate, group = prosoc_left), size = 3/4, color = &quot;#80A0C7&quot;) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, shape = condition), color = &quot;#80A0C7&quot;, fill = &quot;#100F14&quot;, fatten = 8, size = 1/3, show.legend = F) + # annotation for the conditions geom_text(data = text, aes(y = prop, label = labels), color = &quot;#DCA258&quot;, family = &quot;Courier&quot;, size = 3) + scale_shape_manual(values = c(21, 19)) + scale_x_discrete(NULL, breaks = NULL) + scale_y_continuous(&quot;proportion left lever&quot;, breaks = 0:2 / 2, labels = c(&quot;0&quot;, &quot;.5&quot;, &quot;1&quot;)) + labs(subtitle = &quot;Posterior predictions, in light blue, against the raw data, in dark blue, for\\nmodel b14.3, the cross-classified varying effects model.&quot;) + facet_wrap(~ actor, nrow = 1, labeller = label_both) These chimpanzees simply did not behave in any consistently different way in the partner treatments. The model we’ve used here does have some advantages, though. Since it allows for some individuals to differ in how they respond to the treatments, it could reveal a situation in which a treatment has no effect on average, even though some of the individuals respond strongly. That wasn’t the case here. But often we are more interested in the distribution of responses than in the average response, so a model that estimates the distribution of treatment effects is very useful. (p. 452) For more practice with models of this kind, check out my blog post, Multilevel models and the index-variable approach. 14.3 Instruments and causal designs Of course sometimes it won’t be possible to close all of the non-causal paths or rule of unobserved confounds. What can be done in that case? More than nothing. If you are lucky, there are ways to exploit a combination of natural experiments and clever modeling that allow causal inference even when non-causal paths cannot be closed. (p. 455) 14.3.1 Instrumental variables. Say were are interested in the causal impact of education \\(E\\) on wages \\(W\\), \\(E \\rightarrow W\\). Further imagine there is some unmeasured variable \\(U\\) that has causal relations with both, \\(E \\leftarrow U \\rightarrow W\\), creating a backdoor path. We might use good old ggdag to plot the DAG. library(ggdag) dag_coords &lt;- tibble(name = c(&quot;E&quot;, &quot;U&quot;, &quot;W&quot;), x = c(1, 2, 3), y = c(1, 2, 1)) Before we make the plot, we’ll make a custom theme, theme_pearl_dag(), to streamline our DAG plots. theme_pearl_dag &lt;- function(...) { theme_pearl_earring() + theme_dag() + theme(panel.background = element_rect(fill = &quot;#100F14&quot;), ...) } dagify(E ~ U, W ~ E + U, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;U&quot;), shape = 21, stroke = 2, fill = &quot;#FCF9F0&quot;, size = 6, show.legend = F) + geom_dag_text(color = &quot;#100F14&quot;, family = &quot;Courier&quot;) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(&quot;#EEDA9D&quot;, &quot;#A65141&quot;)) + theme_pearl_dag() Instrumental variables will solve some of the difficulties we have in not being able to condition on \\(U\\). Here we’ll call our instrumental variable \\(Q\\). In the terms of the present example, the instrumental variable has the qualities that \\(Q\\) is independent of \\(U\\), \\(Q\\) is not independent of \\(E\\), and \\(Q\\) can only influence \\(W\\) through \\(E\\) (i.e., the effect of \\(Q\\) on \\(W\\) is fully mediated by \\(E\\)). There is what this looks like in a DAG. dag_coords &lt;- tibble(name = c(&quot;Q&quot;, &quot;E&quot;, &quot;U&quot;, &quot;W&quot;), x = c(0, 1, 2, 3), y = c(2, 1, 2, 1)) dagify(E ~ Q + U, W ~ E + U, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;U&quot;), shape = 21, stroke = 2, fill = &quot;#FCF9F0&quot;, size = 6, show.legend = F) + geom_dag_text(color = &quot;#100F14&quot;, family = &quot;Courier&quot;) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(&quot;#EEDA9D&quot;, &quot;#A65141&quot;)) + theme_pearl_dag() Sadly, our condition that \\(Q\\) can only influence \\(W\\) through \\(E\\)–often called the exclusion restriction–generally cannot be tested. Given \\(U\\) is unmeasured, by definition, we also cannot test that \\(Q\\) is independent of \\(U\\). These are model assumptions. Let’s simulate data based on Angrist &amp; Keueger (1991) to get a sense of how this works. # make a standardizing function standardize &lt;- function(x) { (x - mean(x)) / sd(x) } # simulate set.seed(73) n &lt;- 500 dat_sim &lt;- tibble(u_sim = rnorm(n, mean = 0, sd = 1), q_sim = sample(1:4, size = n, replace = T)) %&gt;% mutate(e_sim = rnorm(n, mean = u_sim + q_sim, sd = 1)) %&gt;% mutate(w_sim = rnorm(n, mean = u_sim + 0 * e_sim, sd = 1)) %&gt;% mutate(w = standardize(w_sim), e = standardize(e_sim), q = standardize(q_sim)) dat_sim ## # A tibble: 500 x 7 ## u_sim q_sim e_sim w_sim w e q ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.145 1 1.51 0.216 0.173 -0.575 -1.36 ## 2 0.291 1 0.664 0.846 0.584 -1.09 -1.36 ## 3 0.0938 3 2.44 -0.664 -0.402 -0.0185 0.428 ## 4 -0.127 3 4.09 -0.725 -0.442 0.978 0.428 ## 5 -0.847 4 2.62 -1.24 -0.780 0.0939 1.32 ## 6 0.141 4 3.54 -0.0700 -0.0146 0.651 1.32 ## 7 1.54 2 3.65 1.88 1.26 0.714 -0.464 ## 8 2.74 3 4.91 2.52 1.67 1.48 0.428 ## 9 1.55 3 4.18 0.624 0.439 1.04 0.428 ## 10 0.462 1 0.360 0.390 0.286 -1.27 -1.36 ## # … with 490 more rows \\(Q\\) in this context is like quarter in the school year, but inversely scaled such that larger numbers indicate more quarters. In this simulation, we have set the true effect of education on wages–\\(E \\rightarrow W\\)–to be zero. Any univariate association is through the confounding variable \\(U\\). Also, \\(Q\\) has no direct effect on \\(W\\) or \\(U\\), but it does have a causal relation with \\(E\\), which is \\(Q \\rightarrow E \\leftarrow U\\). First we fit the univariable model corresponding to \\(E \\rightarrow W\\). b14.4 &lt;- brm(data = dat_sim, family = gaussian, w ~ 1 + e, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.04&quot;) print(b14.4) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: w ~ 1 + e ## Data: dat_sim (Number of observations: 500) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.04 -0.08 0.08 1.00 4195 2893 ## e 0.40 0.04 0.32 0.48 1.00 3606 2780 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.92 0.03 0.86 0.98 1.00 4362 3150 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Because we have not conditioned on \\(U\\), then model suggests a moderately large spurious causal relation for \\(E \\rightarrow W\\). Now see what happens when we also condition directly on \\(Q\\), as in \\(Q \\rightarrow W \\leftarrow E\\). b14.5 &lt;- brm(data = dat_sim, family = gaussian, w ~ 1 + e + q, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.05&quot;) print(b14.5) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: w ~ 1 + e + q ## Data: dat_sim (Number of observations: 500) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.00 0.04 -0.07 0.07 1.00 3712 2956 ## e 0.63 0.05 0.54 0.73 1.00 3489 3290 ## q -0.40 0.05 -0.50 -0.31 1.00 3404 3029 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.86 0.03 0.81 0.91 1.00 3585 3149 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Holy smokes that’s a mess. This model suggests both \\(E\\) and \\(Q\\) have moderate to strong causal effects on \\(W\\), even though we know neither do based on the true data-generating model. Like McElreath said, “bad stuff happens” when we condition on an instrumental variable this way. There is no backdoor path through \\(Q\\), as you can see. But there is a non-causal path from \\(Q\\) to \\(W\\) through \\(U\\): \\(Q \\rightarrow E \\leftarrow U \\rightarrow W\\). This is a non-causal path, because changing \\(Q\\) doesn’t result in any change in \\(W\\) through this path. But since we are conditioning on \\(E\\) in the same model, and \\(E\\) is a collider of \\(Q\\) and \\(U\\), the non-causal path is open. This confounds the coefficient on \\(Q\\). It won’t be zero, because it’ll pick up the association between \\(U\\) and \\(W\\). And then, as a result, the coefficient on \\(E\\) can get even more confounded. Used this way, an instrument like \\(Q\\) might be called a bias amplifier. (p. 456, emphasis in the original) The statistical solution to this mess is to express the data-generating DAG as a multivariate statistical model following the form \\[\\begin{align*} \\begin{bmatrix} W_i \\\\ E_i \\end{bmatrix} &amp; \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} \\mu_{\\text W,i} \\\\ \\mu_{\\text E,i} \\end{bmatrix}, \\color{#A65141}{\\mathbf \\Sigma} \\end{pmatrix} \\\\ \\mu_{\\text W,i} &amp; = \\alpha_\\text W + \\beta_\\text{EW} E_i \\\\ \\mu_{\\text E,i} &amp; = \\alpha_\\text E + \\beta_\\text{QE} Q_i \\\\ \\color{#A65141}{\\mathbf\\Sigma} &amp; \\color{#A65141}= \\color{#A65141}{\\begin{bmatrix} \\sigma_\\text W &amp; 0 \\\\ 0 &amp; \\sigma_\\text E \\end{bmatrix} \\mathbf R \\begin{bmatrix} \\sigma_\\text W &amp; 0 \\\\ 0 &amp; \\sigma_\\text E \\end{bmatrix}} \\\\ \\color{#A65141}{\\mathbf R} &amp; \\color{#A65141}= \\color{#A65141}{\\begin{bmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{bmatrix}} \\\\ \\alpha_\\text W \\text{ and } \\alpha_\\text E &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_\\text{EW} \\text{ and } \\beta_\\text{QE} &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma_\\text W \\text{ and } \\sigma_\\text E &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\rho &amp; \\sim \\operatorname{LKJ}(2). \\end{align*}\\] You might not remember, but we’ve actually fit a model like this before. It was b5.3_A from way back in Section 5.1.5.3. The big difference between that earlier model and this one is whereas the former did not include a residual correlation, \\(\\rho\\), this one will. Thus, this time we will make sure to set set_rescor(TRUE) in the formula. Within brms parlance, priors for residual correlations are of class = rescor. e_model &lt;- bf(e ~ 1 + q) w_model &lt;- bf(w ~ 1 + e) b14.6 &lt;- brm(data = dat_sim, family = gaussian, e_model + w_model + set_rescor(TRUE), prior = c(# E model prior(normal(0, 0.2), class = Intercept, resp = e), prior(normal(0, 0.5), class = b, resp = e), prior(exponential(1), class = sigma, resp = e), # W model prior(normal(0, 0.2), class = Intercept, resp = w), prior(normal(0, 0.5), class = b, resp = w), prior(exponential(1), class = sigma, resp = w), # rho prior(lkj(2), class = rescor)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.06&quot;) print(b14.6) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: e ~ 1 + q ## w ~ 1 + e ## Data: dat_sim (Number of observations: 500) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## e_Intercept -0.00 0.04 -0.07 0.07 1.00 2866 2597 ## w_Intercept -0.00 0.04 -0.09 0.09 1.00 2895 2703 ## e_q 0.59 0.04 0.52 0.66 1.00 2809 2527 ## w_e -0.05 0.08 -0.21 0.09 1.00 1969 2492 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_e 0.81 0.03 0.76 0.86 1.00 3419 2895 ## sigma_w 1.02 0.05 0.94 1.12 1.00 2055 2223 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(e,w) 0.54 0.05 0.44 0.64 1.00 1983 2373 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now the parameter for \\(E \\rightarrow W\\), w_e, is just where it should be–near zero. The residual correlation between \\(E\\) and \\(Q\\), rescor(e,w), is positive and large in magnitude, indicating their common influence from the unmeasured variable \\(U\\). Next we’ll take McElreath’s direction to “adjust the simulation and try other scenarios” (p. 459) by adjusting the causal relations, as in his R code 14.28. set.seed(73) n &lt;- 500 dat_sim &lt;- tibble(u_sim = rnorm(n, mean = 0, sd = 1), q_sim = sample(1:4, size = n, replace = T)) %&gt;% mutate(e_sim = rnorm(n, mean = u_sim + q_sim, sd = 1)) %&gt;% mutate(w_sim = rnorm(n, mean = -u_sim + 0.2 * e_sim, sd = 1)) %&gt;% mutate(w = standardize(w_sim), e = standardize(e_sim), q = standardize(q_sim)) dat_sim ## # A tibble: 500 x 7 ## u_sim q_sim e_sim w_sim w e q ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.145 1 1.51 0.809 0.248 -0.575 -1.36 ## 2 0.291 1 0.664 0.396 -0.0563 -1.09 -1.36 ## 3 0.0938 3 2.44 -0.364 -0.615 -0.0185 0.428 ## 4 -0.127 3 4.09 0.347 -0.0922 0.978 0.428 ## 5 -0.847 4 2.62 0.976 0.370 0.0939 1.32 ## 6 0.141 4 3.54 0.357 -0.0852 0.651 1.32 ## 7 1.54 2 3.65 -0.466 -0.690 0.714 -0.464 ## 8 2.74 3 4.91 -1.98 -1.80 1.48 0.428 ## 9 1.55 3 4.18 -1.64 -1.55 1.04 0.428 ## 10 0.462 1 0.360 -0.461 -0.686 -1.27 -1.36 ## # … with 490 more rows We’ll use update() to avoid re-compiling the models. b14.4x &lt;- update(b14.4, newdata = dat_sim, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.04x&quot;) b14.6x &lt;- update(b14.6, newdata = dat_sim, iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.06x&quot;) Just for kicks, let’s examine the results with a coefficient plot. text &lt;- tibble(Estimate = c(fixef(b14.4x)[2, 3], fixef(b14.6x)[4, 4]), y = c(4.35, 3.65), hjust = c(0, 1), fit = c(&quot;b14.4x&quot;, &quot;b14.6x&quot;)) bind_rows( # b_14.4x posterior_summary(b14.4x)[1:3, ] %&gt;% data.frame() %&gt;% mutate(param = c(&quot;alpha[W]&quot;, &quot;beta[EW]&quot;, &quot;sigma[W]&quot;), fit = &quot;b14.4x&quot;), # b_14.6x posterior_summary(b14.6x)[1:7, ] %&gt;% data.frame() %&gt;% mutate(param = c(&quot;alpha[E]&quot;, &quot;alpha[W]&quot;, &quot;beta[QE]&quot;, &quot;beta[EW]&quot;, &quot;sigma[E]&quot;, &quot;sigma[W]&quot;, &quot;rho&quot;), fit = &quot;b14.6x&quot;)) %&gt;% mutate(param = factor(param, levels = c(&quot;rho&quot;, &quot;sigma[W]&quot;, &quot;sigma[E]&quot;, &quot;beta[EW]&quot;, &quot;beta[QE]&quot;, &quot;alpha[W]&quot;, &quot;alpha[E]&quot;))) %&gt;% ggplot(aes(x = param, y = Estimate, color = fit)) + geom_hline(yintercept = 0, color = &quot;#E8DCCF&quot;, alpha = 1/4) + geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5), fatten = 2, position = position_dodge(width = 0.5)) + geom_text(data = text, aes(x = y, label = fit, hjust = hjust)) + scale_color_manual(NULL, values = c(&quot;#E7CDC2&quot;, &quot;#A65141&quot;)) + scale_x_discrete(NULL, labels = ggplot2:::parse_safe) + ylab(&quot;marginal posterior&quot;) + coord_flip() + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank(), legend.position = &quot;none&quot;) With the help from b14.6x, we found “that \\(E\\) and \\(W\\) have a negative correlation in their residual variance, because the confound positively influences one and negatively influences the other” (p. 459). One can use the dagitty() and instrumentalVariables() functions from the dagitty package to first define a DAG and then query whether there are instrumental variables for a given exposure and outcome. library(dagitty) dagIV &lt;- dagitty(&quot;dag{Q -&gt; E &lt;- U -&gt; W &lt;- E}&quot;) instrumentalVariables(dagIV, exposure = &quot;E&quot;, outcome = &quot;W&quot;) ## Q The hardest thing about instrumental variables is believing in any particular instrument. If you believe in your DAG, they are easy to believe. But should you believe in your DAG?… In general, it is not possible to statistically prove whether a variable is a good instrument. As always, we need scientific knowledge outside of the data to make sense of the data. (p. 460) 14.3.1.1 Rethinking: Two-stage worst squares. “The instrumental variable model is often discussed with an estimation procedure known as two-stage least squares (2SLS)” (p. 460, emphasis in the original). For a nice introduction to instrumental variables via 2SLS, see this practical introduction, and also the slides and video-lecture files, from the great Andrew Heiss. 14.3.2 Other designs. There are potentially many ways to find natural experiments. Not all of them are strictly instrumental variables. But they can provide theoretically correct designs for causal inference, if you can believe the assumptions. Let’s consider two more. In addition to the backdoor criterion you met in Chapter 6, there is something called the front-door criterion. (p. 460, emphasis in the original) To get a sense of the front-door criterion, consider the following DAG with observed variables \\(X\\), \\(Y\\), and \\(Z\\) and an unobserved variable, \\(U\\). dag_coords &lt;- tibble(name = c(&quot;X&quot;, &quot;Z&quot;, &quot;U&quot;, &quot;Y&quot;), x = c(1, 2, 2, 3), y = c(1, 1, 2, 1)) dagify(X ~ U, Z ~ X, Y ~ U + Z, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;U&quot;), shape = 21, stroke = 2, fill = &quot;#FCF9F0&quot;, size = 6, show.legend = F) + geom_dag_text(color = &quot;#100F14&quot;, family = &quot;Courier&quot;) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(&quot;#EEDA9D&quot;, &quot;#A65141&quot;)) + theme_pearl_dag() We are interested, as usual, in the causal influence of \\(X\\) on \\(Y\\). But there is an unobserved confound \\(U\\), again as usual. It turns out that, if we can find a perfect mediator \\(Z\\), then we can possibly estimate the causal effect of \\(X\\) on \\(Y\\). It isn’t crazy to think that causes are mediated by other causes. Everything has a mechanism. \\(Z\\) in the DAG above is such a mechanism. If you have a believable \\(Z\\) variable, then the causal effect of \\(X\\) on \\(Y\\) is estimated by expressing the generative model as a statistical model, similar to the instrumental variable example before. (p. 461) McElreath’s second example is the regression discontinuity approach. If you have a time series where the variable of interest is measured before and after some relevant intervention variable, you can estimate intercepts and slopes before and after the intervention, the cutoff. However, in practice, one trend is fit for individuals above the cutoff and another to those below the cutoff. Then an estimate of the causal effect is the average difference between individuals just above and just below the cutoff. While the difference near the cuttoff is of interest, the entire function influences this difference. So some care is needed in choosing functions for the overall relationship between the exposure and the outcome. (p. 461) McElreath’s not kidding about the need for care when fitting regression discontinuity models. Gleman’s blog is littered with awful examples (e.g., here, here, here, here, here). See also Gelman and Imbens’ (2019) paper, Why high-order polynomials should not be used in regression discontinuity designs, or Nick HK’s informative tweet on how this applies to autocorrelated data.5 14.4 Social relations as correlated varying effects It looks like brms is not set up to fit a model like this, at this time. See the Social relations model (SRM) thread on the Stan Forums and issue #502 on the brms GitHub repo for details. In short, the difficulty is brms is not set up to allow covariances among distinct random effects with the same levels and it looks like this will not change any time soon. So, in this section we will fit the model with rethinking, but still use ggplot2 and friends in the post processing. Let’s load the kl_dyads data (Koster &amp; Leckie, 2014). library(rethinking) data(KosterLeckie) Take a look at the data. kl_dyads %&gt;% glimpse() ## Rows: 300 ## Columns: 13 ## $ hidA &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2,… ## $ hidB &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 3, 4… ## $ did &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 2… ## $ giftsAB &lt;int&gt; 0, 6, 2, 4, 8, 2, 1, 0, 10, 1, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 1, 2, 0, 3, 0, 43,… ## $ giftsBA &lt;int&gt; 4, 31, 5, 2, 2, 1, 2, 1, 110, 0, 0, 6, 11, 0, 1, 4, 0, 2, 0, 7, 0, 13, 0, 2, 3, 1, 1, 0, 1, … ## $ offset &lt;dbl&gt; 0.000, -0.003, -0.019, 0.000, -0.003, 0.000, 0.000, 0.000, -0.186, 0.000, -0.471, -0.019, -0… ## $ drel1 &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,… ## $ drel2 &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,… ## $ drel3 &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,… ## $ drel4 &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ dlndist &lt;dbl&gt; -2.790, -2.817, -1.886, -1.892, -3.499, -1.853, -1.475, -1.644, -1.897, -2.379, -2.200, -2.1… ## $ dass &lt;dbl&gt; 0.000, 0.044, 0.025, 0.011, 0.022, 0.071, 0.046, 0.003, 0.552, 0.018, 0.004, 0.004, 0.036, 0… ## $ d0125 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… # kl_households %&gt;% glimpse() “The variables hidA and hidB tell us the household IDs in each dyad, and did is a unique dyad ID number” (p. 462). To get a sense of the interrelation among those three ID variables, we’ll make a tile plot. kl_dyads %&gt;% ggplot(aes(x = hidA, y = hidB, label = did)) + geom_tile(aes(fill = did), show.legend = F) + geom_text(size = 2.25, family = &quot;Courier&quot;) + geom_vline(xintercept = 0:24 + 0.5, color = &quot;#394165&quot;, size = 1/5) + geom_hline(yintercept = 1:25 + 0.5, color = &quot;#394165&quot;, size = 1/5) + scale_fill_gradient(low = &quot;#DCA258&quot;, high = &quot;#EEDA9D&quot;, limits = c(1, NA)) + scale_x_continuous(breaks = 1:24) + scale_y_continuous(breaks = 2:25) + theme(axis.text = element_text(size = 9), axis.ticks = element_blank()) The orange/yellow gradient fill is a little silly, but I couldn’t stop myself. Here is our version of Figure 14.8, the bivariate distribution of dyadic gifts, collapsing across dyads. kl_dyads %&gt;% ggplot(aes(x = giftsAB, y = giftsBA)) + geom_hex(bins = 70) + geom_abline(color = &quot;#DCA258&quot;, linetype = 3) + scale_fill_gradient(low = &quot;#E7CDC2&quot;, high = &quot;#A65141&quot;, limits = c(1, NA)) + scale_x_continuous(&quot;gifts household A to household B&quot;, limits = c(0, 113)) + scale_y_continuous(&quot;gifts from B to A&quot;, limits = c(0, 113)) + ggtitle(&quot;Distribution of dyadic gifts&quot;) + coord_equal() Here’s the overall Pearson’s correlation coefficient, collapsing across grouping levels. cor(kl_dyads$giftsAB, kl_dyads$giftsBA) %&gt;% round(digits = 3) ## [1] 0.239 However, it would be a mistake to take this correlation seriously. It is a disentangled mixture of various kinds of associations, none of which are guaranteed to be even close to \\(r = .24\\). Remember this as we move along with the analyses and let the consequences burn a methodological mark into your soul. kl_data &lt;- list( N = nrow(kl_dyads), N_households = max(kl_dyads$hidB), did = kl_dyads$did, hidA = kl_dyads$hidA, hidB = kl_dyads$hidB, giftsAB = kl_dyads$giftsAB, giftsBA = kl_dyads$giftsBA ) m14.7 &lt;- ulam( alist( giftsAB ~ poisson(lambdaAB), giftsBA ~ poisson(lambdaBA), log(lambdaAB) &lt;- a + gr[hidA, 1] + gr[hidB, 2] + d[did, 1] , log(lambdaBA) &lt;- a + gr[hidB, 1] + gr[hidA, 2] + d[did, 2] , a ~ normal(0, 1), ## gr matrix of varying effects vector[2]:gr[N_households] ~ multi_normal(0, Rho_gr, sigma_gr), Rho_gr ~ lkj_corr(4), sigma_gr ~ exponential(1), ## dyad effects transpars&gt; matrix[N,2]:d &lt;- compose_noncentered(rep_vector(sigma_d, 2), L_Rho_d, z), matrix[2,N]:z ~ normal(0, 1), cholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky(8), sigma_d ~ exponential(1), ## compute correlation matrix for dyads gq&gt; matrix[2, 2]:Rho_d &lt;&lt;- Chol_to_Corr(L_Rho_d) ), data = kl_data, chains = 4, cores = 4, iter = 2000 ) We don’t get a lot of information from the default precis() output for this model, but at least we’ll get a summary for \\(\\alpha\\). precis(m14.7) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a 0.5425828 0.17069448 0.2788992 0.8200983 633.0907 1.003374 ## sigma_d 1.1008880 0.05684433 1.0139311 1.1948653 1106.7396 1.000834 One of the interesting things about this model is we only have one \\(\\alpha\\) parameter for two criterion variables. This makes \\(\\alpha\\) like the grand mean of counts. Here is a focused look at the precis() output when you set depth = 3. precis(m14.7, depth = 3, pars = c(&quot;Rho_gr&quot;, &quot;sigma_gr&quot;)) ## mean sd 5.5% 94.5% n_eff Rhat4 ## Rho_gr[1,1] 1.0000000 0.000000e+00 1.0000000 1.00000000 NaN NaN ## Rho_gr[1,2] -0.4043048 1.971661e-01 -0.6909339 -0.07675351 1335.866 1.0004850 ## Rho_gr[2,1] -0.4043048 1.971661e-01 -0.6909339 -0.07675351 1335.866 1.0004850 ## Rho_gr[2,2] 1.0000000 8.296194e-17 1.0000000 1.00000000 3692.386 0.9989995 ## sigma_gr[1] 0.8303181 1.384847e-01 0.6399042 1.06842157 1860.543 1.0012698 ## sigma_gr[2] 0.4214374 9.036310e-02 0.2870140 0.57051380 1060.689 1.0014661 These are the posterior summaries for the part of the model McElreath defined in the middle of page 463, \\[ \\begin{bmatrix} g_i \\\\ r_i \\end{bmatrix} \\sim \\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_g^2 &amp; \\sigma_g \\sigma_r \\rho_{gr} \\\\ \\sigma_g \\sigma_r \\rho_{rg} &amp; \\sigma_r^2 \\end{bmatrix} \\end{pmatrix}, \\] the population of household effects. But as per usual with Stan, the variance parameters are expressed in a \\(\\sigma\\) metric. Also, rethinking::precis() returned the summary for \\(\\rho_{gr}\\) in matrix form, \\[ \\begin{bmatrix} 1 &amp; \\rho_{gr} \\\\ \\rho_{rg} &amp; 1 \\end{bmatrix}, \\] where \\(\\rho_{gr} = \\rho_{rg}\\). The correlation is negative. Let’s view \\(\\sigma_g\\), \\(\\sigma_r\\), and their correlation in a plot. post &lt;- extract.samples(m14.7) tibble(`sigma[italic(g)]` = post$sigma_gr[, 1], `sigma[italic(r)]` = post$sigma_gr[, 2], `rho[italic(g)][italic(r)]` = post$Rho_gr[, 2, 1]) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name, fill = name)) + geom_vline(xintercept = 0, color = &quot;#FCF9F0&quot;, alpha = 1/3) + stat_halfeye(.width = .89, color = &quot;#FCF9F0&quot;, height = 1.5) + scale_fill_manual(values = c(&quot;#80A0C7&quot;, &quot;#EEDA9D&quot;, &quot;#DCA258&quot;)) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + xlab(&quot;marginal posterior&quot;) + coord_cartesian(ylim = c(1.5, 3.9)) + theme(legend.position = &quot;none&quot;) “This implies that individuals who give more across all dyads tend to receive less[, and ] clear evidence that rates of giving are more variable than rates of receiving” (p. 465). McElreath suggested we “try plot(exp(g[,1]),exp(r[,1])) for example to show the posterior distribution of giving/receiving for household number 1” (p. 465). Here’s a tidyverse version of that plot. g &lt;- sapply( 1:25 , function(i) post$a + post$gr[,i,1] ) r &lt;- sapply( 1:25 , function(i) post$a + post$gr[,i,2] ) tibble(g = exp(g[, 1]), r = exp(r[, 1])) %&gt;% ggplot(aes(x = g, y = r)) + geom_abline(color = &quot;#FCF9F0&quot;, linetype = 2, alpha = 1/3) + # white &quot;#FCF9F0&quot; # gold &quot;#B1934A&quot; geom_point(color = &quot;#B1934A&quot;, alpha = 1/3, size = 1/4) + stat_ellipse(type = &quot;norm&quot;, level = .5, size = 1/2, color = &quot;#80A0C7&quot;) + stat_ellipse(type = &quot;norm&quot;, level = .9, size = 1/2, color = &quot;#80A0C7&quot;) + labs(x = expression(giving[italic(i)==1]), y = expression(receiving[italic(i)==1])) + coord_equal(xlim = c(0, 5), ylim = c(0, 5)) The gold dots are the bivariate posterior draws and the two blue ellipses mark off the 50% and 90% intervals, presuming a bivariate Gaussian distribution. Here’s a programmatic way to make our version of Figure 14.9.a. rbind(exp(g), exp(r)) %&gt;% data.frame() %&gt;% set_names(1:25) %&gt;% mutate(parameter = rep(c(&quot;g&quot;, &quot;r&quot;), each = n() / 2), iter = rep(1:4000, times = 2)) %&gt;% pivot_longer(-c(parameter, iter), names_to = &quot;household&quot;) %&gt;% pivot_wider(names_from = parameter, values_from = value) %&gt;% group_by(household) %&gt;% mutate(mu_g = mean(g), mu_r = mean(r)) %&gt;% nest(data = c(&quot;g&quot;, &quot;r&quot;, &quot;iter&quot;)) %&gt;% ggplot(aes(group = household)) + geom_abline(color = &quot;#FCF9F0&quot;, linetype = 2, alpha = 1/3) + stat_ellipse(data = . %&gt;% unnest(data), aes(x = g, y = r), type = &quot;norm&quot;, level = .5, size = 1/2, alpha = 1/2, color = &quot;#80A0C7&quot;) + geom_point(aes(x = mu_g, y = mu_r), color = &quot;#DCA258&quot;) + labs(x = &quot;generalized giving&quot;, y = &quot;generalized receiving&quot;) + coord_equal(xlim = c(0, 8.5), ylim = c(0, 8.5)) Here is a look at the covariance matrix for the dyadic effects, the posterior summaries for the part of the model McElreath defined in the middle of page 463 as, \\[ \\begin{bmatrix} d_{ij} \\\\ d_{ji} \\end{bmatrix} \\sim \\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_d^2 &amp; \\sigma_d^2 \\rho_d \\\\ \\sigma_d^2 \\rho_d &amp; \\sigma_d^2 \\end{bmatrix} \\end{pmatrix}, \\] where there is only one standard deviation parameter \\(\\sigma_d\\) because the labels for each dyad are arbitrary. precis(m14.7, depth = 3, pars = c(&quot;Rho_d&quot;, &quot;sigma_d&quot;)) ## mean sd 5.5% 94.5% n_eff Rhat4 ## Rho_d[1,1] 1.0000000 0.00000000 1.0000000 1.0000000 NaN NaN ## Rho_d[1,2] 0.8812754 0.03313453 0.8234015 0.9294063 1039.922 1.005997 ## Rho_d[2,1] 0.8812754 0.03313453 0.8234015 0.9294063 1039.922 1.005997 ## Rho_d[2,2] 1.0000000 0.00000000 1.0000000 1.0000000 NaN NaN ## sigma_d 1.1008880 0.05684433 1.0139311 1.1948653 1106.740 1.000834 Here they are in a plot. tibble(`sigma[italic(d)]` = post$sigma_d, `rho[italic(d)]` = post$Rho_d[, 2, 1]) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value, y = name, fill = name)) + geom_vline(xintercept = 0, color = &quot;#FCF9F0&quot;, alpha = 1/3) + stat_halfeye(.width = .89, color = &quot;#FCF9F0&quot;) + scale_fill_manual(values = c(&quot;#A65141&quot;, &quot;#B1934A&quot;)) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + xlab(&quot;marginal posterior&quot;) + coord_cartesian(ylim = c(1.5, 2)) + theme(legend.position = &quot;none&quot;) “The correlation here is positive and strong. And there is more variation among dyads than there is among household in giving rates” (p. 467). Now make the right hand panel of Figure 14.9. tibble(dy1 = apply(post$d[, , 1], 2, mean), dy2 = apply(post$d[, , 2], 2, mean)) %&gt;% ggplot(aes(x = dy1, y = dy2)) + geom_abline(color = &quot;#FCF9F0&quot;, linetype = 2, alpha = 1/3) + geom_vline(xintercept = 0, color = &quot;#FCF9F0&quot;, linetype = 2, alpha = 1/3) + geom_hline(yintercept = 0, color = &quot;#FCF9F0&quot;, linetype = 2, alpha = 1/3) + geom_point(color = &quot;#8B9DAF&quot;, alpha = 1/2, size = 1/2) + geom_text(x = mean(post$d[, 1, 1]), y = mean(post$d[, 1, 2]), label = &quot;1&quot;, color = &quot;#EEDA9D&quot;, family = &quot;Courier&quot;) + labs(x = &quot;household A in dyad&quot;, y = &quot;household B in dyad&quot;) + coord_equal(xlim = c(-2, 3.5), ylim = c(-2, 3.5)) As McElreath pointed out, each dot is the posterior mean for one of the 300 levels of did. Do you see the yellow #1 toward the middle? It’s marking off the posterior mean for did == 1. To give a better sense of the uncertainty in each of the levels of did, here is the full bivariate distribution for did == 1. tibble(dy1 = post$d[, 1, 1], dy2 = post$d[, 1, 2]) %&gt;% ggplot(aes(x = dy1, y = dy2)) + geom_abline(color = &quot;#FCF9F0&quot;, linetype = 2, alpha = 1/3) + geom_vline(xintercept = 0, color = &quot;#FCF9F0&quot;, linetype = 2, alpha = 1/3) + geom_hline(yintercept = 0, color = &quot;#FCF9F0&quot;, linetype = 2, alpha = 1/3) + geom_point(color = &quot;#8B9DAF&quot;, alpha = 1/3, size = 1/4) + stat_ellipse(type = &quot;norm&quot;, level = .5, size = 1/2, color = &quot;#EEDA9D&quot;) + stat_ellipse(type = &quot;norm&quot;, level = .9, size = 1/2, color = &quot;#EEDA9D&quot;) + labs(x = expression(&quot;household A in dyad&quot;[italic(i)==1]), y = expression(&quot;household B in dyad&quot;[italic(i)==1])) + coord_equal(xlim = c(-2, 3.5), ylim = c(-2, 3.5)) The two yellow ellipses mark off the 50% and 90% intervals, again presuming a bivariate Gaussian distribution for the posterior. 14.5 Continuous categories and the Gaussian process There is a way to apply the varying effects approach to continuous categories… The general approach is known as Gaussian process regression. This name is unfortunately wholly uninformative about what it is for and how it works. We’ll proceed to work through a basic example that demonstrates both what it is for and how it works. The general purpose is to define some dimension along which cases differ. This might be individual differences in age. Or it could be differences in location. Then we measure the distance between each pair of cases. What the model then does is estimate a function for the covariance between pairs of cases at different distances. This covariance function provides one continuous category generalization of the varying effects approach. (p. 468, emphasis in the original) 14.5.1 Example: Spatial autocorrelation in Oceanic tools. We start by loading the matrix of geographic distances. # load the distance matrix library(rethinking) data(islandsDistMatrix) # display (measured in thousands of km) d_mat &lt;- islandsDistMatrix colnames(d_mat) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;) round(d_mat, 1) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Malekula 0.0 0.5 0.6 4.4 1.2 2.0 3.2 2.8 1.9 5.7 ## Tikopia 0.5 0.0 0.3 4.2 1.2 2.0 2.9 2.7 2.0 5.3 ## Santa Cruz 0.6 0.3 0.0 3.9 1.6 1.7 2.6 2.4 2.3 5.4 ## Yap 4.4 4.2 3.9 0.0 5.4 2.5 1.6 1.6 6.1 7.2 ## Lau Fiji 1.2 1.2 1.6 5.4 0.0 3.2 4.0 3.9 0.8 4.9 ## Trobriand 2.0 2.0 1.7 2.5 3.2 0.0 1.8 0.8 3.9 6.7 ## Chuuk 3.2 2.9 2.6 1.6 4.0 1.8 0.0 1.2 4.8 5.8 ## Manus 2.8 2.7 2.4 1.6 3.9 0.8 1.2 0.0 4.6 6.7 ## Tonga 1.9 2.0 2.3 6.1 0.8 3.9 4.8 4.6 0.0 5.0 ## Hawaii 5.7 5.3 5.4 7.2 4.9 6.7 5.8 6.7 5.0 0.0 If you wanted to use color to more effectively visualize the values in the matrix, you might do something like this. d_mat %&gt;% data.frame() %&gt;% rownames_to_column(&quot;row&quot;) %&gt;% gather(column, distance, -row) %&gt;% mutate(column = factor(column, levels = colnames(d_mat)), row = factor(row, levels = rownames(d_mat)) %&gt;% fct_rev(), label = formatC(distance, format = &#39;f&#39;, digits = 2)) %&gt;% ggplot(aes(x = column, y = row)) + geom_raster(aes(fill = distance)) + geom_text(aes(label = label), size = 3, family = &quot;Courier&quot;, color = &quot;#100F14&quot;) + scale_fill_gradient(low = &quot;#FCF9F0&quot;, high = &quot;#A65141&quot;) + scale_x_discrete(NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + theme_pearl_earring(axis.text.y = element_text(hjust = 0)) + theme(axis.ticks = element_blank()) Figure 14.10 shows the “shape of the function relating distance to the covariance \\(\\mathbf K_{ij}\\).” tibble(x = seq(from = 0, to = 4, by = .01), linear = exp(-1 * x), squared = exp(-1 * x^2)) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = linear), color = &quot;#B1934A&quot;, linetype = 2) + geom_line(aes(y = squared), color = &quot;#DCA258&quot;) + scale_x_continuous(&quot;distance&quot;, expand = c(0, 0)) + scale_y_continuous(&quot;correlation&quot;, breaks = c(0, .5, 1), labels = c(0, &quot;.5&quot;, 1)) Now load the primary data. data(Kline2) # load the ordinary data, now with coordinates d &lt;- Kline2 %&gt;% mutate(society = 1:10) rm(Kline2) d %&gt;% glimpse() ## Rows: 10 ## Columns: 10 ## $ culture &lt;fct&gt; Malekula, Tikopia, Santa Cruz, Yap, Lau Fiji, Trobriand, Chuuk, Manus, Tonga, Hawaii ## $ population &lt;int&gt; 1100, 1500, 3600, 4791, 7400, 8000, 9200, 13000, 17500, 275000 ## $ contact &lt;fct&gt; low, low, low, high, high, high, high, low, high, low ## $ total_tools &lt;int&gt; 13, 22, 24, 43, 33, 19, 40, 28, 55, 71 ## $ mean_TU &lt;dbl&gt; 3.2, 4.7, 4.0, 5.0, 5.0, 4.0, 3.8, 6.6, 5.4, 6.6 ## $ lat &lt;dbl&gt; -16.3, -12.3, -10.7, 9.5, -17.7, -8.7, 7.4, -2.1, -21.2, 19.9 ## $ lon &lt;dbl&gt; 167.5, 168.8, 166.0, 138.1, 178.1, 150.9, 151.6, 146.9, -175.2, -155.6 ## $ lon2 &lt;dbl&gt; -12.5, -11.2, -14.0, -41.9, -1.9, -29.1, -28.4, -33.1, 4.8, 24.4 ## $ logpop &lt;dbl&gt; 7.003065, 7.313220, 8.188689, 8.474494, 8.909235, 8.987197, 9.126959, 9.472705, 9.769956… ## $ society &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 👋 Heads up: The brms package is capable of handling a variety of Gaussian process models using the gp() function. As we will see throughout this section, this method will depart in important ways from how McElreath fits Gaussian process models with rethinking. Due in large part to these differences, this section and its analogue in the first edition of Statistical rethinking (McElreath, 2015) baffled me, at first. Happily, fellow enthusiasts Louis Bliard and Richard Torkar reached out and helped me hammer this section out behind the scenes. The method to follow is due in large part to their efforts. 🤝 The brms::gp() function takes a handful of arguments. The first and most important argument, ..., accepts the names of one or more predictors from the data. When fitting a spatial Gaussian process of this kind, we’ll enter in the latitude and longitude data for each of levels of culture. This will be an important departure from the text. For his m14.8, McElreath directly entered in the Dmat distance matrix data into ulam(). In so doing, he defined \\(D_{ij}\\), the matrix of distances between each of the societies. When using brms, we instead estimate the distance matrix from the latitude and longitude variables. Before we practice fitting a Gaussian process with the brms::gp() function, we’ll first need to think a little bit about our data. McElreath’s Dmat measured the distances in thousands of km. However, the lat and lon2 variables in the data above are in decimal degrees, which means they need to be transformed to keep our model in the same metric as McElreath’s. Turns out that one decimal degree is 111.32km (at the equator). Thus, we can turn both lat and lon2 into 1,000 km units by multiplying each by 0.11132. Here’s the conversion. d &lt;- d %&gt;% mutate(lat_adj = lat * 0.11132, lon2_adj = lon2 * 0.11132) d %&gt;% select(culture, lat, lon2, lat_adj:lon2_adj) ## culture lat lon2 lat_adj lon2_adj ## 1 Malekula -16.3 -12.5 -1.814516 -1.391500 ## 2 Tikopia -12.3 -11.2 -1.369236 -1.246784 ## 3 Santa Cruz -10.7 -14.0 -1.191124 -1.558480 ## 4 Yap 9.5 -41.9 1.057540 -4.664308 ## 5 Lau Fiji -17.7 -1.9 -1.970364 -0.211508 ## 6 Trobriand -8.7 -29.1 -0.968484 -3.239412 ## 7 Chuuk 7.4 -28.4 0.823768 -3.161488 ## 8 Manus -2.1 -33.1 -0.233772 -3.684692 ## 9 Tonga -21.2 4.8 -2.359984 0.534336 ## 10 Hawaii 19.9 24.4 2.215268 2.716208 Note that because this conversion is valid at the equator, it is only an approximation for latitude and longitude coordinates for our island societies. Now we’ve scaled our two spatial variables, the basic way to use them in a brms Gaussian process is including gp(lat_adj, lon2_adj) into the formula argument within the brm() function. Note however that one of the default gp() settings is scale = TRUE, which scales predictors so that the maximum distance between two points is 1. We don’t want this for our example, so we will set scale = FALSE instead. Our Gaussian process model is an extension of the non-linear model from Section 11.2.1.1, b11.11. Thus our model here will also use the non-linear syntax. Here’s how we might use brms to fit our amended non-centered version of McElreath’s m14.8. b14.8 &lt;- brm(data = d, family = poisson(link = &quot;identity&quot;), bf(total_tools ~ exp(a) * population^b / g, a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE), b + g ~ 1, nl = TRUE), prior = c(prior(normal(0, 1), nlpar = a), prior(exponential(1), nlpar = b, lb = 0), prior(exponential(1), nlpar = g, lb = 0), prior(inv_gamma(2.874624, 2.941204), class = lscale, coef = gplat_adjlon2_adj, nlpar = a), prior(exponential(1), class = sdgp, coef = gplat_adjlon2_adj, nlpar = a)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, sample_prior = T, file = &quot;fits/b14.08&quot;) Check the results. print(b14.8) ## Family: poisson ## Links: mu = identity ## Formula: total_tools ~ exp(a) * population^b/g ## a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE) ## b ~ 1 ## g ~ 1 ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Gaussian Process Terms: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sdgp(a_gplat_adjlon2_adj) 0.47 0.30 0.15 1.27 1.00 1038 1565 ## lscale(a_gplat_adjlon2_adj) 1.64 0.91 0.51 4.00 1.00 1180 2207 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_Intercept 0.34 0.86 -1.41 1.95 1.00 2842 2573 ## b_Intercept 0.26 0.08 0.09 0.42 1.00 1526 1376 ## g_Intercept 0.67 0.65 0.05 2.44 1.00 2185 2118 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The posterior_summary() function will return a summary that looks more like the one in the text. posterior_summary(b14.8)[1:15, ] %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_a_Intercept 0.34 0.86 -1.41 1.95 ## b_b_Intercept 0.26 0.08 0.09 0.42 ## b_g_Intercept 0.67 0.65 0.05 2.44 ## sdgp_a_gplat_adjlon2_adj 0.47 0.30 0.15 1.27 ## lscale_a_gplat_adjlon2_adj 1.64 0.91 0.51 4.00 ## zgp_a_gplat_adjlon2_adj[1] -0.46 0.73 -1.91 0.95 ## zgp_a_gplat_adjlon2_adj[2] 0.44 0.85 -1.23 2.07 ## zgp_a_gplat_adjlon2_adj[3] -0.62 0.72 -1.99 0.90 ## zgp_a_gplat_adjlon2_adj[4] 1.00 0.68 -0.26 2.41 ## zgp_a_gplat_adjlon2_adj[5] 0.25 0.74 -1.18 1.75 ## zgp_a_gplat_adjlon2_adj[6] -1.04 0.77 -2.59 0.48 ## zgp_a_gplat_adjlon2_adj[7] 0.16 0.73 -1.40 1.59 ## zgp_a_gplat_adjlon2_adj[8] -0.22 0.85 -1.86 1.51 ## zgp_a_gplat_adjlon2_adj[9] 0.42 0.91 -1.46 2.12 ## zgp_a_gplat_adjlon2_adj[10] -0.38 0.80 -2.01 1.16 Let’s focus on our three non-linear parameters, first. Happily, both our b_b_Intercept and b_g_Intercept summaries look a lot like those for McElreath’s b and g, respectively. Our b_a_Intercept might look distressingly small, but that’s just because of how we parameterized our model. It’s actually very close to McElreath’s a after you exponentiate. fixef(b14.8, probs = c(.055, .945))[&quot;a_Intercept&quot;, c(1, 3:4)] %&gt;% exp() %&gt;% round(digits = 2) ## Estimate Q5.5 Q94.5 ## 1.41 0.35 5.26 Our Gaussian process parameters are different from McElreath’s. From the gp section of the brms reference manual (Bürkner, 2021i), we learn the brms parameterization follows the form \\[k(x_{i},x_{j}) = sdgp^2 \\exp \\big (-||x_i - x_j||^2 / (2 lscale^2) \\big ),\\] where \\(k(x_{i},x_{j})\\) is the same as McElreath’s \\(\\mathbf K_{ij}\\) and \\(||x_i - x_j||^2\\) is the Euclidean distance, the same as McElreath’s \\(D_{ij}^2\\). Thus we could also express the brms parameterization as \\[\\mathbf K_{ij} = sdgp^2 \\exp \\big (-D_{ij}^2 / (2 lscale^2) \\big ),\\] which is much closer to McElreath’s \\[\\mathbf K_{ij} = \\eta^2 \\exp \\big (-\\rho^2 D_{ij}^2 \\big ) + \\delta_{ij} \\sigma^2\\] On page 470, McElreath explained that the final \\(\\delta_{ij} \\sigma^2\\) term is mute with the Oceanic societies data. Thus we won’t consider it further. This reduces McElreath’s equation to \\[\\mathbf K_{ij} = \\eta^2 \\exp \\big (-\\rho^2 D_{ij}^2 \\big ).\\] Importantly, what McElreath called \\(\\eta\\), Bürkner called \\(sdgp\\). While McElreath estimated \\(\\eta^2\\), brms simply estimated \\(sdgp\\). So we’ll have to square our sdgp(a_gplat_adjlon2_adj) before it’s on the same scale as etasq in the text. Here it is. post &lt;- posterior_samples(b14.8) %&gt;% mutate(etasq = sdgp_a_gplat_adjlon2_adj^2) post %&gt;% mean_hdi(etasq, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) ## etasq .lower .upper .width .point .interval ## 1 0.313 0.001 0.62 0.89 mean hdi Though our posterior is a little bit larger than McElreath’s, we’re in the ballpark. You may have noticed that in our model brm() code, above, we just went with the flow and kept the exponential(1) prior on sdgp. The brms default would have been student_t(3, 0, 15.6). Now look at the denominator of the inner part of Bürkner’s equation, \\(2 lscale^2\\). This appears to be the brms equivalent to McElreath’s \\(\\rho^2\\). Or at least it’s what we’ve got. Anyway, also note that McElreath estimated \\(\\rho^2\\) directly as rhosq. If I’m doing the algebra correctly, we might expect \\[\\begin{align*} \\rho^2 &amp; = 1/(2 \\cdot lscale^2) &amp; \\text{and thus} \\\\ lscale &amp; = \\sqrt{1 / (2 \\cdot \\rho^2)}. \\end{align*}\\] To get a sense of this relationship, it might be helpful to plot. p1 &lt;- tibble(`rho^2` = seq(from = 0, to = 11, by = 0.01)) %&gt;% mutate(lscale = sqrt(1 / (2 * `rho^2`))) %&gt;% ggplot(aes(x = `rho^2`, y = lscale)) + geom_hline(yintercept = 0, color = &quot;#FCF9F0&quot;, size = 1/4, linetype = 2) + geom_vline(xintercept = 0, color = &quot;#FCF9F0&quot;, size = 1/4, linetype = 2) + geom_line(color = &quot;#A65141&quot;) + xlab(expression(rho^2)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) p2 &lt;- tibble(lscale = seq(from = 0, to = 11, by = 0.01)) %&gt;% mutate(`rho^2` = 1 / (2 * lscale^2)) %&gt;% ggplot(aes(x = lscale, y = `rho^2`)) + geom_hline(yintercept = 0, color = &quot;#FCF9F0&quot;, size = 1/4, linetype = 2) + geom_vline(xintercept = 0, color = &quot;#FCF9F0&quot;, size = 1/4, linetype = 2) + geom_line(color = &quot;#80A0C7&quot;) + ylab(expression(rho^2)) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 10)) p1 + p2 The two aren’t quite inverses of one another, but the overall pattern is when one is large, the other is small. Now we have a sense of how they compare and how to covert one to the other, let’s see how our posterior for \\(lscale\\) looks when we convert it to the scale of McElreath’s \\(\\rho^2\\). post &lt;- post %&gt;% mutate(rhosq = 1 / (2 * lscale_a_gplat_adjlon2_adj^2)) post %&gt;% mean_hdi(rhosq, .width = .89) %&gt;% mutate_if(is.double, round, digits = 3) ## rhosq .lower .upper .width .point .interval ## 1 0.422 0.009 0.908 0.89 mean hdi This is about a third of the size of the McElreath’s \\(\\rho^2 = 1.31, 89 \\text{% HDI } [0.08, 4.41]\\). The plot deepens. If you look back, you’ll see we used a very different prior for lscale. Here it is: inv_gamma(2.874624, 2.941204). Use get_prior() to discover where that came from. get_prior(data = d, family = poisson(link = &quot;identity&quot;), bf(total_tools ~ exp(a) * population^b / g, a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE), b + g ~ 1, nl = TRUE)) ## prior class coef group resp dpar nlpar bound source ## (flat) b a default ## (flat) b Intercept a (vectorized) ## (flat) lscale a default ## inv_gamma(2.874624, 2.941204) lscale gplat_adjlon2_adj a default ## student_t(3, 0, 15.6) sdgp a default ## student_t(3, 0, 15.6) sdgp gplat_adjlon2_adj a (vectorized) ## (flat) b b default ## (flat) b Intercept b (vectorized) ## (flat) b g default ## (flat) b Intercept g (vectorized) That is, we used the brms default prior for \\(lscale\\). In a GitHub exchange, Bürkner pointed out that brms uses special priors for \\(lscale\\) parameters based on Michael Betancourt’s (2017) vignette, Robust Gaussian processes in Stan. We can use the dinvgamma() function from the well-named invgamma package (Kahle &amp; Stamey, 2017) to get a sense of what that prior looks like. tibble(lscale = seq(from = 0.01, to = 9, by = 0.01)) %&gt;% mutate(density = invgamma::dinvgamma(lscale, 2.874624, 2.941204)) %&gt;% ggplot(aes(x = lscale, y = density)) + geom_area(fill = &quot;#80A0C7&quot;) + annotate(geom = &quot;text&quot;, x = 4.75, y = 0.75, label = &quot;inverse gamma(2.874624, 2.941204)&quot;, color = &quot;#8B9DAF&quot;, family = &quot;Courier&quot;) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(0, 8)) Anyways, let’s make the subplots for our version of Figure 14.11 to get a sense of what this all means. Start with the left panel, the prior predictive distribution for the covariance. # for `slice_sample()` set.seed(14) # wrangle p1 &lt;- prior_samples(b14.8) %&gt;% mutate(iter = 1:n(), etasq = sdgp_a_gplat_adjlon2_adj^2, rhosq = 1 / (2 * lscale_a_1_gplat_adjlon2_adj^2)) %&gt;% slice_sample(n = 100) %&gt;% expand(nesting(iter, etasq, rhosq), x = seq(from = 0, to = 10, by = .05)) %&gt;% mutate(covariance = etasq * exp(-rhosq * x^2)) %&gt;% # plot ggplot(aes(x = x, y = covariance)) + geom_line(aes(group = iter), size = 1/4, alpha = 1/4, color = &quot;#EEDA9D&quot;) + scale_x_continuous(&quot;distance (thousand km)&quot;, expand = c(0, 0), breaks = 0:5 * 2) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 2)) + labs(subtitle = &quot;Gaussian process prior&quot;) Now make the right panel, the posterior distribution. # for `slice_sample()` set.seed(14) # wrangle p2 &lt;- post %&gt;% transmute(iter = 1:n(), etasq = sdgp_a_gplat_adjlon2_adj^2, rhosq = 1 / (2 * lscale_a_gplat_adjlon2_adj^2)) %&gt;% slice_sample(n = 50) %&gt;% expand(nesting(iter, etasq, rhosq), x = seq(from = 0, to = 10, by = .05)) %&gt;% mutate(covariance = etasq * exp(-rhosq * x^2)) %&gt;% # plot ggplot(aes(x = x, y = covariance)) + geom_line(aes(group = iter), size = 1/4, alpha = 1/4, color = &quot;#EEDA9D&quot;) + stat_function(fun = function(x) mean(post$sdgp_a_gplat_adjlon2_adj)^2 * exp(-(1 / (2 * mean(post$lscale_a_gplat_adjlon2_adj)^2)) * x^2), color = &quot;#DCA258&quot;, size = 1) + scale_x_continuous(&quot;distance (thousand km)&quot;, expand = c(0, 0), breaks = 0:5 * 2) + coord_cartesian(xlim = c(0, 10), ylim = c(0, 2)) + labs(subtitle = &quot;Gaussian process posterior&quot;) Combine the two with patchwork. p1 | p2 Though the Gaussian process parameters from our brms parameterization looked different from McElreath’s, they resulted in a similar decline in spatial covariance. Let’s finish this up and “push the parameters back through the function for \\(\\mathbf{K}\\), the covariance matrix” (p. 473). # compute posterior median covariance among societies k &lt;- matrix(0, nrow = 10, ncol = 10) for (i in 1:10) for (j in 1:10) k[i, j] &lt;- median(post$etasq) * exp(-median(post$rhosq) * islandsDistMatrix[i, j]^2) diag(k) &lt;- median(post$etasq) + 0.01 k %&gt;% round(2) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0.17 0.15 0.14 0.00 0.11 0.06 0.01 0.02 0.07 0.00 ## [2,] 0.15 0.17 0.16 0.00 0.11 0.06 0.02 0.03 0.06 0.00 ## [3,] 0.14 0.16 0.17 0.00 0.09 0.08 0.03 0.04 0.05 0.00 ## [4,] 0.00 0.00 0.00 0.17 0.00 0.04 0.09 0.08 0.00 0.00 ## [5,] 0.11 0.11 0.09 0.00 0.17 0.01 0.00 0.00 0.14 0.00 ## [6,] 0.06 0.06 0.08 0.04 0.01 0.17 0.07 0.13 0.00 0.00 ## [7,] 0.01 0.02 0.03 0.09 0.00 0.07 0.17 0.11 0.00 0.00 ## [8,] 0.02 0.03 0.04 0.08 0.00 0.13 0.11 0.17 0.00 0.00 ## [9,] 0.07 0.06 0.05 0.00 0.14 0.00 0.00 0.00 0.17 0.00 ## [10,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 We’ll continue to follow suit and change these to a correlation matrix. # convert to correlation matrix rho &lt;- round(cov2cor(k), 2) # add row/col names for convenience colnames(rho) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;) rownames(rho) &lt;- colnames(rho) rho %&gt;% round(2) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Ml 1.00 0.89 0.85 0.01 0.65 0.35 0.08 0.14 0.41 0 ## Ti 0.89 1.00 0.92 0.01 0.65 0.36 0.13 0.17 0.37 0 ## SC 0.85 0.92 1.00 0.03 0.53 0.47 0.19 0.25 0.27 0 ## Ya 0.01 0.01 0.03 1.00 0.00 0.22 0.53 0.50 0.00 0 ## Fi 0.65 0.65 0.53 0.00 1.00 0.08 0.02 0.02 0.82 0 ## Tr 0.35 0.36 0.47 0.22 0.08 1.00 0.43 0.79 0.02 0 ## Ch 0.08 0.13 0.19 0.53 0.02 0.43 1.00 0.66 0.00 0 ## Mn 0.14 0.17 0.25 0.50 0.02 0.79 0.66 1.00 0.01 0 ## To 0.41 0.37 0.27 0.00 0.82 0.02 0.00 0.01 1.00 0 ## Ha 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1 Here are those correlations in a plot. rho %&gt;% data.frame() %&gt;% mutate(row = d$culture) %&gt;% pivot_longer(-row, values_to = &quot;distance&quot;) %&gt;% mutate(column = factor(name, levels = colnames(d_mat)), row = factor(row, levels = rownames(d_mat)) %&gt;% fct_rev(), label = formatC(distance, format = &#39;f&#39;, digits = 2) %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;)) %&gt;% # omit this line to keep the diagonal of 1&#39;s filter(distance != 1) %&gt;% ggplot(aes(x = column, y = row)) + geom_raster(aes(fill = distance)) + geom_text(aes(label = label), size = 2.75, family = &quot;Courier&quot;, color = &quot;#100F14&quot;) + scale_fill_gradient(expression(rho), low = &quot;#FCF9F0&quot;, high = &quot;#A65141&quot;, limits = c(0, 1)) + scale_x_discrete(NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + theme_pearl_earring(axis.text.y = element_text(hjust = 0)) + theme(axis.ticks = element_blank()) The correlations in our rho matrix look a little higher than those in the text (p. 474). Before we move on to the next plot, let’s consider psize. If you really want to scale the points in Figure 14.12.a like McElreath did, you can make the psize variable in a tidyverse sort of way as follows. However, if you compare the psize method and the default ggplot2 method using just logpop, you’ll see the difference is negligible. In that light, I’m going to be lazy and just use logpop in my plots. d %&gt;% transmute(psize = logpop / max(logpop)) %&gt;% transmute(psize = exp(psize * 1.5) - 2) ## psize ## 1 0.3134090 ## 2 0.4009582 ## 3 0.6663711 ## 4 0.7592196 ## 5 0.9066890 ## 6 0.9339560 ## 7 0.9834797 ## 8 1.1096138 ## 9 1.2223112 ## 10 2.4816891 As far as I can figure, you still have to get rho into a tidy data frame before feeding it into ggplot2. Here’s my attempt at doing so. tidy_rho &lt;- rho %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% bind_cols(d %&gt;% select(culture, logpop, total_tools, lon2, lat)) %&gt;% pivot_longer(Ml:Ha, names_to = &quot;colname&quot;, values_to = &quot;correlation&quot;) %&gt;% mutate(group = str_c(pmin(rowname, colname), pmax(rowname, colname))) %&gt;% select(rowname, colname, group, culture, everything()) head(tidy_rho) ## # A tibble: 6 x 9 ## rowname colname group culture logpop total_tools lon2 lat correlation ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ml Ml MlMl Malekula 7.00 13 -12.5 -16.3 1 ## 2 Ml Ti MlTi Malekula 7.00 13 -12.5 -16.3 0.89 ## 3 Ml SC MlSC Malekula 7.00 13 -12.5 -16.3 0.85 ## 4 Ml Ya MlYa Malekula 7.00 13 -12.5 -16.3 0.01 ## 5 Ml Fi FiMl Malekula 7.00 13 -12.5 -16.3 0.65 ## 6 Ml Tr MlTr Malekula 7.00 13 -12.5 -16.3 0.35 Okay, here’s the code for our version of Figure 14.12.a. library(ggrepel) p1 &lt;- tidy_rho %&gt;% ggplot(aes(x = lon2, y = lat)) + geom_point(data = d, aes(size = logpop), color = &quot;#DCA258&quot;) + geom_line(aes(group = group, alpha = correlation^2), color = &quot;#EEDA9D&quot;) + geom_text_repel(data = d, aes(label = culture), seed = 14, point.padding = .2, size = 2.75, color = &quot;#FCF9F0&quot;, family = &quot;Courier&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(subtitle = &quot;Among societies in geographic space\\n&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;) + coord_cartesian(xlim = range(d$lon2), ylim = range(d$lat)) + theme(legend.position = &quot;none&quot;) Here’s our the code for our version of Figure 14.12.b. # compute the average posterior predictive relationship between # log population and total tools, summarized by the median and 80% interval f &lt;- post %&gt;% expand(logpop = seq(from = 6, to = 14, length.out = 30), nesting(b_a_Intercept, b_b_Intercept, b_g_Intercept)) %&gt;% mutate(population = exp(logpop)) %&gt;% mutate(lambda = exp(b_a_Intercept) * population^b_b_Intercept / b_g_Intercept) %&gt;% group_by(logpop) %&gt;% median_qi(lambda, .width = .8) # plot p2 &lt;- tidy_rho %&gt;% ggplot(aes(x = logpop)) + geom_smooth(data = f, aes(y = lambda, ymin = .lower, ymax = .upper), stat = &quot;identity&quot;, fill = &quot;#394165&quot;, color = &quot;#100F14&quot;, alpha = .5, size = 1.1) + geom_point(data = d, aes(y = total_tools, size = logpop), color = &quot;#DCA258&quot;) + geom_line(aes(y = total_tools, group = group, alpha = correlation^2), color = &quot;#EEDA9D&quot;) + geom_text_repel(data = d, aes(y = total_tools, label = culture), seed = 14, point.padding = .2, size = 2.75, color = &quot;#FCF9F0&quot;, family = &quot;Courier&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(subtitle = &quot;Shown against the relation between\\ntotal tools and log pop&quot;, x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = range(d$logpop), ylim = range(d$total_tools)) + theme(legend.position = &quot;none&quot;) Now we combine them to make the full version of Figure 14.12. p1 + p2 + plot_annotation(title = &quot;Posterior median correlations&quot;) As expressed by the intensity of the colors of those connecting lines, our correlations are a bit more pronounced than those in the text, making for a more densely webbed plot. It is still the case that the correlations among Malekula, Tikopia, and Santa Cruz are the most pronounced. The next two notable correlations are between Manus and Trobriand and between Lau Fiji and Tonga. 14.5.1.0.1 Rethinking: Dispersion by other names. McElreath remarked it might be a good idea to fit an alternative of this model using the gamma-Poisson likelihood. Let’s take him up on the challenge. Remember that gamma-Poisson models are also referred to as negative binomial models. When using brms, you specify this using family = negbinomial. b14.8_nb &lt;- brm(data = d, family = negbinomial(link = &quot;identity&quot;), bf(total_tools ~ exp(a) * population^b / g, a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE), b + g ~ 1, nl = TRUE), prior = c(prior(normal(0, 1), nlpar = a), prior(exponential(1), nlpar = b, lb = 0), prior(exponential(1), nlpar = g, lb = 0), prior(inv_gamma(2.874624, 2.941204), class = lscale, coef = gplat_adjlon2_adj, nlpar = a), prior(exponential(1), class = sdgp, coef = gplat_adjlon2_adj, nlpar = a), # default prior prior(gamma(0.01, 0.01), class = shape)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, control = list(adapt_delta = .9), file = &quot;fits/b14.08_nb&quot;) Check the summary. print(b14.8_nb) ## Family: negbinomial ## Links: mu = identity; shape = identity ## Formula: total_tools ~ exp(a) * population^b/g ## a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE) ## b ~ 1 ## g ~ 1 ## Data: d (Number of observations: 10) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Gaussian Process Terms: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sdgp(a_gplat_adjlon2_adj) 0.34 0.28 0.01 1.07 1.00 1014 1015 ## lscale(a_gplat_adjlon2_adj) 1.69 1.44 0.45 5.00 1.00 2028 2204 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## a_Intercept 0.32 0.83 -1.35 1.93 1.00 2627 2683 ## b_Intercept 0.27 0.09 0.10 0.44 1.00 1523 1015 ## g_Intercept 0.69 0.65 0.06 2.41 1.00 2029 1936 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## shape 48.39 59.19 4.08 221.46 1.00 1420 2955 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). This resulted in a slightly less pronounced correlation matrix. post &lt;- posterior_samples(b14.8_nb) # compute posterior median covariance among societies k &lt;- matrix(0, nrow = 10, ncol = 10) for (i in 1:10) for (j in 1:10) k[i, j] &lt;- median(post$sdgp_a_gplat_adjlon2_adj^2) * exp(-islandsDistMatrix[i, j]^2 / (2 * median(post$lscale_a_gplat_adjlon2_adj)^2)) diag(k) &lt;- median(post$sdgp_a_gplat_adjlon2_adj^2) + 0.01 # convert to correlation matrix rho &lt;- round(cov2cor(k), 2) # add row/col names for convenience colnames(rho) &lt;- c(&quot;Ml&quot;, &quot;Ti&quot;, &quot;SC&quot;, &quot;Ya&quot;, &quot;Fi&quot;, &quot;Tr&quot;, &quot;Ch&quot;, &quot;Mn&quot;, &quot;To&quot;, &quot;Ha&quot;) rownames(rho) &lt;- colnames(rho) rho %&gt;% round(2) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Ml 1.00 0.83 0.80 0.00 0.58 0.28 0.05 0.10 0.34 0 ## Ti 0.83 1.00 0.87 0.01 0.58 0.29 0.09 0.12 0.30 0 ## SC 0.80 0.87 1.00 0.01 0.45 0.39 0.13 0.19 0.21 0 ## Ya 0.00 0.01 0.01 1.00 0.00 0.16 0.45 0.43 0.00 0 ## Fi 0.58 0.58 0.45 0.00 1.00 0.05 0.01 0.01 0.75 0 ## Tr 0.28 0.29 0.39 0.16 0.05 1.00 0.36 0.73 0.01 0 ## Ch 0.05 0.09 0.13 0.45 0.01 0.36 1.00 0.59 0.00 0 ## Mn 0.10 0.12 0.19 0.43 0.01 0.73 0.59 1.00 0.00 0 ## To 0.34 0.30 0.21 0.00 0.75 0.01 0.00 0.00 1.00 0 ## Ha 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1 Like before, we’ll view them in a plot. rho %&gt;% data.frame() %&gt;% mutate(row = d$culture) %&gt;% pivot_longer(-row, values_to = &quot;distance&quot;) %&gt;% mutate(column = factor(name, levels = colnames(d_mat)), row = factor(row, levels = rownames(d_mat)) %&gt;% fct_rev(), label = formatC(distance, format = &#39;f&#39;, digits = 2) %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;)) %&gt;% # omit this line to keep the diagonal of 1&#39;s filter(distance != 1) %&gt;% ggplot(aes(x = column, y = row)) + geom_raster(aes(fill = distance)) + geom_text(aes(label = label), size = 2.75, family = &quot;Courier&quot;, color = &quot;#100F14&quot;) + scale_fill_gradient(expression(rho), low = &quot;#FCF9F0&quot;, high = &quot;#A65141&quot;, limits = c(0, 1)) + scale_x_discrete(NULL, position = &quot;top&quot;, expand = c(0, 0)) + scale_y_discrete(NULL, expand = c(0, 0)) + theme_pearl_earring(axis.text.y = element_text(hjust = 0)) + theme(axis.ticks = element_blank()) On the whole, the correlation medians appear a just little bit lower than from the Poisson model. Now let’s make a gamma-Poisson alternative to Figure 14.12. # tidy up rho tidy_rho &lt;- rho %&gt;% data.frame() %&gt;% rownames_to_column() %&gt;% bind_cols(d %&gt;% select(culture, logpop, total_tools, lon2, lat)) %&gt;% pivot_longer(Ml:Ha, names_to = &quot;colname&quot;, values_to = &quot;correlation&quot;) %&gt;% mutate(group = str_c(pmin(rowname, colname), pmax(rowname, colname))) # left panel p1 &lt;- tidy_rho %&gt;% ggplot(aes(x = lon2, y = lat)) + geom_point(data = d, aes(size = logpop), color = &quot;#DCA258&quot;) + geom_line(aes(group = group, alpha = correlation^2), color = &quot;#EEDA9D&quot;) + geom_text_repel(data = d, aes(label = culture), seed = 14, point.padding = .2, size = 2.75, color = &quot;#FCF9F0&quot;, family = &quot;Courier&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(subtitle = &quot;Among societies in geographic space\\n&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;) + coord_cartesian(xlim = range(d$lon2), ylim = range(d$lat)) + theme(legend.position = &quot;none&quot;) # compute the average posterior predictive relationship between # log population and total tools, summarized by the median and 80% interval f &lt;- post %&gt;% expand(logpop = seq(from = 6, to = 14, length.out = 30), nesting(b_a_Intercept, b_b_Intercept, b_g_Intercept)) %&gt;% mutate(population = exp(logpop)) %&gt;% mutate(lambda = exp(b_a_Intercept) * population^b_b_Intercept / b_g_Intercept) %&gt;% group_by(logpop) %&gt;% median_qi(lambda, .width = .8) # right panel p2 &lt;- tidy_rho %&gt;% ggplot(aes(x = logpop)) + geom_smooth(data = f, aes(y = lambda, ymin = .lower, ymax = .upper), stat = &quot;identity&quot;, fill = &quot;#394165&quot;, color = &quot;#100F14&quot;, alpha = .5, size = 1.1) + geom_point(data = d, aes(y = total_tools, size = logpop), color = &quot;#DCA258&quot;) + geom_line(aes(y = total_tools, group = group, alpha = correlation^2), color = &quot;#EEDA9D&quot;) + geom_text_repel(data = d, aes(y = total_tools, label = culture), seed = 14, point.padding = .2, size = 2.75, color = &quot;#FCF9F0&quot;, family = &quot;Courier&quot;) + scale_alpha_continuous(range = c(0, 1)) + labs(subtitle = &quot;Shown against the relation between\\ntotal tools and log pop&quot;, x = &quot;log population&quot;, y = &quot;total tools&quot;) + coord_cartesian(xlim = range(d$logpop), ylim = range(d$total_tools)) + theme(legend.position = &quot;none&quot;) # combine p1 + p2 + plot_annotation(title = &quot;Posterior median correlations based on the gamma-Poisson model&quot;) There is very little difference. Finish off by comparing the two models with the WAIC. b14.8 &lt;- add_criterion(b14.8, &quot;waic&quot;) b14.8_nb &lt;- add_criterion(b14.8_nb, &quot;waic&quot;) loo_compare(b14.8, b14.8_nb, criterion = &quot;waic&quot;) %&gt;% print(simplify = F) ## elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic se_waic ## b14.8 0.0 0.0 -33.6 1.4 3.8 0.6 67.1 2.7 ## b14.8_nb -3.7 0.6 -37.2 1.8 3.4 0.7 74.5 3.7 The WAIC comparison suggests we gain little by switching to the gamma-Poisson. If anything, we may have overfit. 14.5.2 Example: Phylogenetic distance. Consider as an example the causal influence of group size (\\(G\\)) on brain size (\\(B\\)). Hypotheses connecting these variables are popular, because primates (including humans) are unusual in both. Most primates live in social groups. Most mammals do not. Second, primates have relatively large brains. There is a family of hypotheses linking these two features. Suppose for example that group living, whatever its cause, could select for larger brains, because once you live with others, a larger brain helps to cope with the complexity of cooperation and manipulation. (p. 477) There are also many potential confounds, which we’ll call \\(U\\). Also imagine there are two time points, indicated by subscripts 1 and 2. Here is the basic DAG. dag_coords &lt;- tibble(name = c(&quot;G1&quot;, &quot;B1&quot;, &quot;U1&quot;, &quot;G2&quot;, &quot;B2&quot;, &quot;U2&quot;), x = rep(1:2, each = 3), y = rep(3:1, times = 2)) dagify(G2 ~ G1 + U1, B2 ~ G1 + B1 + U1, U2 ~ U1, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;G1&quot;, &quot;B1&quot;), &quot;a&quot;, ifelse(name %in% c(&quot;G2&quot;, &quot;B2&quot;), &quot;b&quot;, &quot;c&quot;))) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), shape = 21, stroke = 2, fill = &quot;#FCF9F0&quot;, size = 7, show.legend = F) + geom_dag_text(color = &quot;#100F14&quot;, family = &quot;Courier&quot;, parse = T, label = c(expression(B[1]), expression(G[1]), expression(U[1]), expression(B[2]), expression(G[2]), expression(U[2]))) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(&quot;#80A0C7&quot;, &quot;#EEDA9D&quot;, &quot;#A65141&quot;)) + theme_pearl_dag() However, this will not be our model. Rather, we’ll consider this one. dag_coords &lt;- tibble(name = c(&quot;G&quot;, &quot;U&quot;, &quot;M&quot;, &quot;P&quot;, &quot;B&quot;), x = c(0, 1, 1, 2, 2), y = c(3, 1, 2, 1, 3)) dagify(G ~ U + M, U ~ P, M ~ U, B ~ G + M + U, coords = dag_coords) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = name == &quot;U&quot;), shape = 21, stroke = 2, fill = &quot;#FCF9F0&quot;, size = 6, show.legend = F) + geom_dag_text(color = &quot;#100F14&quot;, family = &quot;Courier&quot;, parse = T) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(&quot;#EEDA9D&quot;, &quot;#A65141&quot;)) + theme_pearl_dag() There’s a lot going on here, but we can take it one piece at a time. Again, we’re interested in \\(G \\rightarrow B\\). There is one confound we know for sure, body mass (\\(M\\)). It possibly influences both \\(G\\) and \\(B\\). So we’ll include that in the model. The unobserved confounds \\(U\\) could potentially influence all three variables. Finally, we let the phylogenetic relationships (\\(P\\)) influence \\(U\\). How is \\(P\\) causal? If we traveled back in time and delayed a split between two species, it could influence the expected differences in their traits. So it is really the timing of the split that is causal, not the phylogeny. Of course \\(P\\) may also influence \\(G\\) and \\(B\\) and \\(M\\) directly. But those arrows aren’t our concern right now, so I’ve omitted them for clarity. (p. 478) Phylogenetic regression models, which “use some function of phylogenetic distance to model the covariation among species” (p. 478) attempt to grapple with all this. To see how, load the primates data and its phylogeny (see Street et al., 2017). data(Primates301, package = &quot;rethinking&quot;) data(Primates301_nex) When working within the ggplot2 framework, one can plot a phylogeny with help from the ggtree package (Yu, 2020a, 2020b; Yu et al., 2018, 2017). To my knowledge, it is not currently available on CRAN but you can download it directly from GitHub with devtools::install_github(). Here’s a basic plot for our version of Figure 14.13. # devtools::install_github(&quot;GuangchuangYu/ggtree&quot;) library(ggtree) Primates301_nex %&gt;% ggtree(layout = &quot;circular&quot;, color = &quot;#394165&quot;, size = 1/4) + geom_tiplab(size = 5/3, color = &quot;#100F14&quot;) Let’s format the data. d &lt;- Primates301 %&gt;% mutate(name = as.character(name)) %&gt;% drop_na(group_size, body, brain) %&gt;% mutate(m = log(body) %&gt;% standardize(), b = log(brain) %&gt;% standardize(), g = log(group_size) %&gt;% standardize()) glimpse(d) ## Rows: 151 ## Columns: 19 ## $ name &lt;chr&gt; &quot;Allenopithecus_nigroviridis&quot;, &quot;Alouatta_belzebul&quot;, &quot;Alouatta_caraya&quot;, &quot;Alouatta… ## $ genus &lt;fct&gt; Allenopithecus, Alouatta, Alouatta, Alouatta, Alouatta, Alouatta, Alouatta, Aotu… ## $ species &lt;fct&gt; nigroviridis, belzebul, caraya, guariba, palliata, pigra, seniculus, azarai, tri… ## $ subspecies &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ spp_id &lt;int&gt; 1, 3, 4, 5, 6, 7, 9, 10, 18, 22, 23, 25, 26, 28, 29, 32, 33, 34, 40, 41, 46, 49,… ## $ genus_id &lt;int&gt; 1, 3, 3, 3, 3, 3, 3, 4, 4, 6, 7, 7, 7, 8, 8, 10, 11, 11, 13, 14, 14, 14, 14, 15,… ## $ social_learning &lt;int&gt; 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 17, 5, 0… ## $ research_effort &lt;int&gt; 6, 15, 45, 37, 79, 25, 82, 22, 58, 1, 12, 58, 30, 10, 6, 24, 11, 8, 43, 16, 161,… ## $ brain &lt;dbl&gt; 58.02, 52.84, 52.63, 51.70, 49.88, 51.13, 55.22, 20.67, 16.85, 6.92, 117.02, 105… ## $ body &lt;dbl&gt; 4655, 6395, 5383, 5175, 6250, 8915, 5950, 1205, 989, 309, 8167, 7535, 8280, 1207… ## $ group_size &lt;dbl&gt; 40.00, 7.40, 8.90, 7.40, 13.10, 5.50, 7.90, 4.10, 3.15, 1.00, 14.50, 42.00, 20.0… ## $ gestation &lt;dbl&gt; NA, NA, 185.92, NA, 185.42, 185.92, 189.90, NA, 133.47, 133.74, 138.20, 226.37, … ## $ weaning &lt;dbl&gt; 106.15, NA, 323.16, NA, 495.60, NA, 370.04, 229.69, 76.21, 109.26, NA, 816.35, 8… ## $ longevity &lt;dbl&gt; 276.0, NA, 243.6, NA, 300.0, 240.0, 300.0, NA, 303.6, 156.0, 336.0, 327.6, 453.6… ## $ sex_maturity &lt;dbl&gt; NA, NA, 1276.72, NA, 1578.42, NA, 1690.22, NA, 736.60, 298.91, NA, 2104.57, 2104… ## $ maternal_investment &lt;dbl&gt; NA, NA, 509.08, NA, 681.02, NA, 559.94, NA, 209.68, 243.00, NA, 1042.72, 1033.59… ## $ m &lt;dbl&gt; 0.36958768, 0.57601585, 0.46403740, 0.43842259, 0.56110778, 0.79196303, 0.529133… ## $ b &lt;dbl&gt; 0.4039485, 0.3285765, 0.3253670, 0.3109981, 0.2821147, 0.3020630, 0.3640841, -0.… ## $ g &lt;dbl&gt; 1.397272713, 0.003132082, 0.155626096, 0.003132082, 0.475005322, -0.242029791, 0… Our first model, which we might call the naïve model, explores the conditional relations of \\(\\log(\\text{body mass})\\) and \\(\\log(\\text{group size})\\) on \\(\\log(\\text{brain size})\\) without accounting for phylogenetic relationships. b14.9 &lt;- brm(data = d, family = gaussian, b ~ 1 + m + g, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.09&quot;) Check the summary of the naïve model. print(b14.9) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: b ~ 1 + m + g ## Data: d (Number of observations: 151) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.00 0.02 -0.03 0.03 1.00 3357 2947 ## m 0.89 0.02 0.85 0.94 1.00 3155 3114 ## g 0.12 0.02 0.08 0.17 1.00 3384 2709 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.22 0.01 0.19 0.24 1.00 3724 3029 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you want \\(\\sigma\\) in the \\(\\sigma^2\\) metric, you can square that by hand. posterior_samples(b14.9) %&gt;% mutate(sigma_sq = sigma^2) %&gt;% mean_qi(sigma_sq) %&gt;% mutate_if(is.double, round, digits = 2) ## sigma_sq .lower .upper .width .point .interval ## 1 0.05 0.04 0.06 0.95 mean qi The oldest and most conservative way to include information about phylogenetic relationships is with a Brownian motion model. Brownian motion just means Gaussian random walks. If species traits drift randomly with respect to one another after speciation, then the covariance between a pair of species ends up being linearly related to the phylogenetic branch distance between them–the further apart, the less covariance, as a proportion of distance. (p. 481) We’ll use functions from the ape package (Emmanuel Paradis et al., 2020; E. Paradis &amp; Schliep, 2019) to make the covariance matrix (V) and the distance matrix (Dmat). library(ape) spp_obs &lt;- d$name tree_trimmed &lt;- keep.tip(Primates301_nex, spp_obs) Rbm &lt;- corBrownian(phy = tree_trimmed) V &lt;- vcv(Rbm) Dmat &lt;- cophenetic( tree_trimmed ) Here’s the distance by covariance scatter plot McElreath alluded to but did not show in the text. full_join( Dmat %&gt;% as_tibble(rownames = &quot;row&quot;) %&gt;% pivot_longer(-row, names_to = &quot;col&quot;, values_to = &quot;distance&quot;), V %&gt;% as_tibble(rownames = &quot;row&quot;) %&gt;% pivot_longer(-row, names_to = &quot;col&quot;, values_to = &quot;covariance&quot;), by = c(&quot;row&quot;, &quot;col&quot;) ) %&gt;% ggplot(aes(x = distance, y = covariance)) + geom_point(color = &quot;#80A0C7&quot;, alpha = 1/10) + labs(subtitle = &quot;These variables are the\\ninverse of one another.&quot;, x = &quot;phylogenetic distance&quot;, y = &quot;covariance&quot;) McElreath suggested executing image(V) and image(Dmat) to plot heat maps of each matrix. We’ll have to work a little harder to make decent-looking head maps within our tidyverse workflow. # headmap of Dmat p1 &lt;- Dmat %&gt;% as_tibble(rownames = &quot;row&quot;) %&gt;% pivot_longer(-row, names_to = &quot;col&quot;, values_to = &quot;distance&quot;) %&gt;% ggplot(aes(x = col, y = row, fill = distance)) + geom_tile() + scale_fill_gradient(low = &quot;#100F14&quot;, high = &quot;#EEDA9D&quot;) + scale_x_discrete(NULL, breaks = NULL) + scale_y_discrete(NULL, breaks = NULL) + theme(legend.position = &quot;top&quot;) # headmap of V p2 &lt;- V %&gt;% as_tibble(rownames = &quot;row&quot;) %&gt;% pivot_longer(-row, names_to = &quot;col&quot;, values_to = &quot;covariance&quot;) %&gt;% ggplot(aes(x = col, y = row, fill = covariance)) + geom_tile() + scale_fill_gradient(low = &quot;#100F14&quot;, high = &quot;#EEDA9D&quot;) + scale_x_discrete(NULL, breaks = NULL) + scale_y_discrete(NULL, breaks = NULL) + theme(legend.position = &quot;top&quot;) # combine (p1 | p2) + plot_annotation(subtitle = &quot;Again, distance is the inverse of covariance.&quot;) Within the brms paradigm, one inserts a known covariance matrix into a model using the fcor() function. For any longer-term brms users, fcor() is the replacement for the now-depreciated cor_fixed() function. Along with fcor(), one tells brms about the data with the data2 function. Here’s how to use it for our version of McElreath’s m14.10. R &lt;- V[spp_obs, spp_obs] / max(V) b14.10 &lt;- brm(data = d, data2 = list(R = R), family = gaussian, b ~ 1 + m + g + fcor(R), prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.10&quot;) Check the summary of the Brownian model. print(b14.10) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: b ~ 1 + m + g + fcor(R) ## Data: d (Number of observations: 151) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.19 0.17 -0.52 0.14 1.00 4104 3302 ## m 0.70 0.04 0.63 0.77 1.00 4074 2779 ## g -0.01 0.02 -0.05 0.03 1.00 4314 3222 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.40 0.02 0.36 0.45 1.00 4129 2922 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Since our residual variance is still in the \\(\\sigma\\) metric, it will be easier to compare it to McElreath’s sigma_sq parameter after transforming the posterior samples. posterior_samples(b14.10) %&gt;% transmute(sigma_sq = sigma^2) %&gt;% mean_hdi(sigma_sq, .width = .89) %&gt;% mutate_if(is.double, round, 2) ## sigma_sq .lower .upper .width .point .interval ## 1 0.16 0.13 0.19 0.89 mean hdi McElreath introduced the Ornstein–Uhlenbeck (OU) process as an alternative to the Brownian motion model. The OU model proposes the covariance between two species \\(i\\) and \\(j\\) is \\[K(i, j) = \\eta^2 \\exp(\\rho^2 D_{ij}),\\] where, in our case, \\(D_{ij}\\) is the distance matrix we’ve saved above as Dmat. Sadly for us, brms only supports the exponentiated-quadratic kernel for Gaussian process models, at this time. However, the Ornstein–Uhlenbeck kernel is one of the alternative kernels Bürkner has on his to-do list (see GitHub issue #234). To follow along with McElreath, we will have to use rethinking or raw Stan. Our approach will be the former. First we make the dat_list. dat_list &lt;- list( N_spp = nrow(d), M = standardize(log(d$body)), B = standardize(log(d$brain)), G = standardize(log(d$group_size)), Imat = diag(nrow(d)), V = V[spp_obs, spp_obs], R = V[spp_obs, spp_obs] / max(V[spp_obs, spp_obs]), Dmat = Dmat[spp_obs, spp_obs] / max(Dmat) ) Now fit the OU model with rethinking::ulam(). m14.11 &lt;- ulam( alist( B ~ multi_normal(mu, SIGMA), mu &lt;- a + bM * M + bG * G, matrix[N_spp,N_spp]: SIGMA &lt;- cov_GPL1(Dmat, etasq, rhosq, 0.01), a ~ normal(0, 1), c(bM,bG) ~ normal(0, 0.5), etasq ~ half_normal(1, 0.25), rhosq ~ half_normal(3, 0.25) ), data = dat_list, chains = 4, cores = 4) Happily, our results are much like those in the text. precis(m14.11) ## mean sd 5.5% 94.5% n_eff Rhat4 ## a -0.06685178 0.078105688 -0.19439289 0.05394255 2395.814 1.0009010 ## bG 0.04879575 0.023524685 0.01150523 0.08657337 1842.324 1.0017992 ## bM 0.83489313 0.029181439 0.78727399 0.87972311 1888.968 0.9998068 ## etasq 0.03501903 0.006619957 0.02554830 0.04624305 1927.700 0.9992260 ## rhosq 2.80390498 0.248733524 2.38440028 3.18522999 2060.497 0.9988617 To get a sense of the covariance function implied by etasq and rhosq, here we’ll plot the posterior against the prior for our tidyverse variant of Figure 14.14. post &lt;- extract.samples(m14.11) set.seed(14) left_join( # posterior post %&gt;% data.frame() %&gt;% slice_sample(n = 30) %&gt;% mutate(iter = 1:n()) %&gt;% tidyr::expand(nesting(iter, etasq, rhosq), d_seq = seq(from = 0, to = 1, length.out = 50)), # prior tibble(eta = abs(rnorm(1e5, mean = 1, sd = 0.25)), rho = abs(rnorm(1e5, mean = 3, sd = 0.25))) %&gt;% tidyr::expand(nesting(eta, rho), d_seq = seq(from = 0, to = 1, length.out = 50)) %&gt;% mutate(k = eta * exp(-rho * d_seq)) %&gt;% group_by(d_seq) %&gt;% mean_hdi(k, .width = .89), # join them by = &quot;d_seq&quot;) %&gt;% # plot! ggplot(aes(x = d_seq)) + geom_line(aes(y = etasq * exp(-rhosq * d_seq), group = iter), color = &quot;#80A0C7&quot;, alpha = 1/2) + geom_lineribbon(data = . %&gt;% filter(iter == 1), aes(y = k, ymin = .lower, ymax = .upper), color = &quot;#A65141&quot;, fill = &quot;#E7CDC2&quot;) + annotate(geom = &quot;text&quot;, x = c(0.2, 0.5), y = c(0.1, 0.5), label = c(&quot;posterior&quot;, &quot;prior&quot;), color = c(&quot;#80A0C7&quot;, &quot;#E7CDC2&quot;), family = &quot;Courier&quot;) + labs(x = &quot;phylogenetic distance&quot;, y = &quot;covariance&quot;) + ylim(0, 1.5) There just isn’t a lot of phylogenetic covariance for brain sizes, at least according to this model and these data. As a result, the phylogenetic distance doesn’t completely explain away the association between group size and brain size, as it did in the Brownian motion model. (p. 485) For a thorough discussion on how to fit phylogenetic models with brms, particularly within the multilevel paradigm, see Bürkner’s (2021f) vignette, Estimating phylogenetic multilevel models with brms. 14.6 Summary Bonus: Multilevel growth models and the MELSM To this point in the chapter and most of the text, the data have largely had a cross-sectional feel. In fairness, we did incorporate an element of time with the café example from model b14.1 by looking at the differences between mornings and evenings. However, even then we collapsed across longer time spans, such as days, weeks, months, and so on. One of the two goals of this bonus section to provide a brief introduction multilevel models designed to express change over time. The particular brand of multilevel models we’ll focus on are often called multilevel growth models. Though we will focus on simple linear models, this basic framework can be generalized along many lines. The second goal is to build on our appreciation of covariance structures by introducing a class of multilevel models designed to investigate variation in variation called the mixed-effects location scale models (MELSM). For our final model, we get a little fancy and fit a multivariate MELSM. 14.6.1 Borrow some data. All the models in this bonus section are based on the preprint by Williams, Liu, et al. (2019), Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity. Williams and colleagues’ data and supporting scripts are available in the example_analyses folder from their OSF project at https://osf.io/3bmdh/. You can also download the data from the data folder of this ebook’s GitHub repo. Load the data. dat &lt;- readr::read_csv(&quot;/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/data/m_melsm_dat.csv&quot;) %&gt;% mutate(day01 = (day - 2) / max((day - 2))) glimpse(dat) ## Rows: 13,033 ## Columns: 10 ## $ X1 &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 27, 28… ## $ P_A.std &lt;dbl&gt; 1.74740876, -0.23109384, 0.34155950, 0.45664827, -0.23484069, 1.12785344, 1.11272629, 0.55… ## $ day &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 27, 28… ## $ P_A.lag &lt;dbl&gt; 0.7478597, 1.4674156, -0.3772641, 0.1286055, 0.3292090, 1.4107233, 0.7696644, 0.7304159, 1… ## $ N_A.lag &lt;dbl&gt; 0.25399356, -0.85363386, 0.96144592, -0.19620339, -0.16047347, -0.90365575, -0.77502805, -… ## $ steps.pm &lt;dbl&gt; 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, … ## $ steps.pmd &lt;dbl&gt; 0.5995578, -0.3947168, -1.5193587, -1.3442335, 0.4175970, -0.3231042, 0.3764198, 0.3176650… ## $ record_id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ N_A.std &lt;dbl&gt; -0.73357975, 0.53856559, 0.60161616, 0.27807249, 0.54674641, 0.05660701, -0.08417053, 0.10… ## $ day01 &lt;dbl&gt; 0.00000000, 0.01020408, 0.02040816, 0.03061224, 0.04081633, 0.05102041, 0.06122449, 0.0714… These data are from 193 participants. distinct(dat, record_id) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 193 Participants were asked to complete self-report ratings once a day for a few months. People varied by how many days they participated in the study, with number of days ranging from 8 to 99 and a median of 74. dat %&gt;% count(record_id) %&gt;% summarise(median = median(n), min = min(n), max = max(n)) ## # A tibble: 1 x 3 ## median min max ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 74 8 99 Here is a plot of that distribution. dat %&gt;% count(record_id) %&gt;% ggplot(aes(x = n)) + geom_bar(fill = &quot;#B1934A&quot;) + scale_x_continuous(&quot;number of days&quot;, limits = c(0, NA)) + theme_pearl_earring() Our primary variables of interest were taken from the Positive and Negative Affect Schedule (PANAS, Watson et al., 1988), which is widely used in certain areas of psychology to measure mood or emotion. In this study, participants completed the PANAS once a day by endorsing the extent to which they experienced various positive (e.g., excited, inspired) and negative (e.g., upset, afraid) emotional states. These responses are summed into two composite scores: positive affect (PA) and negative affect (NA). In the current data, the standardized versions of these scores are in the P_A.std and N_A.std columns, respectively. To get a sense of what these look like, here are the daily N_A.std scores from a random sample of 16 participants. set.seed(14) dat %&gt;% nest(data = c(X1, P_A.std, day, P_A.lag, N_A.lag, steps.pm, steps.pmd, N_A.std, day01)) %&gt;% slice_sample(n = 16) %&gt;% unnest(data) %&gt;% ggplot(aes(x = day, y = N_A.lag)) + geom_line(color = &quot;#80A0C7&quot;) + geom_point(color = &quot;#FCF9F0&quot;, size = 1/2) + ylab(&quot;negative affect (standardized)&quot;) + facet_wrap(~ record_id) 14.6.2 Conventional multilevel growth model. In the social sciences, a typical way to analyze data like these is with a multilevel growth model in which participants vary in their intercepts (starting point) and time slopes (change over time). In the sample of the data, above, it looks like most participants have fairly constant levels of NA over time (i.e., near-zero slopes) but some (e.g., #128 and 147) show some evidence of systemic decreases in NA (i.e., negative slopes). There is also some variation in starting points, though most of the participants in this subset of the data seemed to have endorsed relatively low levels of NA both at baseline and throughout the study. We want a model that can capture all these kinds of variation. Eventually, we will fit a model that accounts for both PA and NA. But to keep things simple while we’re warming up, we will restrict our focus to NA. If we let \\(\\text{NA}_{ij}\\) be the standardized NA score for the \\(i\\)th participant on the \\(j\\)th day, our first Bayesian multilevel growth model will follow the form \\[\\begin{align*} \\text{NA}_{ij} &amp; \\sim \\operatorname{Normal}(\\mu_{ij}, \\sigma) \\\\ \\mu_{ij} &amp; = \\beta_0 + \\beta_1 \\text{time}_{ij} + \\color{#A65141}{u_{0i} + u_{1i} \\text{time}_{ij}} \\\\ \\sigma &amp; = \\sigma_\\epsilon \\\\ \\color{#A65141}{\\begin{bmatrix} u_{0i} \\\\ u_{1i} \\end{bmatrix}} &amp; \\color{#A65141}\\sim \\color{#A65141}{\\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf S \\mathbf R \\mathbf S \\end{pmatrix}} \\\\ \\mathbf S &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 \\\\ 0 &amp; \\sigma_1 \\end{bmatrix} \\\\ \\mathbf R &amp; = \\begin{bmatrix} 1 &amp; \\rho_{12} \\\\ \\rho_{21} &amp; 1 \\end{bmatrix} \\\\ \\beta_0 &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\sigma_0 \\text{ and } \\sigma_1 &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_\\epsilon &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf R &amp; \\sim \\operatorname{LKJ}(2), \\end{align*}\\] where \\(\\beta_0\\) is the intercept (i.e., starting point) and \\(u_{0i}\\) captures variations in that intercept across participants. Similarly, \\(\\beta_1\\) is the slope depicting linear change in \\(\\text{NA}\\) across time and \\(u_{1i}\\) captures variations in that linear change across participants. The \\(u_{0i}\\) and \\(u_{1i}\\) parameters are modeled as multivariate normal with zero means (i.e., they are deviations from the population parameters) and standard deviations \\(\\sigma_0\\) and \\(\\sigma_1\\). We express the correlation between those two group-level \\(\\sigma\\) parameters with \\(\\mathbf R\\), the symmetric correlation matrix. Here we just have one correlation, \\(\\rho_{21}\\), which is the same as \\(\\rho_{12}\\). Finally, variation not accounted for by the other parameters is captured by the single parameter \\(\\sigma_\\epsilon\\), which is often just called \\(\\epsilon\\). We have two variables measuring time in these data. The day variable measures time by integers, ranging from 2 to 100. To make it a little easier to set the priors and fit the model with Stan, we have a rescaled version of the variable, day01, which ranges from 0 to 1. In this way, \\(\\beta_0\\) is the value for the first day in the data set and \\(\\beta_1\\) is the expected change by the end of the collection (i.e., the 100th day). For consistency, we are largely following McElreath’s weakly-regularizing approach to priors. You may have noticed my statistical notation differs a bit from McElreath’s, here. It follows a blend of sensibilities from McElreath, Williams, and from the notation I used in my translation of Singer and Willett’s (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence. I hope it’s clear. Here is how to fit our simple multilevel growth model with brms. b14.12 &lt;- brm(data = dat, family = gaussian, N_A.std ~ 1 + day01 + (1 + day01 | record_id), prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sd), prior(exponential(1), class = sigma), prior(lkj(2), class = cor)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.12&quot;) Check the summary. print(b14.12) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: N_A.std ~ 1 + day01 + (1 + day01 | record_id) ## Data: dat (Number of observations: 13033) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~record_id (Number of levels: 193) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.77 0.04 0.70 0.86 1.00 1696 3160 ## sd(day01) 0.65 0.04 0.57 0.75 1.00 3946 5719 ## cor(Intercept,day01) -0.34 0.08 -0.49 -0.18 1.00 3600 4523 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.03 0.06 -0.08 0.14 1.00 821 1781 ## day01 -0.16 0.06 -0.27 -0.05 1.00 2734 4425 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.61 0.00 0.60 0.62 1.00 13276 5479 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Hopefully it makes sense that the population-level intercept (\\(\\beta_0\\)) is near zero. It would be odd if it wasn’t given these are standardized data. The coefficient for day01 (\\(\\beta_1\\)) is mildly negative, suggesting an overall trend for participants to endorse lower NA scores over time. The two group-level \\(\\sigma\\) parameters are fairly large given the scale of the data. They suggest participants varied quite a bit in terms of both intercepts and slopes. They also have a moderate negative correlation, suggesting that participants with higher intercepts tended to have more negative slopes. To get a sense of the model, we’ll plot the posterior means for each participants’ fitted trajectory across time (thin lines), along with the population-average trajectory (thick line). nd &lt;- dat %&gt;% distinct(record_id, day01) fitted(b14.12, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = day01, y = Estimate, group = record_id)) + geom_line(alpha = 1/3, size = 1/3, color = &quot;#8B9DAF&quot;) + geom_segment(x = 0, xend = 1, y = fixef(b14.12)[1, 1], yend = fixef(b14.12)[1, 1] + fixef(b14.12)[2, 1], size = 3, color = &quot;#80A0C7&quot;) + scale_x_continuous(breaks = c(0, .5, 1)) + ylab(&quot;negative affect (standardized)&quot;) If you look back up to the model summary from before the plot, the one parameter we didn’t focus on was the lone sigma parameter at the bottom. That’s our \\(\\sigma_\\epsilon\\), which captures the variation in NA not accounted for by the intercepts, slopes, and their correlation. An important characteristic of the conventional multilevel growth model is that \\(\\sigma_\\epsilon\\) does not vary across persons, occasions, or other variables. To give a sense of why this might not be the best assumption, let’s take a focused look at the model implied trajectories for two participants. Here we will take cues from some Figure 4.8 from way back in Section 4.4.3.5. We will plot the original data atop both the fitted lines and their 95% intervals, which expresses the mean structure, along with the 95% posterior predictive interval, which expresses the uncertainty of the \\(\\sigma_\\epsilon\\) parameter. nd &lt;- dat %&gt;% filter(record_id %in% c(30, 115)) %&gt;% select(record_id, N_A.std, day01) bind_cols( fitted(b14.12, newdata = nd) %&gt;% data.frame(), predict(b14.12, newdata = nd) %&gt;% data.frame() %&gt;% select(Q2.5:Q97.5) %&gt;% set_names(&quot;p_lower&quot;, &quot;p_upper&quot;) ) %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = day01)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;#8B9DAF&quot;, color = &quot;#8B9DAF&quot;, alpha = 1/2, size = 1/2) + geom_ribbon(aes(ymin = p_lower, ymax = p_upper), fill = &quot;#8B9DAF&quot;, alpha = 1/2) + geom_point(aes(y = N_A.std), color = &quot;#8B9DAF&quot;) + scale_x_continuous(breaks = c(0, .5, 1)) + ylab(&quot;negative affect (standardized)&quot;) + facet_wrap(~ record_id) Because of our fixed \\(\\sigma_\\epsilon\\) parameter, the 95% posterior predictive interval is the same width for both participants. Yet look how closely participant the data points for participant 30 cluster not only within the center region of the posterior prediction intervals, but also almost completely within the 95% interval for the fitted line. In contrast, notice how much more spread out the data points for participant 115 are, and how many of them extend well beyond the posterior predictive interval. This difference in variability is ignored by conventional growth models. However, there’s no reason we can’t adjust our model to capture this kind of person-level variability, too. Enter the MELSM. 14.6.3 Learn more about your data with the MELSM. Mixed-effects location scale models (MELSMs) have their origins in the work of Donald Hedeker and colleagues (Hedeker et al., 2008, 2012). Rast et al. (2012) showcased an early application of the framework to the BUGS/JAGS software. More recently Philippe Rast and colleagues (particularly graduate student, Donald Williams) have adapted this approach for use within the Stan/brms ecosystem (Williams, Liu, et al., 2019; Williams, Rouder, et al., 2019; Williams, Martin, et al., 2019; Williams, Zimprich, et al., 2019). Within the brms framework, MELSMs apply a distributional modeling approach (see Bürkner, 2021b) to the multilevel growth model. Not only are parameters from the mean structure allowed to vary across groups, but parameters applied to \\(\\sigma\\) are allowed to vary across groups, too. Do you remember the little practice model b10.1 from Section 10.2.2? We simulated Gaussian data for two groups with the same mean parameter but different parameters for \\(\\sigma\\). If you check back in that section, you’ll see that the brms default was to model \\(\\log \\sigma\\) in that case. This is smart because when you define a model for \\(\\sigma\\), you want to use a link function that ensures the predictions will be stay at zero and above. The MELSM approach of Hedeker, Rast, Williams and friends applies this logic to \\(\\sigma_\\epsilon\\) in multilevel growth models. However, not only can \\(\\sigma_\\epsilon\\) vary across groups in a fixed-effects sort of way, we can use multilevel partial pooling, too. To get a sense of what this looks like, we’ll augment our previous model, but this time allowing \\(\\sigma_\\epsilon\\) to vary across participants. You might express the updated statistical model as \\[\\begin{align*} \\text{NA}_{ij} &amp; \\sim \\operatorname{Normal}(\\mu_{ij}, \\color{#A65141}{\\sigma_{i}})\\\\ \\mu_{ij} &amp; = \\beta_0 + \\beta_1 \\text{time}_{ij} + u_{0i} + u_{1i} \\text{time}_{ij} \\\\ \\color{#A65141}{\\log (\\sigma_i )} &amp; \\color{#A65141}= \\color{#A65141}{\\eta_0 + u_{2i}} \\\\ \\begin{bmatrix} u_{0i} \\\\ u_{1i} \\\\ \\color{#A65141}{u_{2i}} \\end{bmatrix} &amp; \\sim \\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf S \\mathbf R \\mathbf S \\end{pmatrix} \\\\ \\mathbf S &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_1 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_2 \\end{bmatrix} \\\\ \\mathbf R &amp; = \\begin{bmatrix} 1 &amp; \\rho_{12} &amp; \\rho_{13} \\\\ \\rho_{21} &amp; 1 &amp; \\rho_{23} \\\\ \\rho_{31} &amp; \\rho_{32} &amp; 1 \\end{bmatrix} \\\\ \\beta_0 &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1 \\text{and } \\eta_0 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\sigma_0,\\dots, \\sigma_2 &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf R &amp; \\sim \\operatorname{LKJ}(2). \\end{align*}\\] In the opening likelihood statement from the prior model, we simply set \\(\\text{NA}_{ij} \\sim \\operatorname{Normal}\\begin{pmatrix} \\mu_{ij}, \\sigma \\end{pmatrix}\\). For our first MELSM, we now refer to \\(\\sigma_i\\), meaning the levels of variation not accounted for by the mean structure can vary across participants (hence the \\(i\\) subscript). Two lines down, we see the formula for \\(\\log \\begin{pmatrix} \\sigma_i \\end{pmatrix}\\) contains population-level intercept, \\(\\eta_0\\), and participant-specific deviations around that parameter, \\(u_{2i}\\). In the next three lines, the plot deepens. We see that all three participant-level deviations, \\(u_{0i},\\dots,u_{2i}\\) are multivariate normal with means set to zero and variation expressed in the parameters \\(\\sigma_0,\\dots,\\sigma_2\\) of the \\(\\mathbf S\\) matrix. In the \\(\\mathbf R\\) matrix, we now have three correlation parameters, with \\(\\rho_{31}\\) and \\(\\rho_{32}\\) allowing us to assess the correlations among individual differences in variability and individual differences in starting points and change over time, respectively. Let’s fit the model. b14.13 &lt;- brm(data = dat, family = gaussian, bf(N_A.std ~ 1 + day01 + (1 + day01 |i| record_id), sigma ~ 1 + (1 |i| record_id)), prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sd), prior(normal(0, 1), class = Intercept, dpar = sigma), prior(exponential(1), class = sd, dpar = sigma), prior(lkj(2), class = cor)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.13&quot;) We should note a few things about the brm() syntax. First, because we modeled both \\(\\mu_{ij}\\) and \\(\\sigma_i\\), we nested both model formulas within the bf() function. Second, because the brms default is to use the log link when modeling \\(\\sigma_i\\), there was no need to explicitly set it that way in the family line. However, we could have if we wanted to. Third, notice our use of the |i| syntax within the parentheses in the formula lines. If we had used the conventional | syntax, that would have not allowed our \\(u_{2i}\\) parameters to correlate with \\(u_{0i}\\) and \\(u_{1i}\\) from the mean structure. It would have effectively set \\(\\rho_{31} = \\rho_{32} = 0\\). Finally, notice how within the prior() functions, we explicitly referred to those for the new \\(\\sigma\\) structure with the dpar = sigma operator. Okay, time to check the model summary. print(b14.13) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: N_A.std ~ 1 + day01 + (1 + day01 | i | record_id) ## sigma ~ 1 + (1 | i | record_id) ## Data: dat (Number of observations: 13033) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~record_id (Number of levels: 193) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.76 0.04 0.69 0.84 1.00 565 1406 ## sd(day01) 0.60 0.04 0.53 0.69 1.00 1356 3345 ## sd(sigma_Intercept) 0.69 0.04 0.63 0.77 1.00 678 1284 ## cor(Intercept,day01) -0.34 0.08 -0.49 -0.17 1.01 771 1919 ## cor(Intercept,sigma_Intercept) 0.61 0.05 0.51 0.70 1.00 695 1680 ## cor(day01,sigma_Intercept) -0.10 0.08 -0.26 0.05 1.00 569 1110 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.04 0.05 -0.07 0.14 1.01 193 274 ## sigma_Intercept -0.78 0.05 -0.88 -0.68 1.01 285 424 ## day01 -0.16 0.05 -0.26 -0.06 1.00 596 1071 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The ‘sigma_Intercept’ lines in the ‘Population-Level Effects’ section is the summary for our \\(\\eta_0\\) parameter. To get a sense of what this means out of the log space, just exponentiate. fixef(b14.13)[&quot;sigma_Intercept&quot;, c(1, 3:4)] %&gt;% exp() ## Estimate Q2.5 Q97.5 ## 0.4582024 0.4131828 0.5043478 To get a sense of the variation in that parameter across participants [i.e., \\(\\exp(\\eta_0 + u_{2i})\\)], it’s best to plot. coef(b14.13)$record_id[, , &quot;sigma_Intercept&quot;] %&gt;% exp() %&gt;% data.frame() %&gt;% arrange(Estimate) %&gt;% mutate(rank = 1:n()) %&gt;% ggplot(aes(x = rank, y = Estimate, ymin = Q2.5, ymax = Q97.5)) + geom_pointrange(size = .4, fatten = .3, color = &quot;#EEDA9D&quot;) + scale_x_continuous(&quot;participants ranked by posterior mean&quot;, breaks = NULL) + ylab(expression(exp(eta[0]+italic(u)[2][italic(i)]))) Looks like there’s a lot of variation in a parameter that was formerly fixed across participants as a single value \\(\\sigma_\\epsilon\\). Here’s what this looks like in terms of our posterior predictive distributions for participants 30 and 115, from before. nd &lt;- dat %&gt;% filter(record_id %in% c(30, 115)) %&gt;% select(record_id, N_A.std, day01) bind_cols( fitted(b14.13, newdata = nd) %&gt;% data.frame(), predict(b14.13, newdata = nd) %&gt;% data.frame() %&gt;% select(Q2.5:Q97.5) %&gt;% set_names(&quot;p_lower&quot;, &quot;p_upper&quot;) ) %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = day01)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;#8B9DAF&quot;, color = &quot;#8B9DAF&quot;, alpha = 1/2, size = 1/2) + geom_ribbon(aes(ymin = p_lower, ymax = p_upper), fill = &quot;#8B9DAF&quot;, alpha = 1/2) + geom_point(aes(y = N_A.std), color = &quot;#8B9DAF&quot;) + scale_x_continuous(breaks = c(0, .5, 1)) + ylab(&quot;negative affect (standardized)&quot;) + facet_wrap(~ record_id) That’s a big improvement. Let’s expand our skill set. Within the MELSM paradigm, one can use multiple variables to model participant-specific variation. Here we’ll add in our time variable. \\[\\begin{align*} \\text{NA}_{ij} &amp; \\sim \\operatorname{Normal}(\\mu_{ij}, \\color{#A65141}{\\sigma_{ij}}) \\\\ \\mu_{ij} &amp; = \\beta_0 + \\beta_1 \\text{time}_{ij} + u_{0i} + u_{1i} \\text{time}_{ij} \\\\ \\log(\\sigma_{i\\color{#A65141}j}) &amp; = \\eta_0 + \\color{#A65141}{\\eta_1 \\text{time}_{ij}} + u_{2i} + \\color{#A65141}{u_{3i} \\text{time}_{ij}} \\\\ \\begin{bmatrix} u_{0i} \\\\ u_{1i} \\\\ u_{2i} \\\\ \\color{#A65141}{u_{3i}} \\end{bmatrix} &amp; \\sim \\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf S \\mathbf R \\mathbf S \\end{pmatrix} \\\\ \\mathbf S &amp; = \\begin{bmatrix} \\sigma_0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_3 \\end{bmatrix} \\\\ \\mathbf R &amp; = \\begin{bmatrix} 1 &amp; \\rho_{12} &amp; \\rho_{13} &amp; \\rho_{14} \\\\ \\rho_{21} &amp; 1 &amp; \\rho_{23} &amp; \\rho_{24} \\\\ \\rho_{31} &amp; \\rho_{32} &amp; 1 &amp; \\rho_{34} \\\\ \\rho_{41} &amp; \\rho_{42} &amp; \\rho_{43} &amp; 1 \\end{bmatrix} \\\\ \\beta_0 &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1, \\eta_0, \\text{and } \\eta_1 &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\sigma_0,\\dots, \\sigma_3 &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf R &amp; \\sim \\operatorname{LKJ}(2). \\end{align*}\\] Note how in the very first line we are now speaking in terms of \\(\\sigma_{ij}\\). Variation in the criterion \\(\\text{NA}_{ij}\\) not accounted for by the mean structure can now vary across participants, \\(i\\), and time, \\(j\\). This results in four \\(u_{xi}\\) terms, a \\(4 \\times 4\\) \\(\\mathbf S\\) matrix and a \\(4 \\times 4\\) \\(\\mathbf R\\) matrix. Here’s how you might fit the model with brms::brm(). Warning: it’ll probably take a couple hours. b14.14 &lt;- brm(data = dat, family = gaussian, bf(N_A.std ~ 1 + day01 + (1 + day01 |i| record_id), sigma ~ 1 + day01 + (1 + day01 |i| record_id)), prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 1), class = b), prior(exponential(1), class = sd), prior(normal(0, 1), class = Intercept, dpar = sigma), prior(normal(0, 1), class = b, dpar = sigma), prior(exponential(1), class = sd, dpar = sigma), prior(lkj(2), class = cor)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.14&quot;) At this point, print() is starting to return a lot of output. print(b14.14) ## Family: gaussian ## Links: mu = identity; sigma = log ## Formula: N_A.std ~ 1 + day01 + (1 + day01 | i | record_id) ## sigma ~ 1 + day01 + (1 + day01 | i | record_id) ## Data: dat (Number of observations: 13033) ## Samples: 4 chains, each with iter = 3000; warmup = 1000; thin = 1; ## total post-warmup samples = 8000 ## ## Group-Level Effects: ## ~record_id (Number of levels: 193) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.76 0.04 0.68 0.84 1.00 1172 2283 ## sd(day01) 0.60 0.04 0.52 0.69 1.00 2742 4613 ## sd(sigma_Intercept) 0.70 0.04 0.63 0.78 1.00 2057 3413 ## sd(sigma_day01) 0.36 0.04 0.29 0.44 1.00 4530 6285 ## cor(Intercept,day01) -0.31 0.08 -0.46 -0.14 1.00 1528 2403 ## cor(Intercept,sigma_Intercept) 0.64 0.04 0.55 0.72 1.00 2086 3935 ## cor(day01,sigma_Intercept) -0.20 0.08 -0.35 -0.05 1.00 1651 3773 ## cor(Intercept,sigma_day01) -0.16 0.10 -0.36 0.04 1.00 5208 6235 ## cor(day01,sigma_day01) 0.61 0.09 0.42 0.77 1.00 4316 5267 ## cor(sigma_Intercept,sigma_day01) -0.15 0.10 -0.34 0.04 1.00 5113 5610 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.03 0.05 -0.08 0.13 1.01 464 846 ## sigma_Intercept -0.75 0.05 -0.86 -0.65 1.00 864 1588 ## day01 -0.15 0.05 -0.26 -0.05 1.00 1274 2704 ## sigma_day01 -0.11 0.04 -0.18 -0.03 1.00 4463 6121 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our new line for ‘sigma_day01’ suggests there is a general trend for less variation in negative affect ratings over time. However, the ‘sd(sigma_day01)’ line in the ‘Group-Level Effects’ section indicates even this varies a bit across participants. At this point, a lot of the action is now in the estimates for the \\(\\mathbf R\\) matrix. Here that is in a coefficient plot. posterior_summary(b14.14) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;par&quot;) %&gt;% filter(str_detect(par, &quot;cor_&quot;)) %&gt;% mutate(rho = str_c(&quot;(rho[&quot;, c(21, 31, 32, 41, 42, 43), &quot;])&quot;)) %&gt;% mutate(par = str_c(&quot;&#39;&quot;, par, &quot;&#39;~&quot;, rho)) %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = par)) + geom_vline(xintercept = c(-.5, 0, .5), linetype = c(2, 1, 2), size = c(1/4, 1/2, 1/4), color = &quot;#FCF9F0&quot;, alpha = 1/4) + geom_pointrange(color = &quot;#B1934A&quot;) + scale_y_discrete(labels = ggplot2:::parse_safe) + xlim(-1, 1) + labs(x = &quot;marginal posterior&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0, size = 9), axis.ticks.y = element_blank()) Note how we attached the statistical terms from the lower triangle of the \\(\\mathbf R\\) matrix to the names from the brms output. Coefficient plots like this are somewhat helpful with MELSM parameter summaries, like this. But they leave something to be desired and they won’t scale well. One alternative is to present the posterior means in a correlation matrix plot. Our first step to prepare for the plot is to extract and wrangle the posterior summaries. levels &lt;- c(&quot;beta[0]&quot;, &quot;beta[1]&quot;, &quot;eta[0]&quot;, &quot;eta[1]&quot;) rho &lt;- posterior_summary(b14.14) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;param&quot;) %&gt;% filter(str_detect(param, &quot;cor_&quot;)) %&gt;% mutate(param = str_remove(param, &quot;cor_record_id__&quot;)) %&gt;% separate(param, into = c(&quot;left&quot;, &quot;right&quot;), sep = &quot;__&quot;) %&gt;% mutate( left = case_when( left == &quot;Intercept&quot; ~ &quot;beta[0]&quot;, left == &quot;day01&quot; ~ &quot;beta[1]&quot;, left == &quot;sigma_Intercept&quot; ~ &quot;eta[0]&quot;), right = case_when( right == &quot;day01&quot; ~ &quot;beta[1]&quot;, right == &quot;sigma_Intercept&quot; ~ &quot;eta[0]&quot;, right == &quot;sigma_day01&quot; ~ &quot;eta[1]&quot; ) ) %&gt;% mutate(label = formatC(Estimate, digits = 2, format = &quot;f&quot;) %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;)) %&gt;% mutate(left = factor(left, levels = levels), right = factor(right, levels = levels)) %&gt;% mutate(right = fct_rev(right)) rho ## left right Estimate Est.Error Q2.5 Q97.5 label ## 1 beta[0] beta[1] -0.3063648 0.08118774 -0.4582187 -0.14055804 -.31 ## 2 beta[0] eta[0] 0.6412301 0.04455636 0.5510102 0.72278563 .64 ## 3 beta[1] eta[0] -0.2031335 0.07756338 -0.3525546 -0.05044922 -.20 ## 4 beta[0] eta[1] -0.1627990 0.10257013 -0.3585379 0.03910610 -.16 ## 5 beta[1] eta[1] 0.6093783 0.08922352 0.4216215 0.76836175 .61 ## 6 eta[0] eta[1] -0.1532045 0.09556994 -0.3376225 0.03735320 -.15 Note how instead of naming the correlations in terms of \\(\\rho_{xx}\\), we are now referring to them as the correlations of the deviations among the population parameters, \\(\\beta_0\\) through \\(\\eta_1\\). I’m hoping this will make sense in the plot. Here it is. rho %&gt;% ggplot(aes(x = left, y = right)) + geom_tile(aes(fill = Estimate)) + geom_text(aes(label = label), family = &quot;Courier&quot;, size = 3) + scale_fill_gradient2(expression(rho), low = &quot;#59708b&quot;, mid = &quot;#FCF9F0&quot;, high = &quot;#A65141&quot;, midpoint = 0, labels = c(-1, &quot;&quot;, 0, &quot;&quot;, 1), limits = c(-1, 1)) + scale_x_discrete(NULL, drop = F, labels = ggplot2:::parse_safe, position = &quot;top&quot;) + scale_y_discrete(NULL, drop = F, labels = ggplot2:::parse_safe) + ggtitle(expression(&quot;The lower triangle for &quot;*bold(R)), subtitle = &quot;Note, each cell is summarized by\\nits posterior mean.&quot;) + theme(axis.text = element_text(size = 12), axis.ticks = element_blank(), legend.text = element_text(hjust = 1)) Interestingly, the strongest two associations involve variation around our \\(\\eta\\) parameters. The posterior mean for \\(\\rho_{31}\\) indicates participants with higher baseline levels of \\(\\text{NA}_{ij}\\) tend to vary more in their responses, particularly in the beginning. The posterior mean for \\(\\rho_{42}\\) indicates participants who show greater increases in their \\(\\text{NA}_{ij}\\) responses over time also tend to show greater relative increases in variation in those responses. You can get a little bit of a sense for this by returning once again to our participants 30 and 115. nd &lt;- dat %&gt;% filter(record_id %in% c(30, 115)) %&gt;% select(record_id, N_A.std, day01) bind_cols( fitted(b14.14, newdata = nd) %&gt;% data.frame(), predict(b14.14, newdata = nd) %&gt;% data.frame() %&gt;% select(Q2.5:Q97.5) %&gt;% set_names(&quot;p_lower&quot;, &quot;p_upper&quot;) ) %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = day01)) + geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;#8B9DAF&quot;, color = &quot;#8B9DAF&quot;, alpha = 1/2, size = 1/2) + geom_ribbon(aes(ymin = p_lower, ymax = p_upper), fill = &quot;#8B9DAF&quot;, alpha = 1/2) + geom_point(aes(y = N_A.std), color = &quot;#8B9DAF&quot;) + scale_x_continuous(breaks = c(0, .5, 1)) + ylab(&quot;negative affect (standardized)&quot;) + facet_wrap(~ record_id) With respect to \\(\\rho_{31}\\), participant 115 showed both a higher intercept and higher level of variability toward the beginning of the study than participant 30 did. The meaning of \\(\\rho_{42}\\) is less clear, with these two. But at least you can get a sense of why you might want to include a \\(\\eta_1\\) parameter to allow response variability to change over time, and why you might want to allow that parameter to vary across participants. Whereas response variability increased quite a bit for participant 115 over time, it stayed about the same for participant 30, perhaps even decreasing a bit. 14.6.4 Time to go multivariate. For our final stage in this progression, we will fit what Williams, Liu, et al. (2019) called a M-MELSM, a multivariate mixed-effects location scale model. Recall these data have measures of both negative and positive affect. The standardized values for PA are waiting for us in the P_A.std column. Within the brms framework, this is a combination of sensibilities from Bürkner’s vignettes on distributional models (Bürkner, 2021b) and multivariate models (Bürkner, 2021d). We might express the statistical model as \\[\\begin{align*} \\color{#A65141}{\\begin{bmatrix} \\text{NA}_{ij} \\\\ \\text{PA}_{ij} \\end{bmatrix}} &amp; \\color{#A65141}\\sim \\color{#A65141}{\\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} \\mu_{ij}^\\text{NA} \\\\ \\mu_{ij}^\\text{PA} \\end{bmatrix}, \\mathbf \\Sigma \\end{pmatrix}} \\\\ \\mu_{ij}^{\\color{#A65141}{\\text{NA}}} &amp; = \\beta_0^\\text{NA} + \\beta_1^\\text{NA} \\text{time}_{ij} + u_{0i}^\\text{NA} + u_{1i}^\\text{NA} \\\\ \\mu_{ij}^{\\color{#A65141}{\\text{PA}}} &amp; = \\beta_0^\\text{PA} + \\beta_1^\\text{PA} \\text{time}_{ij} + u_{0i}^\\text{PA} + u_{1i}^\\text{PA} \\\\ \\log \\left ( \\sigma_{ij}^{\\color{#A65141}{\\text{NA}}} \\right ) &amp; = \\eta_0^\\text{NA} + \\eta_1^\\text{NA} \\text{time}_{ij} + u_{2i}^\\text{NA} + u_{3i}^\\text{NA} \\\\ \\log \\left ( \\sigma_{ij}^{\\color{#A65141}{\\text{PA}}} \\right ) &amp; = \\eta_0^\\text{PA} + \\eta_1^\\text{PA} \\text{time}_{ij} + u_{2i}^\\text{PA} + u_{3i}^\\text{PA} \\\\ \\begin{bmatrix} u_{0i}^\\text{NA}, u_{1i}^\\text{NA}, u_{2i}^\\text{NA}, u_{3i}^\\text{NA}, u_{0i}^\\text{PA}, u_{1i}^\\text{PA}, u_{2i}^\\text{PA}, u_{3i}^\\text{PA} \\end{bmatrix}&#39; &amp; \\sim \\operatorname{MVNormal}(\\mathbf 0, \\mathbf S \\mathbf R \\mathbf S) \\\\ \\mathbf S &amp; = \\begin{bmatrix} \\sigma_0^\\text{NA} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\sigma_1^\\text{NA} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma_2^\\text{NA} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma_3^\\text{NA} &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_0^\\text{PA} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_1^\\text{PA} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_2^\\text{PA} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma_3^\\text{PA} \\end{bmatrix} \\\\ \\mathbf R &amp; = \\begin{bmatrix} 1 &amp; \\rho_{12} &amp; \\rho_{13} &amp; \\rho_{14} &amp; \\rho_{15} &amp; \\rho_{16} &amp; \\rho_{17} &amp; \\rho_{18} \\\\ \\rho_{21} &amp; 1 &amp; \\rho_{23} &amp; \\rho_{24} &amp; \\rho_{25} &amp; \\rho_{26} &amp; \\rho_{27} &amp; \\rho_{28} \\\\ \\rho_{31} &amp; \\rho_{32} &amp; 1 &amp; \\rho_{34} &amp; \\rho_{35} &amp; \\rho_{36} &amp; \\rho_{37} &amp; \\rho_{38} \\\\ \\rho_{41} &amp; \\rho_{42} &amp; \\rho_{43} &amp; 1 &amp; \\rho_{45} &amp; \\rho_{46} &amp; \\rho_{47} &amp; \\rho_{48} \\\\ \\rho_{51} &amp; \\rho_{52} &amp; \\rho_{53} &amp; \\rho_{54} &amp; 1 &amp; \\rho_{56} &amp; \\rho_{57} &amp; \\rho_{58} \\\\ \\rho_{61} &amp; \\rho_{62} &amp; \\rho_{63} &amp; \\rho_{64} &amp; \\rho_{65} &amp; 1 &amp; \\rho_{67} &amp; \\rho_{68} \\\\ \\rho_{71} &amp; \\rho_{72} &amp; \\rho_{73} &amp; \\rho_{74} &amp; \\rho_{75} &amp; \\rho_{76} &amp; 1 &amp; \\rho_{78} \\\\ \\rho_{81} &amp; \\rho_{82} &amp; \\rho_{83} &amp; \\rho_{84} &amp; \\rho_{85} &amp; \\rho_{86} &amp; \\rho_{87} &amp; 1 \\end{bmatrix} \\\\ \\beta_0^\\text{NA} \\text{ and } \\beta_0^\\text{PA} &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1^\\text{NA} \\text{ and } \\beta_1^\\text{PA} &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\eta_0^\\text{NA},\\dots, \\eta_1^\\text{PA} &amp; \\sim \\operatorname{Normal}(0, 1) \\\\ \\sigma_0^\\text{NA},\\dots, \\sigma_1^\\text{PA} &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf R &amp; \\sim \\operatorname{LKJ}(2), \\end{align*}\\] where the \\(\\text{NA}\\) and \\(\\text{PA}\\) superscripts indicate which variable is connected with which parameter. This is a straight multivariate generalization from the previous model, b14.14. Now we have eight parameters varying across participants, resulting in an \\(8 \\times 8\\) \\(\\mathbf S\\) matrix and an \\(8 \\times 8\\) \\(\\mathbf R\\) matrix. Here’s the brms::brm() code. b14.15 &lt;- brm(data = dat, family = gaussian, bf(mvbind(N_A.std, P_A.std) ~ 1 + day01 + (1 + day01 |i| record_id), sigma ~ 1 + day01 + (1 + day01 |i| record_id)) + set_rescor(rescor = FALSE), prior = c(prior(normal(0, 0.2), class = Intercept, resp = NAstd), prior(normal(0, 1), class = b, resp = NAstd), prior(exponential(1), class = sd, resp = NAstd), prior(normal(0, 1), class = Intercept, dpar = sigma, resp = NAstd), prior(normal(0, 1), class = b, dpar = sigma, resp = NAstd), prior(exponential(1), class = sd, dpar = sigma, resp = NAstd), prior(normal(0, 0.2), class = Intercept, resp = PAstd), prior(normal(0, 1), class = b, resp = PAstd), prior(exponential(1), class = sd, resp = PAstd), prior(normal(0, 1), class = Intercept, dpar = sigma, resp = PAstd), prior(normal(0, 1), class = b, dpar = sigma, resp = PAstd), prior(exponential(1), class = sd, dpar = sigma, resp = PAstd), prior(lkj(2), class = cor)), iter = 3000, warmup = 1000, chains = 4, cores = 4, seed = 14, file = &quot;fits/b14.15&quot;) Note how we used the resp argument to indicate which priors went with which criterion variables. For the sake of space, I’ll skip showing the print() output. By all means, check that summary out if you fit this model on your own. Though there may be some substantive insights to glean from looking at the population-level parameters and the hierarchical \\(\\sigma\\)’s, I’d argue the main action is in the \\(\\mathbf R\\) matrix. This time we’ll jump straight to showcasing their posterior means in a correlation matrix plot. First we wrangle. levels &lt;- c(&quot;beta[0]^&#39;NA&#39;&quot;, &quot;beta[1]^&#39;NA&#39;&quot;, &quot;eta[0]^&#39;NA&#39;&quot;, &quot;eta[1]^&#39;NA&#39;&quot;, &quot;beta[0]^&#39;PA&#39;&quot;, &quot;beta[1]^&#39;PA&#39;&quot;, &quot;eta[0]^&#39;PA&#39;&quot;, &quot;eta[1]^&#39;PA&#39;&quot;) # two different options for ordering the parameters # levels &lt;- c(&quot;beta[0]^&#39;NA&#39;&quot;, &quot;beta[1]^&#39;NA&#39;&quot;, &quot;beta[0]^&#39;PA&#39;&quot;, &quot;beta[1]^&#39;PA&#39;&quot;, &quot;eta[0]^&#39;NA&#39;&quot;, &quot;eta[1]^&#39;NA&#39;&quot;, &quot;eta[0]^&#39;PA&#39;&quot;, &quot;eta[1]^&#39;PA&#39;&quot;) # levels &lt;- c(&quot;beta[0]^&#39;NA&#39;&quot;, &quot;beta[0]^&#39;PA&#39;&quot;, &quot;beta[1]^&#39;NA&#39;&quot;, &quot;beta[1]^&#39;PA&#39;&quot;,&quot;eta[0]^&#39;NA&#39;&quot;, &quot;eta[0]^&#39;PA&#39;&quot;, &quot;eta[1]^&#39;NA&#39;&quot;, &quot;eta[1]^&#39;PA&#39;&quot;) rho &lt;- posterior_summary(b14.15) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;param&quot;) %&gt;% filter(str_detect(param, &quot;cor_&quot;)) %&gt;% mutate(param = str_remove(param, &quot;cor_record_id__&quot;)) %&gt;% separate(param, into = c(&quot;left&quot;, &quot;right&quot;), sep = &quot;__&quot;) %&gt;% mutate( left = case_when( left == &quot;NAstd_Intercept&quot; ~ &quot;beta[0]^&#39;NA&#39;&quot;, left == &quot;NAstd_day01&quot; ~ &quot;beta[1]^&#39;NA&#39;&quot;, left == &quot;sigma_NAstd_Intercept&quot; ~ &quot;eta[0]^&#39;NA&#39;&quot;, left == &quot;sigma_NAstd_day01&quot; ~ &quot;eta[1]^&#39;NA&#39;&quot;, left == &quot;PAstd_Intercept&quot; ~ &quot;beta[0]^&#39;PA&#39;&quot;, left == &quot;PAstd_day01&quot; ~ &quot;beta[1]^&#39;PA&#39;&quot;, left == &quot;sigma_PAstd_Intercept&quot; ~ &quot;eta[0]^&#39;PA&#39;&quot;, left == &quot;sigma_PAstd_day01&quot; ~ &quot;eta[1]^&#39;PA&#39;&quot; ), right = case_when( right == &quot;NAstd_Intercept&quot; ~ &quot;beta[0]^&#39;NA&#39;&quot;, right == &quot;NAstd_day01&quot; ~ &quot;beta[1]^&#39;NA&#39;&quot;, right == &quot;sigma_NAstd_Intercept&quot; ~ &quot;eta[0]^&#39;NA&#39;&quot;, right == &quot;sigma_NAstd_day01&quot; ~ &quot;eta[1]^&#39;NA&#39;&quot;, right == &quot;PAstd_Intercept&quot; ~ &quot;beta[0]^&#39;PA&#39;&quot;, right == &quot;PAstd_day01&quot; ~ &quot;beta[1]^&#39;PA&#39;&quot;, right == &quot;sigma_PAstd_Intercept&quot; ~ &quot;eta[0]^&#39;PA&#39;&quot;, right == &quot;sigma_PAstd_day01&quot; ~ &quot;eta[1]^&#39;PA&#39;&quot; ) ) %&gt;% mutate(label = formatC(Estimate, digits = 2, format = &quot;f&quot;) %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;)) %&gt;% mutate(left = factor(left, levels = levels), right = factor(right, levels = levels)) %&gt;% mutate(right = fct_rev(right)) rho %&gt;% head() ## left right Estimate Est.Error Q2.5 Q97.5 label ## 1 beta[0]^&#39;NA&#39; beta[1]^&#39;NA&#39; -0.2962979 0.08117798 -0.4484332 -0.13095977 -.30 ## 2 beta[0]^&#39;NA&#39; eta[0]^&#39;NA&#39; 0.6321929 0.04557169 0.5352463 0.71363925 .63 ## 3 beta[1]^&#39;NA&#39; eta[0]^&#39;NA&#39; -0.1859044 0.07794643 -0.3347737 -0.03036644 -.19 ## 4 beta[0]^&#39;NA&#39; eta[1]^&#39;NA&#39; -0.1510637 0.10379261 -0.3536511 0.05184236 -.15 ## 5 beta[1]^&#39;NA&#39; eta[1]^&#39;NA&#39; 0.5788415 0.09107391 0.3870123 0.74229944 .58 ## 6 eta[0]^&#39;NA&#39; eta[1]^&#39;NA&#39; -0.1393943 0.09720892 -0.3263145 0.05486822 -.14 Now we plot! rho %&gt;% full_join(rename(rho, right = left, left = right), by = c(&quot;left&quot;, &quot;right&quot;, &quot;Estimate&quot;, &quot;Est.Error&quot;, &quot;Q2.5&quot;, &quot;Q97.5&quot;, &quot;label&quot;)) %&gt;% ggplot(aes(x = left, y = right)) + geom_tile(aes(fill = Estimate)) + geom_hline(yintercept = 4.5, color = &quot;#100F14&quot;) + geom_vline(xintercept = 4.5, color = &quot;#100F14&quot;) + geom_text(aes(label = label), family = &quot;Courier&quot;, size = 3) + scale_fill_gradient2(expression(rho), low = &quot;#59708b&quot;, mid = &quot;#FCF9F0&quot;, high = &quot;#A65141&quot;, midpoint = 0, labels = c(-1, &quot;&quot;, 0, &quot;&quot;, 1), limits = c(-1, 1)) + scale_x_discrete(NULL, expand = c(0, 0), labels = ggplot2:::parse_safe, position = &quot;top&quot;) + scale_y_discrete(NULL, expand = c(0, 0), labels = ggplot2:::parse_safe) + theme(axis.text = element_text(size = 12), axis.ticks = element_blank(), legend.text = element_text(hjust = 1)) The full_join() business just before the ggplot2 code is how we got the full \\(8 \\times 8\\) matrix. If you’re curious, see what happens if you run the code without that part. To help orient you to the plot, I’ve divided it into four quadrants. The upper left and lower right quadrants are the correlations among the varying parameters for the N_A.std and P_A.std ratings, respectively. The other two quadrants are the correlations for those parameters between N_A.std and P_A.std. As a reminder, this matrix, as with any other correlation matrix, is symmetrical across the diagonal. To my eye, a few things pop out. First, the correlations within N_A.std are generally higher than those within P_A.std. Second, the correlations among the parameters between N_A.std and P_A.std are generally higher than those within them. Finally, all three of the largest correlations have to do with variation in the \\(\\eta\\) parameters. Two of them are basically the same as those we focused on for b14.14. The new one, \\(\\rho_{73}\\), indicates that participants’ baseline ratings for N_A.std tended to vary in a similar way as their baseline ratings for P_A.std. 14.6.4.1 Plot with uncertainty. As fond as I am with that last correlation plot, it has a glaring defect: there is no expression of uncertainty. Sometimes we express uncertainty with percentile-based intervals. Other times we do so with marginal densities. But the correlation plots only describe the marginal posteriors for all the \\(\\rho\\) parameters with their means–no uncertainty. If you have one or a small few correlations to plot, coefficient or density plots might do. However, they don’t scale well. If you don’t believe me, just try. I’m not sure there are any good solutions to this, but it can be helpful to at least try grappling with the issue. Fortunately for us, Matthew Kay (creator of the tidybayes package) has already tried his hand at a few approaches. For all the deets, check out the multivariate-regression.md file in his uncertainty-examples GitHub repo. One of his more imaginative approaches is to use what he calls dithering. Imagine breaking each of the cells in our correlation plot, above, into a \\(50 \\times 50 = 2{,}500\\)-cell grid. Now assign each of the cells within that grid one of the values from the HMC draws of that correlation’s posterior distribution. Then color code each of those cells by that value in the same basic way we color coded our previous correlation plots. Simultaneously do that for all of the correlations within the \\(\\mathbf R\\) matrix and plot them in a faceted plot. That’s the essence of the dithering approach. This is all probably hard to make sense of in words. Hopefully it will all come together with a little code and the resulting plot. Hold on to your hat. levels &lt;- c(&quot;beta[0]^&#39;NA&#39;&quot;, &quot;beta[1]^&#39;NA&#39;&quot;, &quot;eta[0]^&#39;NA&#39;&quot;, &quot;eta[1]^&#39;NA&#39;&quot;, &quot;beta[0]^&#39;PA&#39;&quot;, &quot;beta[1]^&#39;PA&#39;&quot;, &quot;eta[0]^&#39;PA&#39;&quot;, &quot;eta[1]^&#39;PA&#39;&quot;) rho &lt;- posterior_samples(b14.15) %&gt;% select(starts_with(&quot;cor_&quot;)) %&gt;% slice_sample(n = 50 * 50) %&gt;% bind_cols(crossing(x = 1:50, y = 1:50)) %&gt;% pivot_longer(-c(x:y)) %&gt;% mutate(name = str_remove(name, &quot;cor_record_id__&quot;)) %&gt;% separate(name, into = c(&quot;col&quot;, &quot;row&quot;), sep = &quot;__&quot;) %&gt;% mutate( col = case_when( col == &quot;NAstd_Intercept&quot; ~ &quot;beta[0]^&#39;NA&#39;&quot;, col == &quot;NAstd_day01&quot; ~ &quot;beta[1]^&#39;NA&#39;&quot;, col == &quot;sigma_NAstd_Intercept&quot; ~ &quot;eta[0]^&#39;NA&#39;&quot;, col == &quot;sigma_NAstd_day01&quot; ~ &quot;eta[1]^&#39;NA&#39;&quot;, col == &quot;PAstd_Intercept&quot; ~ &quot;beta[0]^&#39;PA&#39;&quot;, col == &quot;PAstd_day01&quot; ~ &quot;beta[1]^&#39;PA&#39;&quot;, col == &quot;sigma_PAstd_Intercept&quot; ~ &quot;eta[0]^&#39;PA&#39;&quot;, col == &quot;sigma_PAstd_day01&quot; ~ &quot;eta[1]^&#39;PA&#39;&quot; ), row = case_when( row == &quot;NAstd_Intercept&quot; ~ &quot;beta[0]^&#39;NA&#39;&quot;, row == &quot;NAstd_day01&quot; ~ &quot;beta[1]^&#39;NA&#39;&quot;, row == &quot;sigma_NAstd_Intercept&quot; ~ &quot;eta[0]^&#39;NA&#39;&quot;, row == &quot;sigma_NAstd_day01&quot; ~ &quot;eta[1]^&#39;NA&#39;&quot;, row == &quot;PAstd_Intercept&quot; ~ &quot;beta[0]^&#39;PA&#39;&quot;, row == &quot;PAstd_day01&quot; ~ &quot;beta[1]^&#39;PA&#39;&quot;, row == &quot;sigma_PAstd_Intercept&quot; ~ &quot;eta[0]^&#39;PA&#39;&quot;, row == &quot;sigma_PAstd_day01&quot; ~ &quot;eta[1]^&#39;PA&#39;&quot; ) ) %&gt;% mutate(col = factor(col, levels = levels), row = factor(row, levels = levels)) rho %&gt;% full_join(rename(rho, col = row, row = col), by = c(&quot;x&quot;, &quot;y&quot;, &quot;col&quot;, &quot;row&quot;, &quot;value&quot;)) %&gt;% ggplot(aes(x = x, y = y, fill = value)) + geom_raster() + scale_fill_gradient2(expression(rho), low = &quot;#59708b&quot;, mid = &quot;#FCF9F0&quot;, high = &quot;#A65141&quot;, midpoint = 0, labels = c(-1, &quot;&quot;, 0, &quot;&quot;, 1), limits = c(-1, 1)) + scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) + scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) + theme(strip.text = element_text(size = 12)) + facet_grid(row ~ col, labeller = label_parsed, switch = &quot;y&quot;) From Kay’s GitHub repo, we read: “This is akin to something like an icon array. You should still be able to see the average color (thanks to the human visual system’s ensembling processing), but also get a sense of the uncertainty by how ‘dithered’ a square looks.” Hopefully this will give you a little inspiration to find new and better ways to express the posterior uncertainty in your Bayesian correlation plots. If you come up with any great solutions, let the rest of us know! 14.6.5 Growth model/MELSM wrap-up. This bonus section introduced a lot of material. To learn more about the conventional multilevel growth model and its extensions, check out Singer and Willett’s (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence; My brms/tidyverse translation of that text, Applied Longitudinal Data Analysis in brms and the tidyverse ; or Hoffman’s (2015) text, Longitudinal analysis: Modeling within-person fluctuation and change. To learn more about the MELSM approach and its extensions, check out Hedeker et al. (2008), An application of a mixed-effects location scale model for analysis of ecological momentary assessment (EMA) data; Hedeker et al. (2012), Modeling between- and within-subject variance in ecological momentary assessment (EMA) data using mixed-effects location scale models; Williams’ and colleagues’ (2019) preprint, Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity; Williams’ and colleagues’ (2019) preprint, Beneath the surface: Unearthing within-person variability and mean relations with Bayesian mixed models; Williams’ and colleagues’ (2019) paper, A Bayesian nonlinear mixed-effects location scale model for learning; Williams’ tutorial blog post, A defining feature of cognitive interference tasks: Heterogeneous within-person variance. From a brms standpoint, it might also be helpful to brush up on Bürkner’s (2021b) vignette, Estimating distributional models with brms and Bürkner’s (2021d) vignette, Estimating multivariate models with brms. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] ape_5.4-1 ggtree_2.5.0.992 ggrepel_0.9.1 rethinking_2.13 dagitty_0.3-1 ## [6] ggdag_0.2.3 posterior_0.1.3 bayesplot_1.8.0 patchwork_1.1.1 brms_2.15.0 ## [11] Rcpp_1.0.6 tidybayes_2.3.1 rstan_2.21.2 StanHeaders_2.21.0-7 dutchmasters_0.1.0 ## [16] forcats_0.5.1 stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 ## [21] tidyr_1.1.3 tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 igraph_1.2.6 lazyeval_0.2.2 ## [6] splines_4.0.4 svUnit_1.0.3 crosstalk_1.1.0.1 TH.data_1.0-10 rstantools_2.1.1 ## [11] inline_0.3.17 digest_0.6.27 invgamma_1.1 htmltools_0.5.1.1 viridis_0.5.1 ## [16] rsconnect_0.8.16 fansi_0.4.2 checkmate_2.0.0 magrittr_2.0.1 graphlayouts_0.7.1 ## [21] modelr_0.1.8 RcppParallel_5.0.2 matrixStats_0.57.0 sandwich_3.0-0 xts_0.12.1 ## [26] prettyunits_1.1.1 colorspace_2.0-0 rvest_0.3.6 ggdist_2.4.0.9000 haven_2.3.1 ## [31] xfun_0.22 hexbin_1.28.1 callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 ## [36] lme4_1.1-25 survival_3.2-7 zoo_1.8-8 glue_1.4.2 polyclip_1.10-0 ## [41] gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 distributional_0.2.2 pkgbuild_1.2.0 ## [46] shape_1.4.5 abind_1.4-5 scales_1.1.1 mvtnorm_1.1-1 emo_0.0.0.9000 ## [51] DBI_1.1.0 miniUI_0.1.1.1 viridisLite_0.3.0 xtable_1.8-4 HDInterval_0.2.2 ## [56] tidytree_0.3.3 stats4_4.0.4 DT_0.16 htmlwidgets_1.5.2 httr_1.4.2 ## [61] threejs_0.3.3 arrayhelpers_1.1-0 ellipsis_0.3.1 pkgconfig_2.0.3 loo_2.4.1 ## [66] farver_2.0.3 dbplyr_2.0.0 utf8_1.1.4 tidyselect_1.1.0 labeling_0.4.2 ## [71] rlang_0.4.10 reshape2_1.4.4 later_1.1.0.1 munsell_0.5.0 cellranger_1.1.0 ## [76] tools_4.0.4 cli_2.3.1 generics_0.1.0 broom_0.7.5 ggridges_0.5.2 ## [81] evaluate_0.14 fastmap_1.0.1 processx_3.4.5 knitr_1.31 fs_1.5.0 ## [86] tidygraph_1.2.0 ggraph_2.0.4 nlme_3.1-152 mime_0.10 projpred_2.0.2 ## [91] aplot_0.0.6 xml2_1.3.2 compiler_4.0.4 shinythemes_1.1.2 rstudioapi_0.13 ## [96] curl_4.3 gamm4_0.2-6 treeio_1.15.2 reprex_0.3.0 tweenr_1.0.1 ## [101] statmod_1.4.35 stringi_1.5.3 highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 ## [106] lattice_0.20-41 Matrix_1.3-2 nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 ## [111] vctrs_0.3.6 pillar_1.5.1 lifecycle_1.0.0 BiocManager_1.30.10 bridgesampling_1.0-0 ## [116] estimability_1.3 httpuv_1.5.4 R6_2.5.0 bookdown_0.21 promises_1.1.1 ## [121] gridExtra_2.3 nleqslv_3.3.2 codetools_0.2-18 boot_1.3-26 colourpicker_1.1.0 ## [126] MASS_7.3-53 gtools_3.8.2 assertthat_0.2.1 withr_2.4.1 shinystan_2.5.0 ## [131] multcomp_1.4-16 mgcv_1.8-33 hms_0.5.3 grid_4.0.4 coda_0.19-4 ## [136] minqa_1.2.4 rvcheck_0.1.8 rmarkdown_2.7 ggforce_0.3.2 shiny_1.5.0 ## [141] lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 We’ll briefly cover autoregressive models in Section 16.4.↩︎ "],["missing-data-and-other-opportunities.html", "15 Missing Data and Other Opportunities 15.1 Measurement error 15.2 Missing data 15.3 Categorical errors and discrete absences 15.4 Summary 15.5 Bonus: Bayesian meta-analysis with odds ratios Session info", " 15 Missing Data and Other Opportunities For the opening example, we’re playing with the conditional probability \\[ \\text{Pr(burnt down | burnt up)} = \\frac{\\text{Pr(burnt up, burnt down)}}{\\text{Pr(burnt up)}}. \\] Given McElreath’s setup, it works out that \\[ \\text{Pr(burnt down | burnt up)} = \\frac{1/3}{1/2} = \\frac{2}{3}. \\] We might express the math toward the bottom of page 489 in tibble form like this. library(tidyverse) p_pancake &lt;- 1/3 ( d &lt;- tibble(pancake = c(&quot;BB&quot;, &quot;BU&quot;, &quot;UU&quot;), p_burnt = c(1, .5, 0)) %&gt;% mutate(p_burnt_up = p_burnt * p_pancake) ) ## # A tibble: 3 x 3 ## pancake p_burnt p_burnt_up ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BB 1 0.333 ## 2 BU 0.5 0.167 ## 3 UU 0 0 d %&gt;% summarise(`p (burnt_down | burnt_up)` = p_pancake / sum(p_burnt_up)) ## # A tibble: 1 x 1 ## `p (burnt_down | burnt_up)` ## &lt;dbl&gt; ## 1 0.667 I understood McElreath’s simulation (R code 15.1) better after breaking it apart. The first part of sim_pancake() takes one random draw from the integers 1, 2, and 3. It just so happens that if we set set.seed(1), the code returns a 1. set.seed(1) sample(x = 1:3, size = 1) ## [1] 1 So here’s what it looks like if we use seeds 2:11. take_sample &lt;- function(seed) { set.seed(seed) sample(x = 1:3, size = 1) } tibble(seed = 2:11) %&gt;% mutate(value_returned = map_dbl(seed, take_sample)) ## # A tibble: 10 x 2 ## seed value_returned ## &lt;int&gt; &lt;dbl&gt; ## 1 2 1 ## 2 3 1 ## 3 4 3 ## 4 5 2 ## 5 6 1 ## 6 7 2 ## 7 8 3 ## 8 9 3 ## 9 10 3 ## 10 11 2 Each of those value_returned values stands for one of the three pancakes: 1 = BB, 2 = BU, and 3 = UU. In the next line, McElreath made slick use of a matrix to specify that. Here’s what the matrix looks like. matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 1 0 ## [2,] 1 0 0 See how the three columns are identified as [,1], [,2], and [,3]? If, say, we wanted to subset the values in the second column, we’d execute matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, 2] ## [1] 1 0 which returns a numeric vector. matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, 2] %&gt;% str() ## num [1:2] 1 0 That 1 0 corresponds to the pancake with one burnt (i.e., 1) and one unburnt (i.e., 0) side. So when McElreath then executed sample(sides), he randomly sampled from one of those two values. In the case of pancake == 2, he randomly sampled one the pancake with one burnt and one unburnt side. Had he sampled from pancake == 1, he would have sampled from the pancake with both sides burnt. Going forward, let’s amend McElreath’s sim_pancake() function so it will take a seed argument, which will allow us to make the output reproducible. # simulate a `pancake` and return randomly ordered `sides` sim_pancake &lt;- function(seed) { set.seed(seed) pancake &lt;- sample(x = 1:3, size = 1) sides &lt;- matrix(c(1, 1, 1, 0, 0, 0), nrow = 2, ncol = 3)[, pancake] sample(sides) } Let’s take this baby for a whirl. # how many simulations would you like? n_sim &lt;- 1e4 d &lt;- tibble(seed = 1:n_sim) %&gt;% mutate(burnt = map(seed, sim_pancake)) %&gt;% unnest(burnt) %&gt;% mutate(side = rep(c(&quot;up&quot;, &quot;down&quot;), times = n() / 2)) Take a look at what we’ve done. head(d, n = 10) ## # A tibble: 10 x 3 ## seed burnt side ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 up ## 2 1 1 down ## 3 2 1 up ## 4 2 1 down ## 5 3 1 up ## 6 3 1 down ## 7 4 0 up ## 8 4 0 down ## 9 5 1 up ## 10 5 0 down Now we use pivot_wider() and summarise() to get the value we’ve been working for. d %&gt;% pivot_wider(names_from = side, values_from = burnt) %&gt;% summarise(`p (burnt_down | burnt_up)` = sum(up == 1 &amp; down == 1) / (sum(up))) ## # A tibble: 1 x 1 ## `p (burnt_down | burnt_up)` ## &lt;dbl&gt; ## 1 0.658 The results are within rounding error of the ideal 2/3. Probability theory is not difficult mathematically. It is just counting. But it is hard to interpret and apply. Doing so often seems to require some cleverness, and authors have an incentive to solve problems in clever ways, just to show off. But we don’t need that cleverness, if we ruthlessly apply conditional probability…. In this chapter, [we’ll] meet two commonplace applications of this assume-and-deduce strategy. The first is the incorporation of measurement error into our models. The second is the estimation of missing data through Bayesian imputation…. In neither application do [we] have to intuit the consequences of measurement errors nor the implications of missing values in order to design the models. All [we] have to do is state your information about the error or about the variables with missing values. Logic does the rest. (McElreath, 2020a, p. 490, emphasis in the original) 15.1 Measurement error Let’s grab those WaffleDivorce data from back in Chapter 5. data(WaffleDivorce, package = &quot;rethinking&quot;) d &lt;- WaffleDivorce rm(WaffleDivorce) In anticipation of R code 15.3 and 15.5, wrangle the data a little. d &lt;- d %&gt;% mutate(D_obs = (Divorce - mean(Divorce)) / sd(Divorce), D_sd = Divorce.SE / sd(Divorce), M = (Marriage - mean(Marriage)) / sd(Marriage), A = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage), M_obs = M, M_sd = Marriage.SE / sd(Marriage)) For the plots in this chapter, we’ll use the dark themes from the ggdark package (Grantham, 2019). Our primary theme will be ggdark::dark_theme_bw(). One way to use the dark_theme_bw() function is to make it part of the code for an individual plot, such as ggplot() + geom_point() + dark_theme_bw(). Another way is to make dark_theme_bw() the default setting with ggplot2::theme_set(). That will be our method. library(ggdark) theme_set( dark_theme_bw() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) ) # to reset the default ggplot2 theme to its default parameters, execute both: # ggplot2::theme_set(theme_gray()) # ggdark::invert_geom_defaults() For the rest of our color palette, we’ll use colors from the viridis package (Garnier, 2018), which provides a variety of colorblind-safe color palettes (see Rudis et al., 2018). library(viridis) The viridis_pal() function gives a list of colors within a given palette. The colors in each palette fall on a spectrum. Within viridis_pal(), the option argument allows one to select a given spectrum, “C,” in our case. The final parentheses, (), allows one to determine how many discrete colors one would like to break the spectrum up by. We’ll choose 7. viridis_pal(option = &quot;C&quot;)(7) ## [1] &quot;#0D0887FF&quot; &quot;#5D01A6FF&quot; &quot;#9C179EFF&quot; &quot;#CC4678FF&quot; &quot;#ED7953FF&quot; &quot;#FDB32FFF&quot; &quot;#F0F921FF&quot; With a little data wrangling, we can put the colors of our palette in a tibble and display them in a plot. tibble(factor = &quot;a&quot;, number = factor(1:7), color_number = str_c(1:7, &quot;. &quot;, viridis_pal(option = &quot;C&quot;)(7))) %&gt;% ggplot(aes(x = factor, y = number)) + geom_tile(aes(fill = number)) + geom_text(aes(label = color_number, color = number %in% c(&quot;5&quot;, &quot;6&quot;, &quot;7&quot;))) + scale_color_manual(values = c(&quot;black&quot;, &quot;white&quot;)) + scale_fill_viridis(option = &quot;C&quot;, discrete = T, direction = -1) + scale_x_discrete(NULL, breaks = NULL, expand = c(0, 0)) + scale_y_discrete(NULL, breaks = NULL, expand = c(0, 0)) + ggtitle(&quot;Behold: viridis C!&quot;) Now, let’s make use of our custom theme and reproduce/reimagine Figure 15.1.a. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] p1 &lt;- d %&gt;% ggplot(aes(x = MedianAgeMarriage, y = Divorce, ymin = Divorce - Divorce.SE, ymax = Divorce + Divorce.SE)) + geom_pointrange(shape = 20, alpha = 2/3, color = color) + labs(x = &quot;Median age marriage&quot; , y = &quot;Divorce rate&quot;) Notice how viridis_pal(option = \"C\")(7)[7] called the seventh color in the color scheme, \"#F0F921FF\". For Figure 15.1.b, we’ll select the sixth color in the palette by coding viridis_pal(option = \"C\")(7)[6]. We’ll then combine the two subplots with patchwork. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[6] p2 &lt;- d %&gt;% ggplot(aes(x = log(Population), y = Divorce, ymin = Divorce - Divorce.SE, ymax = Divorce + Divorce.SE)) + geom_pointrange(shape = 20, alpha = 2/3, color = color) + scale_y_continuous(NULL, breaks = NULL) + xlab(&quot;log population&quot;) library(patchwork) p1 | p2 Just like in the text, our plot shows states with larger populations tend to have smaller measurement error. The relation between measurement error and MedianAgeMarriage is less apparent. 15.1.0.1 Rethinking: Generative thinking, Bayesian inference. Bayesian models are generative, meaning they can be used to simulate observations just as well as they can be used to estimate parameters. One benefit of this fact is that a statistical model can be developed by thinking hard about how the data might have arisen. This includes sampling and measurement, as well as the nature of the process we are studying. Then let Bayesian updating discover the implications. (p. 491, emphasis in the original) 15.1.1 Error on the outcome. Now make a DAG of our data with ggdag. library(ggdag) dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;D&quot;, &quot;Dobs&quot;, &quot;eD&quot;), x = c(1, 2, 2, 3, 4), y = c(2, 3, 1, 1, 1)) dagify(M ~ A, D ~ A + M, Dobs ~ D + eD, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;D&quot;, &quot;eD&quot;), &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;A&quot;, &quot;D&quot;, &quot;M&quot;, expression(italic(e)[D]), expression(D[obs]))) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) + dark_theme_void() Note our use of the dark_theme_void() function. But more to the substance of the matter, there’s a lot going on here. But we can proceed one step at a time. The left triangle of this DAG is the same system that we worked with back in Chapter 5. Age at marriage (\\(A\\)) influences divorce (\\(D\\)) both directly and indirectly, passing through marriage rate (\\(M\\)). Then we have the observation model. The true divorce rate \\(D\\) cannot be observed, so it is circled as an unobserved node. However we do get to observe \\(D_\\text{obs}\\), which is a function of both the true rate \\(D\\) and some unobserved error \\(e_\\text{D}\\). (p. 492) To get a better sense of what we’re about to do, imagine for a moment that each state’s divorce rate is normally distributed with a mean of Divorce and standard deviation Divorce.SE. Those distributions would be like this. d %&gt;% mutate(Divorce_distribution = str_c(&quot;Divorce ~ Normal(&quot;, Divorce, &quot;, &quot;, Divorce.SE, &quot;)&quot;)) %&gt;% select(Loc, Divorce_distribution) %&gt;% head() ## Loc Divorce_distribution ## 1 AL Divorce ~ Normal(12.7, 0.79) ## 2 AK Divorce ~ Normal(12.5, 2.05) ## 3 AZ Divorce ~ Normal(10.8, 0.74) ## 4 AR Divorce ~ Normal(13.5, 1.22) ## 5 CA Divorce ~ Normal(8, 0.24) ## 6 CO Divorce ~ Normal(11.6, 0.94) Here’s how to define the error distribution for each divorce rate. For each observed value \\(D_{\\text{OBS},i}\\), there will be one parameter, \\(D_{\\text{TRUE},i}\\), defined by: \\[D_{\\text{OBS},i} \\sim \\operatorname{Normal}(D_{\\text{TRUE},i}, D_{\\text{SE},i})\\] All this does is define the measurement \\(D_{\\text{OBS},i}\\) as having the specified Gaussian distribution centered on the unknown parameter \\(D_{\\text{TRUE},i}\\). So the above defines a probability for each State \\(i\\)’s observed divorce rate, given a known measurement error. (p. 493) Our model will follow the form \\[\\begin{align*} \\color{#5D01A6FF}{\\text{Divorce}_{\\text{OBS}, i}} &amp; \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(\\text{Divorce}_{\\text{TRUE}, i}, \\text{Divorce}_{\\text{SE}, i})} \\\\ \\color{#5D01A6FF}{\\text{Divorce}_{\\text{TRUE}, i}} &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu &amp; = \\alpha + \\beta_1 \\text A_i + \\beta_2 \\text M_i \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Fire up brms. library(brms) With brms, we accommodate measurement error in the criterion using the mi() syntax, following the general form &lt;response&gt; | mi(&lt;se_response&gt;). This follows a missing data logic, resulting in Bayesian missing data imputation for the criterion values. The mi() syntax is based on the missing data capabilities for brms, which we will cover in greater detail in the second half of this chapter. # put the data into a `list()` dlist &lt;- list( D_obs = d$D_obs, D_sd = d$D_sd, M = d$M, A = d$A) b15.1 &lt;- brm(data = dlist, family = gaussian, D_obs | mi(D_sd) ~ 1 + A + M, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 15, # note this line save_mevars = TRUE, file = &quot;fits/b15.01&quot;) Check the model summary. print(b15.1) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: D_obs | mi(D_sd) ~ 1 + A + M ## Data: dlist (Number of observations: 50) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.06 0.10 -0.23 0.14 1.00 5130 3528 ## A -0.61 0.16 -0.92 -0.31 1.00 3953 3313 ## M 0.05 0.17 -0.28 0.38 1.00 3801 2979 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.58 0.11 0.39 0.81 1.00 1615 1962 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). To return the summaries for the D_true[i] parameters, you might execute posterior_summary(b15.1) or b15.1$fit. posterior_summary(b15.1) %&gt;% round(digits = 2) %&gt;% data.frame() ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.06 0.10 -0.23 0.14 ## b_A -0.61 0.16 -0.92 -0.31 ## b_M 0.05 0.17 -0.28 0.38 ## sigma 0.58 0.11 0.39 0.81 ## Yl[1] 1.16 0.36 0.46 1.89 ## Yl[2] 0.69 0.57 -0.44 1.84 ## Yl[3] 0.42 0.34 -0.24 1.12 ## Yl[4] 1.42 0.48 0.50 2.38 ## Yl[5] -0.90 0.13 -1.15 -0.65 ## Yl[6] 0.65 0.40 -0.12 1.46 ## Yl[7] -1.37 0.35 -2.06 -0.69 ## Yl[8] -0.35 0.49 -1.32 0.60 ## Yl[9] -1.89 0.59 -3.04 -0.75 ## Yl[10] -0.62 0.16 -0.94 -0.30 ## Yl[11] 0.77 0.29 0.22 1.33 ## Yl[12] -0.55 0.48 -1.52 0.32 ## Yl[13] 0.17 0.49 -0.82 1.11 ## Yl[14] -0.87 0.22 -1.32 -0.44 ## Yl[15] 0.55 0.29 -0.02 1.13 ## Yl[16] 0.29 0.37 -0.44 1.00 ## Yl[17] 0.50 0.43 -0.32 1.34 ## Yl[18] 1.25 0.35 0.59 1.94 ## Yl[19] 0.42 0.39 -0.33 1.21 ## Yl[20] 0.40 0.54 -0.59 1.48 ## Yl[21] -0.55 0.32 -1.17 0.08 ## Yl[22] -1.10 0.25 -1.59 -0.60 ## Yl[23] -0.27 0.26 -0.77 0.24 ## Yl[24] -1.00 0.29 -1.57 -0.42 ## Yl[25] 0.42 0.41 -0.36 1.24 ## Yl[26] -0.03 0.31 -0.64 0.59 ## Yl[27] -0.01 0.50 -1.01 1.00 ## Yl[28] -0.15 0.38 -0.93 0.60 ## Yl[29] -0.27 0.50 -1.21 0.71 ## Yl[30] -1.80 0.23 -2.26 -1.35 ## Yl[31] 0.17 0.43 -0.67 1.06 ## Yl[32] -1.66 0.16 -1.98 -1.34 ## Yl[33] 0.12 0.24 -0.36 0.60 ## Yl[34] -0.05 0.51 -1.12 0.93 ## Yl[35] -0.13 0.23 -0.57 0.33 ## Yl[36] 1.28 0.40 0.52 2.08 ## Yl[37] 0.23 0.34 -0.44 0.90 ## Yl[38] -1.02 0.22 -1.45 -0.59 ## Yl[39] -0.93 0.54 -1.93 0.17 ## Yl[40] -0.68 0.31 -1.29 -0.08 ## Yl[41] 0.25 0.54 -0.82 1.38 ## Yl[42] 0.73 0.35 0.06 1.42 ## Yl[43] 0.19 0.18 -0.15 0.54 ## Yl[44] 0.80 0.43 -0.04 1.62 ## Yl[45] -0.40 0.54 -1.45 0.67 ## Yl[46] -0.39 0.26 -0.90 0.12 ## Yl[47] 0.14 0.31 -0.46 0.74 ## Yl[48] 0.55 0.47 -0.38 1.48 ## Yl[49] -0.64 0.28 -1.22 -0.10 ## Yl[50] 0.85 0.60 -0.35 2.04 ## lp__ -78.11 6.47 -91.50 -66.12 Our rows Yl[1] through Yl[50] correspond to what rethinking named D_true[1] through D_true[50]. Here’s the code for our Figure 15.2.a. library(ggrepel) states &lt;- c(&quot;AL&quot;, &quot;AR&quot;, &quot;ME&quot;, &quot;NH&quot;, &quot;RI&quot;, &quot;DC&quot;, &quot;VT&quot;, &quot;AK&quot;, &quot;SD&quot;, &quot;UT&quot;, &quot;ID&quot;, &quot;ND&quot;, &quot;WY&quot;) d_est &lt;- posterior_summary(b15.1) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% mutate(D_est = Estimate) %&gt;% select(term, D_est) %&gt;% filter(str_detect(term, &quot;Yl&quot;)) %&gt;% bind_cols(d) color &lt;- viridis_pal(option = &quot;C&quot;)(7)[5] p1 &lt;- d_est %&gt;% ggplot(aes(x = D_sd, y = D_est - D_obs)) + geom_hline(yintercept = 0, linetype = 2, color = &quot;white&quot;) + geom_point(alpha = 2/3, color = color) + geom_text_repel(data = . %&gt;% filter(Loc %in% states), aes(label = Loc), size = 3, seed = 15, color = &quot;white&quot;) We’ll use a little posterior_samples() + expand() magic to help with our version of Figure 15.2.b. library(tidybayes) states &lt;- c(&quot;AR&quot;, &quot;ME&quot;, &quot;RI&quot;, &quot;ID&quot;, &quot;WY&quot;, &quot;ND&quot;, &quot;MN&quot;) color &lt;- viridis_pal(option = &quot;C&quot;)(7)[4] p2 &lt;- posterior_samples(b15.1) %&gt;% expand(nesting(b_Intercept, b_A), A = seq(from = -3.5, to = 3.5, length.out = 50)) %&gt;% mutate(fitted = b_Intercept + b_A * A) %&gt;% ggplot(aes(x = A)) + stat_lineribbon(aes(y = fitted), .width = .95, size = 1/3, color = &quot;grey50&quot;, fill = &quot;grey20&quot;) + geom_segment(data = d_est, aes(xend = A, y = D_obs, yend = D_est), size = 1/5) + geom_point(data = d_est, aes(y = D_obs), color = color) + geom_point(data = d_est, aes(y = D_est), shape = 1, stroke = 1/3) + geom_text_repel(data = d %&gt;% filter(Loc %in% states), aes(y = D_obs, label = Loc), size = 3, seed = 15, color = &quot;white&quot;) + labs(x = &quot;median age marriage (std)&quot;, y = &quot;divorce rate (std)&quot;) + coord_cartesian(xlim = range(d$A), ylim = range(d$D_obs)) Now combine the two ggplots and plot. p1 | p2 If you look closely, our plot on the left is flipped relative to the one in the text. I’m pretty sure my code is correct, which leaves me to believe McElreath accidentally flipped the ordering in his code and made his \\(y\\)-axis ‘D_obs - D_est.’ Happily, our plot on the right matches up nicely with the one in the text. 15.1.2 Error on both outcome and predictor. Now we update the DAG to account for measurement error in the predictor. dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;Mobs&quot;, &quot;eM&quot;, &quot;D&quot;, &quot;Dobs&quot;, &quot;eD&quot;), x = c(1, 2, 3, 4, 2, 3, 4), y = c(2, 3, 3, 3, 1, 1, 1)) dagify(M ~ A, D ~ A + M, Mobs ~ M + eM, Dobs ~ D + eD, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;A&quot;, &quot;Mobs&quot;, &quot;Dobs&quot;), &quot;b&quot;, &quot;a&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;A&quot;, &quot;D&quot;, &quot;M&quot;, expression(italic(e)[D]), expression(italic(e)[M]), expression(D[obs]), expression(M[obs]))) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) + dark_theme_void() We will express this DAG in an augmented statistical model following the form \\[\\begin{align*} \\text{Divorce}_{\\text{OBS}, i} &amp; \\sim \\operatorname{Normal}(\\text{Divorce}_{\\text{TRUE}, i}, \\text{Divorce}_{\\text{SE}, i}) \\\\ \\text{Divorce}_{\\text{TRUE}, i} &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\text A_i + \\beta_2 \\color{#5D01A6FF}{\\text{Marriage}_{\\text{TRUE}, i}} \\\\ \\color{#5D01A6FF}{\\text{Marriage}_{\\text{OBS}, i}} &amp; \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(\\text{Marriage}_{\\text{TRUE}, i}, \\text{Marriage}_{\\text{SE}, i})} \\\\ \\color{#5D01A6FF}{\\text{Marriage}_{\\text{TRUE}, i}} &amp; \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 1)} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 0.2) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] The current version brms allows users to specify error on predictors with an me() statement in the form of me(predictor, sd_predictor) where sd_predictor is a vector in the data denoting the size of the measurement error, presumed to be in a standard-deviation metric. # put the data into a `list()` dlist &lt;- list( D_obs = d$D_obs, D_sd = d$D_sd, M_obs = d$M_obs, M_sd = d$M_sd, A = d$A) b15.2 &lt;- brm(data = dlist, family = gaussian, D_obs | mi(D_sd) ~ 1 + A + me(M_obs, M_sd), prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(normal(0, 1), class = meanme), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 15, # note this line save_mevars = TRUE, file = &quot;fits/b15.02&quot;) We’ll use posterior_smmary(), again, to get a sense of those depth=2 summaries. posterior_summary(b15.2) %&gt;% round(digits = 2) Due to space concerns, I’m not going to show the results, here. You can do that on your own. Basically, now in addition to the posterior summaries for the Yl[i] parameters (what McElreath called \\(D_{\\text{TRUE}, i}\\)), we now get posterior summaries for Xme_meM_obs[i] (what McElreath called \\(M_{\\text{TRUE}, i}\\)). Note that you’ll need to specify save_mevars = TRUE in the brm() function in order to save the posterior samples of error-adjusted variables obtained by using the me() argument. Without doing so, functions like predict() may give you trouble. Here’s our version of Figure 15.3. color_y &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] color_p &lt;- viridis_pal(option = &quot;C&quot;)(7)[2] # wrangle full_join( # D tibble(Loc = d %&gt;% pull(Loc), D_obs = d %&gt;% pull(D_obs), D_est = posterior_summary(b15.2) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% filter(str_detect(term, &quot;Yl&quot;)) %&gt;% pull(Estimate)) %&gt;% pivot_longer(-Loc, values_to = &quot;d&quot;) %&gt;% mutate(name = if_else(name == &quot;D_obs&quot;, &quot;observed&quot;, &quot;posterior&quot;)), # M tibble(Loc = d %&gt;% pull(Loc), M_obs = d %&gt;% pull(M_obs), M_est = posterior_summary(b15.2) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% filter(str_detect(term, &quot;Xme_&quot;)) %&gt;% pull(Estimate)) %&gt;% pivot_longer(-Loc, values_to = &quot;m&quot;) %&gt;% mutate(name = if_else(name == &quot;M_obs&quot;, &quot;observed&quot;, &quot;posterior&quot;)), by = c(&quot;Loc&quot;, &quot;name&quot;) ) %&gt;% # plot! ggplot(aes(x = m, y = d)) + geom_line(aes(group = Loc), size = 1/4) + geom_point(aes(color = name)) + scale_color_manual(values = c(color_p, color_y)) + labs(subtitle = &quot;Shrinkage of both divorce rate and marriage rate&quot;, x = &quot;Marriage rate (std)&quot; , y = &quot;Divorce rate (std)&quot;) The yellow points are model-implied; the purple ones are of the original data. It turns out our brms model regularized just a little more aggressively than McElreath’s rethinking model. Anyway, The big take home point for this section is that when you have a distribution of values, don’t reduce it down to a single value to use in a regression. Instead, use the entire distribution. Anytime we use an average value, discarding the uncertainty around that average, we risk overconfidence and spurious inference. This doesn’t only apply to measurement error, but also to cases in which data are averaged before analysis. (p. 497) 15.1.3 Measurement terrors. McElreath invited us to consider a few more DAGs. The first is an instance where both sources of measurement error have a common cause, \\(P\\). dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;Mobs&quot;, &quot;eM&quot;, &quot;D&quot;, &quot;Dobs&quot;, &quot;eD&quot;, &quot;P&quot;), x = c(1, 2, 3, 4, 2, 3, 4, 5), y = c(2, 3, 3, 3, 1, 1, 1, 2)) dagify(M ~ A, D ~ A + M, Mobs ~ M + eM, Dobs ~ D + eD, eM ~ P, eD ~ P, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;A&quot;, &quot;Mobs&quot;, &quot;Dobs&quot;, &quot;P&quot;), &quot;b&quot;, &quot;a&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;A&quot;, &quot;D&quot;, &quot;M&quot;, &quot;P&quot;, expression(italic(e)[D]), expression(italic(e)[M]), expression(D[obs]), expression(M[obs]))) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) + dark_theme_void() The second instance is when the true marriage rate \\(M\\) has a causal effect on the measurement error for Divorce, \\(e_\\text{D}\\). dag_coords &lt;- tibble(name = c(&quot;A&quot;, &quot;M&quot;, &quot;Mobs&quot;, &quot;eM&quot;, &quot;D&quot;, &quot;Dobs&quot;, &quot;eD&quot;), x = c(1, 2, 3, 4, 2, 3, 4), y = c(2, 3, 3, 3, 1, 1, 1)) dagify(M ~ A, D ~ A + M, Mobs ~ M + eM, Dobs ~ D + eD, eD ~ M, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;A&quot;, &quot;Mobs&quot;, &quot;Dobs&quot;), &quot;b&quot;, &quot;a&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;A&quot;, &quot;D&quot;, &quot;M&quot;, expression(italic(e)[D]), expression(italic(e)[M]), expression(D[obs]), expression(M[obs]))) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) + dark_theme_void() The final example is when we have negligible measurement error for \\(M\\) and \\(D\\), but known nonignorable measurement error for the causal variable \\(A\\). dag_coords &lt;- tibble(name = c(&quot;eA&quot;, &quot;Aobs&quot;, &quot;A&quot;, &quot;M&quot;, &quot;D&quot;), x = c(1, 2, 3, 4, 4), y = c(2, 2, 2, 3, 1)) dagify(Aobs ~ A + eA, M ~ A, D ~ A, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;A&quot;, &quot;eA&quot;), &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;A&quot;, expression(italic(e)[A]), expression(A[obs]), &quot;D&quot;, &quot;M&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) + dark_theme_void() On page 498, we read: In this circumstance, it can happen that a naive regression of \\(D\\) on \\(A_\\text{obs}\\) and \\(M\\) will strongly suggest that \\(M\\) influences \\(D\\). The reason is that \\(M\\) contains information about the true \\(A\\). And \\(M\\) is measured more precisely than \\(A\\) is. It’s like a proxy \\(A\\). Here’s a small simulation you can toy with that will produce such a frustration: n &lt;- 500 set.seed(15) dat &lt;- tibble(A = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(M = rnorm(n, mean = -A, sd = 1), D = rnorm(n, mean = A, sd = 1), A_obs = rnorm(n, mean = A, sd = 1)) To get a sense of the havoc ignoring measurement error can cause, we’ll fit to models. These aren’t in the text, but, you know, let’s live a little. The first model will include A, the true predictor for D. The second model will include A_obs instead, the version of A with measurement error added in. # the model with A containing no measurement error b15.2b &lt;- brm(data = dat, family = gaussian, D ~ 1 + A + M, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 15, # note this line save_mevars = TRUE, file = &quot;fits/b15.02b&quot;) # The model where A has measurement error, but we ignore it b15.2c &lt;- brm(data = dat, family = gaussian, D ~ 1 + A_obs + M, prior = c(prior(normal(0, 0.2), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 15, # note this line save_mevars = TRUE, file = &quot;fits/b15.02c&quot;) Check the summaries. print(b15.2b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: D ~ 1 + A + M ## Data: dat (Number of observations: 500) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.01 0.04 -0.08 0.10 1.00 3560 2284 ## A 0.89 0.06 0.77 1.01 1.00 2945 2386 ## M -0.04 0.04 -0.13 0.04 1.00 2780 2563 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.01 0.03 0.94 1.07 1.00 3818 2871 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). print(b15.2c) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: D ~ 1 + A_obs + M ## Data: dat (Number of observations: 500) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.05 0.05 -0.05 0.15 1.00 4127 2880 ## A_obs 0.29 0.04 0.21 0.37 1.00 3101 3149 ## M -0.35 0.04 -0.43 -0.28 1.00 3444 3460 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 1.14 0.04 1.08 1.22 1.00 4141 3161 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). b15.2b, the model where A contains no measurement error, comes close to reproducing the data-generating parameters. The second model, b15.2c, which used A infused with measurement error (i.e., A_obs), is a disaster. A coefficient plot might help the comparison. # for annotation text &lt;- tibble(fit = &quot;b15.2b&quot;, term = &quot;beta[0]&quot;, Estimate = fixef(b15.2b, probs = .99)[&quot;Intercept&quot;, 3], label = &quot;In this plot, we like the yellow posteriors.&quot;) # wrangle bind_rows(posterior_summary(b15.2b) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;), posterior_summary(b15.2c) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;)) %&gt;% filter(term != &quot;lp__&quot;) %&gt;% mutate(term = rep(c(str_c(&quot;beta[&quot;, 0:2, &quot;]&quot;), &quot;sigma&quot;), times = 2), fit = rep(c(&quot;b15.2b&quot;, &quot;b15.2c&quot;), each = n() / 2)) %&gt;% # plot! ggplot(aes(x = Estimate, y = fit)) + geom_vline(xintercept = 0, linetype = 3, alpha = 1/2) + geom_pointrange(aes(xmin = Q2.5, xmax = Q97.5, color = fit)) + geom_text(data = text, aes(label = label), hjust = 0, color = color_y) + scale_color_manual(values = c(color_y, &quot;white&quot;)) + labs(x = &quot;marginal posterior&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), strip.background = element_rect(color = &quot;transparent&quot;, fill = &quot;transparent&quot;)) + facet_wrap(~ term, labeller = label_parsed, ncol = 1) 15.2 Missing data With measurement error, the insight is to realize that any uncertain piece of data can be replaced by a distribution that reflects uncertainty. But sometimes data are simply missing–no measurement is available at all. At first, this seems like a lost cause. What can be done when there is no measurement at all, not even one with error?… So what can we do instead? We can think causally about missingness, and we can use the model to impute missing values. A generative model tells you whether the process that produced the missing values will also prevent the identification of causal effects. (p. 499, emphasis in the original) Starting with version 2.2.0, brms supports Bayesian missing data imputation using adaptations of the multivariate syntax (Bürkner, 2021d). Bürkner’s (2021g) vignette, Handle missing values with brms, can provide a nice overview. 15.2.0.1 Rethinking: Missing data are meaningful data. The fact that a variable has an unobserved value is still an observation. It is data, just with a very special value. The meaning of this value depends upon the context. Consider for example a questionnaire on personal income. If some people refuse to fill in their income, this may be associated with low (or high) income. Therefore a model that tries to predict the missing values can be enlightening. (p. 499) 15.2.1 DAG ate my homework. We’ll start this section off with our version of Figure 15.4. It’s going to take a bit of effort on our part to make a nice representation those four DAGs. Here we make panels a, b, and d. # panel a dag_coords &lt;- tibble(name = c(&quot;S&quot;, &quot;H&quot;, &quot;Hs&quot;, &quot;D&quot;), x = c(1, 2, 2, 1), y = c(2, 2, 1, 1)) p1 &lt;- dagify(H ~ S, Hs ~ H + D, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name == &quot;H&quot;, &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(label = c(&quot;D&quot;, &quot;H&quot;, &quot;S&quot;, &quot;H*&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) # panel b p2 &lt;- dagify(H ~ S, Hs ~ H + D, D ~ S, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name == &quot;H&quot;, &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(label = c(&quot;D&quot;, &quot;H&quot;, &quot;S&quot;, &quot;H*&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) # panel d p4 &lt;- dagify(H ~ S, Hs ~ H + D, D ~ H, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name == &quot;H&quot;, &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(label = c(&quot;D&quot;, &quot;H&quot;, &quot;S&quot;, &quot;H*&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) Make panel c. dag_coords &lt;- tibble(name = c(&quot;S&quot;, &quot;H&quot;, &quot;Hs&quot;, &quot;D&quot;, &quot;X&quot;), x = c(1, 2, 2, 1, 1.5), y = c(2, 2, 1, 1, 1.5)) p3 &lt;- dagify(H ~ S + X, Hs ~ H + D, D ~ X, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;H&quot;, &quot;X&quot;), &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(label = c(&quot;D&quot;, &quot;H&quot;, &quot;S&quot;, &quot;X&quot;, &quot;H*&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) Now combine, adjust a little, and plot. (p1 + p2 + p3 + p4) + plot_annotation(tag_levels = &quot;a&quot;, tag_prefix = &quot;(&quot;, tag_suffix = &quot;)&quot;) &amp; scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) &amp; dark_theme_void() + theme(panel.background = element_rect(fill = &quot;grey8&quot;), plot.margin = margin(0.15, 0.15, 0.15, 0.15, &quot;in&quot;)) On page 500, we read: Consider a sample of students, all of whom own dogs. The students produce homework (\\(H\\)). This homework varies in quality, influenced by how much each student studies (\\(S\\)). We could simulate 100 students, their attributes, and their homework like this: n &lt;- 100 set.seed(15) d &lt;- tibble(s = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(h = rbinom(n, size = 10, inv_logit_scaled(s)), d_a = rbinom(n, size = 1, prob = .5), d_b = ifelse(s &gt; 0, 1, 0)) %&gt;% mutate(hm_a = ifelse(d_a == 1, NA, h), hm_b = ifelse(d_b == 1, NA, h)) d ## # A tibble: 100 x 6 ## s h d_a d_b hm_a hm_b ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.259 6 0 1 6 NA ## 2 1.83 8 0 1 8 NA ## 3 -0.340 4 0 0 4 4 ## 4 0.897 6 1 1 NA NA ## 5 0.488 8 1 1 NA NA ## 6 -1.26 3 1 0 NA 3 ## 7 0.0228 3 1 1 NA NA ## 8 1.09 6 1 1 NA NA ## 9 -0.132 6 0 0 6 6 ## 10 -1.08 3 1 0 NA 3 ## # … with 90 more rows In that code block, we simulated the data corresponding to McElreath’s R code 15.8 through 15.10. We have two d and hm variables. d_a and hm_a correspond to McElreath’s R code 15.9 and the DAG in panel a. d_b and hm_b correspond to McElreath’s R code 15.10 and the DAG in panel b. This wasn’t in the text, but here we’ll plot h, hm_a, and hm_b to get a sense of how the first two missing data examples compare to the original data. p1 &lt;- d %&gt;% ggplot(aes(x = s, y = h)) + geom_point(color = viridis_pal(option = &quot;C&quot;)(7)[7], alpha = 2/3) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;true distribution&quot;) p2 &lt;- d %&gt;% ggplot(aes(x = s, y = hm_a)) + geom_point(color = viridis_pal(option = &quot;C&quot;)(7)[6], alpha = 2/3) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;missing completely at random&quot;) p3 &lt;- d %&gt;% ggplot(aes(x = s, y = hm_b)) + geom_point(color = viridis_pal(option = &quot;C&quot;)(7)[6], alpha = 2/3) + scale_y_continuous(breaks = 1:10, limits = c(1, 10)) + labs(subtitle = &quot;missing conditional on s&quot;) p1 + p2 + p3 The left panel is the ideal situation letting us learn what we want to know, what is the effect of studying on the grade you’ll get on your homework (\\(S \\rightarrow H\\)). Once we enter in a missing data process (i.e., dogs \\(D\\) eating homework), we end up with \\(H^*\\), the homework left over after the dogs. Thus the homework outcomes we collect are a combination of the full set of homework and the hungry dogs. The middle panel depicts the scenario where the dogs eat the homework completely at random, \\(H \\rightarrow H^* \\leftarrow D\\). In the right panel, we consider a scenario where the dogs only and always eat the homework on the occasions the students studied more than average, \\(H \\rightarrow H^* \\leftarrow D \\leftarrow S\\). The situation in the third DAG is more complicated. Now homework is conditional on both studying and how noisy it is in a students home, \\(X\\). Also, our new variable \\(X\\) isn’t measured and whether the dogs eat the homework is also conditional on that unmeasured \\(X\\). Here’s the new data simulation. n &lt;- 1000 set.seed(501) d &lt;- tibble(x = rnorm(n, mean = 0, sd = 1), s = rnorm(n, mean = 0, sd = 1)) %&gt;% mutate(h = rbinom(n, size = 10, inv_logit_scaled(2 + s - 2 * x)), d = ifelse(x &gt; 1, 1, 0)) %&gt;% mutate(hm = ifelse(d == 1, NA, h)) d ## # A tibble: 1,000 x 5 ## x s h d hm ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.577 1.15 10 0 10 ## 2 0.617 -0.786 7 0 7 ## 3 0.452 0.958 9 0 9 ## 4 0.226 0.754 8 0 8 ## 5 -0.845 0.689 10 0 10 ## 6 -1.43 0.176 10 0 10 ## 7 -1.65 0.280 10 0 10 ## 8 0.0356 -0.397 8 0 8 ## 9 0.184 0.261 7 0 7 ## 10 1.22 -1.01 2 1 NA ## # … with 990 more rows Those data look like this. p1 &lt;- d %&gt;% ggplot(aes(x = s, y = h)) + geom_point(color = viridis_pal(option = &quot;C&quot;)(7)[7], alpha = 1/4) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;true distribution&quot;) p2 &lt;- d %&gt;% ggplot(aes(x = s, y = hm)) + geom_point(color = viridis_pal(option = &quot;C&quot;)(7)[6], alpha = 1/4) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;missing conditional on x&quot;) p1 + p2 Fit the model using the data with no missingness. b15.3 &lt;- brm(data = d, family = binomial, h | trials(10) ~ 1 + s, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 15, file = &quot;fits/b15.03&quot;) Check the results. print(b15.3) ## Family: binomial ## Links: mu = logit ## Formula: h | trials(10) ~ 1 + s ## Data: d (Number of observations: 1000) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.11 0.02 1.06 1.16 1.00 2839 2619 ## s 0.69 0.03 0.64 0.74 1.00 2702 2456 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Since this is not the data-generating model, we shouldn’t be all that surprised the coefficient for s is off (it should be 1). Because this is an example of where we didn’t collect data on \\(X\\), we can think of our incorrect results as a case of omitted variable bias. Here’s what happens when we run the model on hm, the homework variable after the hungry dogs got to it. b15.4 &lt;- brm(data = d %&gt;% filter(d == 0), family = binomial, h | trials(10) ~ 1 + s, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 15, file = &quot;fits/b15.04&quot;) Check the results. print(b15.4) ## Family: binomial ## Links: mu = logit ## Formula: h | trials(10) ~ 1 + s ## Data: d %&gt;% filter(d == 0) (Number of observations: 820) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 1.80 0.03 1.73 1.86 1.00 2064 2123 ## s 0.83 0.03 0.76 0.89 1.00 1845 2386 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Interestingly, both the intercept and the coefficient for s are now less biased. Because both \\(H\\) and \\(D\\) are conditional on \\(X\\), omitting cases based on \\(X\\) resulted in a model that conditional on \\(X\\), even though \\(X\\) wasn’t directly in the statistical model. This won’t always be the case. Consider what happens when we have a different missing data mechanism. d &lt;- d %&gt;% mutate(d = ifelse(abs(x) &lt; 1, 1, 0)) %&gt;% mutate(hm = ifelse(d == 1, NA, h)) d ## # A tibble: 1,000 x 5 ## x s h d hm ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.577 1.15 10 1 NA ## 2 0.617 -0.786 7 1 NA ## 3 0.452 0.958 9 1 NA ## 4 0.226 0.754 8 1 NA ## 5 -0.845 0.689 10 1 NA ## 6 -1.43 0.176 10 0 10 ## 7 -1.65 0.280 10 0 10 ## 8 0.0356 -0.397 8 1 NA ## 9 0.184 0.261 7 1 NA ## 10 1.22 -1.01 2 0 2 ## # … with 990 more rows Here’s what then updated data look like. p1 &lt;- d %&gt;% ggplot(aes(x = s, y = h)) + geom_point(color = viridis_pal(option = &quot;C&quot;)(7)[7], alpha = 1/4) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;true distribution&quot;) p2 &lt;- d %&gt;% ggplot(aes(x = s, y = hm)) + geom_point(color = viridis_pal(option = &quot;C&quot;)(7)[6], alpha = 1/4) + scale_y_continuous(breaks = 1:10) + labs(subtitle = &quot;missing conditional on x&quot;) p1 + p2 McElreath didn’t fit this model in the text, but he encouraged us to do so on our own (p. 503). Here it is. b15.4b &lt;- brm(data = d %&gt;% filter(d == 0), family = binomial, h | trials(10) ~ 1 + s, prior = c(prior(normal(0, 1), class = Intercept), prior(normal(0, 0.5), class = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 15, file = &quot;fits/b15.04b&quot;) print(b15.4b) ## Family: binomial ## Links: mu = logit ## Formula: h | trials(10) ~ 1 + s ## Data: d %&gt;% filter(d == 0) (Number of observations: 307) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.34 0.04 0.27 0.42 1.00 3312 2749 ## s 0.49 0.04 0.41 0.57 1.00 3664 2767 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Yep, “now missingness makes things worse” (p. 503). 15.2.1.1 Rethinking: Naming completely at random. McElreath briefly mentioned the terms missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). I share his sentiments; these terms are awful. However, they’re peppered throughout the missing data literature and I recommend you familiarize yourself with them. In his endnote #227, McElreath pointed readers to the authoritative work of Donald B. Rubin (1976) and Little and Rubin (2019, though he referenced the second edition, whereas I’m referencing the third). Baraldi &amp; Enders (2010) is a nice primer, too. Also, the great Donald Rubin has several lectures available online. Here’s a link to a talk on causal inference, which includes bits of insights into missing data analysis and lots of historical tidbits, too. 15.2.2 Imputing primates. We return to the milk data. data(milk, package = &quot;rethinking&quot;) d &lt;- milk rm(milk) # transform d &lt;- d %&gt;% mutate(neocortex.prop = neocortex.perc / 100, logmass = log(mass)) %&gt;% mutate(k = (kcal.per.g - mean(kcal.per.g)) / sd(kcal.per.g), b = (neocortex.prop - mean(neocortex.prop, na.rm = T)) / sd(neocortex.prop, na.rm = T), m = (logmass - mean(logmass)) / sd(logmass)) Note how we set na.rm = T within the mean() and sd() functions when computing b. See what happens if you leave that part out. As hinted at above and explicated in the text, we’re missing 12 values for neocortex.prop. d %&gt;% count(is.na(neocortex.prop)) ## is.na(neocortex.prop) n ## 1 FALSE 17 ## 2 TRUE 12 We dropped those values when we fit the models back in Chapter 5. To get a sense of whether this was a bad idea, let’s consider the model with a DAG. Ignoring the missing data, we have this. dag_coords &lt;- tibble(name = c(&quot;M&quot;, &quot;U&quot;, &quot;K&quot;, &quot;B&quot;), x = c(1, 2, 2, 3), y = c(2, 2, 1, 2)) dagify(M ~ U, B ~ U, K ~ M + B, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name == &quot;U&quot;, &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text() + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) + dark_theme_void() “\\(M\\) is body mass, \\(B\\) is neocortex percent, \\(K\\) is milk energy, and \\(U\\) is some unobserved variable that renders \\(M\\) and \\(B\\) positively correlated” (p. 504). Because we have missingness in \\(B\\), our data in hand are actually \\(B^*\\). McElreath considered three processes that may have generated these missing data. Here are the DAGs. dag_coords &lt;- tibble(name = c(&quot;M&quot;, &quot;U&quot;, &quot;K&quot;, &quot;B&quot;, &quot;RB&quot;, &quot;Bs&quot;), x = c(1, 2, 2, 3, 2, 3), y = c(2, 2, 1, 2, 3, 3)) # left p1 &lt;- dagify(M ~ U, B ~ U, K ~ M + B, Bs ~ RB + B, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;U&quot;, &quot;B&quot;), &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;B&quot;, &quot;M&quot;, expression(R[B]), &quot;U&quot;, expression(B^&#39;*&#39;), &quot;K&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) # middle p2 &lt;- dagify(M ~ U, B ~ U, K ~ M + B, Bs ~ RB + B, RB ~ M, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;U&quot;, &quot;B&quot;), &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;B&quot;, &quot;M&quot;, expression(R[B]), &quot;U&quot;, expression(B^&#39;*&#39;), &quot;K&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) # right p3 &lt;- dagify(M ~ U, B ~ U, K ~ M + B, Bs ~ RB + B, RB ~ B, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;U&quot;, &quot;B&quot;), &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;B&quot;, &quot;M&quot;, expression(R[B]), &quot;U&quot;, expression(B^&#39;*&#39;), &quot;K&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) # combine! (p1 + p2 + p3) &amp; scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) &amp; dark_theme_void() &amp; theme(panel.background = element_rect(fill = &quot;black&quot;), plot.background = element_rect(fill = &quot;grey8&quot;, color = &quot;grey8&quot;), plot.margin = margin(0.1, 0.1, 0.1, 0.1, &quot;in&quot;)) In each of the DAGs, the new variable \\(R_B\\) simply indicates whether a given species has missingness in \\(B^*\\), much like our dog variable \\(D\\) indicated the missing data in the DAGs from the earlier DAGs. The big difference between then and now is that whereas we had a sense of what was causing the missing data in the earlier examples (i.e., those hungry \\(D\\) dogs), now we only have a generic missing data mechanism, \\(R_B\\). In the middle of page 505, McElreath asked we consider one more missing data mechanism, this time with a new unmeasured causal variable \\(V\\). dag_coords &lt;- tibble(name = c(&quot;M&quot;, &quot;U&quot;, &quot;K&quot;, &quot;B&quot;, &quot;Bs&quot;, &quot;RB&quot;, &quot;V&quot;), x = c(1, 2, 2, 3, 4, 4, 3.4), y = c(2, 2, 1, 2, 2, 1, 1.45)) dagify(M ~ U, B ~ U + V, K ~ M + B, Bs ~ RB + B, RB ~ V, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name %in% c(&quot;U&quot;, &quot;B&quot;, &quot;V&quot;), &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;B&quot;, &quot;M&quot;, expression(R[B]), &quot;U&quot;, &quot;V&quot;, expression(B^&#39;*&#39;), &quot;K&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) + dark_theme_void() However, our statistical model will follow the form \\[\\begin{align*} K_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\color{#5D01A6FF}{B_i} + \\beta_2 \\log M_i \\\\ \\color{#5D01A6FF}{B_i} &amp; \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(\\nu, \\sigma_B)} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\color{#5D01A6FF}\\nu &amp; \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\ \\color{#5D01A6FF}{\\sigma_B} &amp; \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Exponential}(1)}, \\end{align*}\\] where we simply presume the missing values in \\(B_i\\), which was \\(B^*\\) in our DAGs, are unrelated to any of the other variables in the model. But those missing values in \\(B_i\\) values do get their own prior distribution, \\(\\operatorname{Normal}(\\nu, \\sigma_B)\\). If you look closely, you’ll discover the prior McElreath reported for \\(\\nu\\) \\([\\operatorname{Normal}(0.5, 1)]\\) does not match up with his rethinking::ulam() code in his R code block 15.17, \\(\\operatorname{Normal}(0, 0.5)\\). Here we use the latter. When writing a multivariate model in brms, I find it easier to save the model code by itself and then insert it into the brm() function. Otherwise, things start to feel cluttered. b_model &lt;- # here&#39;s the primary `k` model bf(k ~ 1 + mi(b) + m) + # here&#39;s the model for the missing `b` data bf(b | mi() ~ 1) + # here we set the residual correlations for the two models to zero set_rescor(FALSE) Note the mi(b) syntax in the k model. This indicates that the predictor, b, has missing values that are themselves being modeled. To get a sense of how to specify the priors for such a model in brms, use the get_prior() function. get_prior(data = d, family = gaussian, b_model) ## prior class coef group resp dpar nlpar bound source ## (flat) b default ## (flat) Intercept default ## student_t(3, 0.2, 2.5) Intercept b default ## student_t(3, 0, 2.5) sigma b default ## (flat) b k (vectorized) ## (flat) b m k (vectorized) ## (flat) b mib k (vectorized) ## student_t(3, -0.3, 2.5) Intercept k default ## student_t(3, 0, 2.5) sigma k default With the one-step Bayesian imputation procedure in brms, you might need to use the resp argument when specifying non-default priors. Now fit the model. b15.5 &lt;- brm(data = d, family = gaussian, b_model, # here we insert the model prior = c(prior(normal(0, 0.5), class = Intercept, resp = k), prior(normal(0, 0.5), class = Intercept, resp = b), prior(normal(0, 0.5), class = b, resp = k), prior(exponential(1), class = sigma, resp = k), prior(exponential(1), class = sigma, resp = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 15, file = &quot;fits/b15.05&quot;) With a model like this, print() only gives up part of the picture. print(b15.5) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: k ~ 1 + mi(b) + m ## b | mi() ~ 1 ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## k_Intercept 0.03 0.16 -0.31 0.33 1.00 3841 3213 ## b_Intercept -0.05 0.21 -0.46 0.37 1.00 3503 2561 ## k_m -0.55 0.21 -0.93 -0.13 1.00 1846 2497 ## k_mib 0.50 0.24 -0.00 0.94 1.00 1553 2294 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_k 0.84 0.15 0.60 1.16 1.00 1951 2629 ## sigma_b 1.01 0.17 0.74 1.43 1.00 2185 2374 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Note that for the parameters summarized in the ‘Population-Level Effects:’ section, the criterion is indexed in the prefix. The parameters in the ‘Family Specific Parameters:’ however, have the criteria indexed in the suffix. I don’t know why. Anyway, we can get a summary of the imputed values with posterior_summary(). posterior_summary(b15.5) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_k_Intercept 0.03 0.16 -0.31 0.33 ## b_b_Intercept -0.05 0.21 -0.46 0.37 ## b_k_m -0.55 0.21 -0.93 -0.13 ## bsp_k_mib 0.50 0.24 0.00 0.94 ## sigma_k 0.84 0.15 0.60 1.16 ## sigma_b 1.01 0.17 0.74 1.43 ## Ymi_b[2] -0.57 0.93 -2.37 1.28 ## Ymi_b[3] -0.69 0.92 -2.48 1.15 ## Ymi_b[4] -0.70 0.95 -2.52 1.29 ## Ymi_b[5] -0.31 0.91 -2.08 1.55 ## Ymi_b[9] 0.47 0.91 -1.26 2.20 ## Ymi_b[14] -0.18 0.88 -1.86 1.57 ## Ymi_b[15] 0.20 0.87 -1.53 1.92 ## Ymi_b[17] 0.27 0.89 -1.55 2.05 ## Ymi_b[19] 0.53 0.90 -1.33 2.23 ## Ymi_b[21] -0.46 0.91 -2.24 1.44 ## Ymi_b[23] -0.30 0.88 -2.05 1.41 ## Ymi_b[26] 0.15 0.92 -1.67 1.93 ## lp__ -81.18 4.00 -90.23 -74.73 The imputed b values are indexed by occasion number from the original data. This is in contrast with McElreath’s precis() output, which simply serially indexes the missing values as B_impute[1], B_impute[2], and so on. Before we move on to the next model, let’s plot to get a sense of what we’ve done. posterior_samples(b15.5) %&gt;% select(starts_with(&quot;Ymi_b&quot;)) %&gt;% set_names(filter(d, is.na(b)) %&gt;% pull(species)) %&gt;% pivot_longer(everything(), names_to = &quot;species&quot;) %&gt;% ggplot(aes(x = value, y = reorder(species, value))) + stat_slab(fill = viridis_pal(option = &quot;C&quot;)(7)[4], alpha = 3/4, height = 1.5, slab_color = &quot;black&quot;, slab_size = 1/4) + labs(x = &quot;imputed values for b&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) Here’s the model that drops the cases with NAs on b. b15.6 &lt;- brm(data = d, family = gaussian, k ~ 1 + b + m, prior = c(prior(normal(0, 0.5), class = Intercept), prior(normal(0, 0.5), class = b), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 15, file = &quot;fits/b15.06&quot;) If you run this on your computer, you’ll notice the following message at the top: “Rows containing NAs were excluded from the model.” This time print() gives us the same basic summary information as posterior_summary(). print(b15.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: k ~ 1 + b + m ## Data: d (Number of observations: 17) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.11 0.20 -0.28 0.50 1.00 3093 2745 ## b 0.60 0.29 -0.01 1.11 1.00 2125 2191 ## m -0.64 0.26 -1.11 -0.10 1.00 2173 1715 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.87 0.18 0.60 1.31 1.00 2440 2291 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We can’t use McElreath’s plot(coeftab()) trick with our brms output, but we can still get by. # wrangle bind_rows(fixef(b15.5) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;), fixef(b15.6) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;)) %&gt;% slice(c(4:3, 6:7)) %&gt;% mutate(term = str_c(&quot;beta[&quot;, c(1:2, 1:2), &quot;]&quot;), fit = rep(c(&quot;b15.5&quot;, &quot;b15.6&quot;), each = n() / 2)) %&gt;% # plot! ggplot(aes(x = Estimate, y = fit)) + geom_vline(xintercept = 0, linetype = 3, alpha = 1/2) + geom_pointrange(aes(xmin = Q2.5, xmax = Q97.5)) + labs(x = &quot;marginal posterior&quot;, y = NULL) + theme(axis.ticks.y = element_blank(), strip.background = element_rect(color = &quot;transparent&quot;, fill = &quot;transparent&quot;)) + facet_wrap(~ term, labeller = label_parsed, ncol = 1) The model using Bayesian imputation (b15.5) used more information, resulting in narrower marginal posteriors for \\(\\beta_1\\) and \\(\\beta_2\\). Because it wasted perfectly good information, the conventional b15.6 model was less certain. In order to make our version of Figure 15.5, we’ll want to add the summary values for the imputed b data from b15.1 to the primary data file d. d &lt;- d %&gt;% mutate(row = 1:n()) %&gt;% left_join( posterior_summary(b15.5) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% mutate(row = str_extract(term, &quot;(\\\\d)+&quot;) %&gt;% as.integer()), by = &quot;row&quot; ) d %&gt;% select(species, k:Q97.5) %&gt;% glimpse() ## Rows: 29 ## Columns: 10 ## $ species &lt;fct&gt; Eulemur fulvus, E macaco, E mongoz, E rubriventer, Lemur catta, Alouatta seniculus, A pall… ## $ k &lt;dbl&gt; -0.9400408, -0.8161263, -1.1259125, -1.0019980, -0.2585112, -1.0639553, -0.5063402, 1.5382… ## $ b &lt;dbl&gt; -2.080196025, NA, NA, NA, NA, -0.508641289, -0.508641289, 0.010742472, NA, 0.213469683, -1… ## $ m &lt;dbl&gt; -0.4558357, -0.4150024, -0.3071581, -0.5650254, -0.3874772, 0.1274408, 0.1407505, -0.30715… ## $ row &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,… ## $ term &lt;chr&gt; NA, &quot;Ymi_b[2]&quot;, &quot;Ymi_b[3]&quot;, &quot;Ymi_b[4]&quot;, &quot;Ymi_b[5]&quot;, NA, NA, NA, &quot;Ymi_b[9]&quot;, NA, NA, NA, NA… ## $ Estimate &lt;dbl&gt; NA, -0.5749477, -0.6944976, -0.6971893, -0.3109926, NA, NA, NA, 0.4672010, NA, NA, NA, NA,… ## $ Est.Error &lt;dbl&gt; NA, 0.9324091, 0.9228088, 0.9491488, 0.9071429, NA, NA, NA, 0.9053457, NA, NA, NA, NA, 0.8… ## $ Q2.5 &lt;dbl&gt; NA, -2.368005, -2.483320, -2.519301, -2.078711, NA, NA, NA, -1.264940, NA, NA, NA, NA, -1.… ## $ Q97.5 &lt;dbl&gt; NA, 1.277546, 1.151146, 1.289826, 1.545820, NA, NA, NA, 2.204905, NA, NA, NA, NA, 1.569355… Now make Figure 15.5. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[4] # left p1 &lt;- d %&gt;% ggplot(aes(y = k)) + geom_point(aes(x = b), color = color) + geom_pointrange(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5), shape = 1, size = 1/4, fatten = 8, stroke = 1/4) + labs(x = &quot;neocortex percent (std)&quot;, y = &quot;kcal milk (std)&quot;) + coord_cartesian(xlim = range(d$b, na.rm = T)) # right p2 &lt;- d %&gt;% ggplot(aes(x = m)) + geom_point(aes(y = b), color = color) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), shape = 1, size = 1/4, fatten = 8, stroke = 1/4) + labs(x = &quot;log body mass (std)&quot;, y = &quot;neocortex percent (std)&quot;) + coord_cartesian(ylim = range(d$b, na.rm = T)) # combine and plot! p1 + p2 “We can improve this model by changing the imputation model to estimate the relationship between the two predictors” (p. 509). In the text, McElreath accomplished this with the model \\[\\begin{align*} K_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\color{#5D01A6FF}{B_i} + \\beta_2 \\log M_i \\\\ \\color{#5D01A6FF}{\\begin{bmatrix} M_i \\\\ B_i \\end{bmatrix}} &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} \\mu_M \\\\\\mu_B \\end{bmatrix}, \\mathbf \\Sigma \\end{pmatrix}} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\color{#5D01A6FF}{\\mu_M} &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\ \\color{#5D01A6FF}{\\mu_B} &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\ \\color{#5D01A6FF}{\\mathbf \\Sigma} &amp; \\color{#5D01A6FF} = \\color{#5D01A6FF}{\\operatorname{\\mathbf S \\mathbf R \\mathbf S}} \\\\ \\color{#5D01A6FF}{\\mathbf S} &amp; \\color{#5D01A6FF} = \\color{#5D01A6FF}{\\begin{bmatrix} \\sigma_M &amp; 0 \\\\ 0 &amp; \\sigma_B \\end{bmatrix}} \\\\ \\color{#5D01A6FF}{\\mathbf R} &amp; \\color{#5D01A6FF} = \\color{#5D01A6FF}{\\begin{bmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{bmatrix}} \\\\ \\color{#5D01A6FF}{\\sigma_M} &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Exponential}(1)} \\\\ \\color{#5D01A6FF}{\\sigma_B} &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Exponential}(1)} \\\\ \\color{#5D01A6FF} \\rho &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{LKJ}(2)}, \\end{align*}\\] which expresses the relationship between the two predictors with a residual correlation matrix, \\(\\mathbf \\Sigma\\). Importantly, though \\(\\mathbf \\Sigma\\) involves the variables \\(B_i\\) and \\(M_i\\), it does not directly involve the criterion, \\(K_i\\). As it turns out, the current version of brms cannot handle a model of this form. When you fit multivariate models with residual correlations, you have to set them for either all variables or none of them. For a little more on this topic, you can skim through the Brms and heterogeneous residual covariance - equivalent of “at.level” function thread on the Stan Forums. Bürkner’s response to the initial question indicated this kind of model will be available in brms version 3.0+, which I suspect will entail a substantial reworking of the multivariate syntax. Until then, we can fit the alternative model \\[\\begin{align*} K_i &amp; \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\alpha + \\beta_1 \\color{#5D01A6FF}{B_i} + \\beta_2 \\log M_i \\\\ \\color{#5D01A6FF}{B_i} &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(\\nu_i, \\sigma_B)} \\\\ \\color{#5D01A6FF}{\\nu_i} &amp; \\color{#5D01A6FF} = \\color{#5D01A6FF}{\\gamma + \\delta_1 \\log M_i} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_1 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\beta_2 &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\color{#5D01A6FF}\\gamma &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\ \\color{#5D01A6FF}{\\delta_1} &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\ \\color{#5D01A6FF}{\\sigma_B} &amp; \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Exponential}(1)}, \\end{align*}\\] which captures the relation among the two predictors as a regression of \\(M_i\\) predicting \\(B_i\\). Here’s how to fit the model with brms. b_model &lt;- mvbf(bf(k ~ 1 + mi(b) + m), bf(b | mi() ~ 1 + m), rescor = FALSE) b15.7 &lt;- brm(data = d, family = gaussian, b_model, prior = c(prior(normal(0, 0.5), class = Intercept, resp = k), prior(normal(0, 0.5), class = Intercept, resp = b), prior(normal(0, 0.5), class = b, resp = k), prior(normal(0, 0.5), class = b, resp = b), prior(exponential(1), class = sigma, resp = k), prior(exponential(1), class = sigma, resp = b)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 15, file = &quot;fits/b15.07&quot;) Let’s see what we did. print(b15.7) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: k ~ 1 + mi(b) + m ## b | mi() ~ 1 + m ## Data: d (Number of observations: 29) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## k_Intercept 0.03 0.16 -0.29 0.33 1.00 4147 3012 ## b_Intercept -0.05 0.16 -0.37 0.26 1.00 3735 3243 ## k_m -0.65 0.23 -1.08 -0.20 1.00 2084 2911 ## b_m 0.60 0.15 0.30 0.88 1.00 4515 3473 ## k_mib 0.59 0.26 0.03 1.07 1.00 1765 2708 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_k 0.83 0.14 0.60 1.12 1.00 2317 2776 ## sigma_b 0.71 0.13 0.51 0.99 1.00 1888 2626 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We have two intercepts, k_Intercept (\\(\\alpha\\)) and b_Intercept (\\(\\gamma\\)). Both are near zero because both k and b are standardized. Our k_mib (\\(\\beta_1\\)) and k_m (\\(\\beta_2\\)) parameters correspond with McElreath’s bB and bM parameters, respectively. Our summaries for them are very similar to his. Notice our summary for b_m (\\(\\delta_1\\)). McElreath doesn’t have a parameter exactly like that, but his close analogue is Rho_BM[1,2] (also Rho_BM[2,1], which is really the same thing). Also, notice how our summary for b_m is almost the same as the summary for McElreath’s Rho_BM[1,2]. This is because a univariable regression coefficient between two standardized variables is in the same metric as a correlation and McElreath’s Rho_BM[1,2] is just that–a correlation. If you fit McElreath’s model with rethinking and execute precis(m15.7, depth = 3), you’ll see that our sigma_k (\\(\\sigma\\)) summary corresponds nicely with his summary for sigma. However, our sigma_b (\\(\\sigma_B\\)) is not the same as his Sigma_BM[2]. Why? Because whereas our sigma_b is a residual standard deviation after accounting for the effect of m on b, McElreath’s Sigma_BM[2] is just an estimate of the standard deviation of b. Also, notice that whereas McElreath’s output has a Sigma_BM[1] parameter, we have no direct analogue. Why? Because although m is a predictor variable for both k and b, we did not give it its own likelihood. McElreath, in contrast, entered m into the bivariate likelihood with b. Our workflow for Figure 15.6 is largely the same as for Figure 15.5. The biggest difference is we need to remove the columns term through upper from our earlier model before we can replace them with those from the current model. After that, it’s basically cut and paste. d &lt;- d %&gt;% select(-(term:Q97.5)) %&gt;% left_join( posterior_summary(b15.7) %&gt;% data.frame() %&gt;% rownames_to_column(&quot;term&quot;) %&gt;% filter(str_detect(term, &quot;Ymi&quot;)) %&gt;% mutate(row = str_extract(term, &quot;(\\\\d)+&quot;) %&gt;% as.integer()), by = &quot;row&quot; ) d %&gt;% select(species, k:Q97.5) %&gt;% glimpse() ## Rows: 29 ## Columns: 10 ## $ species &lt;fct&gt; Eulemur fulvus, E macaco, E mongoz, E rubriventer, Lemur catta, Alouatta seniculus, A pall… ## $ k &lt;dbl&gt; -0.9400408, -0.8161263, -1.1259125, -1.0019980, -0.2585112, -1.0639553, -0.5063402, 1.5382… ## $ b &lt;dbl&gt; -2.080196025, NA, NA, NA, NA, -0.508641289, -0.508641289, 0.010742472, NA, 0.213469683, -1… ## $ m &lt;dbl&gt; -0.4558357, -0.4150024, -0.3071581, -0.5650254, -0.3874772, 0.1274408, 0.1407505, -0.30715… ## $ row &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,… ## $ term &lt;chr&gt; NA, &quot;Ymi_b[2]&quot;, &quot;Ymi_b[3]&quot;, &quot;Ymi_b[4]&quot;, &quot;Ymi_b[5]&quot;, NA, NA, NA, &quot;Ymi_b[9]&quot;, NA, NA, NA, NA… ## $ Estimate &lt;dbl&gt; NA, -0.59675787, -0.64115105, -0.77746004, -0.39920411, NA, NA, NA, -0.22797476, NA, NA, N… ## $ Est.Error &lt;dbl&gt; NA, 0.6844536, 0.6769265, 0.6791844, 0.6573925, NA, NA, NA, 0.6922921, NA, NA, NA, NA, 0.6… ## $ Q2.5 &lt;dbl&gt; NA, -1.9581199, -1.9841119, -2.1180278, -1.6832233, NA, NA, NA, -1.6138249, NA, NA, NA, NA… ## $ Q97.5 &lt;dbl&gt; NA, 0.7762222, 0.7100079, 0.6052781, 0.9105329, NA, NA, NA, 1.1177218, NA, NA, NA, NA, 0.5… Now make Figure 15.6. color &lt;- viridis_pal(option = &quot;D&quot;)(7)[3] p1 &lt;- d %&gt;% ggplot(aes(y = k)) + geom_point(aes(x = b), color = color) + geom_pointrange(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5), shape = 1, size = 1/4, fatten = 8, stroke = 1/4) + labs(x = &quot;neocortex percent (std)&quot;, y = &quot;kcal milk (std)&quot;) + coord_cartesian(xlim = range(d$b, na.rm = T)) p2 &lt;- d %&gt;% ggplot(aes(x = m)) + geom_point(aes(y = b), color = color) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), shape = 1, size = 1/4, fatten = 8, stroke = 1/4) + labs(x = &quot;log body mass (std)&quot;, y = &quot;neocortex percent (std)&quot;) + coord_cartesian(ylim = range(d$b, na.rm = T)) p1 + p2 The results further show that our fully standardized regression coefficient (\\(\\delta_1\\)) had the same effect on the Bayesian imputation as McElreath’s residual correlation. Our \\(\\delta_1\\) coefficient is basically just a correlation in disguise. 15.2.2.1 Rethinking: Multiple imputations. Missing data imputation has a messy history. There are many forms of imputation… A common non-Bayesian procedure is multiple imputation. Multiple imputation was developed in the context of survey non-response, and it actually has a Bayesian justification. But it was invented when Bayesian imputation on the desktop was impractical, so it tries to approximate the full Bayesian solution to a “missing at random” missingness model. If you aren’t comfortable dropping incomplete cases, then you shouldn’t be comfortable using multiple imputation either. The procedure performs multiple draws from an approximate posterior distribution of the missing values, performs separate analyses with these draws, and then combines the analyses in a way that approximates full Bayesian imputation. Multiple imputation is more limited than full Bayesian imputation, so now we just use the real thing. (p. 511) We won’t be walking through an example in this ebook, but you should know that brms is capable of multiple imputation, too. You can find an example of multiple imputation in Bürkner’s (2021g) vignette, Handle missing values with brms. To learn about the origins of this approach, check out the authoritative work by Rubin (1996, 1987) and Little and Rubin (2019). 15.2.3 Where is your god now? “Sometimes there are no statistical solutions to scientific problems. But even then, careful statistical thinking can be useful because it will tell us that there is no statistical solution” (p. 512). Let’s load the Moralizing_gods data from Whitehouse et al. (2019). data(Moralizing_gods, package = &quot;rethinking&quot;) d &lt;- Moralizing_gods rm(Moralizing_gods) Take a look at the new data. glimpse(d) ## Rows: 864 ## Columns: 5 ## $ polity &lt;fct&gt; Big Island Hawaii, Big Island Hawaii, Big Island Hawaii, Big Island Hawaii, Big Isla… ## $ year &lt;int&gt; 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, -600, -500, -400, -300, -200, … ## $ population &lt;dbl&gt; 3.729643, 3.729643, 3.598340, 4.026240, 4.311767, 4.205113, 4.373960, 5.157593, 4.99… ## $ moralizing_gods &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ writing &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … The bulk of the values for moralizing_gods are missing and very few of the remaining values are 0’s. d %&gt;% count(moralizing_gods) %&gt;% mutate(`%` = 100 * n / sum(n)) ## moralizing_gods n % ## 1 0 17 1.967593 ## 2 1 319 36.921296 ## 3 NA 528 61.111111 To get a sense of how these values are distributed, here’s our version of Figure 15.7. d %&gt;% mutate(mg = factor(ifelse(is.na(moralizing_gods), 2, 1 - moralizing_gods), levels = 0:2, labels = c(&quot;present&quot;, &quot;absent&quot;, &quot;unknown&quot;))) %&gt;% ggplot(aes(x = year, y = population, color = mg)) + geom_point(alpha = 2/3) + scale_color_manual(&quot;Moralizing gods&quot;, values = viridis_pal(option = &quot;D&quot;)(7)[c(7, 4, 1)]) + labs(subtitle = &#39;&quot;This is a highly non-random missingness pattern&quot; (p. 514).&#39;, x = &quot;Time (year)&quot;, y = &quot;Population size (log)&quot;) + theme(legend.position = c(.125, .67)) Here are the counts broken down by gods and literacy status. d %&gt;% mutate(gods = moralizing_gods, literacy = writing) %&gt;% count(gods, literacy) %&gt;% mutate(`%` = 100 * n / sum(n)) ## gods literacy n % ## 1 0 0 16 1.8518519 ## 2 0 1 1 0.1157407 ## 3 1 0 9 1.0416667 ## 4 1 1 310 35.8796296 ## 5 NA 0 442 51.1574074 ## 6 NA 1 86 9.9537037 The bulk of the missing moralizing_gods values are from non-literate polities and the figure above shows that smaller polities also tend to have missing values. We can try to make sense of all this with McElreath’s DAG. dag_coords &lt;- tibble(name = c(&quot;P&quot;, &quot;W&quot;, &quot;G&quot;, &quot;RG&quot;, &quot;Gs&quot;), x = c(1, 2.33, 4, 5.67, 7), y = c(2, 1, 2, 1, 2)) dagify(P ~ G, W ~ P, RG ~ W, Gs ~ G + RG, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name == &quot;G&quot;, &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;G&quot;, &quot;P&quot;, expression(R[G]), &quot;W&quot;, expression(G^&#39;*&#39;))) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) + dark_theme_void() Here \\(P\\) is rate of population growth (not the same as the population size variable in the data), \\(G\\) is the presence of belief in moralizing gods (which is unobserved), \\(G^*\\) is the observed variable with missing values, \\(W\\) is writing, and \\(R_G\\) is the missing values indicator. This is an optimistic scenario, because it assumes there are no unobserved confounds among \\(P\\), \\(G\\), and \\(W\\). These are purely observational data, recall. But the goal is to use this example to think through the impact of missing data. If we can’t recover from missing data with the DAG above, adding confounds isn’t going to help. (p. 515) Consider the case of Hawaii. d %&gt;% filter(polity == &quot;Big Island Hawaii&quot;) %&gt;% select(year, writing, moralizing_gods) ## year writing moralizing_gods ## 1 1000 0 NA ## 2 1100 0 NA ## 3 1200 0 NA ## 4 1300 0 NA ## 5 1400 0 NA ## 6 1500 0 NA ## 7 1600 0 NA ## 8 1700 0 NA ## 9 1800 0 1 What happened in 1778? Captain James Cook and his crew finally made contact…. After Captain Cook, Hawaii is correctly coded with 1 for belief in moralizing gods. It is also a fact that Hawaii never developed its own writing system. So there is no direct evidence of when moralizing gods appeared in Hawaii. Any imputation model needs to decide how to fill in those NA values. With so much missing data, any imputation model would necessarily make very strong assumptions. (pp. 515–516) 15.2.3.1 Rethinking: Present details about missing data. Clear documentation of missing data and its treatment is necessary. This is best done with a causal model that makes transparent what is being assumed about the source of missing values and simultaneously justifies how they are handled. But the minimum is to report the counts of missing values in each variable and what was done with them. (p. 516, emphasis added) I strongly agree with McElreath on this point. When doing peer-reviews, I regularly inquire about missing data and how, if present, they were handled. 15.3 Categorical errors and discrete absences Discrete unobserved variables require discrete parameters. There are two issues with discrete parameters. First, a discrete variable will not produce a smooth surface for Hamiltonian Monte Carlo to glide around on. HMC just doesn’t do discrete variables. Second, other estimation approaches also have problems with discrete parameter spaces, because discrete jumps are difficult to calibrate. Chains tend to get stuck for long periods. But that doesn’t mean we are stuck. In almost every case, we don’t need to sample discrete parameters at all. Instead we can use a special technique, known to experts as a “weighted average,” to remove discrete parameters from the model. After sampling the other parameters, we can then use their samples to compute the posterior distribution of any dis- crete parameter that we removed. So no information is given up. (pp. 516–517) 15.3.1 Discrete cats. Here is McElreath’s next DAG. dag_coords &lt;- tibble(name = c(&quot;Rc&quot;, &quot;Cs&quot;, &quot;C&quot;, &quot;N&quot;), x = 1:4, y = 1) dagify(Cs ~ Rc + C, N ~ C, coords = dag_coords) %&gt;% tidy_dagitty() %&gt;% mutate(color = ifelse(name == &quot;C&quot;, &quot;a&quot;, &quot;b&quot;)) %&gt;% ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + geom_dag_point(aes(color = color), size = 7, show.legend = F) + geom_dag_text(parse = T, label = c(&quot;C&quot;,expression(R[C]), expression(C^&quot;*&quot;), &quot;N&quot;)) + geom_dag_edges(edge_colour = &quot;#FCF9F0&quot;) + scale_color_manual(values = c(viridis_pal(option = &quot;C&quot;)(7)[2], &quot;black&quot;)) + dark_theme_void() “The presence/absence of a cat \\(C\\) influences the number of sung notes \\(N\\). Because of missing values \\(R_C\\) however, we only observe \\(C^*\\)” (p. 517). McElreath’s proposed generative model was \\[\\begin{align*} N_i &amp; \\sim \\operatorname{Poisson}(\\lambda_i) \\\\ \\log \\lambda_i &amp; = \\alpha + \\beta C_i \\\\ C_i &amp; \\sim \\operatorname{Bernoulli}(k) \\\\ R_{C, i} &amp; \\sim \\operatorname{Bernoulli}(r). \\end{align*}\\] We can simulate data along those lines with a few lines of code. set.seed(9) n_houses &lt;- 1000L alpha &lt;- 5 beta &lt;- (-3) k &lt;- 0.5 r &lt;- 0.2 dat &lt;- tibble(cat = rbinom(n_houses, size = 1, prob = k)) %&gt;% mutate(notes = rpois(n_houses, lambda = alpha + beta * cat), r_c = rbinom(n_houses, size = 1, prob = r)) %&gt;% mutate(cat_obs = if_else(r_c == 1, (-9L), cat)) glimpse(dat) ## Rows: 1,000 ## Columns: 4 ## $ cat &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0,… ## $ notes &lt;int&gt; 3, 5, 11, 6, 9, 6, 6, 6, 1, 3, 4, 6, 3, 7, 6, 3, 6, 3, 4, 4, 2, 0, 4, 7, 3, 1, 7, 8, 2, 2, 5… ## $ r_c &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,… ## $ cat_obs &lt;int&gt; 0, 0, 0, 0, 0, 0, -9, 0, 1, 1, 0, -9, 1, -9, 0, 1, 0, 1, 0, 0, 1, -9, -9, -9, -9, -9, 0, -9,… Note how we constructed cat_obs by replacing several values of cat with -9. There is nothing special about this value. The model will skip them. But it is usually good to use some invalid value, so that if you make a mistake in coding, an error will result. In this case, since cat has a Bernoulli distribution, if the model ever asks for the probability of observing -9, there should be an error, because -9 is impossible. (p. 518) 😢 Sadly, this is as far as I’m going in this section. I still haven’t made sense of McElreath’s weighted average approach to missing data and I’m not sure whether it’s even possible with brms. If you have insights in to how one might accommodate categorical missing data with brms, share them with the rest of us on GitHub. 15.4 Summary This chapter has been a quick introduction to the design and implementation of measurement error and missing data models. Measurement error and missing data have causes. Incorporating those causes into the generative model helps us decide how error and missingness impact inference as well as how to design a statistical procedure. (p. 521) If modern missing data methods are new to you, you might also check out van Burren’s great online (2018) text, Flexible imputation of missing data. I’m also a fan of Enders’s (2010) Applied missing data analysis, for which you can find a free sample chapter here. 15.5 Bonus: Bayesian meta-analysis with odds ratios If your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use brms::brm() to fit Bayesian meta-analyses, too. Before we proceed, I should acknowledge that this section is heavily influenced by Matti Vourre’s great blog post, Meta-analysis is a special case of Bayesian multilevel modeling. Since neither editions of McElreath’s text directly address meta-analyses, we’ll also have to borrow a bit from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s (2013) Bayesian data analysis, Third edition. 15.5.1 How do meta-analyses fit into the picture? Let Gelman and colleagues introduce the topic: Discussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected a priori to have predictable effects favoring one study over another…. this third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis…. The first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125–126, emphasis in the original) The basic version of a Bayesian meta-analysis follows the form \\[y_j \\sim \\operatorname{Normal}(\\theta_j, \\sigma_j),\\] where \\(y_j\\) = the point estimate for the effect size of a single study, \\(j\\), which is presumed to have been a draw from a Normal distribution centered on \\(\\theta_j\\). The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. The standard error from study \\(j\\) is specified \\(\\sigma_j\\), which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating \\(\\sigma_j\\), here. Those values we take directly from the original studies. Building on the model, we further presume that study \\(j\\) is itself just one draw from a population of related studies, each of which have their own effect sizes. As such, we presume \\(\\theta_j\\) itself has a distribution following the form \\[\\theta_j \\sim \\operatorname{Normal}(\\mu, \\tau),\\] where \\(\\mu\\) is the meta-analytic effect (i.e., the population mean) and \\(\\tau\\) is the variation around that mean, what you might also think of as \\(\\sigma_\\tau\\). 15.5.2 We need some data. Our data in this section come from the second large-scale replication project by the Many Labs team (Klein et al., 2018). Of the 28 studies replicated in the study, we will focus on the replication of the trolley experiment from Hauser et al. (2007). Here’s how the study was described by Klein and colleagues: According to the principle of double effect, an act that harms other people is more morally permissible if the act is a foreseen side effect rather than the means to the greater good. Hauser et al. (2007) compared participants’ reactions to two scenarios to test whether their judgments followed this principle. In the foreseen-side-effect scenario, a person on an out-of-control train changed the train’s trajectory so that the train killed one person instead of five. In the greater-good scenario, a person pushed a fat man in front of a train, killing him, to save five people. Whereas \\(89\\%\\) of participants judged the action in the foreseen-side-effect scenario as permissible \\((95 \\% \\; \\text{CI} = [87\\%, 91\\%]),\\) only \\(11\\%\\) of participants in the greater-good scenario judged it as permissible \\((95 \\% \\; \\text{CI} = [9\\%, 13\\%])\\). The difference between the percentages was significant\\(,\\) \\(\\chi^2(1, N = 2,646) = 1,615.96,\\) \\(p &lt; .001,\\) \\(w = .78,\\) \\(d = 2.50,\\) \\(95 \\% \\; \\text{CI} = [2.22, 2.86]\\). Thus, the results provided evidence for the principle of double effect. (p. 459, emphasis in the original) You can find supporting materials for the replication project on the Open Science Framework at https://osf.io/8cd4r/. The relevant subset of the data for the replication of Hauser et al. come from the Trolley Dilemma 1 (Hauser et al., 2007) folder within the OSFdata.zip (https://osf.io/ag2pd/). I’ve downloaded the file and saved it on GitHub. Here we load the data. d &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/master/data/Hauser_1_study_by_order_all_CLEAN_CASE.csv&quot;) d &lt;- d %&gt;% mutate(y = ifelse(variable == &quot;Yes&quot;, 1, 0), loc = factor(Location, levels = distinct(d, Location) %&gt;% pull(Location), labels = 1:59)) glimpse(d) ## Rows: 6,842 ## Columns: 29 ## $ uID &lt;dbl&gt; 65, 68, 102, 126, 145, 263, 267, 298, 309, 318, 350, 356, 376, 431, 438, 447, 483, … ## $ variable &lt;chr&gt; &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;… ## $ factor &lt;chr&gt; &quot;SideEffect&quot;, &quot;SideEffect&quot;, &quot;SideEffect&quot;, &quot;SideEffect&quot;, &quot;SideEffect&quot;, &quot;SideEffect&quot;,… ## $ .id &lt;chr&gt; &quot;ML2_Slate1_Brazil__Portuguese_execution_illegal_r.csv&quot;, &quot;ML2_Slate1_Brazil__Portug… ## $ source &lt;chr&gt; &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;wilfredlaur&quot;, &quot;wilfredlaur&quot;, &quot;ubc&quot;, &quot;ubc&quot;, &quot;to… ## $ haus1.1 &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2,… ## $ haus1.1t_1 &lt;dbl&gt; 39.054, 36.792, 56.493, 21.908, 25.635, 50.633, 58.661, 50.137, 51.717, 28.122, 41.… ## $ haus2.1 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ haus2.1t_1 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ Source.Global &lt;chr&gt; &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;wilfredlaur&quot;, &quot;wilfredlaur&quot;, &quot;ubc&quot;, &quot;ubc&quot;, &quot;to… ## $ Source.Primary &lt;chr&gt; &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;wilfredlaur&quot;, &quot;wilfredlaur&quot;, &quot;ubc&quot;, &quot;ubc&quot;, &quot;to… ## $ Source.Secondary &lt;chr&gt; &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;brasilia&quot;, &quot;wilfredlaur&quot;, &quot;wilfredlaur&quot;, &quot;ubc&quot;, &quot;ubc&quot;, &quot;to… ## $ Country &lt;chr&gt; &quot;Brazil&quot;, &quot;Brazil&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Canada&quot;, &quot;Ca… ## $ Location &lt;chr&gt; &quot;Social and Work Psychology Department, University of Brasilia, DF, Brazil&quot;, &quot;Socia… ## $ Language &lt;chr&gt; &quot;Portuguese&quot;, &quot;Portuguese&quot;, &quot;Portuguese&quot;, &quot;English&quot;, &quot;English&quot;, &quot;English&quot;, &quot;English… ## $ Weird &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ Execution &lt;chr&gt; &quot;illegal&quot;, &quot;illegal&quot;, &quot;illegal&quot;, &quot;illegal&quot;, &quot;illegal&quot;, &quot;illegal&quot;, &quot;illegal&quot;, &quot;illeg… ## $ SubjectPool &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Y… ## $ Setting &lt;chr&gt; &quot;In a classroom&quot;, &quot;In a classroom&quot;, &quot;In a classroom&quot;, &quot;In a lab&quot;, &quot;In a lab&quot;, &quot;In a… ## $ Tablet &lt;chr&gt; &quot;Computers&quot;, &quot;Computers&quot;, &quot;Computers&quot;, &quot;Computers&quot;, &quot;Computers&quot;, &quot;Computers&quot;, &quot;Comp… ## $ Pencil &lt;chr&gt; &quot;No, the whole study was on the computer (except maybe consent/debriefing)&quot;, &quot;No, t… ## $ StudyOrderN &lt;chr&gt; &quot;Hauser|Ross.Slate1|Rottenstrich|Graham|Kay|Inbar|Anderson|VanLange|Huang|Bauer|Cri… ## $ IDiffOrderN &lt;chr&gt; &quot;ID: Global self-esteem SISE|ID: Mood|ID: Subjective wellbeing|ID: Disgust scale (s… ## $ study.order &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ analysis.type &lt;chr&gt; &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;Order&quot;, &quot;O… ## $ subset &lt;chr&gt; &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;, &quot;all&quot;,… ## $ case.include &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE,… ## $ y &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,… ## $ loc &lt;fct&gt; 1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 4, 4,… The total sample size is \\(N = 6{,}842\\). d %&gt;% distinct(uID) %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 6842 All cases are to be included. d %&gt;% count(case.include) ## # A tibble: 1 x 2 ## case.include n ## &lt;lgl&gt; &lt;int&gt; ## 1 TRUE 6842 The data were collected in 59 locations with sample sizes ranging from 34 to 325. d %&gt;% count(Location) %&gt;% arrange(desc(n)) ## # A tibble: 59 x 2 ## Location n ## &lt;chr&gt; &lt;int&gt; ## 1 University of Toronto, Scarborough 325 ## 2 MTurk India Workers 308 ## 3 MTurk US Workers 304 ## 4 University of Illinois at Urbana-Champaign, Champaign, IL 198 ## 5 Eotvos Lorand University, in Budapest, Hungary 180 ## 6 Department of Social Psychology, Tilburg University, P.O. Box 90153, Tilburg, 5000 LE, Netherlands 173 ## 7 Department of Psychology, San Diego State University, San Diego, CA 92182 171 ## 8 Department of Psychology, Pennsylvania State University Abington, Abington, PA 19001 166 ## 9 American University of Sharjah, United Arab Emirates 162 ## 10 University of British Columbia, Vancouver, Canada 147 ## # … with 49 more rows 15.5.3 Our effect size will be an odds ratio. Here’s how Klein and colleagues summarized their primary results: In the aggregate replication sample \\((N = 6,842\\) after removing participants who responded in less than \\(4\\) s\\(),\\) \\(71\\%\\) of participants judged the action in the foreseen-side-effect scenario as permissible, but only \\(17\\%\\) of participants in the greater-good scenario judged it as permissible. The difference between the percentages was significant, \\(p = 2.2 \\text e^{-16},\\) \\(\\text{OR} = 11.54,\\) \\(d = 1.35,\\) \\(95\\% \\; \\text{CI} = [1.28, 1.41]\\). The replication results were consistent with the double-effect hypothesis, and the effect was about half the magnitude of the original \\((d = 1.35,\\) \\(95\\% \\; \\text{CI} = [1.28, 1.41],\\) vs. original \\(d = 2.50)\\). (p. 459) Here is the breakdown of the outcome and primary experimental condition, which will confirm the two empirical percentages mentioned, above. d %&gt;% count(variable, factor) %&gt;% group_by(factor) %&gt;% mutate(percent = 100 * n / sum(n)) ## # A tibble: 4 x 4 ## # Groups: factor [2] ## variable factor n percent ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 No GreaterGood 2781 82.8 ## 2 No SideEffect 1026 29.4 ## 3 Yes GreaterGood 577 17.2 ## 4 Yes SideEffect 2458 70.6 Though the authors presented their overall effect size with a \\(p\\)-value, an odds-ratio (OR), and a Cohen’s \\(d\\) (i.e., a kind of standardized mean difference), we will focus on the OR. The primary data are binomial counts, which are well-handled with logistic regression. When you perform a logistic regression where a control condition is compared with some experimental condition, the difference between those conditions may be expressed as an OR. To get a sense of what that is, we’ll first practice fitting a logistic regression model with the frequentist glm() function. Here are the results based on the subset of data from the first location. glm0 &lt;- glm(y ~ factor, family = binomial(logit), data = d %&gt;% filter(loc == 1)) summary(glm0) ## ## Call: ## glm(formula = y ~ factor, family = binomial(logit), data = d %&gt;% ## filter(loc == 1)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5227 -0.6231 -0.6231 0.8677 1.8626 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.5404 0.3673 -4.194 2.74e-05 *** ## factorSideEffect 2.3232 0.4754 4.887 1.02e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 139.47 on 101 degrees of freedom ## Residual deviance: 110.98 on 100 degrees of freedom ## AIC: 114.98 ## ## Number of Fisher Scoring iterations: 4 Just like with brms, the base-R glm() function returns the results of a logistic regression model in the log-odds metric. The intercept is the log-odds probability of selecting yes in the study for participants in the GreaterGood condition. The ‘factorSideEffect’ parameter is the difference in log-odds probability for participants in the SideEffect condition. Here’s what happens when you exponentiate that coefficient. coef(glm0)[2] %&gt;% exp() ## factorSideEffect ## 10.20833 That, my friends, is an odds ratio (OR). Odds ratios are simply exponentiated logistic regression coefficients. The implication of this particular OR is that those in the SideEffect condition have about 10 times the odds of selecting yes compared to those in the GreaterGood condition. In the case of this subset of the data, that’s 18% yeses versus 69%, which seems like a large difference, to me. d %&gt;% filter(loc == 1) %&gt;% count(variable, factor) %&gt;% group_by(factor) %&gt;% mutate(percent = 100 * n / sum(n)) %&gt;% filter(variable == &quot;Yes&quot;) ## # A tibble: 2 x 4 ## # Groups: factor [2] ## variable factor n percent ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Yes GreaterGood 9 17.6 ## 2 Yes SideEffect 35 68.6 15.5.4 Log-odds, odds ratios, and modeling effect sizes. Though it’s common for researchers to express their effect sizes as odds ratios, we don’t want to work directly with odds ratios in a meta-analysis. Why? Well, think back on why we model binomial data with the logit link. The logit link transforms a bounded \\([0, 1]\\) parameter space into an unbounded parameter space ranging from negative to positive infinity. For us Bayesians, it also provides a context in which our \\(\\beta\\) parameters are approximately Gaussian. However, when we exponentiate those approximately Gaussian log-odds coefficients, the resulting odds ratios aren’t so Gaussian any more. This is why, even if our ultimate goal is to express a meta-analytic effect as an OR, we want to work with effect sizes in the log-odds metric. It allows us to use the Bayesian meta-analytic framework outlined by Gelman and colleagues, above, \\[\\begin{align*} y_j &amp; \\sim \\operatorname{Normal}(\\theta_j, \\sigma_j) \\\\ \\theta_j &amp; \\sim \\operatorname{Normal}(\\mu, \\tau), \\end{align*}\\] where \\(y_j\\) is the point estimate in the \\(j\\)th study still in the log-odds scale. After fitting the model, we can then exponentiate the meta-analytic parameter \\(\\mu\\) into the OR metric. 15.5.5 Compute the study-specific effect sizes. Our d data from the Klein et al replication study includes the un-aggregated data from all of the study locations combined. Before we compute our meta-analysis, we’ll need to compute the study-specific effect sizes and standard errors. Here we do so within a nested tibble. library(broom) glms &lt;- d %&gt;% select(loc, y, factor) %&gt;% nest(data = c(y, factor)) %&gt;% mutate(glm = map(data, ~update(glm0, data = .))) %&gt;% mutate(coef = map(glm, tidy)) %&gt;% select(-data, -glm) %&gt;% unnest(coef) %&gt;% filter(term == &quot;factorSideEffect&quot;) # what did we do? glms %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 59 x 6 ## loc term estimate std.error statistic p.value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 factorSideEffect 2.32 0.475 4.89 0 ## 2 2 factorSideEffect 3.64 0.644 5.64 0 ## 3 3 factorSideEffect 2.37 0.399 5.96 0 ## 4 4 factorSideEffect 2.24 0.263 8.54 0 ## 5 5 factorSideEffect 2.02 0.505 4.00 0 ## 6 6 factorSideEffect 2.49 0.571 4.36 0 ## 7 7 factorSideEffect 2.53 0.658 3.84 0 ## 8 8 factorSideEffect 1.78 0.459 3.87 0 ## 9 9 factorSideEffect 1.81 0.378 4.79 0 ## 10 10 factorSideEffect 2.37 0.495 4.79 0 ## # … with 49 more rows In the estimate column we have all the \\(y_j\\) values and std.error contains the corresponding \\(\\sigma_j\\) values. Here they are in a plot. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[5] glms %&gt;% ggplot(aes(x = std.error, y = estimate)) + geom_point(color = color) + labs(x = expression(sigma[italic(j)]~(&quot;log-odds&quot;)), y = expression(italic(y[j])~(&quot;log-odds&quot;))) 15.5.6 Fit the Bayesian meta-analysis. Now are data are ready, we can express our first Bayesian meta-analysis with the formula \\[\\begin{align*} \\text{estimate}_j &amp; \\sim \\operatorname{Normal}(\\theta_j, \\; \\text{std.error}_j) \\\\ \\theta_j &amp; \\sim \\operatorname{Normal}(\\mu, \\tau) \\\\ \\mu &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\tau &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] where the last two lines spell out our priors. As we learned in Section 11.1, the \\(\\operatorname{Normal}(0, 1.5)\\) prior in the log-odds space is just about flat on the probability space. If you wanted to be more conservative, consider something like \\(\\operatorname{Normal}(0, 1)\\). Here’s how to fit the model with brms. b15.10 &lt;- brm(data = glms, family = gaussian, estimate | se(std.error) ~ 1 + (1 | loc), prior = c(prior(normal(0, 1.5), class = Intercept), prior(exponential(1), class = sd)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 15, file = &quot;fits/b15.10&quot;) se() is one of the brms helper functions designed to provide additional information about the criterion variable. Here it informs brm() that each estimate value has an associated measurement error defined in the std.error column. Unlike the mi() function, which we used earlier in the chapter to accommodate measurement error and the Bayesian imputation of missing data, the se() function is specially designed to handle meta-analyses. se() contains a sigma argument which is set to FALSE by default. This will return a model with no estimate for sigma, which is what we want. The uncertainty around the estimate-value for each study \\(j\\) has already been encoded in the data as std.error. Let’s look at the model results. print(b15.10) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: estimate | se(std.error) ~ 1 + (1 | loc) ## Data: glms (Number of observations: 59) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~loc (Number of levels: 59) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.43 0.09 0.26 0.62 1.00 1956 2389 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 2.55 0.09 2.38 2.72 1.00 3443 2631 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.00 0.00 0.00 0.00 1.00 4000 4000 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our estimate for heterogeneity across studies, \\(\\tau\\), is about 0.4, suggesting modest differences across the studies. The meta-analytic effect, \\(\\mu\\), is about 2.5. Both, recall, are in the log-odds metric. Here we exponentiate \\(\\mu\\) to get our odds ratio. fixef(b15.10) %&gt;% exp() ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 12.79272 1.091829 10.85899 15.25431 If you look back up to the results reported by Klein and colleagues, you’ll see this is rather close to their OR estimate of 11.54. 15.5.7 Fit the Bayesian muiltilevel alternative. We said earlier that meta-analysis is just a special case of the multilevel model, applied to summary data. We typically perform meta-analyses on data summaries because historically it has not been the norm among researchers to make their data publicly available. So effect size summaries were the best we typically had for aggregating study results. However, times are changing (e.g., here, here). In this case, Klein and colleagues engaged in open-science practices and reported all their data. Thus we can just directly fit the model \\[\\begin{align*} \\text{y}_{ij} &amp; \\sim \\operatorname{Binomial}(n = 1, p_{ij}) \\\\ \\operatorname{logit}(p_{ij}) &amp; \\sim \\alpha + \\beta \\text{factor}_{ij} + u_{\\alpha j} + u_{\\beta j} \\text{factor}_{ij} \\\\ \\begin{bmatrix} u_{\\alpha j} \\\\ u_{\\beta j} \\end{bmatrix} &amp; \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{SRS} \\end{pmatrix} \\\\ \\mathbf S &amp; = \\begin{bmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{bmatrix} \\\\ \\mathbf R &amp; = \\begin{bmatrix} 0 &amp; \\rho_{\\alpha \\beta} \\\\ \\rho_{\\beta \\alpha} &amp; 0 \\end{bmatrix} \\\\ \\alpha &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\beta &amp; \\sim \\operatorname{Normal}(0, 1.5) \\\\ \\sigma_\\alpha &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_\\beta &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\mathbf R &amp; \\sim \\operatorname{LKJ}(2), \\end{align*}\\] where the criterion variable, \\(y\\), is nested in \\(i\\) participants within \\(j\\) locations. The \\(\\beta\\) parameter is analogous to the meta-analytic effect (\\(\\mu\\)) and \\(\\sigma_\\beta\\) is analogous to the expression of heterogeneity in the meta-analytic effect (\\(\\tau\\)). Here is how to fit the model with brms. b15.11 &lt;- brm(data = d, family = binomial, y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc), prior = c(prior(normal(0, 1.5), class = b), prior(exponential(1), class = sd), prior(lkj(2), class = cor)), iter = 2000, warmup = 1000, cores = 4, chains = 4, seed = 15, file = &quot;fits/b15.11&quot;) The results for the focal parameters are very similar to those from b15.10. print(b15.11) ## Family: binomial ## Links: mu = logit ## Formula: y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc) ## Data: d (Number of observations: 6842) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Group-Level Effects: ## ~loc (Number of levels: 59) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.42 0.07 0.30 0.57 1.00 2120 2870 ## sd(factorSideEffect) 0.48 0.09 0.32 0.66 1.01 1107 2010 ## cor(Intercept,factorSideEffect) -0.31 0.19 -0.62 0.08 1.00 1487 2276 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.66 0.08 -1.82 -1.52 1.00 2012 2675 ## factorSideEffect 2.57 0.09 2.39 2.76 1.00 2044 2623 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here’s the multilevel version of the effect size as an odds ratio. fixef(b15.11)[2, -2] %&gt;% exp() ## Estimate Q2.5 Q97.5 ## 13.02704 10.93772 15.73129 Here we compare the study specific effect sizes, \\(\\theta_j\\), by our two modeling approaches. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[3] # how many levels are there? n_loc &lt;- distinct(d, loc) %&gt;% count() %&gt;% pull(n) # rank by meta-analysis ranks &lt;- tibble(Estimate = coef(b15.10)$loc[, 1, &quot;Intercept&quot;], index = 1:n_loc) %&gt;% arrange(Estimate) %&gt;% mutate(rank = 1:n_loc) # combine parameter summaries rbind(coef(b15.10)$loc[, , &quot;Intercept&quot;], coef(b15.11)$loc[, , &quot;factorSideEffect&quot;]) %&gt;% data.frame() %&gt;% mutate(index = rep(1:n_loc, times = 2), type = rep(c(&quot;meta-analysis&quot;, &quot;multilevel model&quot;), each = n_loc)) %&gt;% # add the ranks left_join(select(ranks, -Estimate), by = &quot;index&quot;) %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) + geom_pointrange(fatten = 1, color = color) + scale_x_continuous(expression(log-odds~effect~size~(theta[italic(j)])), limits = c(0, 4.5)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ type) The results are very similar. You might be curious how to show these results in a more conventional looking forest plot where the names of the groups (typically studies) for the \\(\\theta_j\\) values are listed on the left, the point estimate and 95% interval summaries are listed on the right, and the summary for the population level effect, \\(\\mu\\), is listed beneath all all the \\(\\theta_j\\)’s. That’ll require some prep work. First we’ll need to reformat the location names. I’ll save the results in an object called labs. labs &lt;- d %&gt;% mutate(lab = case_when( Location == &quot;Social and Work Psychology Department, University of Brasilia, DF, Brazil&quot; ~ &quot;University of Brasilia&quot;, Location == &quot;Wilfrid Laurier University, Waterloo, Ontario, Canada&quot; ~ &quot;Wilfrid Laurier University&quot;, Location == &quot;University of British Columbia, Vancouver, Canada&quot; ~ &quot;University of British Columbia&quot;, Location == &quot;University of Toronto, Scarborough&quot; ~ &quot;University of Toronto&quot;, Location == &quot;Division of Social Science, The Hong Kong University of Science and Technology, Hong Kong, China&quot; ~ &quot;Hong Kong University of Science and Technology&quot;, Location == &quot;Chinese Academy of Science, Beijing, China&quot; ~ &quot;Chinese Academy of Science&quot;, Location == &quot;Shanghai International Studies University, SISU Intercultural Institute, Shanghai, China&quot; ~ &quot;Shanghai International Studies University&quot;, Location == &quot;Guangdong Literature &amp; Art Vocational College, Guangzhou, China&quot; ~ &quot;Guangdong Literature &amp; Art Vocational College&quot;, Location == &quot;The University of J. E. Purkyně, Ústí nad Labem, Czech Republic&quot; ~ &quot;The University of J. E. Purkyně&quot;, Location == &quot;University of Leuven, Belgium&quot; ~ &quot;University of Leuven&quot;, Location == &quot;Department of Experimental and Applied Psychology, VU Amsterdam, 1081BT, Amsterdam, The Netherlands&quot; ~ &quot;VU Amsterdam&quot;, Location == &quot;Department of Social Psychology, Tilburg University, P.O. Box 90153, Tilburg, 5000 LE, Netherlands&quot; ~ &quot;Department of Social Psychology, Tilburg University&quot;, Location == &quot;Eindhoven University of Technology, Eindhoven, Netherlands&quot; ~ &quot;Eindhoven University of Technology&quot;, Location == &quot;Department of Communication and Information Sciences, P.O. Box 90153, Tilburg, 5000 LE, Netherlands&quot; ~ &quot;Department of Communication and Information Sciences, Tilburg University&quot;, Location == &quot;University of Navarra, Spain&quot; ~ &quot;University of Navarra&quot;, Location == &quot;University of Lausanne, Switzerland&quot; ~ &quot;University of Lausanne&quot;, Location == &quot;Université de Poitiers, France&quot; ~ &quot;Université de Poitiers&quot;, Location == &quot;Eotvos Lorand University, in Budapest, Hungary&quot; ~ &quot;Eotvos Lorand University&quot;, Location == &quot;MTurk India Workers&quot; ~ &quot;MTurk India Workers&quot;, Location == &quot;University of Winchester, Winchester, Hampshire, England&quot; ~ &quot;University of Winchester&quot;, Location == &quot;Doshisha University, Kyoto, Japan&quot; ~ &quot;Doshisha University&quot;, Location == &quot;Victoria University of Wellington, New Zealand&quot; ~ &quot;Victoria University of Wellington&quot;, Location == &quot;University of Social Sciences and Humanities, Wroclaw, Poland&quot; ~ &quot;University of Social Sciences and Humanities&quot;, Location == &quot;Department of Psychology, SWPS University of Social Sciences and Humanities Campus Sopot, Sopot, Poland&quot; ~ &quot;SWPS University of Social Sciences and Humanities Campus Sopot&quot;, Location == &quot;badania.net&quot; ~ &quot;badania.net&quot;, Location == &quot;Universidade do Porto, Portugal&quot; ~ &quot;Universidade do Porto&quot;, Location == &quot;University of Belgrade, Belgrade, Serbia&quot; ~ &quot;University of Belgrade&quot;, Location == &quot;University of Johannesburg, Johanneburg, South Africa&quot; ~ &quot;University of Johannesburg&quot;, Location == &quot;Santiago, Chile&quot; ~ &quot;Santiago, Chile&quot;, Location == &quot;Universidad de Costa Rica, Costa Rica&quot; ~ &quot;Universidad de Costa Rica&quot;, Location == &quot;National Autonomous University of Mexico in Mexico City&quot; ~ &quot;National Autonomous University of Mexico&quot;, Location == &quot;University of the Republic, Montevideo, Uruguay&quot; ~ &quot;University of the Republic&quot;, Location == &quot;Lund University, Lund, Sweden&quot; ~ &quot;Lund University&quot;, Location == &quot;Academia Sinica, Taiwan National Taiwan Normal University, Taiwan&quot; ~ &quot;Taiwan National Taiwan Normal University&quot;, Location == &quot;Bilgi University, Istanbul, Turkey&quot; ~ &quot;Bilgi University&quot;, Location == &quot;Koç University, Istanbul, Turkey&quot; ~ &quot;Koç University&quot;, Location == &quot;American University of Sharjah, United Arab Emirates&quot; ~ &quot;American University of Sharjah&quot;, Location == &quot;University of Hawaii, Honolulu, HI&quot; ~ &quot;University of Hawaii&quot;, Location == &quot;Social Science and Policy Studies Department, Worcester Polytechnic Institute, Worcester, MA 01609&quot; ~ &quot;Worcester Polytechnic Institute&quot;, Location == &quot;Department of Psychology, Washington and Lee University, Lexington, VA 24450&quot; ~ &quot;Washington and Lee University&quot;, Location == &quot;Department of Psychology, San Diego State University, San Diego, CA 92182&quot; ~ &quot;San Diego State University&quot;, Location == &quot;Tufts&quot; ~ &quot;Tufts&quot;, Location == &quot;University of Florida, Florida&quot; ~ &quot;University of Florida&quot;, Location == &quot;University of Illinois at Urbana-Champaign, Champaign, IL&quot; ~ &quot;University of Illinois at Urbana-Champaign&quot;, Location == &quot;Pacific Lutheran University, Tacoma, WA&quot; ~ &quot;Pacific Lutheran University&quot;, Location == &quot;University of Virginia, VA&quot; ~ &quot;University of Virginia&quot;, Location == &quot;Marian University, Indianapolis, IN&quot; ~ &quot;Marian University&quot;, Location == &quot;Department of Psychology, Ithaca College, Ithaca, NY 14850&quot; ~ &quot;Ithaca College&quot;, Location == &quot;University of Michigan&quot; ~ &quot;University of Michigan&quot;, Location == &quot;Department of Psychology, Pennsylvania State University Abington, Abington, PA 19001&quot; ~ &quot;Pennsylvania State University Abington&quot;, Location == &quot;Department of Psychology, Texas A&amp;M University, College Station, TX 77843&quot; ~ &quot;Texas A&amp;M University&quot;, Location == &quot;William Paterson University, Wayne, NJ&quot; ~ &quot;William Paterson University&quot;, Location == &quot;Department of Cognitive Science, Occidental College, Los Angeles, CA&quot; ~ &quot;Occidental College&quot;, Location == &quot;The Pennsylvania State University&quot; ~ &quot;The Pennsylvania State University&quot;, Location == &quot;MTurk US Workers&quot; ~ &quot;MTurk US Workers&quot;, Location == &quot;University of Graz AND the Universty of Vienna&quot; ~ &quot;University of Graz and the Universty of Vienna&quot;, Location == &quot;University of Potsdam, Germany&quot; ~ &quot;University of Potsdam&quot;, Location == &quot;Open University of Hong Kong&quot; ~ &quot;Open University of Hong Kong&quot;, Location == &quot;Concepción, Chile&quot; ~ &quot;Concepción&quot; )) %&gt;% distinct(loc, lab) # what is this? labs %&gt;% glimpse() ## Rows: 59 ## Columns: 2 ## $ loc &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2… ## $ lab &lt;chr&gt; &quot;University of Brasilia&quot;, &quot;Wilfrid Laurier University&quot;, &quot;University of British Columbia&quot;, &quot;Unive… Now we’ll do some tricky wrangling with the output from coef() and fixef() to arrange the odds ratio summaries for the population average and the location-specific results. # this will help us format the labels on the secondary y-axis my_format &lt;- function(number) { formatC(number, digits = 2, format = &quot;f&quot;) } # grab the theta_j summaries groups &lt;- coef(b15.11)$loc[, , &quot;factorSideEffect&quot;] %&gt;% data.frame() %&gt;% mutate(loc = distinct(d, loc) %&gt;% pull()) %&gt;% arrange(Estimate) # grab the mu summary average &lt;- fixef(b15.11) %&gt;% data.frame() %&gt;% slice(2) %&gt;% mutate(loc = &quot;Average&quot;) # combine and wrangle post &lt;- bind_rows(groups, average) %&gt;% mutate(rank = c(1:59, 0), Estimate = exp(Estimate), Q2.5 = exp(Q2.5), Q97.5 = exp(Q97.5)) %&gt;% left_join(labs, by = &quot;loc&quot;) %&gt;% arrange(rank) %&gt;% mutate(label = ifelse(is.na(lab), &quot;POPULATION AVERAGE&quot;, lab), summary = str_c(my_format(Estimate), &quot; [&quot;, my_format(Q2.5), &quot;, &quot;, my_format(Q97.5), &quot;]&quot;)) # what have we done? post %&gt;% glimpse() ## Rows: 60 ## Columns: 9 ## $ Estimate &lt;dbl&gt; 13.027040, 5.994537, 7.225509, 7.894728, 7.896201, 7.989348, 8.158148, 8.425675, 8.637662,… ## $ Est.Error &lt;dbl&gt; 0.09183827, 0.23712115, 0.35418533, 0.32549107, 0.35978096, 0.23125168, 0.34382994, 0.3229… ## $ Q2.5 &lt;dbl&gt; 10.937724, 3.752456, 3.537577, 4.170577, 3.898701, 5.147465, 4.109451, 4.488119, 4.401112,… ## $ Q97.5 &lt;dbl&gt; 15.731289, 9.501053, 14.080042, 15.016368, 15.701244, 12.588834, 16.272517, 15.886559, 16.… ## $ loc &lt;chr&gt; &quot;Average&quot;, &quot;19&quot;, &quot;38&quot;, &quot;8&quot;, &quot;32&quot;, &quot;55&quot;, &quot;5&quot;, &quot;34&quot;, &quot;22&quot;, &quot;9&quot;, &quot;6&quot;, &quot;58&quot;, &quot;24&quot;, &quot;7&quot;, &quot;50&quot;, … ## $ rank &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, … ## $ lab &lt;chr&gt; NA, &quot;MTurk India Workers&quot;, &quot;University of Hawaii&quot;, &quot;Guangdong Literature &amp; Art Vocational … ## $ label &lt;chr&gt; &quot;POPULATION AVERAGE&quot;, &quot;MTurk India Workers&quot;, &quot;University of Hawaii&quot;, &quot;Guangdong Literature… ## $ summary &lt;chr&gt; &quot;13.03 [10.94, 15.73]&quot;, &quot;5.99 [3.75, 9.50]&quot;, &quot;7.23 [3.54, 14.08]&quot;, &quot;7.89 [4.17, 15.02]&quot;, &quot;… Here’s our custom forest plot. post %&gt;% ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) + geom_interval(aes(color = label == &quot;POPULATION AVERAGE&quot;), size = 1/2) + geom_point(aes(size = 1 - Est.Error, color = label == &quot;POPULATION AVERAGE&quot;), shape = 15) + scale_color_viridis_d(option = &quot;C&quot;, begin = .33, end = .67) + scale_size_continuous(range = c(1, 3.5)) + scale_x_continuous(&quot;odds ratio&quot;, breaks = 1:6 * 10, expand = expansion(mult = c(0.005, 0.005))) + scale_y_continuous(NULL, breaks = 0:59, limits = c(-1, 60), expand = c(0, 0), labels = pull(post, label), sec.axis = dup_axis(labels = pull(post, summary))) + theme(text = element_text(family = &quot;Times&quot;), axis.text.y = element_text(hjust = 0, color = &quot;white&quot;, size = 7), axis.text.y.right = element_text(hjust = 1, size = 7), axis.ticks.y = element_blank(), panel.background = element_rect(fill = &quot;grey8&quot;), panel.border = element_rect(color = &quot;transparent&quot;)) You may have noticed this plot is based on the results of our multilevel model, b15.11. We could have done the same basic thing with the results from the more conventional meta-analysis model, b15.10, too. I’m not aware this it typical in random effect meta-analyses, but it might be useful to further clarify the meaning of the two primary parameters, \\(\\mu\\) and \\(\\tau\\). Like with the forest plot, above, we could examine these with either b15.10 or b15.11. For kicks, we’ll use b15.10 (the conventional Bayesian meta-analysis). In the output from posterior_samples(b15.10), \\(\\mu\\) and \\(\\tau\\) are in the columns named b_Intercept and sd_loc__Intercept, respectively. post &lt;- posterior_samples(b15.10) post %&gt;% select(b_Intercept:sd_loc__Intercept) %&gt;% head() ## b_Intercept sd_loc__Intercept ## 1 2.378526 0.4688289 ## 2 2.562858 0.4555103 ## 3 2.435846 0.3252279 ## 4 2.658129 0.3895584 ## 5 2.451356 0.3583352 ## 6 2.672061 0.5595212 If you scroll back above, you’ll see our random effect meta-analysis explicitly presumed our empirical effect-size estimates \\(y_j\\) are approximations of the true effect sizes \\(\\theta_j\\), which are themselves normally distributed in the population of possible effect sizes from similar studies: \\(\\theta_j \\sim \\operatorname{Normal}(\\mu, \\tau)\\). Why not use our posterior samples to simulate draws from \\(\\operatorname{Normal}(\\mu, \\tau)\\) to get a sense of what this distribution might look like? Recall that the parameters are in the log-odds metric. We’ll present the distribution in that metric and as odds ratios. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[6] set.seed(15) post %&gt;% transmute(lo = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept), or = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept) %&gt;% exp()) %&gt;% slice(1:1e3) %&gt;% pivot_longer(lo:or, values_to = &quot;effect size&quot;) %&gt;% mutate(name = factor(name, labels = c(&quot;log-odds&quot;, &quot;odds ratio&quot;))) %&gt;% ggplot(aes(x = `effect size`, y = 0)) + geom_dots(color = color, fill = color) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(Normal(mu*&#39;, &#39;*tau))) + theme(text = element_text(family = &quot;Times&quot;), strip.background = element_rect(color = &quot;transparent&quot;)) + facet_wrap(~ name, scales = &quot;free&quot;) Both panels show 1,000 draws, each of which is depicted by a single dot. If we were to run this experiment 1,000 times and compute the effect size separately for each one, this is what we’d expect those distributions of effect sizes to look like. Seems like there’s a lot of variation in there, eh? The next time you observe your fellow scientists debating over whether a study replicated or not, keep these distributions in mind. Once you start thinking about distributions, replication becomes a tricky notion. 15.5.8 Relative to the muiltilevel model, meta-analyses have limitations. Although the Bayesian meta-analysis and multilevel models produced very similar results for the effect size, the two are not equivalent. Because the meta-analysis was based on summary information from the effect sizes along, it does not offer all of the insights available from the multilevel model. Other than the random effects, the only population parameters offered by the meta-analysis are the effect size, \\(\\mu\\), and the standard deviation around that effect size, \\(\\tau\\). These parameters are equivalent to our multilevel model parameters \\(\\beta\\) and \\(\\sigma_\\beta\\), respectively. In addition, the multilevel model added group-level parameter for the average performance in the control condition, \\(\\alpha\\), the population-level standard deviation around that mean, \\(\\sigma_\\alpha\\), and the correlation between the group-specific \\(\\alpha_j\\)’s and \\(\\beta_j\\)’s, which we called \\(\\rho_{\\alpha \\beta}\\). To build a sense of why we might to want these parameters, too, we’ll extract the posterior samples for the multilevel model b15.11. post &lt;- posterior_samples(b15.11) Here we’ll make a scatter plot the posterior means of the random intercepts, \\(\\alpha_j\\)’s, and the random slopes, \\(\\beta_j\\)’s. color &lt;- viridis_pal(option = &quot;C&quot;)(7)[7] c &lt;- full_join( # intercepts post %&gt;% select(`r_loc[1,Intercept]`:`r_loc[59,Intercept]`) %&gt;% set_names(1:59) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, values_to = &quot;intercepts&quot;), # slopes post %&gt;% select(`r_loc[1,factorSideEffect]`:`r_loc[59,factorSideEffect]`) %&gt;% set_names(1:59) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, values_to = &quot;slopes&quot;), by = c(&quot;iter&quot;, &quot;name&quot;) ) # summarize by the means c %&gt;% group_by(name) %&gt;% summarise(intercepts = mean(intercepts), slopes = mean(slopes)) %&gt;% # plot! ggplot(aes(x = intercepts, y = slopes)) + geom_point(color = color) + labs(x = expression(alpha[italic(j)]), y = expression(beta[italic(j)])) This provides some information, which we wouldn’t get from a meta-analysis. However, the plot is a little deceiving. First, it doesn’t provide an expression of the uncertainty on the effects. Second, it doesn’t give full justice to the correlation between the two effects, \\(\\rho_{\\alpha \\beta}\\). If you look back up at the print() output from Section 15.5.7, you’ll see that correlation was about -0.31. We can take some cues from Section 14.4 and express the bivariate posteriors as ellipses, rather than simple mean-level points. c %&gt;% ggplot(aes(x = intercepts, y = slopes)) + stat_ellipse(aes(group = name), geom = &quot;polygon&quot;, level = .05, size = 0, fill = color, alpha = 1/2) + stat_ellipse(level = .95, color = color) + labs(x = expression(alpha[italic(j)]), y = expression(beta[italic(j)])) The small semitransparent ellipses depict the inner-most 5% of each of the \\(j\\)-level bivariate posteriors. The larger ellipse is the 95% boundary, collapsing across all levels of \\(j\\). Both kinds of summaries help depict the negative correlation between the \\(\\alpha_j\\)’s and \\(\\beta_j\\)’s. That negative correlation means suggests the studies which had larger effect sizes for the control condition tended to have smaller effect sizes for the experimental condition. There’s no way you would know that from the meta-analysis approach. Another way to explore the relation between the model intercepts and slopes is to plot their implications on the probability scale. The way we parameterized the model, the greater-good condition is expressed the \\(\\alpha_j\\) and the side-effect condition is expressed as \\(\\alpha_j + \\beta_j\\). If we were working directly with the posteior_samples(), we’d have to do fancy conversions with the inv_logit_scaled() function. An easier way is to just use fitted(). breaks &lt;- c(0, inv_logit_scaled(c(fixef(b15.11)[1, 1], fixef(b15.11)[1, 1] + fixef(b15.11)[2, 1])), 1) nd &lt;- d %&gt;% distinct(loc, factor) fitted(b15.11, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) %&gt;% mutate(factor = if_else(factor == &quot;SideEffect&quot;, &quot;Side effect&quot;, &quot;Greater good&quot;)) %&gt;% ggplot(aes(x = factor, y = Estimate, group = loc)) + geom_line(alpha = 1/2, color = color) + scale_y_continuous(&quot;probability&quot;, limits = 0:1, breaks = breaks, labels = round(breaks, digits = 2)) + xlab(NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.x = element_blank()) Each of the 59 levels of our grouping variable loc is depicted in one of those 59 lines. Sadly, this approach doesn’t do a good job expressing the uncertainty in those lines. Each one is based on the posterior mean. Regardless, this plot offers more insights that were not available from the meta-analysis. 15.5.9 Parting thoughts. There are other things you might do with these data. For example, you might inspect how much the effect size varies between those from WEIRD and non-WEIRD countries. You might also model the data as clustered by Language rather than by Location. But I think we’ve gone far enough to get you started. If you’d like to learn more about these methods, do check out Vourre’s Meta-analysis is a special case of Bayesian multilevel modeling. You might also read Williams, Rast, and Bürkner’s (2018) manuscript, Bayesian meta-analysis with weakly informative prior distributions. For an alternative workflow, consider the baggr package (Wiecek &amp; Meager, 2020), which is designed to fit hierarchical Bayesian meta-analyses with Stan under the hood. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] broom_0.7.5 tidybayes_2.3.1 ggrepel_0.9.1 brms_2.15.0 Rcpp_1.0.6 ## [6] ggdag_0.2.3 patchwork_1.1.1 viridis_0.5.1 viridisLite_0.3.0 ggdark_0.2.1 ## [11] forcats_0.5.1 stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 ## [16] tidyr_1.1.3 tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 plyr_1.8.6 igraph_1.2.6 svUnit_1.0.3 ## [6] splines_4.0.4 crosstalk_1.1.0.1 TH.data_1.0-10 rstantools_2.1.1 inline_0.3.17 ## [11] digest_0.6.27 htmltools_0.5.1.1 rsconnect_0.8.16 fansi_0.4.2 magrittr_2.0.1 ## [16] graphlayouts_0.7.1 modelr_0.1.8 RcppParallel_5.0.2 matrixStats_0.57.0 sandwich_3.0-0 ## [21] xts_0.12.1 prettyunits_1.1.1 colorspace_2.0-0 rvest_0.3.6 ggdist_2.4.0.9000 ## [26] haven_2.3.1 xfun_0.22 callr_3.5.1 crayon_1.4.1 jsonlite_1.7.2 ## [31] lme4_1.1-25 survival_3.2-7 zoo_1.8-8 glue_1.4.2 polyclip_1.10-0 ## [36] gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 distributional_0.2.2 pkgbuild_1.2.0 ## [41] rstan_2.21.2 abind_1.4-5 scales_1.1.1 mvtnorm_1.1-1 emo_0.0.0.9000 ## [46] DBI_1.1.0 miniUI_0.1.1.1 xtable_1.8-4 StanHeaders_2.21.0-7 stats4_4.0.4 ## [51] DT_0.16 htmlwidgets_1.5.2 httr_1.4.2 threejs_0.3.3 arrayhelpers_1.1-0 ## [56] ellipsis_0.3.1 pkgconfig_2.0.3 loo_2.4.1 farver_2.0.3 dbplyr_2.0.0 ## [61] utf8_1.1.4 tidyselect_1.1.0 labeling_0.4.2 rlang_0.4.10 reshape2_1.4.4 ## [66] later_1.1.0.1 munsell_0.5.0 dagitty_0.3-1 cellranger_1.1.0 tools_4.0.4 ## [71] cli_2.3.1 generics_0.1.0 ggridges_0.5.2 evaluate_0.14 fastmap_1.0.1 ## [76] processx_3.4.5 knitr_1.31 fs_1.5.0 tidygraph_1.2.0 ggraph_2.0.4 ## [81] nlme_3.1-152 mime_0.10 projpred_2.0.2 xml2_1.3.2 compiler_4.0.4 ## [86] bayesplot_1.8.0 shinythemes_1.1.2 rstudioapi_0.13 curl_4.3 gamm4_0.2-6 ## [91] reprex_0.3.0 statmod_1.4.35 tweenr_1.0.1 stringi_1.5.3 ps_1.6.0 ## [96] highr_0.8 Brobdingnag_1.2-6 lattice_0.20-41 Matrix_1.3-2 nloptr_1.2.2.2 ## [101] markdown_1.1 shinyjs_2.0.0 vctrs_0.3.6 pillar_1.5.1 lifecycle_1.0.0 ## [106] bridgesampling_1.0-0 estimability_1.3 httpuv_1.5.4 R6_2.5.0 bookdown_0.21 ## [111] promises_1.1.1 gridExtra_2.3 codetools_0.2-18 boot_1.3-26 colourpicker_1.1.0 ## [116] MASS_7.3-53 gtools_3.8.2 assertthat_0.2.1 withr_2.4.1 shinystan_2.5.0 ## [121] multcomp_1.4-16 mgcv_1.8-33 parallel_4.0.4 hms_0.5.3 grid_4.0.4 ## [126] coda_0.19-4 minqa_1.2.4 rmarkdown_2.7 ggforce_0.3.2 shiny_1.5.0 ## [131] lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 "],["generalized-linear-madness.html", "16 Generalized Linear Madness 16.1 Geometric people 16.2 Hidden minds and observed behavior 16.3 Ordinary differential nut cracking 16.4 Population dynamics Session info", " 16 Generalized Linear Madness Applied statistics has to apply to all the sciences, and so it is often much vaguer about models. Instead it focuses on average performance, regardless of the model. The generalized linear models in the preceding chapters are not credible scientific models of most natural processes. They are powerful, geocentric (Chapter 4) descriptions of associations. In combination with a logic of causal inference, for example DAGs and do-calculus, generalized linear models can nevertheless be unreasonably powerful. But there are problems with this GLMs-plus-DAGs approach. Not everything can be modeled as a GLM—a linear combination of variables mapped onto a non-linear outcome. But if it is the only approach you know, then you have to use it…. In this chapter, I will go beyond generalized linear madness. I’ll work through examples in which the scientific context provides a causal model that will breathe life into the statistical model. I’ve chosen examples which are individually distinct and highlight different challenges in developing and translating causal models into bespoke (see the Rethinking box [section] below) statistical models. You won’t require any specialized scientific expertise to grasp these examples. And the basic strategy is the same as it has been from the start: Define a generative model of a phenomenon and then use that model to design strategies for causal inference and statistical estimation. (McElreath, 2020a, p. 525, emphasis in the original) McElreath then reported he was going to work with Stan code, via rstan::stan(), in this chapter because of the unique demands of some of the models. Our approach will be mixed. We can fit at least a few of the models with brms, particularly with help from the non-linear syntax. However, some of the models to come are either beyond the current scope of brms or are at least beyond my current skill set. In those cases, we’ll follow McElreath’s approach and fit the models with stan(). 16.0.0.1 Rethinking: Bespoken for. Mass production has some advantages, but it also makes our clothes fit badly. Garments bought off-the-shelf are not manufactured with you in mind. They are not bespoke products, designed for any particular person with a particular body. Unless you are lucky to have a perfectly average body shape, you will need a tailor to get better. Statistical analyses are similar. Generalized linear models are off-the-shelf products, mass produced for a consumer market of impatient researchers with diverse goals. Science asked statisticians for tools that could be used anywhere. And so they delivered. But the clothes don’t always fit. (p. 526, emphasis in the original) 16.1 Geometric people Back in Chapter 4, you met linear regression in the context of building a predictive model of height using weight. You even saw how to measure non-linear associations between the two variables. But nothing in that example was scientifically satisfying. The height-weight model was just a statistical device. It contains no biological information and tells us nothing about how the association between height and weight arises. Consider for example that weight obviously does not cause height, at least not in humans. If anything, the causal relationship is the reverse. So now let’s try to do better. Why? Because when the model is scientifically inspired, rather than just statistically required, disagreements between model and data are informative of real causal relationships. Suppose for example that a person is shaped like a cylinder. Of course a person isn’t exactly shaped like a cylinder. There are arms and a head. But let’s see how far this cylinder model gets us. The weight of the cylinder is a consequence of the volume of the cylinder. And the volume of the cylinder is a consequence of growth in the height and width of the cylinder. So if we can relate the height to the volume, then we’d have a model to predict weight from height. (p. 526, emphasis in the original) 16.1.1 The scientific model. If we let \\(V\\) stand for volume, \\(r\\) stand for a radius, and \\(h\\) stand for height, we can solve for volume by \\[V = \\pi r^2 h.\\] If we further presume a person’s radius is unknown, but some proportion (\\(p\\)) of height (\\(ph\\)), we can rewrite the formula as \\[\\begin{align*} V &amp; = \\pi (ph)^2 h \\\\ &amp; = \\pi p^2 h^3. \\end{align*}\\] Though we’re not interested in volume per se, we might presume weight is some proportion of volume. Thus we could include a final parameter \\(k\\) to stand for the conversion form weight to volume, leaving us with the formula \\[W = kV = k \\pi p^2 h^3,\\] where \\(W\\) denotes weight. 16.1.2 The statistical model. For one last time, together, let’s load the Howell1 data. library(tidyverse) data(Howell1, package = &quot;rethinking&quot;) d &lt;- Howell1 rm(Howell1) # scale observed variables d &lt;- d %&gt;% mutate(w = weight / mean(weight), h = height / mean(height)) McElreath’s proposed statistical model follows the form \\[\\begin{align*} \\text{w}_i &amp; \\sim \\operatorname{Log-Normal}(\\mu_i, \\sigma) \\\\ \\exp(\\mu_i) &amp; = k \\pi p^2 \\text{h}_i^3 \\\\ k &amp; \\sim \\operatorname{Exponential}(0.5) \\\\ p &amp; \\sim \\operatorname{Beta}(2, 18) \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), &amp;&amp; \\text{where} \\\\ \\text w_i &amp; = \\text{weight}_i \\big / \\overline{\\text{weight}}, &amp;&amp; \\text{and} \\\\ \\text h_i &amp; = \\text{height}_i \\big / \\overline{\\text{height}}. \\end{align*}\\] The Log-Normal likelihood ensures the predictions for \\(\\text{weight}_i\\) will always be non-negative. Because our parameter \\(p\\) is the ratio of radius to height, \\(p = r / h\\), it must be positive. Since people are typically taller than their width, it should also be less than one, and probably substantially less than that. Our next step will be taking a look at our priors. For the plots in this chapter, we’ll give a nod the minimalistic plots in the authoritative text by Gelman et al. (2013), Bayesian data analysis: Third edition. Just to be a little kick, we’ll set the font to family = \"Times\". Most of the adjustments will come from ggthemes::theme_base(). library(ggthemes) theme_set( theme_base(base_size = 12) + theme(text = element_text(family = &quot;Times&quot;), axis.text = element_text(family = &quot;Times&quot;), axis.ticks = element_line(size = 0.25), axis.ticks.length = unit(0.1, &quot;cm&quot;), panel.background = element_rect(size = 0.1), plot.background = element_blank(), ) ) Now we have our theme, let’s get a sense of our priors. library(tidybayes) library(brms) c(prior(beta(2, 18), nlpar = p, coef = italic(p)), prior(exponential(0.5), nlpar = p, coef = italic(k)), prior(exponential(1), class = sigma, coef = sigma)) %&gt;% parse_dist(prior) %&gt;% ggplot(aes(y = 0, dist = .dist, args = .args)) + stat_dist_halfeye(.width = .5, size = 1, p_limits = c(0, 0.9995), n = 2e3, normalize = &quot;xy&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + facet_wrap(~ coef, scales = &quot;free_x&quot;, labeller = label_parsed) Here the points are the posterior medians and the horizontal lines the quantile-based 50% intervals. Turns out that \\(\\operatorname{Beta}(2, 18)\\) prior for \\(p\\) pushes the bulk of the prior mass down near zero. The beta distribution also forces the parameter space for \\(p\\) to range between 0 and 1. If we denote the two parameters of the beta distribution as \\(\\alpha\\) and \\(\\beta\\), we can compute the mean for any beta distribution as \\(\\alpha / (\\alpha + \\beta)\\). Thus the mean for our \\(\\operatorname{Beta}(2, 18)\\) prior is \\(2 / (2 + 18) = 2 / 20 = 0.1\\). Because we computed our weight and height variables, w and h, by dividing the original variables by their respective means, each now has a mean of 1. d %&gt;% pivot_longer(w:h) %&gt;% group_by(name) %&gt;% summarise(mean = mean(value)) ## # A tibble: 2 x 2 ## name mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 h 1 ## 2 w 1 Here’s their bivariate distribution in a scatter plot. d %&gt;% ggplot(aes(x = h, y = w)) + geom_vline(xintercept = 1, linetype = 2, size = 1/4, color = &quot;grey50&quot;) + geom_hline(yintercept = 1, linetype = 2, size = 1/4, color = &quot;grey50&quot;) + geom_point(size = 1/4) With this scaling, here is the formula for an individual with average weight and height: \\[\\begin{align*} 1 &amp; = k \\pi p^2 1^3 \\\\ &amp; = k \\pi p^2. \\end{align*}\\] If you assume \\(p &lt; .5\\), \\(k\\) must be greater than 1. \\(k\\) also has to be positive. To get a sense of this, we can further work the algebra: \\[\\begin{align*} 1 &amp; = k \\pi p^2 \\\\ 1/k &amp; = \\pi p^2 \\\\ k &amp; = 1 / \\pi p^2. \\end{align*}\\] To get a better sense of that relation, we might plot. tibble(p = seq(from = 0.001, to = 0.499, by = 0.001)) %&gt;% mutate(k = 1 / (pi * p^2)) %&gt;% ggplot(aes(x = p, y = k)) + geom_line() + labs(x = expression(italic(p)), y = expression(italic(k))) + coord_cartesian(ylim = c(0, 500)) McElreath’s quick and dirty solution was to set \\(k \\sim \\operatorname{Exponential}(0.5)\\), which has a prior predictive mean of 2. By setting up his model formula as exp(mu) = ..., McElreath effectively used the log link. It turns out that brms only supports the identity and inverse links for family = lognormal. However, we can sneak in the log link by nesting the right-hand side of the formula within log(). b16.1 &lt;- brm(data = d, family = lognormal, bf(w ~ log(3.141593 * k * p^2 * h^3), k + p ~ 1, nl = TRUE), prior = c(prior(beta(2, 18), nlpar = p, lb = 0, ub = 1), prior(exponential(0.5), nlpar = k, lb = 0), prior(exponential(1), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 16, file = &quot;fits/b16.01&quot;) Check the parameter summary. print(b16.1) ## Family: lognormal ## Links: mu = identity; sigma = identity ## Formula: w ~ log(3.141593 * k * p^2 * h^3) ## k ~ 1 ## p ~ 1 ## Data: d (Number of observations: 544) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## k_Intercept 5.79 2.72 2.16 12.49 1.00 1177 1488 ## p_Intercept 0.25 0.06 0.16 0.37 1.00 1178 1459 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 0.21 0.01 0.19 0.22 1.00 1345 1133 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). McElreath didn’t show the parameter summary for his m16.1 in the text. If you fit the model with both rethinking and brms, you’ll see our b16.1 matches up quite well. To make our version of Figure 16.2, we’ll use a GGally::ggpairs() workflow. First we’ll save our customizes settings for the three subplot types. my_lower &lt;- function(data, mapping, ...) { # get the x and y data to use the other code x &lt;- eval_data_col(data, mapping$x) y &lt;- eval_data_col(data, mapping$y) # compute the correlations corr &lt;- cor(x, y, method = &quot;p&quot;, use = &quot;pairwise&quot;) abs_corr &lt;- abs(corr) # plot the cor value ggally_text( label = formatC(corr, digits = 2, format = &quot;f&quot;) %&gt;% str_replace(., &quot;0.&quot;, &quot;.&quot;), mapping = aes(), size = 3.5, color = &quot;black&quot;, family = &quot;Times&quot;) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) } my_diag &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_histogram(size = 1/4, color = &quot;white&quot;, fill = &quot;grey67&quot;, bins = 20) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) } my_upper &lt;- function(data, mapping, ...) { ggplot(data = data, mapping = mapping) + geom_point(size = 1/4, alpha = 1/4) + scale_x_continuous(NULL, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) } Now we make our version of Figure 16.2.a. library(GGally) posterior_samples(b16.1) %&gt;% select(-lp__) %&gt;% set_names(c(&quot;italic(k)&quot;, &quot;italic(p)&quot;, &quot;sigma&quot;)) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower), labeller = label_parsed) + theme(strip.text = element_text(size = 8), strip.text.y = element_text(angle = 0)) We see the lack of identifiability of \\(k\\) and \\(p\\) resulted in a strong inverse relation between them. Now here’s how we might make Figure 16.2.b. nd &lt;- tibble(h = seq(from = 0, to = 1.5, length.out = 50)) p &lt;- predict(b16.1, newdata = nd) %&gt;% data.frame() %&gt;% bind_cols(nd) d %&gt;% ggplot(aes(x = h)) + geom_smooth(data = p, aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), stat = &quot;identity&quot;, fill = &quot;grey67&quot;, color = &quot;black&quot;, size = 1/4) + geom_point(aes(y = w), size = 1/3) + coord_cartesian(xlim = c(0, max(d$h)), ylim = c(0, max(d$w))) + labs(x = &quot;height (scaled)&quot;, y = &quot;weight (scaled)&quot;) Overall the model did okay, but the poor fit for the cases with lower values of height and weight suggests we might be missing important differences between children and adults. 16.1.3 GLM in disguise. Recall that because brms does not support the log link for the Log-Normal likelihood, we recast our b16.1 likelihood as \\[\\begin{align*} \\text{w}_i &amp; \\sim \\operatorname{Log-Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\log(k \\pi p^2 \\text{h}_i^3). \\end{align*}\\] Because multiplication becomes addition on the log scale, we can also express this as \\[\\begin{align*} \\text{w}_i &amp; \\sim \\operatorname{Log-Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp; = \\log(k) + \\log(\\pi) + 2 \\log(p) + 3 \\log(\\text{h}_i), \\end{align*}\\] which means our fancy non-linear model is just linear regression on the log scale. McElreath pointed this out to highlight one of the reasons that generalized linear models are so powerful. Lots of natural relationships are GLM relationships, on a specific scale of measurement. At the same time, the GLM approach wants to simply estimate parameters which may be informed by a proper theory, as in this case. (p. 531) 16.2 Hidden minds and observed behavior Load the Boxes data (van Leeuwen et al., 2018). data(Boxes, package = &quot;rethinking&quot;) d &lt;- Boxes rm(Boxes) rethinking::precis(d) ## mean sd 5.5% 94.5% histogram ## y 2.1208267 0.7279860 1 3 ▃▁▁▁▇▁▁▁▁▅ ## gender 1.5055644 0.5003669 1 2 ▇▁▁▁▁▁▁▁▁▇ ## age 8.0302067 2.4979055 5 13 ▇▃▅▃▃▃▂▂▂▁ ## majority_first 0.4848967 0.5001696 0 1 ▇▁▁▁▁▁▁▁▁▇ ## culture 3.7519873 1.9603189 1 8 ▃▂▁▇▁▂▁▂▁▂▁▁▁▁ The data are from 629 children. d %&gt;% count() ## n ## 1 629 Their ages ranged from 4 to 14 years and, as indicated by the histogram above, the bulk were on the younger end. d %&gt;% summarise(min = min(age), max = max(age)) ## min max ## 1 4 14 Here’s a depiction of our criterion variable y. # wrangle d %&gt;% mutate(color = factor(y, levels = 1:3, labels = c(&quot;unchosen&quot;, &quot;majority&quot;, &quot;minority&quot;))) %&gt;% mutate(y = str_c(y, &quot; (&quot;, color, &quot;)&quot;)) %&gt;% count(y) %&gt;% mutate(percent = (100 * n / sum(n))) %&gt;% mutate(label = str_c(round(percent, digits = 1), &quot;%&quot;)) %&gt;% # plot ggplot(aes(y = y)) + geom_col(aes(x = n)) + geom_text(aes(x = n + 4, label = label), hjust = 0, family = &quot;Times&quot;) + scale_x_continuous(expression(italic(n)), expand = expansion(mult = c(0, 0.1))) + labs(title = &quot;The criterion variable y indexes three kinds of color choices&quot;, subtitle = &quot;Children tended to prefer the &#39;majority&#39; color.&quot;, y = NULL) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) 16.2.1 The scientific model. The key, as always, is to think generatively. Consider for example a group of children in which half of them choose at random and the other half follow the majority. If we simulate choices for these children, we can figure out how often we might see the “2” choice, the one that indicates the majority color. (p. 532) Here’s an alternative way to set up McEreath’s R code 16.6. set.seed(16) n &lt;- 30 # number of children tibble(name = rep(c(&quot;y1&quot;, &quot;y2&quot;), each = n / 2), value = c(sample(1:3, size = n / 2, replace = T), rep(2, n / 2))) %&gt;% mutate(y = sample(value)) %&gt;% summarise(`number of 2s` = sum(y == 2) / n) ## # A tibble: 1 x 1 ## `number of 2s` ## &lt;dbl&gt; ## 1 0.633 We’ll consider 5 different strategies children might use. Follow the Majority: Copy the majority demonstrated color. Follow the Minority: Copy the minority demonstrated color. Maverick: Choose the color that no demonstrator chose. Random: Choose a color at random, ignoring the demonstrators. Follow First: Copy the color that was demonstrated first. This was either the majority color (when majority_first equals 1) or the minority color (when 0). (p. 533) 16.2.2 The statistical model. Our statistical will follow the form \\[\\begin{align*} y_i &amp; \\sim \\operatorname{Categorical}(\\theta) \\\\ \\theta_j &amp; = \\sum_{s = 1}^5 p_s \\operatorname{Pr}(j | s) \\;\\;\\; \\text{for} \\; j = 1, \\dots, 3 \\\\ p &amp; \\sim \\operatorname{Dirichlet}(4, 4, 4, 4, 4), \\end{align*}\\] where \\(s\\) indexes one of our five latent strategies and \\(\\operatorname{Pr}(j | s)\\) is the probability of one of the three color choices given a child is using the \\(s\\)th strategy. The \\(\\sum_{s = 1}^5 p_s\\) portion conveys these five probabilities are treated as a simplex, which means they will sum to one. We give these probabilities a Dirichlet prior, which we learned about back in Section 12.4. Here’s what that prior looks like. set.seed(16) rdirichlet(n = 1e5, alpha = rep(4, times = 5)) %&gt;% data.frame() %&gt;% set_names(1:5) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = name %&gt;% as.double(), alpha = str_c(&quot;alpha[&quot;, name, &quot;]&quot;)) %&gt;% ggplot(aes(x = value, group = name)) + geom_histogram(fill = &quot;grey67&quot;, binwidth = .02, boundary = 0) + scale_x_continuous(expression(italic(p[s])), limits = c(0, 1), breaks = c(0, .2, .5, 1), labels = c(&quot;0&quot;, &quot;.2&quot;, &quot;.5&quot;, &quot;1&quot;), ) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = expression(&quot;Dirichlet&quot;*(4*&quot;, &quot;*4*&quot;, &quot;*4*&quot;, &quot;*4*&quot;, &quot;*4))) + facet_wrap(~ alpha, labeller = label_parsed, nrow = 1) 16.2.3 Coding the statistical model. Let’s take a look at McElreath’s Stan code. library(rethinking) data(Boxes_model) cat(Boxes_model) ## ## data{ ## int N; ## int y[N]; ## int majority_first[N]; ## } ## parameters{ ## simplex[5] p; ## } ## model{ ## vector[5] phi; ## ## // prior ## p ~ dirichlet( rep_vector(4,5) ); ## ## // probability of data ## for ( i in 1:N ) { ## if ( y[i]==2 ) phi[1]=1; else phi[1]=0; // majority ## if ( y[i]==3 ) phi[2]=1; else phi[2]=0; // minority ## if ( y[i]==1 ) phi[3]=1; else phi[3]=0; // maverick ## phi[4]=1.0/3.0; // random ## if ( majority_first[i]==1 ) // follow first ## if ( y[i]==2 ) phi[5]=1; else phi[5]=0; ## else ## if ( y[i]==3 ) phi[5]=1; else phi[5]=0; ## ## // compute log( p_s * Pr(y_i|s ) ## for ( j in 1:5 ) phi[j] = log(p[j]) + log(phi[j]); ## // compute average log-probability of y_i ## target += log_sum_exp( phi ); ## } ## } I’m not aware that one can fit this model directly with brms. My guess is that if it’s possible, it would require a custom likelihood (see Bürkner, 2021a). If you can reproduce McElreath’s Stan code with this or some other approach in brms, please share your code on GitHub. In the mean time, we’re going to follow along with the text and fit the model with rstan::stan(). # prep data dat_list &lt;- list( N = nrow(d), y = d$y, majority_first = d$majority_first ) # run the sampler m16.2 &lt;- stan(model_code = Boxes_model, data = dat_list, chains = 3, cores = 3, seed = 16) Here’s what it looks like if you print() a fit object from the stan() function. print(m16.2) ## Inference for Stan model: 27b25f2c261c65f6230541a266cbbdbd. ## 3 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=3000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## p[1] 0.26 0.00 0.04 0.19 0.23 0.26 0.28 0.32 986 1.00 ## p[2] 0.14 0.00 0.03 0.07 0.12 0.14 0.16 0.20 1087 1.01 ## p[3] 0.15 0.00 0.03 0.09 0.13 0.15 0.17 0.20 1066 1.01 ## p[4] 0.20 0.00 0.08 0.07 0.14 0.19 0.25 0.37 792 1.01 ## p[5] 0.26 0.00 0.03 0.20 0.24 0.26 0.28 0.32 2559 1.00 ## lp__ -667.17 0.04 1.44 -670.69 -667.91 -666.81 -666.12 -665.32 1203 1.00 ## ## Samples were drawn using NUTS(diag_e) at Sat Nov 7 14:32:59 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). Here’s the summary with rethinking::precis(). precis(m16.2, depth = 2) ## mean sd 5.5% 94.5% n_eff Rhat4 ## p[1] 0.2576792 0.03642940 0.19925939 0.3143086 985.7502 1.0042274 ## p[2] 0.1390785 0.03299730 0.08584531 0.1916790 1086.6851 1.0056198 ## p[3] 0.1479720 0.03021546 0.09678031 0.1942334 1066.2915 1.0066600 ## p[4] 0.1961164 0.07966720 0.08178310 0.3356874 791.8116 1.0078264 ## p[5] 0.2591538 0.03212438 0.20681421 0.3103560 2559.2139 0.9991496 Here we show marginal posterior for \\(p_s\\). label &lt;- c(&quot;Majority~(italic(s)[1])&quot;, &quot;Minority~(italic(s)[2])&quot;, &quot;Maverick~(italic(s)[3])&quot;, &quot;Random~(italic(s)[4])&quot;, &quot;Follow~First~(italic(s)[5])&quot;) precis(m16.2, depth = 2) %&gt;% data.frame() %&gt;% mutate(name = factor(label, levels = label)) %&gt;% mutate(name = fct_rev(name)) %&gt;% ggplot(aes(x = mean, xmin = X5.5., xmax = X94.5., y = name)) + geom_vline(xintercept = .2, size = 1/4, linetype = 2) + geom_pointrange(size = 1/4, fatten = 6/4) + scale_x_continuous(expression(italic(p[s])), limits = c(0, 1), breaks = 0:5 / 5) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + theme(axis.text.y = element_text(hjust = 0)) As an alternative, this might be a good time to revisit the tidybayes::stat_ccdfinterval() approach (see Section 12.3.3), which will depict those posteriors with a bar plot where the ends of the bars depict our uncertainty in terms of cumulative density curves. extract.samples(m16.2) %&gt;% data.frame() %&gt;% select(-lp__) %&gt;% set_names(label) %&gt;% pivot_longer(everything()) %&gt;% mutate(name = factor(name, levels = label)) %&gt;% mutate(name = fct_rev(name)) %&gt;% ggplot(aes(x = value, y = name)) + geom_vline(xintercept = .2, size = 1/4, linetype = 2) + stat_ccdfinterval(.width = .95, size = 1/4, alpha = .8) + scale_x_continuous(expression(italic(p[s])), expand = c(0, 0), limits = c(0, 1), breaks = 0:5 / 5) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + theme(axis.text.y = element_text(hjust = 0), axis.ticks.y = element_blank()) 16.2.4 State space models. In this section of the text, McElreath mentioned state space models and hidden Markov models. Based on this thread in the Stan forums and this issue in the brms GitHub repo, it looks like state space models are not supported in brms at this time. As for hidden Markov models, I’m not sure whether they are supported by brms. The best I could find was this thread on the Stan forums. If you know more about either of these topics, please share your knowledge on this book’s GitHub repo. 16.3 Ordinary differential nut cracking Load the Panda_nuts data (Boesch et al., 2019). data(Panda_nuts, package = &quot;rethinking&quot;) d &lt;- Panda_nuts rm(Panda_nuts) Anticipating McElreath’s R code 16.11, we’ll wrangle a little. d &lt;- d %&gt;% mutate(n = nuts_opened, age_s = age / max(age)) glimpse(d) ## Rows: 84 ## Columns: 9 ## $ chimpanzee &lt;int&gt; 11, 11, 18, 18, 18, 11, 11, 17, 7, 1, 22, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 7, 9, 1, 7, 13,… ## $ age &lt;int&gt; 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6… ## $ sex &lt;fct&gt; m, m, f, f, f, m, m, f, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m… ## $ hammer &lt;fct&gt; G, G, wood, G, L, Q, Q, wood, G, L, wood, G, G, G, G, G, G, G, G, G, G, G, G, L, G, wood… ## $ nuts_opened &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 58, 4, 21, 9, 2, 30, 19, 13, 6, 11, 1, 2, 0, 1, 0, 0, 0… ## $ seconds &lt;dbl&gt; 61.0, 37.0, 20.0, 14.0, 13.0, 24.0, 30.5, 135.0, 24.0, 13.0, 34.0, 66.5, 5.0, 24.0, 20.0… ## $ help &lt;fct&gt; N, N, N, y, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, y, N, N, N, N, N, y, y, N… ## $ n &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 58, 4, 21, 9, 2, 30, 19, 13, 6, 11, 1, 2, 0, 1, 0, 0, 0… ## $ age_s &lt;dbl&gt; 0.1875, 0.1875, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.3125, 0.3125, 0.3125, 0.3125, … Our criterion is n, the number of Panda nuts opened by a chimpanzee on a given occasion. The two focal predictor variables are age and seconds. Here they are depicted in a pairs plot. d %&gt;% select(n, age, seconds) %&gt;% ggpairs(upper = list(continuous = my_upper), diag = list(continuous = my_diag), lower = list(continuous = my_lower)) + theme(strip.text.y = element_text(hjust = 0, angle = 0)) 16.3.1 Scientific model. As a starting point, McElreath proposed we the strength of a chimpanzee would relate to the number of nuts they might open. We don’t have a measure of strength, but we do have age, which is a proxy for how close a chimp might be to their maximum body size, and we presume body size would be proportional to strength. If we let \\(t\\) index time, \\(M_\\text{max}\\) be the maximum body size (mass), \\(M_t\\) be the current body size, and \\(k\\) stand for the rate of skill gain the comes with age, we can write \\[M_t = M_\\text{max} [1 - \\exp(-kt) ]\\] to solve for mass at a given age (von Bertalanffy, 1934). But again, we actually care about strength, not mass. Letting \\(S_t\\) be strength at time \\(t\\), we can express a proportional relation between the two as \\(S_t = \\beta M_t\\). Now if we let \\(\\lambda\\) stand in for the number of nuts opening, \\(\\alpha\\) express the relation of strength to nut opening, we can write \\[\\lambda = \\alpha S_t^\\theta = \\alpha \\big ( \\beta M_\\text{max} [1 - \\exp(-kt) ] \\big ) ^\\theta,\\] “where \\(\\theta\\) is some exponent greater than 1” (p. 538). If we rescale \\(M_\\text{max} = 1\\), we can simplify the equation to \\[\\lambda = \\alpha \\beta^\\theta [1 - \\exp(-kt) ]^\\theta.\\] As “the product \\(\\alpha \\beta^\\theta\\) in the front just rescales strength to nuts-opened-per-second” (p. 538), we can collapse it to a single parameter, \\(\\phi\\), which leaves us with \\[\\lambda = \\phi [1 - \\exp(-kt) ]^\\theta.\\] This is our scientific model. 16.3.2 Statistical model. Now if we let \\(n_i\\) be the number of nuts opened, we can write our statistical model as \\[\\begin{align*} n_i &amp; \\sim \\operatorname{Poisson}(\\lambda_i) \\\\ \\lambda_i &amp; = \\text{seconds}_i \\, \\phi [1 - \\exp(-k \\,\\text{age}_i) ]^\\theta, \\end{align*}\\] where we have replaced our time index, \\(t\\), with the variable age. By including the variable seconds in the equation, we have scaled the results to be nuts per second. McElreath proposed the priors: \\[\\begin{align*} \\phi &amp; \\sim \\operatorname{Log-Normal}(\\log 1, 0.10) \\\\ k &amp; \\sim \\operatorname{Log-Normal}(\\log 2, 0.25) \\\\ \\theta &amp; \\sim \\operatorname{Log-Normal}(\\log 5, 0.25), \\end{align*}\\] all of which were Log-Normal to ensure the parameters were positive and continuous. To get a sense of what these priors implied, he simulated. Here’s our version of his simulations, which make up Figure 16.4. n &lt;- 1e4 # define the x-axis breaks at &lt;- 0:6 / 4 # how many prior draws would you like? n_samples &lt;- 50 # simulate set.seed(16) prior &lt;- tibble(index = 1:n, phi = rlnorm(n, meanlog = log(1), sdlog = 0.1), k = rlnorm(n, meanlog = log(2), sdlog = 0.25), theta = rlnorm(n, meanlog = log(5), sdlog = 0.25)) %&gt;% slice_sample(n = n_samples) %&gt;% expand(nesting(index, phi, k, theta), age = seq(from = 0, to = 1.5, length.out = 1e2)) %&gt;% mutate(bm = 1 - exp(-k * age), ns = phi * (1 - exp(-k * age))^theta) # left panel p1 &lt;- prior %&gt;% ggplot(aes(x = age, y = bm, group = index)) + geom_line(size = 1/4, alpha = 1/2) + scale_x_continuous(breaks = at, labels = round(at * max(d$age))) + ylab(&quot;body mass&quot;) # right panel p2 &lt;- prior %&gt;% ggplot(aes(x = age, y = ns, group = index)) + geom_line(size = 1/4, alpha = 1/2) + scale_x_continuous(breaks = at, labels = round(at * max(d$age))) + ylab(&quot;nuts per second&quot;) # combine and plot library(patchwork) p1 + p2 + plot_annotation(title = &quot;Prior predictive simulation for the nut opening model&quot;, subtitle = &quot;Each panel shows the results from 50 prior draws.&quot;) McElreath suggested we inspect the distributions of these priors. Here they are in a series of histograms. set.seed(16) tibble(phi = rlnorm(n, meanlog = log(1), sdlog = 0.1), `italic(k)` = rlnorm(n, meanlog = log(2), sdlog = 0.25), theta = rlnorm(n, meanlog = log(5), sdlog = 0.25)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_histogram(fill = &quot;grey67&quot;, bins = 40, boundary = 0) + scale_x_continuous(&quot;marginal prior&quot;, limits = c(0, NA)) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~ name, scales = &quot;free&quot;, labeller = label_parsed) Happily, we can fit this model using the non-linear brms syntax. b16.4 &lt;- brm(data = d, family = poisson(link = identity), bf(n ~ seconds * phi * (1 - exp(-k * age_s))^theta, phi + k + theta ~ 1, nl = TRUE), prior = c(prior(lognormal(log(1), 0.1), nlpar = phi, lb = 0), prior(lognormal(log(2), 0.25), nlpar = k, lb = 0), prior(lognormal(log(5), 0.25), nlpar = theta, lb = 0)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 16, file = &quot;fits/b16.04&quot;) Check the parameter summary. print(b16.4) ## Family: poisson ## Links: mu = identity ## Formula: n ~ seconds * phi * (1 - exp(-k * age_s))^theta ## phi ~ 1 ## k ~ 1 ## theta ~ 1 ## Data: d (Number of observations: 84) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## phi_Intercept 0.86 0.04 0.79 0.95 1.00 1045 1589 ## k_Intercept 5.97 0.56 4.88 7.09 1.00 787 1151 ## theta_Intercept 9.81 2.00 6.50 14.31 1.00 858 1276 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). No we might get a sense of what this posterior means by plotting nuts per second as a function of age in our version of Figure 16.5. posterior_samples(b16.4) %&gt;% mutate(iter = 1:n()) %&gt;% slice_sample(n = n_samples) %&gt;% expand(nesting(iter, b_phi_Intercept, b_k_Intercept, b_theta_Intercept), age = seq(from = 0, to = 1.5, length.out = 1e2)) %&gt;% mutate(ns = b_phi_Intercept * (1 - exp(-b_k_Intercept * age))^b_theta_Intercept) %&gt;% ggplot() + geom_line(aes(x = age, y = ns, group = iter), size = 1/4, alpha = 1/2) + geom_jitter(data = d, aes(x = age_s, y = n / seconds, size = seconds), shape = 1, width = 0.01, color = &quot;grey50&quot;) + scale_size_continuous(breaks = c(1, 50, 100), limits = c(1, NA)) + scale_x_continuous(breaks = at, labels = round(at * max(d$age))) + labs(title = &quot;Posterior predictive distribution for the\\nnut opening model&quot;, y = &quot;nuts per second&quot;) + theme(legend.background = element_blank(), legend.position = c(.9, .25)) Looks like things flatten out around age == 16. Yet since the data drop off at that age, we probably shouldn’t get overconfident. 16.4 Population dynamics Load the Lynx_Hare population dynamics data (Hewitt, 1921). data(Lynx_Hare, package = &quot;rethinking&quot;) glimpse(Lynx_Hare) ## Rows: 21 ## Columns: 3 ## $ Year &lt;int&gt; 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915,… ## $ Lynx &lt;dbl&gt; 4.0, 6.1, 9.8, 35.2, 59.4, 41.7, 19.0, 13.0, 8.3, 9.1, 7.4, 8.0, 12.3, 19.5, 45.7, 51.1, 29.7, … ## $ Hare &lt;dbl&gt; 30.0, 47.2, 70.2, 77.4, 36.3, 20.6, 18.1, 21.4, 22.0, 25.4, 27.1, 40.3, 57.0, 76.6, 52.3, 19.5,… As McElreath indicated in his endnote #238 (p. 570), this example is based on Stan case study by the great Bob Carpenter, Predator-prey population dynamics: The Lotka-Volterra model in Stan. You might bookmark that link. It’ll come up later in this section. Figure 6.6 will give us a sense of how the lynx and hare populations ebbed and flowed. # for annotation text &lt;- tibble(name = c(&quot;Hare&quot;, &quot;Lynx&quot;), label = c(&quot;Lepus&quot;, &quot;Lynx&quot;), Year = c(1913.5, 1915.5), value = c(78, 52)) # wrangle Lynx_Hare %&gt;% pivot_longer(-Year) %&gt;% # plot! ggplot(aes(x = Year, y = value)) + geom_line(aes(color = name), size = 1/4) + geom_point(aes(fill = name), size = 3, shape = 21, color = &quot;white&quot;) + geom_text(data = text, aes(label = label, color = name), hjust = 0, family = &quot;Times&quot;) + scale_fill_grey(start = 0, end = .5) + scale_color_grey(start = 0, end = .5) + scale_y_continuous(&quot;thousands of pelts&quot;, breaks = 0:4 * 20, limits = c(0, 90)) + theme(legend.position = &quot;none&quot;) Note, however, that these are numbers of pelts, not of actual animals. This will become important when we start modeling. A typical way to model evenly-spaced time series data like this would be with an autoregressive model with the basic structure \\[\\operatorname{E}(y_t) = \\alpha + \\beta_1 y_{t-1},\\] where \\(t\\) indexes time and \\(t - 1\\) is the time point immediately before \\(t\\). Models following this form are called first-order autoregressive models, AR(1), meaning that the current time point is only influenced by the previous time point, but none of the earlier ones. You can build on this format by adding other predictors. A natural way would be to use a predictor from \\(t - 1\\) to predict \\(y_t\\), following the form \\[\\operatorname{E}(y_t) = \\alpha + \\beta_1 y_{t-1} + \\beta_2 x_{t-1}.\\] But that’s still a first-order model. A second-order model, AR(2), would include a term for \\(y_{t - 2}\\), such as \\[\\operatorname{E}(y_t) = \\alpha + \\beta_1 y_{t-1} + \\beta_2 x_{t-1} + \\beta_3 y_{t-2}.\\] McElreath isn’t a huge fan of these models, particularly from the scientific modeling perspective he developed in this chapter. But brms can fit them and we’ll practice a little in a bonus section, later on. In the mean time, we’ll follow along and learn about ordinary differential equations (ODEs). 16.4.1 The scientific model. We’ll start off simple and focus first on hares. If we let \\(H_t\\) be the number of hares at time \\(t\\), we can express the rate of change in the hare population as \\[\\frac{\\mathrm{d} H}{\\mathrm{d} t} = H_t \\times (\\text{birth rate}) - H_t \\times (\\text{death rate}).\\] If we presume both birth rates and death rates (mortality rates) are constants, we might denote them \\(b_H\\) and \\(m_H\\), respectively, and re-express the formula as \\[\\frac{\\mathrm{d} H}{\\mathrm{d} t} = H_t b_H - H_t m_H = H_t (b_H - m_H).\\] Building, if we let \\(L_t\\) stand for the number of lynx present at time \\(t\\), we can allow the mortality rate depend on that variable with the expanded formula \\[\\frac{\\mathrm{d} H}{\\mathrm{d} t} = H_t (b_H - L_t m_H).\\] We can expand this even further to model how the number of hares at a given time influence the birth rate for lynx (\\(b_L\\)) to help us model the rate of change in the lynx population as \\[\\frac{\\mathrm{d} L}{\\mathrm{d} t} = L_t (H_t b_L - m_L),\\] where the lynx mortality rate (\\(m_l\\)) is now constant. This is called the Lotka-Volterra model (Lotka, 1925; Volterra, 1926). You may have noticed how the above equations shifted our focus from what were were originally interested in, \\(\\operatorname{E}(H_t)\\), to a rate of change, \\(\\mathrm{d} H / \\mathrm{d} t\\). Happily, our equation for \\(\\mathrm{d} H / \\mathrm{d} t\\), “tells us how to update \\(H\\) after each tiny unit of passing time \\(\\mathrm d t\\)” (p. 544). You update by \\[H_{t +\\mathrm d t} = H_t + \\mathrm d t \\frac{\\mathrm d H}{\\mathrm d t} = H_t + \\mathrm d t H_t (b_H - L_t m_H).\\] Here we’ll use a custom function called sim_lynx_hare() to simulate how this can work. Our version of the function is very similar to the one McElreath displayed in his R code 16.14, but we changed it so it returns a tibble which includes a time index, t. sim_lynx_hare &lt;- function(n_steps, init, theta, dt = 0.002) { L &lt;- rep(NA, n_steps) H &lt;- rep(NA, n_steps) # set initial values L[1] &lt;- init[1] H[1] &lt;- init[2] for (i in 2:n_steps) { H[i] &lt;- H[i - 1] + dt * H[i - 1] * (theta[1] - theta[2] * L[i - 1]) L[i] &lt;- L[i - 1] + dt * L[i - 1] * (theta[3] * H[i - 1] - theta[4]) } # return a tibble tibble(t = 1:n_steps, H = H, L = L) } Now we simulate. # set the four theta values theta &lt;- c(0.5, 0.05, 0.025, 0.5) # simulate z &lt;- sim_lynx_hare(n_steps = 1e4, init = c(filter(Lynx_Hare, Year == 1900) %&gt;% pull(&quot;Lynx&quot;), filter(Lynx_Hare, Year == 1900) %&gt;% pull(&quot;Hare&quot;)), theta = theta) # what did we do? glimpse(z) ## Rows: 10,000 ## Columns: 3 ## $ t &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27,… ## $ H &lt;dbl&gt; 30.00000, 30.01800, 30.03600, 30.05401, 30.07203, 30.09005, 30.10807, 30.12610, 30.14413, 30.16217… ## $ L &lt;dbl&gt; 4.000000, 4.002000, 4.004005, 4.006014, 4.008028, 4.010046, 4.012069, 4.014097, 4.016129, 4.018166… Each row is a stand-in index for time. Here we’ll explicitly add a time column and them plot the results in our version of Figure 16.7. z %&gt;% pivot_longer(-t) %&gt;% ggplot(aes(x = t, y = value, color = name)) + geom_line(size = 1/4) + scale_color_grey(start = 0, end = .5) + scale_x_continuous(expression(time~(italic(t))), breaks = NULL) + scale_y_continuous(&quot;number (thousands)&quot;, breaks = 0:4 * 10, limits = c(0, 45)) + theme(legend.position = &quot;none&quot;) “This model produces cycles, similar to what we see in the data. The model behaves this way, because lynx eat hares. Once the hares are eaten, the lynx begin to die off. Then the cycle repeats (p. 545).” 16.4.2 The statistical model. If we continue to let \\(H_t\\) and \\(L_t\\) be the number of hares and lynx at time \\(t\\), we might also want to rehearse the distinction between those numbers and our observations by letting \\(h_t\\) and \\(l_t\\) stand for the observed numbers of hares and lynx. These observed numbers, recall, are from counts of pelts. We want a statistical model that can connect \\(h_t\\) to \\(H_t\\) and connect \\(l_t\\) to \\(L_t\\). Part of that model would include the probability a hare was trapped on a given year, \\(p_h\\), and a similar probability for a lynx getting trapped, \\(p_l\\). To make things worse, further imagine the number of pelts for each, in a given year, was rounded to the nearest \\(100\\) and divided by \\(1{,}000\\). Those are our values. We practice simulating all this in Figure 16.8. Here we propose a population of \\(H_t = 10^4\\) hares and an average trapping rate of about \\(10\\%\\), as expressed by \\(p_t \\sim \\operatorname{Beta}(2, 18)\\). As described above, we then divide the number of observed pelts by \\(1{,}000\\) and round the results, yielding \\(h_t\\). n &lt;- 1e4 Ht &lt;- 1e4 set.seed(16) # simulate tibble(pt = rbeta(n, shape1 = 2, shape2 = 18)) %&gt;% mutate(ht = rbinom(n, size = Ht, prob = pt)) %&gt;% mutate(ht = round(ht / 1000, digits = 2)) %&gt;% # plot ggplot(aes(x = ht)) + geom_histogram(size = 1/4, binwidth = 0.1, color = &quot;white&quot;, fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(thousand~of~pelts~(italic(h[t])))) On page 546, McElreath encouraged us to try the simulation with different values of \\(H_t\\) and \\(p_t\\). Here we’ll do so with a \\(3 \\times 3\\) grid of \\(H_t = \\{5{,}000, 10{,}000, 15{,}000\\}\\) and \\(p_t \\sim \\{ \\operatorname{Beta}(2, 18), \\operatorname{Beta}(10, 10), \\operatorname{Beta}(18, 2) \\}\\). set.seed(16) # define the 3 X 3 grid tibble(shape1 = c(2, 10, 18), shape2 = c(18, 10, 2)) %&gt;% expand(nesting(shape1, shape2), Ht = c(5e3, 1e4, 15e3)) %&gt;% # simulate mutate(pt = purrr::map2(shape1, shape2, ~rbeta(n, shape1 = .x, shape2 = .y))) %&gt;% mutate(ht = purrr::map2(Ht, pt, ~rbinom(n, size = .x, prob = .y))) %&gt;% unnest(c(pt, ht)) %&gt;% # wrangle mutate(ht = round(ht / 1000, digits = 2), beta = str_c(&quot;italic(p[t])%~%&#39;Beta &#39;(&quot;, shape1, &quot;, &quot;, shape2, &quot;)&quot;), Htlab = str_c(&quot;italic(H[t])==&quot;, Ht)) %&gt;% mutate(beta = factor(beta, levels = c(&quot;italic(p[t])%~%&#39;Beta &#39;(2, 18)&quot;, &quot;italic(p[t])%~%&#39;Beta &#39;(10, 10)&quot;, &quot;italic(p[t])%~%&#39;Beta &#39;(18, 2)&quot;)), Htlab = factor(Htlab, levels = c(&quot;italic(H[t])==15000&quot;, &quot;italic(H[t])==10000&quot;, &quot;italic(H[t])==5000&quot;))) %&gt;% # plot! ggplot(aes(x = ht)) + geom_histogram(aes(fill = beta == &quot;italic(p[t])%~%&#39;Beta &#39;(2, 18)&quot; &amp; Htlab == &quot;italic(H[t])==10000&quot;), size = 1/10, binwidth = 0.25, boundary = 0) + geom_vline(aes(xintercept = Ht / 1000), size = 1/4, linetype = 2) + scale_fill_grey(start = .67, end = 0, breaks = NULL) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(thousand~of~pelts~(italic(h[t])))) + facet_grid(Htlab ~ beta, labeller = label_parsed, scales = &quot;free_y&quot;) The vertical dashed lines mark off the maximum values in each panel. The histogram in black is of the simulation parameters based on our version of Figure 16.8, above. McElreath’s proposed model is \\[\\begin{align*} h_t &amp; \\sim \\operatorname{Log-Normal} \\big (\\log(p_H H_t), \\sigma_H \\big) \\\\ l_t &amp; \\sim \\operatorname{Log-Normal} \\big (\\log(p_L L_t), \\sigma_L \\big) \\\\ H_1 &amp; \\sim \\operatorname{Log-Normal}(\\log 10, 1) \\\\ L_1 &amp; \\sim \\operatorname{Log-Normal}(\\log 10, 1) \\\\ H_{T &gt;1} &amp; = H_1 + \\int_1^T H_t (b_H - m_H L_t) \\mathrm{d} t \\\\ L_{T &gt;1} &amp; = L_1 + \\int_1^T L_t (b_L H_T - m_L) \\mathrm{d} t \\\\ \\sigma_H &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_L &amp; \\sim \\operatorname{Exponential}(1) \\\\ p_H &amp; \\sim \\operatorname{Beta}(\\alpha_H, \\beta_H) \\\\ p_L &amp; \\sim \\operatorname{Beta}(\\alpha_L, \\beta_L) \\\\ b_H &amp; \\sim \\operatorname{Half-Normal}(1, 0.5) \\\\ b_L &amp; \\sim \\operatorname{Half-Normal}(0.5, 0.5) \\\\ m_H &amp; \\sim \\operatorname{Half-Normal}(0.5, 0.5) \\\\ m_L &amp; \\sim \\operatorname{Half-Normal}(1, 0.5). \\end{align*}\\] It’s not immediately clear from the text, but if you look closely at the output from cat(Lynx_Hare_model) (see below), you’ll see \\(\\alpha_H = \\alpha_L = 40\\) and \\(\\beta_H = \\beta_L = 200\\). If you’re curious, here’s a plot of what the \\(\\operatorname{Beta}(40, 200)\\) prior looks like. set.seed(16) tibble(p = rbeta(n = 1e6, shape1 = 40, shape2 = 200)) %&gt;% ggplot(aes(x = p)) + geom_histogram(size = 1/6, binwidth = 0.005, boundary = 0, color = &quot;white&quot;, fill = &quot;grey67&quot;) + scale_x_continuous(expression(prior~predictive~distribution~of~italic(p[H])~and~italic(p[L])), breaks = 0:5 / 5, expand = c(0, 0), limits = c(0, 1)) + scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) + labs(subtitle = expression(&quot;1,000,000 draws from Beta&quot;*(40*&quot;, &quot;*200))) The \\(\\operatorname{Beta}(40, 200)\\) prior suggests an average trapping rate near 16%. ️ The content to follow is going to diverge from the text, a bit. As you can see from the equation, above, McElreath’s statistical model is a beast. We can fit this model with brms, but the workflow is more complicated than usual. To make this material more approachable, I am going to divide the remainder of this section into two subsections. In the first subsection, we’ll fit a simplified version of McElreath’s m16.5, which does not contain the measurement-error portion. In the second subsection, we’ll tack on the measurement-error portion and fit the full model. ️ 16.4.2.1 The simple Lotka-Volterra model. Before we get into it, I should acknowledge that this brms approach to fitting ODE’s is a direct result of the generous contributions from Markus Gesmann. It was one of his older blog posts, PK/PD reserving models, that led me to believe one could fit an ODE model with brms. When I reached out to Gesmann on GitHub (see Issue #18), he went so far as to write a new blog post on exactly this model: Fitting multivariate ODE models with brms. The workflow to follow is something of a blend of the methods in his blog post, McElreath’s model in the text, and the original post by Carpenter that started this all. As far as the statistical model goes, we might express the revision of McElreath’s model omitting the measurement-error portion as \\[\\begin{align*} h_t &amp; \\sim \\operatorname{Log-Normal} \\big (\\log(H_t), \\sigma_H \\big) \\\\ l_t &amp; \\sim \\operatorname{Log-Normal} \\big (\\log(L_t), \\sigma_L \\big) \\\\ H_1 &amp; \\sim \\operatorname{Log-Normal}(\\log 10, 1) \\\\ L_1 &amp; \\sim \\operatorname{Log-Normal}(\\log 10, 1) \\\\ H_{T &gt;1} &amp; = H_1 + \\int_1^T H_t (b_H - m_H L_t) \\mathrm{d} t \\\\ L_{T &gt;1} &amp; = L_1 + \\int_1^T L_t (b_L H_T - m_L) \\mathrm{d} t \\\\ b_H &amp; \\sim \\operatorname{Half-Normal}(1, 0.5) \\\\ b_L &amp; \\sim \\operatorname{Half-Normal}(0.5, 0.5) \\\\ m_H &amp; \\sim \\operatorname{Half-Normal}(0.5, 0.5) \\\\ m_L &amp; \\sim \\operatorname{Half-Normal}(1, 0.5) \\\\ \\sigma_H &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_L &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] With the exception of the priors for the \\(\\sigma\\) parameters, this is basically the same model Carpenter fit with his original Stan code. Carpenter expressed his model using a different style of notation, but the parts are all there. As for our brms, the first issue we need to address is that, at the time of this writing, brms is only set up to fit a univariate ODE model. As Gesmann pointed out, the way around this is to convert the Lynx_Hare data into the long format where the pelt values from the Lynx and Hare columns are all listed in a pelts columns and the two animal populations are differentiated in a population column. We’ll call this long version of the data Lynx_Hare_long. Lynx_Hare_long &lt;- Lynx_Hare %&gt;% pivot_longer(-Year, names_to = &quot;population&quot;, values_to = &quot;pelts&quot;) %&gt;% mutate(delta = if_else(population == &quot;Lynx&quot;, 1, 0), t = Year - min(Year) + 1) %&gt;% arrange(delta, Year) # what did we do? head(Lynx_Hare_long) ## # A tibble: 6 x 5 ## Year population pelts delta t ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1900 Hare 30 0 1 ## 2 1901 Hare 47.2 0 2 ## 3 1902 Hare 70.2 0 3 ## 4 1903 Hare 77.4 0 4 ## 5 1904 Hare 36.3 0 5 ## 6 1905 Hare 20.6 0 6 You’ll note how we converted the information in the population column into a dummy variable, delta, which is coded 0 = hares, 1 = lynxes. It’s that dummy variable that will allow us to adjust our model formula so we express a bivariate model as if it were univariate. You’ll see. Also notice how we added a t index for time. This is because the Stan code to follow will expect us to index time in that way. The next step is to write a script that will tell brms how to tell Stan how to fit a Lotka-Volterra model. In his blog, Gesmann called this LotkaVolterra. Our script to follow is a very minor adjustment of his. LotkaVolterra &lt;- &quot; // Sepcify dynamical system (ODEs) real[] ode_LV(real t, // time real [] y, // the system rate real [] theta, // the parameters (i.e., the birth and mortality rates) real [] x_r, // data constant, not used here int [] x_i) { // data constant, not used here // the outcome real dydt[2]; // differential equations dydt[1] = (theta[1] - theta[2] * y[2]) * y[1]; // Hare process dydt[2] = (theta[3] * y[1] - theta[4]) * y[2]; // Lynx process return dydt; // return a 2-element array } // Integrate ODEs and prepare output real LV(real t, real Hare0, real Lynx0, real brHare, real mrHare, real brLynx, real mrLynx, real delta) { real y0[2]; // Initial values real theta[4]; // Parameters real y[1, 2]; // ODE solution // Set initial values y0[1] = Hare0; y0[2] = Lynx0; // Set parameters theta[1] = brHare; theta[2] = mrHare; theta[3] = brLynx; theta[4] = mrLynx; // Solve ODEs y = integrate_ode_rk45(ode_LV, y0, 0, rep_array(t, 1), theta, rep_array(0.0, 0), rep_array(1, 1), 0.001, 0.001, 100); // tolerances, steps // Return relevant population values based on our dummy-variable coding method return (y[1, 1] * (1 - delta) + y[1, 2] * delta); } &quot; If you study this, you’ll see echos of Carpenter’s original Stan code and connections to McElreath’s Stan code (execute cat(Lynx_Hare_model) from his R code 16.17 block), too. But take special notice of the last two lines, above. Those lines use the delta dummy to differentiate the model results for the hare and lynx populations, respectively. Next we define our formula input. To keep from overwhelming the brm() code, we’ll save it, here, as an independent object called lv_formula. lv_formula &lt;- bf(pelts ~ log(eta), # use our LV() function from above nlf(eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta)), # initial population state H1 ~ 1, L1 ~ 1, # hare parameters bh ~ 1, mh ~ 1, # lynx parameters bl ~ 1, ml ~ 1, # population-based measurement errors sigma ~ 0 + population, nl = TRUE ) Note our use of the LV() function in the nlf() line. That’s a function defined in the LotkaVolterra script, above, which will allow us to connect the variables and parameters in our formula code to the underlying statistical model. Next we define our priors and save them as an independent object called lv_priors. lv_priors &lt;- c( prior(lognormal(log(10), 1), nlpar = H1, lb = 0), prior(lognormal(log(10), 1), nlpar = L1, lb = 0), prior(normal(1, 0.5), nlpar = bh, lb = 0), prior(normal(0.05, 0.05), nlpar = bl, lb = 0), prior(normal(0.05, 0.05), nlpar = mh, lb = 0), prior(normal(1, 0.5), nlpar = ml, lb = 0), prior(exponential(1), dpar = sigma, lb = 0) ) Now whether you can fit this model in brms may depend on which version you’re using. For details, see the endnote6. In short, just update to the current version. Within our brm() code, notice our stanvars settings. Also, I find these models benefit from setting inits = 0. Happily, this model fit in just about four minutes on my 2019 MacBook Pro. b16.5a &lt;- brm(data = Lynx_Hare_long, family = brmsfamily(&quot;lognormal&quot;, link_sigma = &quot;identity&quot;), formula = lv_formula, prior = lv_priors, iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = 0, stanvars = stanvar(scode = LotkaVolterra, block = &quot;functions&quot;), file = &quot;fits/b16.05a&quot;) On page 548, McElreath recommend we check the chains. Here we’ll pretty them up with help from bayesplot. library(bayesplot) color_scheme_set(&quot;gray&quot;) col_names &lt;- c(&quot;italic(H)[1]&quot;, &quot;italic(L)[1]&quot;, str_c(&quot;italic(&quot;, c(&quot;b[H]&quot;, &quot;m[H]&quot;, &quot;b[L]&quot;, &quot;m[L]&quot;), &quot;)&quot;), &quot;sigma[italic(H)]&quot;, &quot;sigma[italic(L)]&quot;, &quot;lp__&quot;, &quot;chain&quot;, &quot;iter&quot;) posterior_samples(b16.5a, add_chain = T) %&gt;% set_names(col_names) %&gt;% mcmc_trace(pars = vars(-iter, -lp__), facet_args = list(labeller = label_parsed), size = .15) + scale_x_continuous(breaks = NULL) + theme(legend.key.size = unit(0.15, &#39;in&#39;), legend.position = c(.97, .13)) They look like a dream. Now inspect the parameter summary. print(b16.5a) ## Family: lognormal ## Links: mu = identity; sigma = identity ## Formula: pelts ~ log(eta) ## eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta) ## H1 ~ 1 ## L1 ~ 1 ## bh ~ 1 ## mh ~ 1 ## bl ~ 1 ## ml ~ 1 ## sigma ~ 0 + population ## Data: Lynx_Hare_long (Number of observations: 42) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## H1_Intercept 23.52 2.17 19.58 28.03 1.00 2019 2274 ## L1_Intercept 6.69 0.70 5.45 8.20 1.00 1826 2445 ## bh_Intercept 0.55 0.07 0.43 0.69 1.00 1003 1321 ## mh_Intercept 0.03 0.00 0.02 0.04 1.00 1078 1609 ## bl_Intercept 0.02 0.00 0.02 0.03 1.00 1087 1498 ## ml_Intercept 0.80 0.09 0.63 1.00 1.00 1016 1361 ## sigma_populationHare 0.25 0.05 0.18 0.36 1.00 2895 2070 ## sigma_populationLynx 0.25 0.05 0.18 0.37 1.00 2540 2384 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As Gesmann covered in his blog, we need to use the brms::expose_functions() function to expose Stan functions to R before we use some of our favorite post-processing functions. expose_functions(b16.5a, vectorize = TRUE) Now we’re ready to plot our results like McElreath did in Figure 16.9a. Our first step will be to use predict(). p &lt;- predict(b16.5a, summary = F, # how many posterior predictive draws would you like? nsamples = 21) str(p) ## num [1:21, 1:42] 17.8 67.2 41.8 37.4 24.4 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : NULL We’re ready to plot! # for annotation text &lt;- tibble(population = c(&quot;Hare&quot;, &quot;Lynx&quot;), label = c(&quot;Lepus&quot;, &quot;Lynx&quot;), Year = c(1914, 1916.5), value = c(92, 54)) # wrangle p %&gt;% data.frame() %&gt;% set_names(1:42) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, names_to = &quot;row&quot;) %&gt;% mutate(row = as.double(row)) %&gt;% left_join(Lynx_Hare_long %&gt;% mutate(row = 1:n()), by = &quot;row&quot;) %&gt;% # plot! ggplot(aes(x = Year, y = value)) + geom_line(aes(group = interaction(iter, population), color = population), size = 1/3, alpha = 1/2) + geom_point(data = . %&gt;% filter(iter == 1), aes(x = Year, fill = population), size = 3, shape = 21, stroke = 1/5, color = &quot;white&quot;) + geom_text(data = text, aes(label = label, color = population), hjust = 0, family = &quot;Times&quot;) + scale_color_grey(start = 0, end = .5, breaks = NULL) + scale_fill_grey(start = 0, end = .5, breaks = NULL) + scale_y_continuous(&quot;thousands of pelts&quot;, breaks = 0:6 * 20) + coord_cartesian(ylim = c(0, 120)) Since this version of the model didn’t include a measurement-error process, we don’t have a clear way to make an analogue of Figure 16.9b. We’ll contend with that in the next section. 16.4.2.2 16.4.2.2 Add a measurement-error process to the Lotka-Volterra model. Now we have a sense of what the current Lotka-Volterra workflow looks like for brms, we’re ready to complicate our model a bit. Happily, we won’t need to update our LotkaVolterra code. That’s good as it is. But we will need to make a couple minor adjustments to our model formula object, which we now call lv_formula_error. Make special note of the first bf() line and the last line before we set nl = TRUE. That’s where all the measurement-error action is at. lv_formula_error &lt;- # this is new bf(pelts ~ log(eta * p), nlf(eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta)), H1 ~ 1, L1 ~ 1, bh ~ 1, mh ~ 1, bl ~ 1, ml ~ 1, sigma ~ 0 + population, # this is new, too p ~ 0 + population, nl = TRUE ) Update the priors and save them as lv_priors_error. lv_priors_error &lt;- c( prior(lognormal(log(10), 1), nlpar = H1, lb = 0), prior(lognormal(log(10), 1), nlpar = L1, lb = 0), prior(normal(1, 0.5), nlpar = bh, lb = 0), prior(normal(0.05, 0.05), nlpar = bl, lb = 0), prior(normal(0.05, 0.05), nlpar = mh, lb = 0), prior(normal(1, 0.5), nlpar = ml, lb = 0), prior(exponential(1), dpar = sigma, lb = 0), # here&#39;s our new prior setting prior(beta(40, 200), nlpar = p, lb = 0, ub = 1) ) That wasn’t all that bad, was it? Okay, fit the full brms analogue to McElreath’s m16.5. If you followed along closely, all should go well. b16.5b &lt;- brm(data = Lynx_Hare_long, family = brmsfamily(&quot;lognormal&quot;, link_sigma = &quot;identity&quot;), formula = lv_formula_error, prior = lv_priors_error, iter = 2000, warmup = 1000, chains = 4, cores = 4, inits = 0, stanvars = stanvar(scode = LotkaVolterra, block = &quot;functions&quot;), file = &quot;fits/b16.05b&quot;) Once again, check the quality of the chains. col_names &lt;- c(&quot;italic(H)[1]&quot;, &quot;italic(L)[1]&quot;, str_c(&quot;italic(&quot;, c(&quot;b[H]&quot;, &quot;m[H]&quot;, &quot;b[L]&quot;, &quot;m[L]&quot;), &quot;)&quot;), &quot;italic(p[H])&quot;, &quot;italic(p[L])&quot;, &quot;sigma[italic(H)]&quot;, &quot;sigma[italic(L)]&quot;, &quot;lp__&quot;, &quot;chain&quot;, &quot;iter&quot;) posterior_samples(b16.5b, add_chain = T) %&gt;% set_names(col_names) %&gt;% mcmc_trace(pars = vars(-iter, -lp__), facet_args = list(labeller = label_parsed), size = .15) + scale_x_continuous(breaks = NULL) + theme(legend.key.size = unit(0.15, &#39;in&#39;), legend.position = c(.55, .13)) They look great! Now inspect the model parameter summary. print(b16.5b) ## Family: lognormal ## Links: mu = identity; sigma = identity ## Formula: pelts ~ log(eta * p) ## eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta) ## H1 ~ 1 ## L1 ~ 1 ## bh ~ 1 ## mh ~ 1 ## bl ~ 1 ## ml ~ 1 ## sigma ~ 0 + population ## p ~ 0 + population ## Data: Lynx_Hare_long (Number of observations: 42) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## H1_Intercept 131.94 21.91 94.77 180.94 1.00 1963 2133 ## L1_Intercept 38.69 6.69 27.31 53.56 1.00 2251 2725 ## bh_Intercept 0.55 0.07 0.43 0.68 1.00 1958 1965 ## mh_Intercept 0.00 0.00 0.00 0.01 1.00 1931 1922 ## bl_Intercept 0.00 0.00 0.00 0.01 1.00 1757 2117 ## ml_Intercept 0.80 0.09 0.64 1.00 1.00 1921 1939 ## p_populationHare 0.18 0.02 0.13 0.23 1.00 1939 2156 ## p_populationLynx 0.17 0.02 0.13 0.22 1.00 2033 2357 ## sigma_populationHare 0.25 0.04 0.18 0.35 1.00 3424 2558 ## sigma_populationLynx 0.25 0.05 0.18 0.36 1.00 3017 2365 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If you fit McElreath’s m16.5, you’ll see our parameter summaries are very similar to his. Okay, we’re now ready to make the real analogue of McElreath’s Figure 16.9. First we’ll make and save the plot for the top panel. p1 &lt;- # get the posterior predictive draws predict(b16.5b, summary = F, # how many posterior predictive draws would you like? nsamples = 21) %&gt;% # wrangle data.frame() %&gt;% set_names(1:42) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, names_to = &quot;row&quot;) %&gt;% mutate(row = as.double(row)) %&gt;% left_join(Lynx_Hare_long %&gt;% mutate(row = 1:n()), by = &quot;row&quot;) %&gt;% # plot! ggplot(aes(x = Year, y = value)) + geom_line(aes(group = interaction(iter, population), color = population), size = 1/3, alpha = 1/2) + geom_point(data = . %&gt;% filter(iter == 1), aes(x = Year, fill = population), size = 3, shape = 21, stroke = 1/5, color = &quot;white&quot;) + geom_text(data = text, aes(label = label, color = population), hjust = 0, family = &quot;Times&quot;) + scale_color_grey(start = 0, end = .5, breaks = NULL) + scale_fill_grey(start = 0, end = .5, breaks = NULL) + scale_y_continuous(&quot;thousands of pelts&quot;, breaks = 0:6 * 20) + scale_x_continuous(NULL, breaks = NULL) + coord_cartesian(ylim = c(0, 120)) Our workflow for the second panel will differ a bit from above and a lot from McElreath’s rethinking-based workflow. In essence, we won’t get the same kind of output McElreath got when he executed post &lt;- extract.samples(m16.5). Our post &lt;- posterior_samples(b16.5b) call only get’s us part of the way there. So we’ll have to be tricky and supplement those results with a little fitted() magic. post &lt;- posterior_samples(b16.5b) f &lt;- fitted(b16.5b, summary = F) Now we’re ready to make our version of the bottom panel of Figure 16.9. The trick is to divide our fitted() based results by the appropriate posterior draws from our \\(p\\) parameters. This is a way of hand computing the post$pop values McElreath showed off in his R code 16.20 block. p2 &lt;- cbind(f[, 1:21] / post$b_p_populationHare, f[, 22:42] / post$b_p_populationLynx) %&gt;% data.frame() %&gt;% set_names(1:42) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter, names_to = &quot;row&quot;) %&gt;% mutate(row = as.double(row)) %&gt;% left_join(Lynx_Hare_long %&gt;% mutate(row = 1:n()), by = &quot;row&quot;) %&gt;% filter(iter &lt; 22) %&gt;% # plot! ggplot(aes(x = Year, y = value)) + geom_line(aes(group = interaction(iter, population), color = population), size = 1/3, alpha = 1/2) + scale_color_grey(start = 0, end = .5, breaks = NULL) + scale_fill_grey(start = 0, end = .5, breaks = NULL) + scale_y_continuous(&quot;thousands of animals&quot;, breaks = 0:5 * 100) + coord_cartesian(ylim = c(0, 500)) Now combine the two ggplot2 with patchwork to make the full Figure 16.9 in all its glory. p1 / p2 Boom! 16.4.3 Lynx lessons Bonus: Practice with the autoregressive model. Back in Section 16.4, we briefly discussed how autoregressive models are a typical way to explore processes like those in lynx-hare data. In this bonus section, we’ll practice fitting a few of these. To start off, we’ll restrict ourselves to focusing on just one of the criteria, Hare. Our basic autoregressive model will follow the form \\[\\begin{align*} \\text{Hare}_t &amp; \\sim \\operatorname{Normal}(\\mu_t, \\sigma) \\\\ \\mu_t &amp; = \\alpha + \\beta_1 \\text{Hare}_{t - 1} \\\\ \\alpha &amp; \\sim \\; ? \\\\ \\beta_1 &amp; \\sim \\; ? \\\\ \\sigma &amp; \\sim \\operatorname{Exponential}(1), \\end{align*}\\] were \\(\\beta_1\\) is the first-order autoregressive coefficient and the question marks in the third and fourth lines indicate we’re not wedding ourselves to specific priors, at the moment. Also, note the \\(t\\) subscripts, which denote which time period the observation is drawn from, which in these data is \\(\\text{Year} = 1900, 1901, \\dots, 1920\\). Conceptually, \\(t\\) is now and \\(t - 1\\) the time point just before now. So if we were particularly interested in \\(\\operatorname E (\\text{Hare}_{t = 1920})\\), \\(\\text{Hare}_{t - 1}\\) would be the same as \\(\\text{Hare}_{t = 1919}\\). With brms, you can fit a model like this using the ar() function. By default, ar() presumes the criterion variable (Hare, in this case) is ordered chronologically. If you’re unsure or just want to be on the safe side, you can enter your time variable in the time argument. Also, though the ar() function presumes a first-order autoregressive structure by default, it is capable of fitting models with higher-order autoregressive structures. You can manually specify this with the p argument. Here’s how to fit our simple AR(1) model with explicit ar() syntax. b16.6 &lt;- brm(data = Lynx_Hare, family = gaussian, Hare ~ 1 + ar(time = Year, p = 1), prior(exponential(0.04669846), class = sigma), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 16, file = &quot;fits/b16.06&quot;) You may have noticed we just went with the default brms priors for \\(\\alpha\\) and \\(\\beta_1\\). We got the value for the exponential prior for \\(\\sigma\\) by executing the following. 1 / sd(Lynx_Hare$Hare) ## [1] 0.04669846 Here’s the model summary. print(b16.6) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Hare ~ 1 + ar(time = Year, p = 1) ## Data: Lynx_Hare (Number of observations: 21) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Correlation Structures: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## ar[1] 0.74 0.17 0.38 1.05 1.00 2313 1813 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 30.22 9.68 9.34 48.88 1.00 2044 1739 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 16.42 2.87 12.00 23.19 1.00 2516 1985 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Our autoregressive \\(\\beta_1\\) parameter is summarized in the ‘Correlation Structures,’ in which it’s called ‘ar[1].’ Another more old-school way to fit a autoregressive model is by manually computing a lagged version of your criterion variable. In R, you can do this with the lag() function. Lynx_Hare &lt;- Lynx_Hare %&gt;% mutate(Hare_1 = lag(Hare)) head(Lynx_Hare) ## Year Lynx Hare Hare_1 ## 1 1900 4.0 30.0 NA ## 2 1901 6.1 47.2 30.0 ## 3 1902 9.8 70.2 47.2 ## 4 1903 35.2 77.4 70.2 ## 5 1904 59.4 36.3 77.4 ## 6 1905 41.7 20.6 36.3 Look closely at the relation between the values in the Hare and Hare_1 columns. They are set up such that \\(\\text{Hare}_{\\text{Year} = 1901} = \\text{Hare_1}_{\\text{Year} = 1900}\\), \\(\\text{Hare}_{\\text{Year} = 1902} = \\text{Hare_1}_{\\text{Year} = 1901}\\), and so on. Unfortunately, this approach does produce a single missing value in the first time point for the lagged variable, Hare_1. Here’s how you might use such a variable to manually fit an autoregressive model with brms::brm(). b16.7 &lt;- brm(data = Lynx_Hare, family = gaussian, Hare ~ 1 + Hare_1, prior = c(prior(normal(0, 1), class = b), prior(exponential(0.04669846), class = sigma)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 16, file = &quot;fits/b16.07&quot;) print(b16.7) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: Hare ~ 1 + Hare_1 ## Data: Lynx_Hare (Number of observations: 20) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 10.17 7.36 -4.65 24.86 1.00 3244 2561 ## Hare_1 0.68 0.18 0.32 1.04 1.00 3123 2681 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 16.96 2.99 12.33 23.87 1.00 2777 2193 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Did you notice how the fourth line in the output read ‘Number of observations: 20?’ That’s because we had that one missing value for Hare_1. One quick and dirty hack might be to use the missing data syntax we learned from Chapter 15. Here’s how that might look like. b16.8 &lt;- brm(data = Lynx_Hare, family = gaussian, bf(Hare ~ 1 + mi(Hare_1)) + bf(Hare_1 | mi() ~ 1) + set_rescor(FALSE), prior = c(prior(normal(0, 1), class = b), prior(exponential(0.04669846), class = sigma, resp = Hare), prior(exponential(0.04669846), class = sigma, resp = Hare1)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 16, file = &quot;fits/b16.08&quot;) Check the model summary. print(b16.8) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: Hare ~ 1 + mi(Hare_1) ## Hare_1 | mi() ~ 1 ## Data: Lynx_Hare (Number of observations: 21) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Hare_Intercept 11.90 7.06 -2.36 25.76 1.00 2430 2477 ## Hare1_Intercept 34.44 5.12 24.39 44.43 1.00 3926 2758 ## Hare_miHare_1 0.65 0.17 0.29 0.98 1.00 2317 2319 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_Hare 16.80 2.81 12.32 23.21 1.00 2660 2192 ## sigma_Hare1 22.43 3.65 16.67 30.67 1.00 3394 2674 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Now we have a model based on all 21 observations, again. I’m still not in love with this fix, because it presumes that value in Hare_1 was missing at random, with no accounting for the autoregressive structure. This is why, when you can, it’s probably better to use the ar() syntax. Anyway, here’s how we might use fitted() to get a sense of \\(\\operatorname{E}(\\text{Hare}_t)\\) from our first autoregressive model, b16.6. set.seed(16) fitted(b16.6, summary = F, nsamples = 21) %&gt;% data.frame() %&gt;% set_names(1900:1920) %&gt;% mutate(iter = 1:n()) %&gt;% pivot_longer(-iter) %&gt;% mutate(Year = as.integer(name)) %&gt;% ggplot(aes(x = Year)) + geom_line(aes(y = value, group = iter), size = 1/3, alpha = 1/2) + geom_point(data = Lynx_Hare, aes(y = Hare), size = 3, shape = 21, stroke = 1/4, fill = &quot;black&quot;, color = &quot;white&quot;) + annotate(geom = &quot;text&quot;, x = 1913.5, y = 85, label = &quot;Lepus&quot;, family = &quot;Times&quot;) + scale_y_continuous(&quot;thousands of hare pelts&quot;, breaks = 0:6 * 20, limits = c(0, 120)) The model did a pretty good job capturing the non-linear trends in the data. But notice how the fitted lines appear to be one step off from the data. This is actually expected behavior for a simple AR(1) model. For insights on why, check out this thread on Stack Exchange. So far we’ve been fitting the autoregressive models with the Gaussian likelihood, which is a typical approach. If you look at McElreath’s practice problem 16H3, you’ll see he proposed a bivariate autoregressive model using the Log-Normal likelihood. His approach used hand-made lagged predictors and ignored the missing value problem by dropping the first case. That model followed the form \\[\\begin{align*} L_t &amp; \\sim \\operatorname{Log-Normal}(\\log \\mu_{L, t}, \\sigma_L) \\\\ H_t &amp; \\sim \\operatorname{Log-Normal}(\\log \\mu_{H, t}, \\sigma_H) \\\\ \\mu_{L, t} &amp; = \\alpha_L + \\beta_{L1} L_{t - 1} + \\beta_{L2} H_{t - 1} \\\\ \\mu_{H, t} &amp; = \\alpha_H + \\beta_{H1} H_{t - 1} + \\beta_{H2} L_{t - 1}, \\end{align*}\\] where \\(\\beta_{L1}\\) and \\(\\beta_{H1}\\) are the autoregressive parameters and \\(\\beta_{L2}\\) and \\(\\beta_{H2}\\) are what are sometimes called the cross-lag parameters. McElreath left the priors up to us. I propose something like this: \\[\\begin{align*} \\alpha_L &amp; \\sim \\operatorname{Normal}(\\log 10, 1) \\\\ \\alpha_H &amp; \\sim \\operatorname{Normal}(\\log 10, 1) \\\\ \\beta_{L1}, \\dots, \\beta_{H2} &amp; \\sim \\operatorname{Normal}(0, 0.5) \\\\ \\sigma_L &amp; \\sim \\operatorname{Exponential}(1) \\\\ \\sigma_H &amp; \\sim \\operatorname{Exponential}(1). \\end{align*}\\] Before we fit the model, we’ll need to make a lagged version of Lynx. Lynx_Hare &lt;- Lynx_Hare %&gt;% mutate(Lynx_1 = lag(Lynx)) Because the predictor variables are not centered at zero, we’ll want to use the 0 + Intercept... syntax. Now fit the bivariate autoregressive model. b16.9 &lt;- brm(data = Lynx_Hare, family = lognormal, bf(Hare ~ 0 + Intercept + Hare_1 + Lynx_1) + bf(Lynx ~ 0 + Intercept + Lynx_1 + Hare_1) + set_rescor(FALSE), prior = c(prior(normal(log(10), 1), class = b, resp = Hare, coef = Intercept), prior(normal(log(10), 1), class = b, resp = Lynx, coef = Intercept), prior(normal(0, 0.5), class = b, resp = Hare), prior(normal(0, 0.5), class = b, resp = Lynx), prior(exponential(1), class = sigma, resp = Hare), prior(exponential(1), class = sigma, resp = Lynx)), iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 16, file = &quot;fits/b16.09&quot;) Check the summary. print(b16.9) ## Family: MV(lognormal, lognormal) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: Hare ~ 0 + Intercept + Hare_1 + Lynx_1 ## Lynx ~ 0 + Intercept + Lynx_1 + Hare_1 ## Data: Lynx_Hare (Number of observations: 20) ## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Hare_Intercept 3.01 0.17 2.68 3.34 1.00 2253 2480 ## Hare_Hare_1 0.02 0.00 0.02 0.03 1.00 3418 2454 ## Hare_Lynx_1 -0.02 0.00 -0.03 -0.01 1.00 2884 2557 ## Lynx_Intercept 1.44 0.12 1.21 1.69 1.00 2663 2450 ## Lynx_Lynx_1 0.03 0.00 0.02 0.04 1.00 4299 2897 ## Lynx_Hare_1 0.02 0.00 0.02 0.03 1.00 3921 2789 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_Hare 0.33 0.06 0.23 0.47 1.00 3030 3019 ## sigma_Lynx 0.23 0.04 0.17 0.34 1.00 3151 2663 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Here we’ll use fitted() to make a variant of the posterior predictions from the top portion of Figure 16.9. rbind(fitted(b16.9, resp = &quot;Hare&quot;), fitted(b16.9, resp = &quot;Lynx&quot;)) %&gt;% data.frame() %&gt;% mutate(name = rep(c(&quot;Hare&quot;, &quot;Lynx&quot;), each = n() / 2), year = rep(1901:1920, times = 2)) %&gt;% ggplot(aes(x = year)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, group = name, fill = name), alpha = 1/3) + geom_line(aes(y = Estimate, group = name, color = name)) + geom_point(data = Lynx_Hare %&gt;% pivot_longer(Lynx:Hare), aes(x = Year, y = value, color = name), size = 2) + scale_color_grey(start = 0, end = .5, breaks = NULL) + scale_fill_grey(start = 0, end = .5, breaks = NULL) + scale_x_continuous(limits = c(1900, 1920)) + scale_y_continuous(&quot;thousands of pelts&quot;, breaks = 0:6 * 20) In the next practice problem (16H4), McElreath suggested we “adapt the autoregressive model to use a two-step lag variable” (p. 551, emphasis added). Using the verbiage from above, we might also refer to that as second-order autoregressive model, AR(2). That would be a straight generalization of the approach we just took. I’ll leave the exercise to the interested reader. The kinds autoregressive models we fit in this section are special cases of what are called autoregressive moving average (ARMA) models. If you’re in the social sciences, Hamaker and Brose have a nice (2009) chapter explaining AR, ARMA, and other related models, which you can download from ReserachGate here. ARMA models are available in brms with help from the arma() function. However, if you want to dive deep into models of this kind, you might want to check out the varstan package (Matamoros &amp; Torres, 2020), which is designed to fit a variety of Bayesian structured time series models. Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bayesplot_1.8.0 patchwork_1.1.1 rethinking_2.13 rstan_2.21.2 StanHeaders_2.21.0-7 ## [6] GGally_2.1.1 brms_2.15.0 Rcpp_1.0.6 tidybayes_2.3.1 ggthemes_4.2.4 ## [11] forcats_0.5.1 stringr_1.4.0 dplyr_1.0.5 purrr_0.3.4 readr_1.4.0 ## [16] tidyr_1.1.3 tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0 ## ## loaded via a namespace (and not attached): ## [1] readxl_1.3.1 backports_1.2.1 RcppEigen_0.3.3.7.0 plyr_1.8.6 igraph_1.2.6 ## [6] splines_4.0.4 svUnit_1.0.3 crosstalk_1.1.0.1 TH.data_1.0-10 rstantools_2.1.1 ## [11] inline_0.3.17 digest_0.6.27 htmltools_0.5.1.1 rsconnect_0.8.16 fansi_0.4.2 ## [16] BH_1.75.0-0 magrittr_2.0.1 modelr_0.1.8 RcppParallel_5.0.2 matrixStats_0.57.0 ## [21] xts_0.12.1 sandwich_3.0-0 prettyunits_1.1.1 colorspace_2.0-0 rvest_0.3.6 ## [26] ggdist_2.4.0.9000 haven_2.3.1 xfun_0.22 callr_3.5.1 crayon_1.4.1 ## [31] jsonlite_1.7.2 lme4_1.1-25 survival_3.2-7 zoo_1.8-8 glue_1.4.2 ## [36] gtable_0.3.0 emmeans_1.5.2-1 V8_3.4.0 distributional_0.2.2 pkgbuild_1.2.0 ## [41] shape_1.4.5 abind_1.4-5 scales_1.1.1 mvtnorm_1.1-1 emo_0.0.0.9000 ## [46] DBI_1.1.0 miniUI_0.1.1.1 xtable_1.8-4 stats4_4.0.4 DT_0.16 ## [51] htmlwidgets_1.5.2 httr_1.4.2 threejs_0.3.3 RColorBrewer_1.1-2 arrayhelpers_1.1-0 ## [56] ellipsis_0.3.1 reshape_0.8.8 pkgconfig_2.0.3 loo_2.4.1 farver_2.0.3 ## [61] dbplyr_2.0.0 utf8_1.1.4 labeling_0.4.2 tidyselect_1.1.0 rlang_0.4.10 ## [66] reshape2_1.4.4 later_1.1.0.1 munsell_0.5.0 cellranger_1.1.0 tools_4.0.4 ## [71] cli_2.3.1 generics_0.1.0 broom_0.7.5 ggridges_0.5.2 evaluate_0.14 ## [76] fastmap_1.0.1 processx_3.4.5 knitr_1.31 fs_1.5.0 nlme_3.1-152 ## [81] mime_0.10 projpred_2.0.2 xml2_1.3.2 compiler_4.0.4 shinythemes_1.1.2 ## [86] rstudioapi_0.13 gamm4_0.2-6 curl_4.3 reprex_0.3.0 statmod_1.4.35 ## [91] stringi_1.5.3 highr_0.8 ps_1.6.0 Brobdingnag_1.2-6 lattice_0.20-41 ## [96] Matrix_1.3-2 nloptr_1.2.2.2 markdown_1.1 shinyjs_2.0.0 vctrs_0.3.6 ## [101] pillar_1.5.1 lifecycle_1.0.0 bridgesampling_1.0-0 estimability_1.3 httpuv_1.5.4 ## [106] R6_2.5.0 bookdown_0.21 promises_1.1.1 gridExtra_2.3 codetools_0.2-18 ## [111] boot_1.3-26 colourpicker_1.1.0 MASS_7.3-53 gtools_3.8.2 assertthat_0.2.1 ## [116] withr_2.4.1 shinystan_2.5.0 multcomp_1.4-16 mgcv_1.8-33 hms_0.5.3 ## [121] grid_4.0.4 coda_0.19-4 minqa_1.2.4 rmarkdown_2.7 shiny_1.5.0 ## [126] lubridate_1.7.9.2 base64enc_0.1-3 dygraphs_1.1.1.6 We first fit this model with brms version 2.14.4. At that time, the only way to get it to run successfully was by using backend = \"cmdstan\". I’m not really interested in tackling what that setting means, but rest assured that exciting things are happening for brms and Stan. If you’d like to learn more, check out the (2021) vignette by Webber and Bürkner, Running brms models with within-chain parallelization. But anyways, the current version of brms (2.15.0) no longer requires that backend setting and the complications it entails. The default works fine.↩︎ "],["horoscopes-insights.html", "17 Horoscopes Insights 17.1 Use R Notebooks 17.2 Save your model fits 17.3 Build your models slowly 17.4 Look at your data 17.5 Consider using the 0 + Intercept syntax 17.6 Annotate your workflow 17.7 Annotate your code 17.8 Break up your workflow 17.9 Code in public 17.10 Check out social media 17.11 Extra Rethinking-related resources 17.12 But, like, how do I do this for real? 17.13 Parting wisdom Session info", " 17 Horoscopes Insights Statistical inference is indeed critically important. But only as much as every other part of research. Scientific discovery is not an additive process, in which sin in one part can be atoned by virtue in another. Everything interacts. So equally when science works as intended as when it does not, every part of the process deserves attention. (McElreath, 2020a, p. 553) In this final chapter, there are no models for us to fit and no figures for use to reimagine. McElreath took the opportunity to comment more broadly on the scientific process. He made a handful of great points, some of which I’ll quote in a bit. But for the bulk of this chapter, I’d like to take the opportunity to pass on a few of my own insights about workflow. I hope they’re of use. 17.1 Use R Notebooks OMG I first started using R in the winter of 2015/2016. Right from the start, I learned how to code from within the RStudio environment. But within RStudio I was using simple scripts. No longer. I now use R Notebooks for just about everything, including my scientific projects, this bookdown-powered ebook, and even my academic webpage and blog. Nathan Stephens wrote a nice blog post, Why I love R Notebooks. I agree. This has fundamentally changed my workflow as a scientist. I only wish I’d learned about this before starting my dissertation project. So it goes… Do yourself a favor, adopt R Notebooks into your workflow. Do it today. If you prefer to learn with videos, here’s a nice intro by Kristine Yu and another one by JJ Allaire. Try it out for like one afternoon and you’ll be hooked. 17.2 Save your model fits It’s embarrassing how long it took for this to dawn on me. Unlike classical statistics, Bayesian models using MCMC take a while to compute. Most of the simple models in McElreath’s text take 30 seconds up to a couple minutes. If your data are small, well-behaved and of a simple structure, you might have a lot of wait times in that range in your future. It hasn’t been that way, for me. Most of my data have a complicated multilevel structure and often aren’t very well behaved. It’s normal for my models to take an hour or several to fit. Once you start measuring your model fit times in hours, you do not want to fit these things more than once. So, it’s not enough to document my code in a nice R Notebook file. I need to save my brm() fit objects in external files. Consider this model. It’s taken from Bürkner’s (2021d) vignette, Estimating multivariate models with brms. It took about three minutes for my personal laptop to fit. library(brms) data(&quot;BTdata&quot;, package = &quot;MCMCglmm&quot;) b17.1 &lt;- brm(data = BTdata, family = gaussian, mvbind(tarsus, back) ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam), chains = 2, cores = 2, seed = 17) Three minutes isn’t terribly long to wait, but still. I’d prefer to never have to wait for another three minutes, again. Sure, if I save my code in a document like this, I will always be able to fit the model again. But I can work smarter. Here I’ll save my b17.1 object outside of R with the save() function. save(b17.1, file = &quot;fits/b17.01.rda&quot;) Hopefully y’all are savvy Bayesian R users and find this insultingly remedial. But if it’s new to you like it was me, you can learn more about .rda files here. Now b17.1 is saved outside of R, I can safely remove it and then reload it. rm(b17.1) load(&quot;fits/b17.01.rda&quot;) The file took a fraction of a second to reload. Once reloaded, I can perform typical operations, like examine summaries of the model parameters or refreshing my memory on what data I used. print(b17.1) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: tarsus ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) ## back ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) ## Data: BTdata (Number of observations: 828) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Group-Level Effects: ## ~dam (Number of levels: 106) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(tarsus_Intercept) 0.48 0.05 0.40 0.58 1.00 857 1208 ## sd(back_Intercept) 0.25 0.08 0.09 0.40 1.03 243 625 ## cor(tarsus_Intercept,back_Intercept) -0.52 0.22 -0.93 -0.08 1.00 505 842 ## ## ~fosternest (Number of levels: 104) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(tarsus_Intercept) 0.26 0.05 0.16 0.37 1.00 703 835 ## sd(back_Intercept) 0.35 0.06 0.23 0.47 1.01 350 743 ## cor(tarsus_Intercept,back_Intercept) 0.71 0.20 0.25 0.99 1.01 180 433 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## tarsus_Intercept -0.41 0.07 -0.54 -0.27 1.00 1488 1397 ## back_Intercept -0.01 0.06 -0.14 0.11 1.00 2310 1681 ## tarsus_sexMale 0.77 0.06 0.66 0.89 1.00 3596 1534 ## tarsus_sexUNK 0.23 0.13 -0.04 0.49 1.00 2856 1388 ## tarsus_hatchdate -0.04 0.06 -0.16 0.08 1.00 1315 1436 ## back_sexMale 0.01 0.07 -0.12 0.15 1.00 3571 1627 ## back_sexUNK 0.15 0.15 -0.17 0.46 1.00 2716 1390 ## back_hatchdate -0.09 0.05 -0.19 0.02 1.00 1744 1443 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_tarsus 0.76 0.02 0.72 0.80 1.00 2368 1604 ## sigma_back 0.90 0.02 0.85 0.95 1.00 2628 1317 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(tarsus,back) -0.05 0.04 -0.13 0.02 1.00 2300 1598 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). head(b17.1$data) ## tarsus sex hatchdate fosternest dam back ## 1 -1.89229718 Fem -0.6874021 F2102 R187557 1.1464212 ## 2 1.13610981 Male -0.6874021 F1902 R187559 -0.7596521 ## 3 0.98468946 Male -0.4279814 A602 R187568 0.1449373 ## 4 0.37900806 Male -1.4656641 A1302 R187518 0.2555847 ## 5 -0.07525299 Fem -1.4656641 A2602 R187528 -0.3006992 ## 6 -1.13519543 Fem 0.3502805 C2302 R187945 1.5577219 The other option, which we’ve been using extensively throughout the earlier chapters, is to use file argument within the brms::brm() function. You can read about the origins of the argument in issue #472 on the brms GitHub repo. To make use of the file argument, specify a character string. brm() will then save your fitted model object in an external .rds file via the saveRDS() function. Let’s give it a whirl, this time with an interaction. b17.2 &lt;- brm(data = BTdata, family = gaussian, mvbind(tarsus, back) ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam), chains = 2, cores = 2, seed = 17, file = &quot;fits/b17.02&quot;) Now b17.2 is saved outside of R, I can safely remove it and then reload it. rm(b17.2) We might load b17.2 with the readRDS() function. b17.2 &lt;- readRDS(&quot;fits/b17.02.rds&quot;) Now we can work with b17.2 as desired. fixef(b17.2) ## Estimate Est.Error Q2.5 Q97.5 ## tarsus_Intercept -0.407978346 0.06886640 -0.53697160 -0.26934530 ## back_Intercept -0.010887826 0.06517174 -0.13971337 0.11402170 ## tarsus_sexMale 0.770696149 0.05935318 0.65440422 0.88679912 ## tarsus_sexUNK 0.191067789 0.14825959 -0.09870164 0.48952403 ## tarsus_hatchdate -0.053516008 0.06662924 -0.18533521 0.07750452 ## tarsus_sexMale:hatchdate 0.011741501 0.06006519 -0.10458836 0.13400483 ## tarsus_sexUNK:hatchdate 0.062469696 0.12139710 -0.17678732 0.29220534 ## back_sexMale 0.004347074 0.06648164 -0.12444057 0.13386272 ## back_sexUNK 0.149935239 0.17311024 -0.18821742 0.48691806 ## back_hatchdate -0.047850895 0.06221408 -0.16974086 0.06967660 ## back_sexMale:hatchdate -0.081434205 0.06909402 -0.21426763 0.05106719 ## back_sexUNK:hatchdate -0.040903073 0.14882849 -0.33749536 0.25284227 The file method has another handy feature. Let’s remove b17.2 one more time to see. rm(b17.2) If you’ve fit a brm() model once and saved the results with file, executing the same brm() code will not re-fit the model. Rather, it will just load and return the model from the .rds file. b17.2 &lt;- brm(data = BTdata, family = gaussian, mvbind(tarsus, back) ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam), chains = 2, cores = 2, seed = 15, file = &quot;fits/b17.02&quot;) It takes just a fraction of a second. Once again, we’re ready to work with b17.2. b17.2$formula ## tarsus ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam) ## back ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam) And if you’d like to remind yourself what the name of that external file was or what folder you saved it in, you can extract it from the brm() fit object. b17.2$file ## [1] &quot;fits/b17.02.rds&quot; Also, see Gavin Simpson’s blog post, A better way of saving and loading objects in R, for a discussion on the distinction between .rda and .rds files. 17.3 Build your models slowly The model from Bürkner’s vignette, b17.1, was no joke. If you wanted to be verbose about it, it was a multilevel, multivariate, multivariable model. It had a cross-classified multilevel structure, two predictors (for each criterion), and two criteria. Not only is that a lot to keep track of, there’s a whole lot of places for things to go wrong. Even if that was the final model I was interested in as a scientist, I still wouldn’t start with it. I’d build up incrementally, just to make sure nothing looked fishy. One place to start would be a simple intercepts-only model. b17.0 &lt;- brm(mvbind(tarsus, back) ~ 1, data = BTdata, chains = 2, cores = 2, file = &quot;fits/b17.00&quot;) plot(b17.0) print(b17.0) ## Family: MV(gaussian, gaussian) ## Links: mu = identity; sigma = identity ## mu = identity; sigma = identity ## Formula: tarsus ~ 1 ## back ~ 1 ## Data: BTdata (Number of observations: 828) ## Samples: 2 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup samples = 2000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## tarsus_Intercept -0.00 0.03 -0.07 0.07 1.00 2650 1451 ## back_Intercept -0.00 0.04 -0.08 0.07 1.00 2916 1345 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma_tarsus 1.00 0.02 0.95 1.05 1.00 2331 1433 ## sigma_back 1.00 0.02 0.95 1.05 1.00 2761 1337 ## ## Residual Correlations: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## rescor(tarsus,back) -0.03 0.04 -0.10 0.03 1.00 2494 1594 ## ## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). If the chains look good and the summary statistics are about what I’d expect, I’m on good footing to keep building up to the model I really care about. The results from this model, for example, suggest that both criteria were standardized (i.e., intercepts at 0 and \\(\\sigma\\)’s at 1). If that wasn’t what I intended, I’d rather catch it here than spend several minutes fitting the more complicated b17.1 model, the parameters for which are sufficiently complicated that I may have had trouble telling what scale the data were on. Note, this is not the same as \\(p\\)-hacking (Simmons et al., 2011) or wandering aimlessly down the garden of forking paths (Gelman &amp; Loken, 2013). We are not chasing the flashiest model to put in a paper. Rather, this is just good pragmatic data science. If you start off with a theoretically-justified but complicated model and run into computation problems or produce odd-looking estimates, it won’t be clear where things went awry. When you build up, step by step, it’s easier to catch data cleaning failures, coding goofs and the like. So, when I’m working on a project, I fit one or a few simplified models before fitting my complicated model of theoretical interest. This is especially the case when I’m working with model types that are new to me or that I haven’t worked with in a while. I document each step in my R Notebook files and I save the fit objects for each in external files. I have caught surprises this way. Hopefully this will help you catch your mistakes, too. 17.4 Look at your data Relatedly, and perhaps even a precursor, you should always plot your data before fitting a model. There were plenty examples of this in the text, but it’s worth it making explicit. Simple summary statistics are great, but they’re not enough. For an entertaining exposition, check out Matejka and Fitzmaurice’s (2017) Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing. Though it might make for a great cocktail party story, I’d hate to pollute the scientific literature with a linear model based on a set of dinosaur-shaped data. 17.5 Consider using the 0 + Intercept syntax We covered this a little in the last couple chapters (e.g., Section 15.4.7, Section 16.4.3), but it’s easy to miss. If your real-world model has predictors (i.e., isn’t an intercept-only model), it’s important to keep track of how you have centered those predictors. When you specify a prior for a brms Intercept (i.e., an intercept resulting from the y ~ x or y ~ 1 + x style of syntax), that prior is applied under the presumption all the predictors are mean centered. In the Population-level (‘fixed’) effects subsection of the set_prior section of the brms reference manual (Bürkner, 2021i), we read: Note that technically, this prior is set on an intercept that results when internally centering all population-level predictors around zero to improve sampling efficiency. On this centered intercept, specifying a prior is actually much easier and intuitive than on the original intercept, since the former represents the expected response value when all predictors are at their means. To treat the intercept as an ordinary population-level effect and avoid the centering parameterization, use 0 + Intercept on the right-hand side of the model formula. We get a little more information from the Parameterization of the population-level intercept subsection of the brmsformula section: This behavior can be avoided by using the reserved (and internally generated) variable Intercept. Instead of y ~ x, you may write y ~ 0 + Intercept + x. This way, priors can be defined on the real intercept, directly. In addition, the intercept is just treated as an ordinary population-level effect and thus priors defined on b will also apply to it. Note that this parameterization may be less efficient than the default parameterization discussed above. We didn’t bother using the 0 + Intercept syntax for most of our models because McElreath chose to emphasize mean-centered and standardized predictors in the second edition of his text. But this will not always be the case. Sometimes you might have a good reason not to center your predictors. In those cases, the 0 + Intercept syntax can make a difference. Regardless, do set your Intercept priors with care. 17.6 Annotate your workflow In a typical model-fitting file, I’ll load my data, perhaps transform the data a bit, fit several models, and examine the output of each with trace plots, model summaries, information criteria, and the like. In my early days, I just figured each of these steps were self-explanatory. Nope. “In every project you have at least one other collaborator; future-you. You don’t want future-you to curse past-you.” My experience was that even a couple weeks between taking a break from a project and restarting it was enough time to make my earlier files confusing. And they were my files. I now start each R Notebook document with an introductory paragraph or two explaining exactly what the purpose of the file is. I separate my major sections by headers and subheaders. My working R Notebook files are peppered with bullets, sentences, and full on paragraphs between code blocks. 17.7 Annotate your code This idea is implicit in McElreath’s text, but it’s easy to miss the message. I know I did, at first. I find this is especially important for data wrangling. I’m a tidyverse guy and, for me, the big-money verbs like mutate(), pivot_longer(), select(), filter(), group_by(), and summarise() take care of the bulk of my data wrangling. But every once and a while I need to do something less common, like with str_extract() or case_when(). When I end up using a new or less familiar function, I typically annotate right in the code and even sometimes leave a hyperlink to some R-bloggers post or stackoverflow question that explained how to use it. 17.8 Break up your workflow I’ve also learned to break up my projects into multiple R Notebook files. If you have a small project for which you just want a quick and dirty plot, fine, do it all in one file. My typical scientific projects have: a primary data cleaning file; a file with basic descriptive statistics and the like; at least one primary analysis file; possible secondary and tertiary analysis files; a file or two for my major figures; and a file explaining and depicting my priors, often accompanied by my posteriors, for comparison. Putting all that information in one R Notebook file would be overwhelming. Your workflow might well look different, but hopefully you get the idea. You don’t want working files with thousands of lines of code. To get a sense of what this can look like in practice, you might follow this link, https://osf.io/vekpf/, to the OSF project site for one of my recent conference presentations (Kurz et al., 2019). In the wiki, I explained the contents of 10 files supporting the analyses we presented at the conference. Each of those 10 .html files has its origin in its own .Rmd file. Mainly to keep Jenny Bryan from setting my computer on fire, I’m also in the habit of organizing interconnected project files with help from RStudio Projects. You can learn more about these from Chapter 8 in R4DS (Grolemund &amp; Wickham, 2017). They might seem trivial, at first, but I’ve come to value the simple ways in which RStudio Projects help streamline my workflow. 17.9 Code in public If you would like to improve the code you write for data-wrangling, modeling, and/or visualizing, code in public. Yes, it can be intimidating. Yes, you will probably make mistakes. If you’re lucky, others will point them out and you will revise and learn and grow. 🌱 You can do this on any number of mediums, such as GitHub (e.g., here), personal blogs (e.g., here), the Open Science Framework (e.g., here), online books (e.g., here), full-blown YouTube lectures (e.g., here), or even with brief Twitter GIFs (e.g., here). I’ve found that just the possibility that others might look at my code makes it more likely I’ll slow down, annotate, and try to use more consistent formatting. Hopefully it will benefit you, too. 17.10 Check out social media If you’re not on it, consider joining academic Twitter (see the related blog posts by Sarah Mojarad and Dan Quintana). The word on the street is correct. Twitter can be rage-fueled dumpster fire. But if you’re selective about who you follow, it’s a great place to lean from and connect with your academic heroes. If you’re a fan of this project, here’s a list of some of the people you might want to follow: Mara Averick Michael Bentacourt Jenny Bryan Paul Bürkner Nathaniel Haines Frank Harrell Alison Presmanes Hill Matthew Kay Tristan Mahr Richard McElreath Danielle Navarro Chelsea Parlett-Pelleriti Roger Peng David Robinson Julia Silge Dan Simpson Aki Vehtari Matti Vuorre Hadley Wickham Claus Wilke Donald Williams Yihui Xie I’m on twitter, too. There are also some great blogs out there. Of primary interest, McElreath blogs once in a blue moon at https://elevanth.org/blog/. Andrew Gelman regularly blogs at https://statmodeling.stat.columbia.edu/. Many of Gelman’s posts are great, but don’t miss the action in the comments sections. Every so often, I post on brms-related content at https://solomonkurz.netlify.app/post/. Also, do check out the Stan Forums. They have a special brms tag there, under which you can find all kinds of hot brms talk. But if you’re new to the world of asking for help with your code online, you might acquaint yourself with the notion of a minimally reproducible example. In short, a good minimally reproducible example helps others help you. If you fail to do this, prepare for some snark. 17.11 Extra Rethinking-related resources You may have noticed I don’t offer solutions to the homework problems. Happily, David Salazar has a blog series on the homework problems highlighting a tiydverse- and rethinking-oriented workflow; find his first post here. Ania Kawiecki made a beautiful website solving the homework problems using the tiydverse and INLA (Rue et al., 2009), which you can find here. Also, Vincent Arel-Bundock has a nice website where we worked through the primary material in the book chapters using rstan and the tidyverse, which you can find here. If you find other resources along these lines, please tell me about it in a GitHub issue. 17.12 But, like, how do I do this for real? McElreath’s text and my ebook are designed to teach you how to get started doing applied Bayesian statistics. Neither does a great job teaching you how to write them up for a professional presentation, like a peer-reviewed journal article. What I will do, however, is give you some places to look. Just before releasing the 0.2.0 version of this ebook, I asked the good people on twitter to share their favorite examples of applied Bayesian statistics in the scientific literature. What are your favorite **applied** articles using Bayesian statistics (via Stan, #brms, …)? I’m particularly interested in great walk-outs in Methods sections, Discussion sections, and supplemental materials.&mdash; Solomon Kurz (@SolomonKurz) March 15, 2021 At first, folks were a little shy, but eventually the crowd came through in spades! For a full list, feel free to peruse my tweet thread. For your convenience, here’s a list of a good bunch of the works people shared7: Allen et al. (2020, PDF link) Amlie-Lefond et al. (2020, PDF link) Casillas (2021, PDF link) Davis et al. (2020, PDF link) Girard et al. (2021, PDF link) Haines et al. (2018, PDF link) Kale et al. (2020, PDF link) Nogueira et al. (2018, PDF link) Ross et al. (2020, PDF link) Silbiger et al. (2019, PDF link) At a quick glance, these papers cover a reasonably broad range of topics. I hope they give you a sense of how to write up your work. 17.13 Parting wisdom Okay, that’s enough from me. Let’s start wrapping this project up with some McElreath. There is an aspect of science that you do personally control: openness. Pre-plan your research together with the statistical analysis. Doing so will improve both the research design and the statistics. Document it in the form of a mock analysis that you would not be ashamed to share with a colleague. Register it publicly, perhaps in a simple repository, like Github or any other. But your webpage will do just fine, as well. Then collect the data. Then analyze the data as planned. If you must change the plan, that’s fine. But document the changes and justify them. Provide all of the data and scripts necessary to repeat your analysis. Do not provide scripts and data “on request,” but rather put them online so reviewers of your paper can access them without your interaction. There are of course cases in which full data cannot be released, due to privacy concerns. But the bulk of science is not of that sort. The data and its analysis are the scientific product. The paper is just an advertisement. If you do your honest best to design, conduct, and document your research, so that others can build directly upon it, you can make a difference. (p. 555) Toward that end, also check out the OSF and their YouTube channel, here. Katie Corker gets the last words: “Open science is stronger because we’re doing this together.” Session info sessionInfo() ## R version 4.0.4 (2021-02-15) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Catalina 10.15.7 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] brms_2.15.0 Rcpp_1.0.6 ## ## loaded via a namespace (and not attached): ## [1] TH.data_1.0-10 minqa_1.2.4 colorspace_2.0-0 ellipsis_0.3.1 ggridges_0.5.2 ## [6] rsconnect_0.8.16 estimability_1.3 markdown_1.1 base64enc_0.1-3 farver_2.0.3 ## [11] rstan_2.21.2 DT_0.16 lubridate_1.7.9.2 fansi_0.4.2 mvtnorm_1.1-1 ## [16] bridgesampling_1.0-0 codetools_0.2-18 splines_4.0.4 knitr_1.31 shinythemes_1.1.2 ## [21] bayesplot_1.8.0 projpred_2.0.2 jsonlite_1.7.2 nloptr_1.2.2.2 shiny_1.5.0 ## [26] httr_1.4.2 compiler_4.0.4 emmeans_1.5.2-1 backports_1.2.1 assertthat_0.2.1 ## [31] Matrix_1.3-2 fastmap_1.0.1 cli_2.3.1 later_1.1.0.1 htmltools_0.5.1.1 ## [36] prettyunits_1.1.1 tools_4.0.4 igraph_1.2.6 coda_0.19-4 gtable_0.3.0 ## [41] glue_1.4.2 reshape2_1.4.4 dplyr_1.0.5 V8_3.4.0 vctrs_0.3.6 ## [46] nlme_3.1-152 crosstalk_1.1.0.1 xfun_0.22 stringr_1.4.0 ps_1.6.0 ## [51] lme4_1.1-25 mime_0.10 miniUI_0.1.1.1 lifecycle_1.0.0 gtools_3.8.2 ## [56] statmod_1.4.35 MASS_7.3-53 zoo_1.8-8 scales_1.1.1 colourpicker_1.1.0 ## [61] promises_1.1.1 Brobdingnag_1.2-6 parallel_4.0.4 sandwich_3.0-0 inline_0.3.17 ## [66] shinystan_2.5.0 gamm4_0.2-6 curl_4.3 gridExtra_2.3 ggplot2_3.3.3 ## [71] loo_2.4.1 StanHeaders_2.21.0-7 tweetrmd_0.0.8 stringi_1.5.3 highr_0.8 ## [76] dygraphs_1.1.1.6 boot_1.3-26 pkgbuild_1.2.0 rlang_0.4.10 pkgconfig_2.0.3 ## [81] matrixStats_0.57.0 evaluate_0.14 lattice_0.20-41 purrr_0.3.4 labeling_0.4.2 ## [86] rstantools_2.1.1 htmlwidgets_1.5.2 processx_3.4.5 tidyselect_1.1.0 plyr_1.8.6 ## [91] magrittr_2.0.1 bookdown_0.21 R6_2.5.0 generics_0.1.0 multcomp_1.4-16 ## [96] DBI_1.1.0 pillar_1.5.1 withr_2.4.1 mgcv_1.8-33 xts_0.12.1 ## [101] survival_3.2-7 abind_1.4-5 tibble_3.1.0 crayon_1.4.1 utf8_1.1.4 ## [106] rmarkdown_2.7 emo_0.0.0.9000 grid_4.0.4 callr_3.5.1 threejs_0.3.3 ## [111] digest_0.6.27 xtable_1.8-4 httpuv_1.5.4 RcppParallel_5.0.2 stats4_4.0.4 ## [116] munsell_0.5.0 shinyjs_2.0.0 This list is not exhaustive of the links people shared on twitter. My basic inclusion criteria were that they were (a) peer-reviewed articles (b) with a substantive focus that I could (c) easily find a PDF link for. When someone shared several of their works, I simply pulled the first one that fulfilled those criteria. Also, inclusion on this list is not a personal endorsement. I haven’t read most of them. You get what you pay for, friends.↩︎ "],["references.html", "References", " References Aden-Buie, G. (2020). ggpomological: Pomological plot theme for ggplot2 [Manual]. https://github.com/gadenbuie/ggpomological Agresti, A. (2015). Foundations of linear and generalized linear models. John Wiley &amp; Sons. https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034 Akaike, H. (1998). Information theory and an extension of the maximum likelihood principle. In Selected papers of Hirotugu Akaike (pp. 199–213). Springer. https://www.springer.com/gp/book/9780387983554 Allen, M. A., Flynn, M. E., Machain, C. M., &amp; Stravers, A. (2020). Outside the wire: US military deployments and public opinion in host states. American Political Science Review, 114(2), 326–341. https://doi.org/10.1017/S0003055419000868 Amlie-Lefond, C., Shaw, D. W., Cooper, A., Wainwright, M. S., Kirton, A., Felling, R. J., Abraham, M. G., Mackay, M. T., Dowling, M. M., Torres, M., &amp; others. (2020). Risk of intracranial hemorrhage following intravenous tPA (Tissue-Type Plasminogen Activator) for acute stroke is low in children. Stroke, 51(2), 542–548. https://doi.org/10.1161/STROKEAHA.119.027225 Amrhein, V., Greenland, S., &amp; McShane, B. (2019). Scientists rise up against statistical significance. Nature, 567(7748), 305–307. https://doi.org/10.1038/d41586-019-00857-9 Angrist, J. D., &amp; Keueger, A. B. (1991). Does compulsory school attendance affect schooling and earnings? The Quarterly Journal of Economics, 106(4), 979–1014. https://doi.org/10.2307/2937954 Aono, Y. (2012). Long-term change in climate and floral phenophase. Chikyu Kankyo (Global Environment), 17. http://atmenv.envi.osakafu-u.ac.jp/aono/kyophenotemp4/ Aono, Y., &amp; Kazui, K. (2008). Phenological data series of cherry tree flowering in Kyoto, Japan, and its application to reconstruction of springtime temperatures since the 9th century. International Journal of Climatology, 28(7), 905–914. https://doi.org/10.1002/joc.1594 Aono, Y., &amp; Saito, S. (2010). Clarifying springtime temperature reconstructions of the medieval period by gap-filling the cherry blossom phenological data series at Kyoto, Japan. International Journal of Biometeorology, 54(2), 211–219. https://doi.org/10.1007/s00484-009-0272-x Arnold, J. B. (2019). ggthemes: Extra themes, scales and geoms for ’ggplot2’. https://CRAN.R-project.org/package=ggthemes Atkins, D. C., Baldwin, S. A., Zheng, C., Gallop, R. J., &amp; Neighbors, C. (2013). A tutorial on count regression and zero-altered count models for longitudinal substance use data. Psychology of Addictive Behaviors : Journal of the Society of Psychologists in Addictive Behaviors, 27(1), 166–177. https://doi.org/10.1037/a0029508 Baraldi, A. N., &amp; Enders, C. K. (2010). An introduction to modern missing data analyses. Journal of School Psychology, 48(1), 5–37. https://doi.org/10.1016/j.jsp.2009.10.001 Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001 Barrett, M. (2021a). An introduction to ggdag. https://CRAN.R-project.org/package=ggdag/vignettes/intro-to-ggdag.html Barrett, M. (2021b). ggdag: Analyze and create elegant directed acyclic graphs. https://CRAN.R-project.org/package=ggdag Baumer, B. S., Kaplan, D. T., &amp; Horton, N. J. (2021). Modern data science with R (2nd edition). Taylor &amp; Francis Group, LLC. https://mdsr-book.github.io/mdsr2e/ Betancourt, M. (2018). Bayes sparse regression. https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html Betancourt, M. (2017). Robust Gaussian processes in Stan. https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html Bickel, P. J., Hammel, E. A., &amp; O’Connell, J. W. (1975). Sex bias in graduate admissions: Data from Berkeley. Science, 187(4175), 398–404. https://doi.org/10.1126/science.187.4175.398 Boesch, C., Bombjaková, D., Meier, A., &amp; Mundry, R. (2019). Learning curves and teaching when acquiring nut-cracking in humans and chimpanzees. Scientific Reports, 9(1), 1515. https://doi.org/10.1038/s41598-018-38392-8 Borges, JL. (1941). El jardin de senderos que se bifurcan. Buenos Aires: Sur. Translated by D. A. Yates (1964). In Labyrinths: Selected Stories &amp; Other Writings (pp. 19–29). New Directions. Brilleman, S., Crowther, M., Moreno-Betancur, M., Buros Novik, J., &amp; Wolfe, R. (2018). Joint longitudinal and time-to-event models via Stan. https://github.com/stan-dev/stancon_talks/ Bryan, J., the STAT 545 TAs, &amp; Hester, J. (2020). Happy Git and GitHub for the useR. https://happygitwithr.com Bürkner, P.-C. (2021a). Define custom response distributions with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html Bürkner, P.-C. (2021b). Estimating distributional models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html Bürkner, P.-C. (2021c). Estimating monotonic effects with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_monotonic.html Bürkner, P.-C. (2021d). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html Bürkner, P.-C. (2021e). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html Bürkner, P.-C. (2021f). Estimating phylogenetic multilevel models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_phylogenetics.html Bürkner, P.-C. (2021g). Handle missing values with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html Bürkner, P.-C. (2021h). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html Bürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01 Bürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017 Bürkner, P.-C. (2020). brms: Bayesian regression models using ’Stan’. https://CRAN.R-project.org/package=brms Bürkner, P.-C. (2021i). brms reference manual, Version 2.15.0. https://CRAN.R-project.org/package=brms/brms.pdf Bürkner, P.-C., &amp; Charpentier, E. (2020). Modelling monotonic effects of ordinal predictors in Bayesian regression models. British Journal of Mathematical and Statistical Psychology. https://doi.org/10.1111/bmsp.12195 Bürkner, P.-C., Gabry, J., Kay, M., &amp; Vehtari, A. (2020). posterior: Tools for working with posterior distributions. https://mc-stan.org/posterior Bürkner, P.-C., &amp; Vuorre, M. (2019). Ordinal regression models in psychology: A tutorial. Advances in Methods and Practices in Psychological Science, 2(1), 77–101. https://doi.org/10.1177/2515245918823199 Carvalho, C. M., Polson, N. G., &amp; Scott, J. G. (2009). Handling sparsity via the horseshoe. Artificial Intelligence and Statistics, 73–80. http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf Casella, G., &amp; George, E. I. (1992). Explaining the Gibbs sampler. The American Statistician, 46(3), 167–174. https://doi.org/10.1080/00031305.1992.10475878 Casillas, J. V. (2021). Interlingual interactions elicit performance mismatches not “compromise” categories in early bilinguals: Evidence from meta-analysis and coronal stops. Languages, 6(1), 9. https://doi.org/10.3390/languages6010009 Cohen, J., Cohen, P., West, S. G., &amp; Aiken, L. S. (2013). Applied multiple regression/correlation analysis for the behavioral sciences (Third Edition). Routledge. https://doi.org/10.4324/9780203774441 Cover, T. M., &amp; Thomas, J. A. (2006). Elements of information theory (2nd Edition). John Wiley &amp; Sons. https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959 Cumming, G. (2014). The new statistics: Why and how. Psychological Science, 25(1), 7–29. https://doi.org/10.1177/0956797613504966 Cushman, F., Young, L., &amp; Hauser, M. (2006). The role of conscious reasoning and intuition in moral judgment: Testing three principles of harm. Psychological Science, 17(12), 1082–1089. https://doi.org/10.1111/j.1467-9280.2006.01834.x Davis, F. P., Nern, A., Picard, S., Reiser, M. B., Rubin, G. M., Eddy, S. R., &amp; Henry, G. L. (2020). A genetic, genomic, and computational resource for exploring neural circuit function. Elife, 9, e50901. https://doi.org/10.7554/eLife.50901 de Rooij, M., &amp; Weeda, W. (2020). Cross-validation: A method every psychologist should know. Advances in Methods and Practices in Psychological Science, 3(2), 248–263. https://doi.org/10.1177/2515245919898466 Dunn, P. K., &amp; Smyth, G. K. (2018). Generalized linear models with examples in R. Springer. https://link.springer.com/book/10.1007/978-1-4419-0118-7 Efron, B., &amp; Morris, C. (1977). Stein’s paradox in statistics. Scientific American, 236(5), 119–127. https://doi.org/10.1038/scientificamerican0577-119 Enders, C. K. (2010). Applied missing data analysis. Guilford press. http://www.appliedmissingdata.com/ Fernández i Marín, X. (2016). ggmcmc: Analysis of MCMC samples and Bayesian inference. Journal of Statistical Software, 70(9), 1–20. https://doi.org/10.18637/jss.v070.i09 Fernández i Marín, X. (2020). ggmcmc: Tools for analyzing MCMC simulations from Bayesian inference [Manual]. https://CRAN.R-project.org/package=ggmcmc Gabry, J. (2021). Plotting MCMC draws using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/plotting-mcmc-draws.html Gabry, J., &amp; Goodrich, B. (2020). rstanarm: Bayesian applied regression modeling via stan [Manual]. https://CRAN.R-project.org/package=rstanarm Gabry, J., &amp; Mahr, T. (2021). bayesplot: Plotting for Bayesian models. https://CRAN.R-project.org/package=bayesplot Gabry, J., &amp; Modrák, M. (2021). Visual MCMC diagnostics using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/visual-mcmc-diagnostics.html Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., &amp; Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378 Garnier, S. (2018). viridis: Default color maps from ’matplotlib’ [Manual]. https://CRAN.R-project.org/package=viridis Gelman, A. (2005). Analysis of variance–Why it is more important than ever. Annals of Statistics, 33(1), 1–53. https://doi.org/10.1214/009053604000001048 Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Bayesian Analysis, 1(3), 515–534. https://doi.org/10.1214/06-BA117A Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/ Gelman, A., Goodrich, B., Gabry, J., &amp; Vehtari, A. (2019). R-squared for Bayesian regression models. The American Statistician, 73(3), 307–309. https://doi.org/10.1080/00031305.2018.1549100 Gelman, A., &amp; Greenland, S. (2019). Are confidence intervals better termed “uncertainty intervals?” BMJ, l5381. https://doi.org/10.1136/bmj.l5381 Gelman, A., Hill, J., &amp; Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879 Gelman, A., &amp; Imbens, G. (2019). Why high-order polynomials should not be used in regression discontinuity designs. Journal of Business &amp; Economic Statistics, 37(3), 447–456. https://doi.org/10.1080/07350015.2017.1366909 Gelman, A., &amp; Little, T. C. (1997). Postratification into many categories using hierarchical logistic regression. Survey Methodology, 23, 127–135. https://stat.columbia.edu/~gelman/research/published/poststrat3.pdf Gelman, A., &amp; Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-hacking” and the research hypothesis was posited ahead of time. 17. https://stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf Gelman, A., Simpson, D., &amp; Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. Entropy, 19(10), 555. https://doi.org/10.3390/e19100555 Gelman, A., &amp; Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60(4), 328–331. https://doi.org/10.1198/000313006X152649 Geman, S., &amp; Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6(6), 721–741. https://doi.org/10.1109/TPAMI.1984.4767596 Girard, J. M., Cohn, J. F., Yin, L., &amp; Morency, L.-P. (2021). Reconsidering the duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion. Affective Science, 1–16. https://doi.org/10.1007/s42761-020-00030-w Gohel, D. (2021a). flextable: Functions for tabular reporting [Manual]. https://CRAN.R-project.org/package=flextable Gohel, D. (2021b). Using the flextable R package. https://ardata-fr.github.io/flextable-book/ Grafen, A., &amp; Hails, R. (2002). Modern statistics for the life sciences. Oxford University Press. https://global.oup.com/academic/product/modern-statistics-for-the-life-sciences-9780199252312? Grantham, N. (2019). ggdark: Dark mode for ’ggplot2’ themes [Manual]. https://CRAN.R-project.org/package=ggdark Grolemund, G., &amp; Wickham, H. (2017). R for data science. O’Reilly. https://r4ds.had.co.nz Haines, N., Vassileva, J., &amp; Ahn, W.-Y. (2018). The outcome-representation learning model: A novel reinforcement learning model of the Iowa Gambling Task. Cognitive Science, 42(8), 2534–2561. https://doi.org/10.1111/cogs.12688 Hamaker, E. L., &amp; Dolan, C. V. (2009). Idiographic data analysis: Quantitative methodsfrom simple to advanced. In J. Valsiner, P. C. M. Molenaar, M. C. D. P. Lyra, &amp; N. Chaudhary (Eds.), Dynamic process methodology in the social and developmental sciences (pp. 191–216). Springer. https://doi.org/10.1007/978-0-387-95922-1_9 Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer Science &amp; Business Media. https://doi.org/10.1007/978-0-387-84858-7 Hauer, E. (2004). The harm done by tests of significance. Accident Analysis &amp; Prevention, 36(3), 495–500. https://doi.org/10.1016/S0001-4575(03)00036-8 Hauser, M., Cushman, F., Young, L., Jin, R. K.-X., &amp; Mikhail, J. (2007). A dissociation between moral judgments and justifications. Mind &amp; Language, 22(1), 1–21. https://doi.org/10.1111/j.1468-0017.2006.00297.x Hayes, A. F. (2017). Introduction to mediation, moderation, and conditional process analysis: A regression-based approach. Guilford publications. https://www.guilford.com/books/Introduction-to-Mediation-Moderation-and-Conditional-Process-Analysis/Andrew-Hayes/9781462534654 Healy, K. (2018). Data visualization: A practical introduction. Princeton University Press. https://socviz.co/ Hedeker, D., Mermelstein, R. J., &amp; Demirtas, H. (2008). An application of a mixed-effects location scale model for analysis of ecological momentary assessment (EMA) data. Biometrics, 64(2), 627–634. https://doi.org/10.1111/j.1541-0420.2007.00924.x Hedeker, D., Mermelstein, R. J., &amp; Demirtas, H. (2012). Modeling between- and within-subject variance in ecological momentary assessment (EMA) data using mixed-effects location scale models. Statistics in Medicine, 31(27). https://doi.org/10.1002/sim.5338 Henderson, E. (2020). ghibli: Studio ghibli colour palettes [Manual]. https://CRAN.R-project.org/package=ghibli Henry, L., &amp; Wickham, H. (2020). purrr: Functional programming tools. https://CRAN.R-project.org/package=purrr Hewitt, C. G. (1921). The conservation of the wild life of Canada. Charles Scribner’s Sons. Hinde, K., &amp; Milligan, L. A. (2011). Primate milk: Proximate mechanisms and ultimate perspectives. Evolutionary Anthropology: Issues, News, and Reviews, 20(1), 9–23. https://doi.org/10.1002/evan.20289 Hoffman, L. (2015). Longitudinal analysis: Modeling within-person fluctuation and change (1 edition). Routledge. https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025 Howell, N. (2001). Demography of the dobe! Kung (2nd Edition). Routledge. https://www.routledge.com/Demography-of-the-Dobe-Kung/Howell/p/book/9780202306490 Howell, N. (2010). Life histories of the Dobe! Kung: Food, fatness, and well-being over the life span (Vol. 4). Univ of California Press. https://www.ucpress.edu/book/9780520262348/life-histories-of-the-dobe-kung Johnson, W., Carothers, A., &amp; Deary, I. J. (2008). Sex differences in variability in general intelligence: A new look at the old question. Perspectives on Psychological Science, 3(6), 518–531. https://doi.org/10.1111/j.1745-6924.2008.00096.x Kahle, D., &amp; Stamey, J. (2017). invgamma: The inverse gamma distribution [Manual]. https://CRAN.R-project.org/package=invgamma Kale, A., Kay, M., &amp; Hullman, J. (2020). Visual reasoning strategies for effect size judgments and decisions. IEEE Transactions on Visualization and Computer Graphics. https://doi.org/10.1109/TVCG.2020.3030335 Kay, M. (2020a). Extracting and visualizing tidy draws from brms models. https://mjskay.github.io/tidybayes/articles/tidy-brms.html Kay, M. (2020b). Marginal distribution of a single correlation from an LKJ distribution. https://mjskay.github.io/ggdist/reference/lkjcorr_marginal.html Kay, M. (2020c). tidybayes: Tidy data and ’geoms’ for Bayesian models. http://mjskay.github.io/tidybayes Kennedy, L., &amp; Gelman, A. (2020). Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample. arXiv:1906.11323 [Stat]. http://arxiv.org/abs/1906.11323 Kievit, R., Frankenhuis, W. E., Waldorp, L., &amp; Borsboom, D. (2013). Simpson’s paradox in psychological science: A practical guide. Frontiers in Psychology, 4. https://doi.org/10.3389/fpsyg.2013.00513 Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A. (2018). Many Labs 2: Investigating variation in replicability across samples and settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225 Kline, M. A., &amp; Boyd, R. (2010). Population size predicts technological complexity in Oceania. Proceedings of the Royal Society B: Biological Sciences, 277(1693), 2559–2564. https://doi.org/10.1098/rspb.2010.0452 Kolczynska, M., Bürkner, P.-C., Kennedy, L., &amp; Vehtari, A. (2020). Trust in state institutions in Europe, 1989-2019. SocArXiv. https://doi.org/10.31235/osf.io/3v5g7 Koster, J. M., &amp; Leckie, G. (2014). Food sharing networks in lowland Nicaragua: An application of the social relations model to count data. Social Networks, 38, 100–110. https://doi.org/10.1016/j.socnet.2014.02.002 Kruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/ Kullback, S., &amp; Leibler, R. A. (1951). On information and sufficiency. Annals of Mathematical Statistics, 22(1), 79–86. https://doi.org/10.1214/aoms/1177729694 Kurz, A. S. (2020a). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.2.0). https://doi.org/10.5281/zenodo.3693202 Kurz, A. S. (2020b). Applied Longitudinal Data Analysis in brms and the tidyverse (version 0.0.1). https://bookdown.org/content/4253/ Kurz, A. S. (2020c). Doing Bayesian data analysis in brms and the tidyverse (version 0.3.0). https://bookdown.org/content/3686/ Kurz, A. S. (2019). Recoding Introduction to mediation, moderation, and conditional process analysis (version 1.1.0). https://doi.org/10.5281/zenodo.3589999 Kurz, A. S., DeBeer, B. B., Kimbrel, N. A., Morissette, S. B., &amp; Meyer, E. C. (2019, October). Even with treatment, functional impairment and quality of life remain remarkably stable over two years in post-9/11 Iraq and Afghanistan war veterans. The 4th Annual San Antonio Combat PTSD Conference. https://osf.io/vekpf/ Legler, J., &amp; Roback, P. (2019). Broadening your statistical horizons: Generalized linear models and multilevel models. https://bookdown.org/roback/bookdown-bysh/ Linnebo, Ø. (2018). Platonism in the philosophy of mathematics. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Spring 2018). Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/spr2018/entries/platonism-mathematics/ Little, R. J. A., &amp; Rubin, D. B. (2019). Statistical analysis with missing data. John Wiley &amp; Sons. https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798 Lotka, A. J. (1925). Principles of physical biology. Waverly. Matamoros, I. A. A., &amp; Torres, C. A. C. (2020). varstan: An R package for Bayesian analysis of structured time series models with Stan. arXiv:2005.10361 [Stat]. http://arxiv.org/abs/2005.10361 Matejka, J., &amp; Fitzmaurice, G. (2017). Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing. https://www.autodesk.com/research/publications/same-stats-different-graphs McElreath, R. (2020a). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/ McElreath, R. (2020b). rethinking R package. https://xcelab.net/rm/software/ Meehl, P. E. (1990). Why summaries of research on psychological theories are often uninterpretable. Psychological Reports, 66(1), 195–244. https://doi.org/10.2466/pr0.1990.66.1.195 Merkle, E. C., &amp; Rosseel, Y. (2018). blavaan: Bayesian structural equation models via parameter expansion. Journal of Statistical Software, 85(4), 1–30. https://doi.org/10.18637/jss.v085.i04 Merkle, E. C., Rosseel, Y., &amp; Goodrich, B. (2020). blavaan: Bayesian latent variable analysis. https://CRAN.R-project.org/package=blavaan Müller, K., &amp; Wickham, H. (2020). tibble: Simple data frames. https://CRAN.R-project.org/package=tibble Navarro, D. (2019). Learning statistics with R. https://learningstatisticswithr.com Navarro, D. J. (2019). Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection. Computational Brain &amp; Behavior, 2(1), 28–34. https://doi.org/10.1007/s42113-018-0019-z Nogueira, R. G., Jadhav, A. P., Haussen, D. C., Bonafe, A., Budzik, R. F., Bhuva, P., Yavagal, D. R., Ribo, M., Cognard, C., Hanel, R. A., &amp; others. (2018). Thrombectomy 6 to 24 hours after stroke with a mismatch between deficit and infarct. New England Journal of Medicine, 378(1), 11–21. https://doi.org/10.1056/NEJMoa1706442 Nowosad, J. (2019). rcartocolor: ’CARTOColors’ palettes. https://CRAN.R-project.org/package=rcartocolor Nunn, N., &amp; Puga, D. (2012). Ruggedness: The blessing of bad geography in Africa. Review of Economics and Statistics, 94(1), 20–36. https://doi.org/10.1162/REST_a_00161 Paananen, T., Bürkner, P.-C., Vehtari, A., &amp; Gabry, J. (2020). Avoiding model refits in leave-one-out cross-validation with moment matching. https://CRAN.R-project.org/package=loo/vignettes/loo2-moment-matching.html Paananen, T., Piironen, J., Bürkner, P.-C., &amp; Vehtari, A. (2020). Implicitly adaptive importance sampling. arXiv:1906.08850 [Stat]. http://arxiv.org/abs/1906.08850 Paradis, Emmanuel, Blomberg, S., Bolker, B., Brown, J., Claramunt, S., Claude, J., Cuong, H. S., Desper, R., Didier, G., Durand, B., Dutheil, J., Ewing, R., Gascuel, O., Guillerme, T., Heibl, C., Ives, A., Jones, B., Krah, F., Lawson, D., … de Vienne, D. (2020). ape: Analyses of phylogenetics and evolution [Manual]. https://CRAN.R-project.org/package=ape Paradis, E., &amp; Schliep, K. (2019). ape 5.0: An environment for modern phylogenetics and evolutionary analyses in R. Bioinformatics, 35, 526–528. https://doi.org/10.1093/bioinformatics/bty633 Park, D. K., Gelman, A., &amp; Bafumi, J. (2004). Bayesian multilevel estimation with poststratification: State-level estimates from national polls. Political Analysis, 12(4), 375–385. https://www.jstor.org/stable/25791784 Pedersen, T. L. (2019). patchwork: The composer of plots. https://CRAN.R-project.org/package=patchwork Peng, R. D. (2019). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/ Peng, R. D., Kross, S., &amp; Anderson, B. (2017). Mastering software development in {}R{}. https://github.com/rdpeng/RProgDA Pivot data from wide to long pivot_longer. (2020). https://tidyr.tidyverse.org/reference/pivot_longer.html Pivoting. (2020). https://tidyr.tidyverse.org/articles/pivot.html Plummer, M. (2003). JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling. Working Papers, 8. http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Plummer.pdf R Core Team. (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ R Library Contrast Coding Systems for categorical variables. (n.d.). In UCLA: Statistical Consulting Group. Retrieved October 14, 2020, from https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/ Ram, K., &amp; Wickham, H. (2018). wesanderson: A Wes Anderson palette generator [Manual]. https://CRAN.R-project.org/package=wesanderson Rast, P., Hofer, S. M., &amp; Sparks, C. (2012). Modeling individual differences in within-person variation of negative and positive affect in a mixed effects location scale model using BUGS/JAGS. Multivariate Behavioral Research, 47(2), 177–200. https://doi.org/10.1080/00273171.2012.658328 Revelle, W. (2020). psych: Procedures for psychological, psychometric, and personality research. https://CRAN.R-project.org/package=psych Ripley, B. (2019). MASS: Support functions and datasets for venables and ripley’s MASS. https://CRAN.R-project.org/package=MASS Robert, C., &amp; Casella, G. (2011). A short history of Markov chain Monte Carlo: Subjective recollections from incomplete data. Statistical Science, 26(1), 102–115. https://arxiv.org/pdf/0808.2902.pdf Robinson, D., &amp; Hayes, A. (2020). broom: Convert statistical analysis objects into tidy tibbles [Manual]. https://CRAN.R-project.org/package=broom Ross, C. T., Winterhalder, B., &amp; McElreath, R. (2020). Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates. Social Psychological and Personality Science, 1948550620916071. https://doi.org/10.1177/1948550620916071 Rubin, Donald B. (1996). Multiple imputation after 18+ years. Journal of the American Statistical Association, 91(434), 473–489. https://doi.org/10.1080/01621459.1996.10476908 Rubin, Donald B. (1976). Inference and missing data. Biometrika, 63(3), 581–592. https://doi.org/10.1093/biomet/63.3.581 Rubin, Donald B. (1987). Multiple imputation for nonresponse in surveys. John Wiley &amp; Sons Inc. https://doi.org/10.1002/9780470316696 Rudis, B. (2020). statebins: Create united states uniform cartogram heatmaps [Manual]. https://CRAN.R-project.org/package=statebins Rudis, B., Ross, N., &amp; Garnier, S. (2018). The viridis color palettes. https://cran.r-project.org/package=viridis/vignettes/intro-to-viridis.html Rue, H., Martino, S., &amp; Chopin, N. (2009). Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series b (Statistical Methodology), 71(2), 319–392. https://doi.org/10.1111/j.1467-9868.2008.00700.x Schloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp; Larmarange, J. (2020). GGally: Extension to ’ggplot2’. https://CRAN.R-project.org/package=GGally Shannon, C. E. (1948). A mathematical theory of communication. Bell System Technical Journal, 27(3), 379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x Silbiger, N. J., Goodbody-Gringley, G., Bruno, J. F., &amp; Putnam, H. M. (2019). Comparative thermal performance of the reef-building coral Orbicella franksi at its latitudinal range limits. Marine Biology, 166(10), 1–14. https://doi.org/10.1007/s00227-019-3573-6 Silk, J. B., Brosnan, S. F., Vonk, J., Henrich, J., Povinelli, D. J., Richardson, A. S., Lambeth, S. P., Mascaro, J., &amp; Schapiro, S. J. (2005). Chimpanzees are indifferent to the welfare of unrelated group members. Nature, 437(7063), 1357–1359. https://doi.org/10.1038/nature04243 Simmons, J. P., Nelson, L. D., &amp; Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632 Singer, J. D., &amp; Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968 Slowikowski, K. (2020). ggrepel: Automatically position non-overlapping text labels with ’ggplot2’. https://CRAN.R-project.org/package=ggrepel Spiegelhalter, D. J., Best, N. G., Carlin, B. P., &amp; Linde, A. V. D. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(4), 583–639. https://doi.org/10.1111/1467-9868.00353 Spiegelhalter, D., Thomas, A., Best, N., &amp; Lunn, D. (2003). WinBUGS user manual. https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/manual14.pdf Stan Development Team. (2020). RStan: The R interface to Stan. https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html Stan Development Team. (2021a). Stan functions reference. https://mc-stan.org/docs/2_26/functions-reference/index.html Stan Development Team. (2021b). Stan reference manual, Version 2.26. https://mc-stan.org/docs/2_26/reference-manual/ Stan Development Team. (2021c). Stan user’s guide, Version 2.26. https://mc-stan.org/docs/2_26/stan-users-guide/index.html Street, S. E., Navarrete, A. F., Reader, S. M., &amp; Laland, K. N. (2017). Coevolution of cultural intelligence, extended life history, sociality, and brain size in primates. Proceedings of the National Academy of Sciences, 114(30), 7908–7914. https://doi.org/10.1073/pnas.1620734114 Subramanian, S. V., Kim, R., &amp; Christakis, N. A. (2018). The “average” treatment effect: A construct ripe for retirement. A commentary on Deaton and Cartwright. Social Science &amp; Medicine, 210, 77–82. https://doi.org/10.1016/j.socscimed.2018.04.027 Textor, J., &amp; der Zander, B. van. (2016). dagitty: Graphical analysis of structural causal models. https://CRAN.R-project.org/package=dagitty Thoen, E. (2019). dutchmasters [Manual]. https://github.com/EdwinTh/dutchmasters Tufte, E. R. (2001). The visual display of quantitative information (Second Edition). Graphics Press. https://www.edwardtufte.com/tufte/books_vdqi Urban Institute. (2020). urbnmapr: State and county maps with Alaska and Hawaii. https://github.com/UrbanInstitute/urbnmapr van Buuren, S. (2018). Flexible imputation of missing data (Second Edition). CRC Press. https://stefvanbuuren.name/fimd/ van Leeuwen, E. J. C., Cohen, E., Collier-Baker, E., Rapold, C. J., Schäfer, M., Schütte, S., &amp; Haun, D. B. M. (2018). The development of human social learning across seven societies. Nature Communications, 9(1), 2076. https://doi.org/10.1038/s41467-018-04468-2 Vehtari, A., Gabry, J., Magnusson, M., Yao, Y., &amp; Gelman, A. (2019). loo: Efficient leave-one-out cross-validation and WAIC for bayesian models. https://CRAN.R-project.org/package=loo/ Vehtari, A., Gelman, A., &amp; Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4 Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved \\(\\widehat{R}\\) for assessing convergence of MCMC. arXiv Preprint arXiv:1903.08008. https://arxiv.org/abs/1903.08008? Venables, W. N., &amp; Ripley, B. D. (2002). Modern applied statistics with S (Fourth Edition). Springer. http://www.stats.ox.ac.uk/pub/MASS4 Vermeer, J. (1665). Girl with a pearl earring. Volterra, V. (1926). Fluctuations in the abundance of a species considered mathematically. Nature, 118(2972), 558–560. https://doi.org/10.1038/118558a0 von Bertalanffy, L. (1934). Untersuchungen Über die Gesetzlichkeit des Wachstums. Wilhelm Roux’ Archiv für Entwicklungsmechanik Der Organismen, 131(4), 613–652. https://doi.org/10.1007/BF00650112 Vonesh, J. R., &amp; Bolker, B. M. (2005). Compensatory larval responses shift trade-offs associated with predator-induced hatching plasticity. Ecology, 86(6), 1580–1591. https://doi.org/10.1890/04-0535 Wasserstein, R. L., &amp; Lazar, N. A. (2016). The ASA statement on p-values: Context, process, and purpose. The American Statistician, 70(2), 129–133. https://doi.org/10.1080/00031305.2016.1154108 Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. Journal of Machine Learning Research, 11(116), 3571–3594. http://jmlr.org/papers/v11/watanabe10a.html Watson, D., Clark, L. A., &amp; Tellegen, A. (1988). Development and validation of brief measures of positive and negative affect: The PANAS scales. Journal of Personality and Social Psychology, 54(6), 1063–1070. https://doi.org/10.1037/0022-3514.54.6.1063 Weber, S., &amp; Bürkner, P.-C. (2021). Running brms models with within-chain parallelization. https://CRAN.R-project.org/package=brms/vignettes/brms_threading.html Whitehouse, H., François, P., Savage, P. E., Currie, T. E., Feeney, K. C., Cioni, E., Purcell, R., Ross, R. M., Larson, J., Baines, J., ter Haar, B., Covey, A., &amp; Turchin, P. (2019). Complex societies precede moralizing gods throughout world history. Nature, 568(7751), 226–229. https://doi.org/10.1038/s41586-019-1043-4 Wickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2-book.org/ Wickham, H. (2019). tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse Wickham, H. (2020). The tidyverse style guide. https://style.tidyverse.org/ Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., &amp; Dunnington, D. (2020). ggplot2: Create elegant data visualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2 Wiecek, W., &amp; Meager, R. (2020). baggr: Bayesian aggregate treatment effects [Manual]. https://CRAN.R-project.org/package=baggr Wilke, C. O. (2019). Fundamentals of data visualization. https://clauswilke.com/dataviz/ Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses. The Annals of Mathematical Statistics, 9(1), 60–62. https://doi.org/10.1214/aoms/1177732360 Williams, D. R., Liu, S., Martin, S. R., &amp; Rast, P. (2019). Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity. https://doi.org/10.31234/osf.io/4kfjp Williams, D. R., Martin, S. R., &amp; Rast, P. (2019). Putting the individual into reliability: Bayesian testing of homogeneous within-person variance in hierarchical models. https://doi.org/10.31234/osf.io/hpq7w Williams, D. R., Rast, P., &amp; Bürkner, P.-C. (2018). Bayesian meta-analysis with weakly informative prior distributions. https://doi.org/10.31234/osf.io/7tbrm Williams, D. R., Rouder, J., &amp; Rast, P. (2019). Beneath the surface: Unearthing within-Person variability and mean relations with Bayesian mixed models. https://doi.org/10.31234/osf.io/gwatq Williams, D. R., Zimprich, D. R., &amp; Rast, P. (2019). A Bayesian nonlinear mixed-effects location scale model for learning. Behavior Research Methods, 51(5), 1968–1986. https://doi.org/10.3758/s13428-019-01255-9 Wood, S. N. (2017a). Generalized additive models: An introduction with R (Second Edition). CRC Press. https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331 Wood, S. N. (2003). Thin-plate regression splines. Journal of the Royal Statistical Society (B), 65(1), 95–114. https://doi.org/10.1111/1467-9868.00374 Wood, S. N. (2004). Stable and efficient multiple smoothing parameter estimation for generalized additive models. Journal of the American Statistical Association, 99(467), 673–686. https://doi.org/10.1198/016214504000000980 Wood, S. N. (2011). Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. Journal of the Royal Statistical Society (B), 73(1), 3–36. https://doi.org/10.1111/j.1467-9868.2010.00749.x Wood, S. N. (2017b). Generalized additive models: An introduction with r (Second). Chapman and Hall/CRC. https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331?utm_source=crcpress.com&amp;utm_medium=referral Wood, S. N. (2019). mgcv: Mixed GAM computation vehicle with automatic smoothness estimation. https://CRAN.R-project.org/package=mgcv Wood, S. N., Pya, N., &amp; Säfken, B. (2016). Smoothing parameter and model selection for general smooth models (with discussion). Journal of the American Statistical Association, 111, 1548–1575. https://doi.org/10.1080/01621459.2016.1180986 Xie, Y. (2020). bookdown: Authoring books and technical documents with R Markdown. https://CRAN.R-project.org/package=bookdown Xie, Y., Allaire, J. J., &amp; Grolemund, G. (2020). R markdown: The definitive guide. Chapman and Hall/CRC. https://bookdown.org/yihui/rmarkdown/ Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A. (2018). Using stacking to average Bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091 Yarkoni, T., &amp; Westfall, J. (2017). Choosing prediction over explanation in psychology: Lessons from machine learning. Perspectives on Psychological Science : A Journal of the Association for Psychological Science, 12(6), 1100–1122. https://doi.org/10.1177/1745691617693393 Yu, G. (2020a). Data integration, manipulation and visualization of phylogenetic trees. https://yulab-smu.github.io/treedata-book/ Yu, G. (2020b). Using ggtree to visualize data on tree-like structures. Current Protocols in Bioinformatics, 69(1), e96. https://doi.org/10.1002/cpbi.96 Yu, G., Lam, T. T.-Y., Zhu, H., &amp; Guan, Y. (2018). Two methods for mapping and visualizing associated data on phylogeny using ggtree. Molecular Biology and Evolution, 35(12), 3041–3043. https://doi.org/10.1093/molbev/msy194 Yu, G., Smith, D. K., Zhu, H., Guan, Y., &amp; Lam, T. T.-Y. (2017). ggtree: An R package for visualization and annotation of phylogenetic trees with their covariates and other associated data. Methods in Ecology and Evolution, 8(1), 28–36. https://doi.org/10.1111/2041-210X.12628 Zhang, Y., &amp; Yang, Y. (2015). Cross-validation for selecting a model selection procedure. Journal of Econometrics, 187(1), 95–112. https://doi.org/10.1016/j.jeconom.2015.02.006 "]]
