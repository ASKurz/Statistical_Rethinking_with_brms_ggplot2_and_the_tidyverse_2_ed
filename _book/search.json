[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical rethinking 2 with brms and the tidyverse",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#my-assumptions-about-you",
    "href": "index.html#my-assumptions-about-you",
    "title": "Statistical rethinking 2 with brms and the tidyverse",
    "section": "My assumptions about you",
    "text": "My assumptions about you\nIf you’re looking at this project, I’m guessing you’re either a graduate student, a post-graduate academic or a researcher of some sort, which suggests you have at least a 101-level foundation in statistics. If you’re rusty, consider checking out the free text books by Roback and Legler (2021) or Navarro (2019) before diving into Statistical rethinking. I’m also assuming you understand the rudiments of R and have at least a vague idea about what the tidyverse is. If you’re totally new to R, consider starting with Peng’s (2022) R programming for data science. For an introduction to the tidyvese-style of data analysis, the best source I’ve found is Grolemund and Wickham’s (2017) R for data science (R4DS), which I extensively link to throughout this project. Another nice alternative is Baumer, Kaplan, and Horton’s (2021), Modern data science with R.\nHowever, you do not need to be totally fluent in statistics or R. Otherwise why would you need this project, anyway? IMO, the most important things are curiosity, a willingness to try, and persistent tinkering. I love this stuff. Hopefully you will, too.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-and-understand-this-project",
    "href": "index.html#how-to-use-and-understand-this-project",
    "title": "Statistical rethinking 2 with brms and the tidyverse",
    "section": "How to use and understand this project",
    "text": "How to use and understand this project\nThis project is not meant to stand alone. It’s a supplement to the second edition of McElreath’s text. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse-style code. However, some of the sections in the text are composed entirely of equations or prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project will sometimes be blank or omitted, though I do highlight some of the important points in quotes and prose of my own. So I imagine students might reference this project as they progress through McElreath’s text. I also imagine working data analysts might use this project in conjunction with the text as they flip to the specific sections that seem relevant to solving their data challenges.\nI reproduce the bulk of the figures in the text, too. The plots in the first few chapters are the closest to those in the text. However, I’m passionate about data visualization and like to play around with color palettes, formatting templates, and other conventions quite a bit. As a result, the plots in each chapter have their own look and feel. For more on some of these topics, check out chapters 3, 7, and 28 in R4DS; Healy’s (2018) Data visualization: A practical introduction; Wilke’s (2019) Fundamentals of data visualization; or Wickham’s (2016) ggplot2: Elegant graphics for data analysis.\nIn this project, I use a handful of formatting conventions gleaned from R4DS, The tidyverse style guide (Wickham, 2020), and R markdown: The definitive guide (Xie et al., 2020).\n\nR code blocks and their output appear in a gray background. E.g.,\n\n\n2 + 2 == 5\n\n[1] FALSE\n\n\n\nR and the names of specific package (e.g., brms) are in boldface font.\nFunctions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()).\nWhen I want to make explicit the package a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidybayes::median_qi()).\nR objects, such as data or function arguments, are in typewriter font atop gray backgrounds (e.g., chimpanzees, .width = 0.5).\nYou can detect hyperlinks by their typical blue-colored font.\nIn the text, McElreath indexed his models with names like m4.1 (i.e., the first model of 4  Geocentric Models). I primarily followed that convention, but replaced the m with a b to stand for the brms package.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#r-setup",
    "href": "index.html#r-setup",
    "title": "Statistical rethinking 2 with brms and the tidyverse",
    "section": "R setup",
    "text": "R setup\nTo get the full benefit from this ebook, you’ll need some software. Happily, everything will be free (provided you have access to a decent personal computer and an good internet connection).\nFirst, you’ll need to install R, which you can learn about at https://cran.r-project.org/.\nThough not necessary, your R experience might be more enjoyable if done through the free RStudio interface, which you can learn about at https://posit.co/products/open-source/rstudio/.\nOnce you have installed R, execute the following to install the bulk of the add-on packages. This will probably take a few minutes to finish. Go make yourself a coffee.\n\npackages &lt;- c(\"ape\", \"bayesplot\", \"brms\", \"broom\", \"dagitty\", \"devtools\", \"flextable\", \"GGally\", \"ggdag\", \"ggmcmc\", \"ggrepel\", \"ggthemes\", \"ggtree\", \"ghibli\", \"gtools\", \"invgamma\", \"loo\", \"patchwork\", \"posterior\", \"psych\", \"rcartocolor\", \"Rcpp\", \"remotes\", \"rstan\", \"santoku\", \"StanHeaders\", \"statebins\", \"tidybayes\", \"tidyverse\", \"viridis\", \"viridisLite\", \"wesanderson\")\n\ninstall.packages(packages, dependencies = T)\n\nA few of the other packages are not officially available via the Comprehensive R Archive Network (CRAN; https://cran.r-project.org/). You can download them directly from GitHub by executing the following.\n\ndevtools::install_github(\"stan-dev/cmdstanr\")\ndevtools::install_github(\"EdwinTh/dutchmasters\")\ndevtools::install_github(\"gadenbuie/ggpomological\")\ndevtools::install_github(\"GuangchuangYu/ggtree\")\ndevtools::install_github(\"rmcelreath/rethinking\")\ndevtools::install_github(\"UrbanInstitute/urbnmapr\")\n\nIt’s possible you’ll have problems installing some of these packages. Here are some likely suspects and where you can find help:\n\nfor difficulties installing brms, go to https://github.com/paul-buerkner/brms#how-do-i-install-brms or search around in the brms section of the Stan forums;\nfor difficulties installing cmdstanr, go to https://mc-stan.org/cmdstanr/articles/cmdstanr.html;\nfor difficulties installing rethinking, go to https://github.com/rmcelreath/rethinking#installation; and\nfor difficulties installing rstan, go to https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#we-have-updates",
    "href": "index.html#we-have-updates",
    "title": "Statistical rethinking 2 with brms and the tidyverse",
    "section": "We have updates",
    "text": "We have updates\nFor a brief rundown of the version history, we have:\n\nVersion 0.1.0\nI released the 0.1.0 version of this project in November 24, 2020. It was the first full-length and nearly complete draft including material from all the 17 chapters in McElreath’s source material. All brms models were fit with version 2.14.0+.\n\n\nVersion 0.1.1\nOn December 2, 2020, I released a mini update designed to\n\nfix code breaks resulting from updates to the broom package (Robinson et al., 2022), caught by Jenny Bigman;\nreplace the soon-to-be retired sample_n() code with slice_sample(), caught by Randall Pruim;\nand correct a few typos along the way.\n\n\n\nVersion 0.2.0\nOn March 16, 2021, I released version 0.2.0, which included the following major improvements:\n\na corrected workflow for fitting single-level b-spline models (Splines), thanks to Stephen Wild;\na refined workflow for fitting multilevel b-spline models using the s() function (Summary First bonus: Smooth functions with brms::s()), thanks to Gavin Simpson;\na new bonus section (Second bonus: Group predictors with matrix columns) on grouping predictors within matrix columns, thanks to insights from Gelman et al. (2020) and hints from Paul-Christian Bürkner;\na brms solution to the \\(\\sigma = 0.01\\) sixth-order polynomial model in More parameters (almost) always improve fit, thanks again to Stephen Wild;\na workflow to reproduce the Metropolis simulation for High-dimensional problems, thanks to James Henegan;\na corrected workflow for taking fitted()-based random draws for the bonus material in Bonus: Let’s use fitted() this time, thanks to an exchange with Ladislas Nalborczyk;\na brms solution to McElreath’s bivariate differential equation model in Population dynamics, thanks to Markus Gesmann;\nbetter use of geom_area() throughout the book, thanks to insights from Randall Pruim;\nthe adoption of a CONTRIBUTING section on GitHub, thanks to Brenton M. Wiernik;\nimproved table workflow with the flextable package (Gohel, 2022); and\nall models have been refit using brms version 2.15.0.\n\n\n\nVersion 0.3.0\nOn September 28, 2022, I released version 0.3.0, which included the following major improvements:\n\nan as_draws_df()-oriented workflow to replace the depreciated posterior_samples() function;\nbetter sampling in models in the first few due to changes in the ub argument;\nmore use of the uniform prior for \\(\\sigma\\) to better match the text, thanks to the improved ub argument;\na more tidyverse-oriented version of the sim_happiness() function in Collider bias, thanks to Randall Pruim\na more accurate depiction of entropy in Figure 10.2.b in Gaussian, thanks to help from Hamed Bastan-Hagh,\na cross link to the first edition, thanks to a suggestion from Will Petry;\nsome corrected typos and updates to the links and citations; and\nall models have been refit using brms version 2.18.0.\n\n\n\nVersion 0.4.0\nOn January 26, 2023, I released version 0.4.0, which included:\n\nreplacing my incorrect use of tidyr::expand() with a more appropriate tidyr::expand_grid() workflow, thanks to insights from Desislava Petkova;\nadopting the new linewidth argument for several ggplot2 geoms (see here); and\nminor prose, hyperlink, and code edits throughout.\n\n\n\nVersion 0.5.0\nWelcome to version 0.5.0! Noteworthy changes include:\n\nswitching to a Quarto ebook format;\nupdating the code to better reflect tidyverse-style guidelines;\nswitching to the base pipe (|&gt;);\nadding comments sections (e.g., here) via giscus;\nupdating all brms model fits with version 2.23.0;\nupdating the Stan code for two of the bespoke models in 16  Generalized Linear Madness;\nintroduced the very flat \\(\\operatorname{logistic}(0, 1)\\) prior for logistic regression models in Bonus: A truly flat prior;\ncorrected the syntax for dplyr::lag(), thanks to James Fern;\ncorrecting some of the labels in Figure 15.4, thanks to Liam Haller;\ncorrecting an table in Counting possibilities, thanks to Christopher Rider; and\na few other minor prose and code edits throughout.\n\n\n\nWe’re not done yet and I could use your help\nSome areas of the book could use some fleshing out. The sections I’m particularly anxious to improve are\n\nCategorical errors and discrete absences, which may someday include a brms workflow for categorical missing data; and\nCoding the statistical model, which contains a mixture model that McElreath fit directly in Stan and I suspect may be possible in brms with a custom likelihood.\n\nIf you have insights on how to improve these or any other sections, please share your thoughts on GitHub at https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/issues. The contribution guidelines for this book are listed at https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/blob/master/CONTRIBUTING.md.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#thank-yous-are-in-order",
    "href": "index.html#thank-yous-are-in-order",
    "title": "Statistical rethinking 2 with brms and the tidyverse",
    "section": "Thank-you’s are in order",
    "text": "Thank-you’s are in order\nI’d like to thank the following for their helpful contributions:\n\nE. David Aja (@edavidaja),\nMonica Alexander (@MJAlexander),\nShaan Amin (@Shaan-Amin),\nMalcolm Barrett (@malcolmbarrett),\nHamed Bastan-Hagh (@hamedbh),\nAdam Bear (@adambear91),\nJenny Bigman (@jennybigman),\nLouis Bliard (@lbiard),\nPhilipp Boersch-Supan (@pboesu),\nPaul-Christian Bürkner (@paul-buerkner),\nDamien Croteau-Chonka (@dcroteau-chonka),\nJames Fern (@JFern83),\nMarkus Gesmann (@mages),\nLiam Haller (@liamhaller),\nJames Henegan (@jameshenegan),\nMatthijs Hollanders (@mhollanders),\nMathieu Jones (@00mathieu),\nWilliam Lai (@williamlai2),\nSebastian Lobentanzer (@slobentanzer),\nEd Merkle (@ecmerkle),\nLadislas Nalborczyk (@lnalborczyk),\nDesislava Petkova (@dipetkov),\nWill Petry (@wpetry),\nRandall Pruim (@rpruim),\nChristopher Rider (@chrisrider),\nGavin Simpson (@gavinsimpson),\nKathleen Sprouffske (@sprouffske)\nRichard Torkar (@torkar),\nMichael S. Truong (@emstruong),\nBrenton M. Wiernik (@bwiernik),\nStephen Wild (@sjwild),\nDonald R. Williams (@donaldRwilliams), and\nCarson Zhang (@cxzhang4).\n\nScience is better when we work together.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license-and-citation",
    "href": "index.html#license-and-citation",
    "title": "Statistical rethinking 2 with brms and the tidyverse",
    "section": "License and citation",
    "text": "License and citation\nThis book is licensed under the Creative Commons Zero v1.0 Universal license. You can learn the details, here. In short, you can use my work. Just make sure you give me the appropriate credit the same way you would for any other scholarly resource. Here’s the citation information:\n\n@book{kurzStatisticalRethinkingSecondEd2026,\n  title = {Statistical rethinking 2 with brms and the tidyverse},\n  author = {Kurz, A. Solomon},\n  year = {2026},\n  month = {jan},\n  edition = {version 0.5.0},\n  url = {https://solomon.quarto.pub/sr2/}\n}",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#comments",
    "href": "index.html#comments",
    "title": "Statistical rethinking 2 with brms and the tidyverse",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBaumer, B. S., Kaplan, D. T., & Horton, N. J. (2021). Modern data science with R (2nd edition). Taylor & Francis Group, LLC. https://mdsr-book.github.io/mdsr2e/\n\n\nBürkner, P.-C. (2017). brms: An R package for Bayesian multilevel models using Stan. Journal of Statistical Software, 80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2018). Advanced Bayesian multilevel modeling with the R package brms. The R Journal, 10(1), 395–411. https://doi.org/10.32614/RJ-2018-017\n\n\nBürkner, P.-C. (2022). brms: Bayesian regression models using ’Stan’. https://CRAN.R-project.org/package=brms\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGohel, D. (2022). flextable: Functions for tabular reporting [Manual]. https://CRAN.R-project.org/package=flextable\n\n\nGrolemund, G., & Wickham, H. (2017). R for data science. O’Reilly. https://r4ds.had.co.nz\n\n\nHealy, K. (2018). Data visualization: A practical introduction. Princeton University Press. https://socviz.co/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nNavarro, D. (2019). Learning statistics with R. https://learningstatisticswithr.com\n\n\nPeng, R. D. (2022). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/\n\n\nR Core Team. (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nRoback, P., & Legler, J. (2021). Beyond multiple linear regression: Applied generalized linear models and multilevel models in R. CRC Press. https://bookdown.org/roback/bookdown-BeyondMLR/\n\n\nRobinson, D., Hayes, A., & Couch, S. (2022). broom: Convert statistical objects into tidy tibbles [Manual]. https://CRAN.R-project.org/package=broom\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2-book.org/\n\n\nWickham, H. (2020). The tidyverse style guide. https://style.tidyverse.org/\n\n\nWickham, H. (2022). tidyverse: Easily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686\n\n\nWilke, C. O. (2019). Fundamentals of data visualization. https://clauswilke.com/dataviz/\n\n\nXie, Y., Allaire, J. J., & Grolemund, G. (2020). R markdown: The definitive guide. Chapman and Hall/CRC. https://bookdown.org/yihui/rmarkdown/",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  The Golem of Prague",
    "section": "",
    "text": "Session info\nAs he opened the chapter, McElreath told us that\nThere are a lot of great points, themes, methods, and factoids in this text. For me, one of the most powerful themes interlaced throughout the pages is how we should be skeptical of our models. Yes, learn Bayes. Pore over this book. Fit models until late into the night. But please don’t fall into blind love with their elegance and power. If we all knew what we were doing, there’d be no need for science. For more wise deflation along these lines, do check out A personal essay on Bayes factors, Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection (Navarro, 2019) and Science, statistics and the problem of “pretty good inference”, a blog, paper and talk by the inimitable Danielle Navarro.\nAnyway, McElreath left us no code or figures to translate in this chapter. But before you skip off to the next one, why not invest a little time soaking in this chapter’s material by way of a lecture by McElreath, himself? He’s an engaging speaker and the material in his online lectures does not entirely overlap with that in the text. Here’s the first lecture from his Winter 2019 course:\nAt the end of every chapter, I use the sessionInfo() function to help make my results more reproducible.\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] assertthat_0.2.1  digest_0.6.39     R6_2.6.1          fastmap_1.2.0     xfun_0.55         magrittr_2.0.4    glue_1.8.0       \n [8] stringr_1.6.0     knitr_1.51        htmltools_0.5.9   rmarkdown_2.30    lifecycle_1.0.5   cli_3.6.5         vctrs_0.7.0      \n[15] compiler_4.5.1    httr_1.4.7        vembedr_0.1.5     rstudioapi_0.17.1 tools_4.5.1       evaluate_1.0.5    yaml_2.3.12      \n[22] rlang_1.1.7       jsonlite_2.0.0    htmlwidgets_1.6.4 stringi_1.8.7",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "01.html#comments",
    "href": "01.html#comments",
    "title": "1  The Golem of Prague",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nNavarro, D. J. (2019). Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection. Computational Brain & Behavior, 2(1), 28–34. https://doi.org/10.1007/s42113-018-0019-z",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Golem of Prague</span>"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Small Worlds and Large Worlds",
    "section": "",
    "text": "2.1 The garden of forking data\nA while back The Oatmeal put together an infographic on Christopher Columbus. I’m no historian and cannot vouch for its accuracy, so make of it what you will.\nMcElreath described the thrust of this chapter this way:\nIndeed.\nGelman and Loken (2013) wrote a great paper of a similar name and topic. The titles from this section and Gelman and Loken’s paper have their origins in the short story by Jorge Luis Borges (1941), The garden of forking paths. You can find copies of the original short story here or here. Here’s a snip:\nThe choices we make in our data analyses proliferate and fork in this way, too.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "02.html#the-garden-of-forking-data",
    "href": "02.html#the-garden-of-forking-data",
    "title": "2  Small Worlds and Large Worlds",
    "section": "",
    "text": "In all fictional works, each time a man is confronted with several alternatives, he chooses one and eliminates the others; in the fiction of Ts’ui Pên, he chooses–simultaneously–all of them. He creates, in this way, diverse futures, diverse times which themselves also proliferate and fork.\n\n\n\n2.1.1 Counting possibilities\nThroughout this project, we’ll make extensive use packages from the tidyverse for data wrangling and plotting.\n\nlibrary(tidyverse)\n\nIf you are new to tidyverse-style syntax, possibly the oddest component is the pipe (i.e., |&gt;). I’m not going to explain the |&gt; in this project, but you might learn more about in this brief clip, starting around minute 21:25 in this talk by Wickham, or in Section 5.6.1 from Grolemund and Wickham’s (2017) R for data science. Really, all of Chapter 5 of R4DS is just great for new R and new tidyverse users. And R4DS Chapter 3 is a nice introduction to plotting with ggplot2 (Wickham, 2016; Wickham et al., 2022).\nOther than the pipe, the other big thing to be aware of is tibbles (Müller & Wickham, 2022). For our purposes, think of a tibble as a data object with two dimensions defined by rows and columns. Importantly, tibbles are just special types of data frames. So, whenever we talk about data frames, we’re usually talking about tibbles. For more on the topic, check out R4SD, Chapter 10.\nIf we’re willing to code the marbles as 0 = “white” 1 = “blue”, we can arrange the possibility data in a tibble as follows.\n\nd &lt;- tibble(p1 = 0,\n            p2 = rep(1:0, times = c(1, 3)),\n            p3 = rep(1:0, times = c(2, 2)),\n            p4 = rep(1:0, times = c(3, 1)),\n            p5 = 1)\n\nhead(d)\n\n# A tibble: 4 × 5\n     p1    p2    p3    p4    p5\n  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1     0     1     1     1     1\n2     0     0     1     1     1\n3     0     0     0     1     1\n4     0     0     0     0     1\n\n\nYou might depict the possibility data in a plot.\n\nd |&gt; \n  set_names(1:5) |&gt; \n  mutate(x = 1:4) |&gt; \n  pivot_longer(-x, names_to = \"possibility\") |&gt; \n  mutate(value = value |&gt; as.character()) |&gt; \n  \n  ggplot(aes(x = x, y = possibility, fill = value)) +\n  geom_point(shape = 21, size = 5) +\n  scale_x_discrete(NULL, breaks = NULL) +\n  scale_fill_manual(values = c(\"white\", \"navy\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nAs a quick aside, check out Suzan Baert’s blog post Data wrangling part 2: Transforming your columns into the right shape for an extensive discussion on dplyr::mutate() and tidyr::gather(). The tidyr::pivot_longer() function is an updated variant of gather(), which we’ll be making extensive use of throughout this project. If you’re new to reshaping data with pivoting, check out the vignettes here and here (Pivot Data from Wide to Long — Pivot_longer, 2020; Pivoting, 2020).\nHere’s the basic structure of the possibilities per marble draw.\n\nlibrary(flextable)\n\ntibble(draw    = 1:3,\n       marbles = 4) |&gt; \n  mutate(possibilities = marbles ^ draw) |&gt; \n  flextable()\n\ndrawmarblespossibilities14424163464\n\n\nNote our use of the flextable package (Gohel, 2022, 2023) to format the output into a nice table. We’ll get more practice with this throughout this chapter.\nIf you walk that out a little, you can structure the data required to approach Figure 2.2.\n\nd &lt;- tibble(position = c((1:4^1) / 4^0, \n                         (1:4^2) / 4^1, \n                         (1:4^3) / 4^2),\n            draw = rep(1:3, times = c(4^1, 4^2, 4^3)),\n            fill = rep(c(\"b\", \"w\"), times = c(1, 3)) |&gt; \n              rep(times = c(4^0 + 4^1 + 4^2)))\n\n# What?\nhead(d)\n\n# A tibble: 6 × 3\n  position  draw fill \n     &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;\n1     1        1 b    \n2     2        1 w    \n3     3        1 w    \n4     4        1 w    \n5     0.25     2 b    \n6     0.5      2 w    \n\n\nHere’s the initial plot.\n\nd |&gt; \n  ggplot(aes(x = position, y = draw, fill = fill)) +\n  geom_point(shape = 21, size = 3) +\n  scale_fill_manual(values  = c(\"navy\", \"white\")) +\n  scale_y_continuous(breaks = 1:3) +\n  theme(legend.position = \"none\",\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nTo my mind, the easiest way to connect the dots in the appropriate way is to make two auxiliary tibbles.\n\n# These will connect the dots from the first and second draws\nlines_1 &lt;- tibble(x    = rep(1:4, each = 4),\n                  xend = ((1:4^2) / 4),\n                  y    = 1,\n                  yend = 2)\n\n# These will connect the dots from the second and third draws\nlines_2 &lt;- tibble(x    = rep((1:4^2) / 4, each = 4),\n                  xend = (1:4^3) / (4^2),\n                  y    = 2,\n                  yend = 3)\n\nglimpse(lines_1)\n\nRows: 16\nColumns: 4\n$ x    &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4\n$ xend &lt;dbl&gt; 0.25, 0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 2.00, 2.25, 2.50, 2.75, 3.00, 3.25, 3.50, 3.75, 4.00\n$ y    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ yend &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n\nglimpse(lines_2)\n\nRows: 64\nColumns: 4\n$ x    &lt;dbl&gt; 0.25, 0.25, 0.25, 0.25, 0.50, 0.50, 0.50, 0.50, 0.75, 0.75, 0.75, 0.75, 1.00, 1.00, 1.00, 1.00, 1.25, 1.25, 1.25, 1…\n$ xend &lt;dbl&gt; 0.0625, 0.1250, 0.1875, 0.2500, 0.3125, 0.3750, 0.4375, 0.5000, 0.5625, 0.6250, 0.6875, 0.7500, 0.8125, 0.8750, 0.9…\n$ y    &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ yend &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n\n\nWe can use the lines_1 and lines_2 data in the plot with two geom_segment() functions.\n\nd |&gt; \n  ggplot(aes(x = position, y = draw)) +\n  geom_segment(data = lines_1,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               linewidth = 1/3) +\n  geom_segment(data = lines_2,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               linewidth = 1/3) +\n  geom_point(aes(fill = fill),\n             shape = 21, size = 3) +\n  scale_y_continuous(breaks = 1:3) +\n  scale_fill_manual(values  = c(\"navy\", \"white\")) +\n  theme(legend.position = \"none\",\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nWe’ve generated the values for position (i.e., the \\(x\\)-axis), in such a way that they’re all justified to the right, so to speak. But we’d like to center them. For draw == 1, we’ll need to subtract 0.5 from each. For draw == 2, we need to reduce the scale by a factor of 4 and we’ll then need to reduce the scale by another factor of 4 for draw == 3. The ifelse() function will be of use for that.\n\nd &lt;- d |&gt; \n  mutate(denominator = ifelse(draw == 1, .5,\n                              ifelse(draw == 2, .5 / 4,\n                                     .5 / 4^2))) |&gt; \n  mutate(position = position - denominator)\n\nd\n\n# A tibble: 84 × 4\n   position  draw fill  denominator\n      &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1    0.5       1 b           0.5  \n 2    1.5       1 w           0.5  \n 3    2.5       1 w           0.5  \n 4    3.5       1 w           0.5  \n 5    0.125     2 b           0.125\n 6    0.375     2 w           0.125\n 7    0.625     2 w           0.125\n 8    0.875     2 w           0.125\n 9    1.12      2 b           0.125\n10    1.38      2 w           0.125\n# ℹ 74 more rows\n\n\nWe’ll follow the same logic for the lines_1 and lines_2 data.\n\nlines_1 &lt;- lines_1 |&gt; \n  mutate(x    = x - 0.5,\n         xend = xend - 0.5 / 4^1)\n\nlines_2 &lt;- lines_2 |&gt; \n  mutate(x    = x - 0.5 / 4^1,\n         xend = xend - 0.5 / 4^2)\n\nglimpse(lines_1)\n\nRows: 16\nColumns: 4\n$ x    &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 1.5, 1.5, 1.5, 1.5, 2.5, 2.5, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5\n$ xend &lt;dbl&gt; 0.125, 0.375, 0.625, 0.875, 1.125, 1.375, 1.625, 1.875, 2.125, 2.375, 2.625, 2.875, 3.125, 3.375, 3.625, 3.875\n$ y    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n$ yend &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n\nglimpse(lines_2)\n\nRows: 64\nColumns: 4\n$ x    &lt;dbl&gt; 0.125, 0.125, 0.125, 0.125, 0.375, 0.375, 0.375, 0.375, 0.625, 0.625, 0.625, 0.625, 0.875, 0.875, 0.875, 0.875, 1.1…\n$ xend &lt;dbl&gt; 0.03125, 0.09375, 0.15625, 0.21875, 0.28125, 0.34375, 0.40625, 0.46875, 0.53125, 0.59375, 0.65625, 0.71875, 0.78125…\n$ y    &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ yend &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n\n\nNow the plot’s looking closer.\n\nd |&gt; \n  ggplot(aes(x = position, y = draw)) +\n  geom_segment(data = lines_1,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               linewidth = 1/3) +\n  geom_segment(data = lines_2,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               linewidth = 1/3) +\n  geom_point(aes(fill = fill),\n             shape = 21, size = 3) +\n  scale_y_continuous(breaks = 1:3) +\n  scale_fill_manual(values  = c(\"navy\", \"white\")) +\n  theme(legend.position = \"none\",\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nFor the final step, we’ll use coord_polar() to change the coordinate system, giving the plot a mandala-like feel.\n\nd |&gt; \n  ggplot(aes(x = position, y = draw)) +\n  geom_segment(data = lines_1,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               linewidth = 1/3) +\n  geom_segment(data = lines_2,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               linewidth = 1/3) +\n  geom_point(aes(fill = fill),\n             shape = 21, size = 4) +\n  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +\n  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +\n  scale_fill_manual(values = c(\"navy\", \"white\")) +\n  coord_polar() +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTo make our version of Figure 2.3, we’ll have to add an index to tell us which paths remain logically valid after each choice. We’ll call the index remain.\n\nlines_1 &lt;- lines_1 |&gt; \n  mutate(remain = c(rep(0:1, times = c(1, 3)),\n                    rep(0,   times = 4 * 3)))\nlines_2 &lt;- lines_2 |&gt; \n  mutate(remain = c(rep(0,   times = 4),\n                    rep(1:0, times = c(1, 3)) |&gt; rep(times = 3),\n                    rep(0,   times = 12 * 4)))\nd &lt;-\n  d |&gt; \n  mutate(remain = c(rep(1:0, times = c(1, 3)),\n                    rep(0:1, times = c(1, 3)),\n                    rep(0,   times = 4 * 4),\n                    rep(1:0, times = c(1, 3)) |&gt; rep(times = 3),\n                    rep(0,   times = 12 * 4))) \n# Finally, we plot\nd |&gt; \n  ggplot(aes(x = position, y = draw)) +\n  geom_segment(data = lines_1,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend,\n                   alpha = remain |&gt; as.character()),\n               linewidth = 1/3) +\n  geom_segment(data = lines_2,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend,\n                   alpha = remain |&gt; as.character()),\n               linewidth = 1/3) +\n  geom_point(aes(fill = fill, alpha = remain |&gt; as.character()),\n             shape = 21, size = 4) +\n  # It's the `alpha` argument that makes elements semitransparent\n  scale_x_continuous(NULL, limits = c(0, 4), breaks = NULL) +\n  scale_y_continuous(NULL, limits = c(0.75, 3), breaks = NULL) +\n  scale_alpha_manual(values = c(1/5, 1)) +\n  scale_fill_manual(values = c(\"navy\", \"white\")) +\n  coord_polar() +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nLetting “w” = a white dot and “b” = a blue dot, we might recreate the table in the middle of page 23 like so.\n\n# If we make two custom functions, here, \n# it will simplify the `mutate()` code, below\nn_blue  &lt;- function(x) sum(x == \"b\")\nn_white &lt;- function(x) sum(x == \"w\")\n\n# Make the data\nt &lt;- tibble(d1 = rep(c(\"w\", \"b\"), times = c(1, 4)),\n            d2 = rep(c(\"w\", \"b\"), times = c(2, 3)),\n            d3 = rep(c(\"w\", \"b\"), times = c(3, 2)),\n            d4 = rep(c(\"w\", \"b\"), times = c(4, 1))) |&gt; \n  rowwise() |&gt;\n  mutate(blue1 = n_blue(c_across(d1:d4)),\n         white = n_white(c_across(d1:d4))) |&gt; \n  mutate(blue2 = blue1) |&gt; \n  ungroup() |&gt; \n  mutate(product = blue1 * white * blue2)\n\n# Format the table\nt |&gt;\n  transmute(conjecture = str_c(\"[\", d1, \" \", d2, \" \", d3, \" \", d4, \"]\"),\n            `Ways to produce [b w b]` = str_c(blue1, \" * \", white, \" * \", blue2, \" = \", product)) |&gt; \n  flextable() |&gt; \n  width(j = 1:2, width = c(1, 2)) |&gt; \n  align(align = \"center\", part = \"all\")\n\nconjectureWays to produce [b w b][w w w w]0 * 4 * 0 = 0[b w w w]1 * 3 * 1 = 3[b b w w]2 * 2 * 2 = 8[b b b w]3 * 1 * 3 = 9[b b b b]4 * 0 * 4 = 0\n\n\nNote our use of the rowwise() function. The tidyverse is primarily designed for operating on columns. But when you want to perform an operation across several columns within a row, this requires a row-wise operation. The rowwise() function effectively groups a data frame such that each row becomes a group. The operations to follow, such as those within mutate(), are done separately within each row. The final ungroup() step un-groups the data frame, and all subsequent operations are performed at the column level, as usual. For more on this kind of workflow, see the Row-wise operations vignette (Row-Wise Operations, 2026).\nWe’ll need new data for Figure 2.4. Here’s the initial primary data, d.\n\n# Start simple\nd &lt;- tibble(position = c((1:4^1) / 4^0, \n                         (1:4^2) / 4^1, \n                         (1:4^3) / 4^2),\n            draw = rep(1:3, times = c(4^1, 4^2, 4^3)))\n\n# Expand\nd &lt;- d |&gt; \n  bind_rows(d, d) |&gt; \n  # Here are the fill colors\n  mutate(fill = c(rep(c(\"w\", \"b\"), times = c(1, 3)) |&gt; rep(times = c(4^0 + 4^1 + 4^2)),\n                  rep(c(\"w\", \"b\"), each  = 2)       |&gt; rep(times = c(4^0 + 4^1 + 4^2)),\n                  rep(c(\"w\", \"b\"), times = c(3, 1)) |&gt; rep(times = c(4^0 + 4^1 + 4^2)))) |&gt; \n  # Now we need to shift the positions over in accordance with draw, like before\n  mutate(denominator = ifelse(draw == 1, 0.5,\n                              ifelse(draw == 2, 0.5 / 4,\n                                     0.5 / 4^2))) |&gt; \n  mutate(position = position - denominator) |&gt; \n  # Here we'll add an index for which pie wedge we're working with\n  mutate(pie_index = rep(letters[1:3], each = n() / 3)) |&gt; \n  # To get the position axis correct for pie_index == \"b\" or \"c\", we'll need to offset\n  mutate(position = ifelse(pie_index == \"a\", position,\n                           ifelse(pie_index == \"b\", position + 4,\n                                  position + 4 * 2)))\n\nglimpse(d)\n\nRows: 252\nColumns: 5\n$ position    &lt;dbl&gt; 0.50000, 1.50000, 2.50000, 3.50000, 0.12500, 0.37500, 0.62500, 0.87500, 1.12500, 1.37500, 1.62500, 1.87500, …\n$ draw        &lt;int&gt; 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …\n$ fill        &lt;chr&gt; \"w\", \"b\", \"b\", \"b\", \"w\", \"b\", \"b\", \"b\", \"w\", \"b\", \"b\", \"b\", \"w\", \"b\", \"b\", \"b\", \"w\", \"b\", \"b\", \"b\", \"w\", \"b\"…\n$ denominator &lt;dbl&gt; 0.50000, 0.50000, 0.50000, 0.50000, 0.12500, 0.12500, 0.12500, 0.12500, 0.12500, 0.12500, 0.12500, 0.12500, …\n$ pie_index   &lt;chr&gt; \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\"…\n\n\nBoth lines_1 and lines_2 require adjustments for x and xend. Our current approach is a nested ifelse(). Rather than copy and paste that multi-line ifelse() code for all four, let’s wrap it in a compact function, which we’ll call move_over().\n\nmove_over &lt;- function(position, index) {\n  ifelse(\n    index == \"a\", position,\n    ifelse(index == \"b\", position + 4, position + 4 * 2)\n  )\n}\n\nIf you’re new to making your own R functions, check out Chapter 19 of R4DS or Chapter 14 of R programming for data science (Peng, 2022).\nAnyway, now we’ll make our new lines_1 and lines_2 data, for which we’ll use move_over() to adjust their x and xend positions to the correct spots.\n\nlines_1 &lt;- tibble(x    = rep(1:4, each = 4) |&gt; rep(times = 3),\n                  xend = ((1:4^2) / 4)      |&gt; rep(times = 3),\n                  y    = 1,\n                  yend = 2) |&gt; \n  mutate(x    = x - 0.5,\n         xend = xend - 0.5 / 4^1) |&gt; \n  # Here we'll add an index for which pie wedge we're working with\n  mutate(pie_index = rep(letters[1:3], each = n() / 3)) |&gt; \n  # To get the position axis correct for `pie_index == \"b\"` or `\"c\"`, we'll need to offset\n  mutate(x    = move_over(position = x,    index = pie_index),\n         xend = move_over(position = xend, index = pie_index))\n\nlines_2 &lt;- tibble(x    = rep((1:4^2) / 4, each = 4) |&gt; rep(times = 3),\n                  xend = (1:4^3 / 4^2)              |&gt; rep(times = 3),\n                  y    = 2,\n                  yend = 3) |&gt; \n  mutate(x    = x - 0.5 / 4^1,\n         xend = xend - 0.5 / 4^2) |&gt; \n  # Here we'll add an index for which pie wedge we're working with\n  mutate(pie_index = rep(letters[1:3], each = n() / 3)) |&gt; \n  # To get the position axis correct for `pie_index == \"b\"` or `\"c\"`, we'll need to offset\n  mutate(x    = move_over(position = x,    index = pie_index),\n         xend = move_over(position = xend, index = pie_index))\n\nglimpse(lines_1)\n\nRows: 48\nColumns: 5\n$ x         &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 1.5, 1.5, 1.5, 1.5, 2.5, 2.5, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5, 4.5, 4.5, 4.5, 4.5, 5.5, 5.5, …\n$ xend      &lt;dbl&gt; 0.125, 0.375, 0.625, 0.875, 1.125, 1.375, 1.625, 1.875, 2.125, 2.375, 2.625, 2.875, 3.125, 3.375, 3.625, 3.875…\n$ y         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ yend      &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ pie_index &lt;chr&gt; \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\", \"b\", …\n\nglimpse(lines_2)\n\nRows: 192\nColumns: 5\n$ x         &lt;dbl&gt; 0.125, 0.125, 0.125, 0.125, 0.375, 0.375, 0.375, 0.375, 0.625, 0.625, 0.625, 0.625, 0.875, 0.875, 0.875, 0.875…\n$ xend      &lt;dbl&gt; 0.03125, 0.09375, 0.15625, 0.21875, 0.28125, 0.34375, 0.40625, 0.46875, 0.53125, 0.59375, 0.65625, 0.71875, 0.…\n$ y         &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ yend      &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,…\n$ pie_index &lt;chr&gt; \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", \"a\", …\n\n\nFor the last data wrangling step, we add the remain indices to help us determine which parts to make semitransparent. I’m not sure of a slick way to do this, so these are the result of brute force counting.\n\nd &lt;- d |&gt; \n  mutate(remain = c(\n    # pie_index == \"a\"\n    rep(0:1, times = c(1, 3)),\n    rep(0,   times = 4),\n    rep(1:0, times = c(1, 3)) |&gt; rep(times = 3),\n    rep(0,   times = 4 * 4),\n    rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) |&gt; rep(times = 3),\n    # pie_index == \"b\"\n    rep(0:1, each = 2),\n    rep(0,   times = 4 * 2),\n    rep(1:0, each = 2) |&gt; rep(times = 2),\n    rep(0,   times = 4 * 4 * 2),\n    rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) |&gt; rep(times = 2),\n    # pie_index == \"c\",\n    rep(0:1, times = c(3, 1)),\n    rep(0,   times = 4 * 3),\n    rep(1:0, times = c(3, 1)), \n    rep(0,   times = 4 * 4 * 3),\n    rep(0:1, times = c(3, 1)) |&gt; rep(times = 3),\n    rep(0,   times = 4)\n  )\n  )\n\nlines_1 &lt;- lines_1 |&gt; \n  mutate(remain = c(rep(0,   times = 4),\n                    rep(1:0, times = c(1, 3)) |&gt; rep(times = 3),\n                    rep(0,   times = 4 * 2),\n                    rep(1:0, each  = 2) |&gt; rep(times = 2),\n                    rep(0,   times = 4 * 3),\n                    rep(1:0, times = c(3, 1))\n  )\n  )\n\nlines_2 &lt;- lines_2 |&gt; \n  mutate(remain = c(rep(0,   times = 4 * 4),\n                    rep(c(0, 1, 0), times = c(1, 3, 4 * 3)) |&gt; rep(times = 3),\n                    rep(0,   times = 4 * 8),\n                    rep(c(0, 1, 0, 1, 0), times = c(2, 2, 2, 2, 8)) |&gt; rep(times = 2),\n                    rep(0,   times = 4 * 4 * 3),\n                    rep(0:1, times = c(3, 1)) |&gt; rep(times = 3),\n                    rep(0,   times = 4)\n  )\n  )\n\nWe’re finally ready to plot our Figure 2.4.\n\nd |&gt; \n  ggplot(aes(x = position, y = draw)) +\n  geom_vline(xintercept = c(0, 4, 8), color = \"white\", linewidth = 2/3) +\n  geom_segment(data = lines_1,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend,\n                   alpha = remain |&gt; as.character()),\n               linewidth = 1/3) +\n  geom_segment(data = lines_2,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend,\n                   alpha = remain |&gt; as.character()),\n               linewidth = 1/3) +\n  geom_point(aes(fill = fill, size = draw, alpha = remain |&gt; as.character()),\n             shape = 21) +\n  scale_x_continuous(NULL, limits = c(0, 12), breaks = NULL) +\n  scale_y_continuous(NULL, limits = c(0.75, 3.5), breaks = NULL) +\n  scale_alpha_manual(values = c(0.2, 1)) +\n  scale_fill_manual(values = c(\"navy\", \"white\")) +\n  scale_size_continuous(range = c(3, 1.5)) +\n  coord_polar() +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n2.1.2 Combining other information\n\nWe may have additional information about the relative plausibility of each conjecture. This information could arise from knowledge of how the contents of the bag were generated. It could also arise from previous data. Whatever the source, it would help to have a way to combine different sources of information to update the plausibilities. Luckily there is a natural solution: Just multiply the counts. (p. 25)\n\nHere’s how to make a version of the table in the middle of page 25.\n\n# Update `t`\nt &lt;- t |&gt; \n  mutate(nc = blue1 * product)\n\n# Format the table\nt |&gt;\n  transmute(Conjecture            = str_c(\"[\", d1, \" \", d2, \" \", d3, \" \", d4, \"]\"),\n            `Ways to produce [b]` = blue1,\n            `Prior counts`        = product,\n            `New count`           = str_c(blue1, \" * \", product, \" = \", nc)) |&gt; \n  flextable() |&gt; \n  width(width = c(1, 1, 0.8, 1)) |&gt; \n  align(align = \"center\", part = \"all\") |&gt; \n  align(j = 4, align = \"left\", part = \"all\") |&gt; \n  valign(valign = \"bottom\", part = \"header\")\n\nConjectureWays to produce [b]Prior countsNew count[w w w w]000 * 0 = 0[b w w w]131 * 3 = 3[b b w w]282 * 8 = 16[b b b w]393 * 9 = 27[b b b b]404 * 0 = 0\n\n\nWe might update to reproduce the table a the top of page 26, like this.\n\n# Update `t`\nt &lt;- t |&gt; \n  rename(pc = nc) |&gt; \n  mutate(fc = c(0, 3:0)) |&gt; \n  mutate(nc = pc * fc) \n\n# Format the table\nt |&gt; \n  transmute(Conjecture      = str_c(\"[\", d1, \" \", d2, \" \", d3, \" \", d4, \"]\"),\n            `Prior count`   = pc,\n            `Factory count` = fc,\n            `New count`     = str_c(pc, \" * \", fc, \" = \", nc)) |&gt; \n  flextable() |&gt; \n  width(width = c(1, 1, 0.8, 1)) |&gt; \n  align(align = \"center\", part = \"all\") |&gt; \n  align(j = 4, align = \"left\", part = \"all\") |&gt; \n  valign(valign = \"bottom\", part = \"header\")\n\nConjecturePrior countFactory countNew count[w w w w]000 * 0 = 0[b w w w]333 * 3 = 9[b b w w]16216 * 2 = 32[b b b w]27127 * 1 = 27[b b b b]000 * 0 = 0\n\n\nTo learn more about dplyr::select() and dplyr::rename(), check out Baert’s exhaustive blog post, Data wrangling part 1: Basic to advanced ways to select columns.\n\n2.1.2.1 Rethinking: Original ignorance\n\nWhich assumption should we use, when there is no previous information about the conjectures? The most common solution is to assign an equal number of ways that each conjecture could be correct, before seeing any data. This is sometimes known as the principle of indifference: When there is no reason to say that one conjecture is more plausible than another, weigh all of the conjectures equally. This book does not use nor endorse “ignorance” priors. As we’ll see in later chapters, the structure of the model and the scientific context always provide information that allows us to do better than ignorance. (p. 26, emphasis in the original)\n\n\n\n\n2.1.3 From counts to probability\nThe opening sentences in this subsection are important: “It is helpful to think of this strategy as adhering to a principle of honest ignorance: When we don’t know what caused the data, potential causes that may produce the data in more ways are more plausible” (p. 26, emphasis in the original).\nWe can define our updated plausibility as\n\nplausibility of  after seeing \n\\(\\propto\\)\nways  can produce \n\\(\\times\\)\nprior plausibility of .\n\nIn other words,\n\nplausibility of \\(p\\) after \\(D_\\text{new}\\) \\(\\propto\\) ways \\(p\\) can produce \\(D_\\text{new} \\times\\) prior plausibility of \\(p\\).\n\nBut since we have to standardize the results to get them into a probability metric, the full equation is\n\\[\\text{plausibility of } p \\text{ after } D_\\text{new} = \\frac{\\text{ ways } p \\text{ can produce } D_\\text{new} \\times \\text{ prior plausibility of } p}{\\text{sum of the products}}.\\]\nYou might make a version of the table in the middle of page 27 like this.\n\n# Update `t`\nt |&gt; \n  rename(ways = product) |&gt; \n  mutate(p = blue1 / 4) |&gt; \n  mutate(pl = ways / sum(ways)) |&gt; \n  transmute(`Possible composition` = str_c(\"[\", d1, \" \", d2, \" \", d3, \" \", d4, \"]\"),\n            p                      = p,\n            `Ways to produce data` = ways,\n            `Plausibility`         = pl) |&gt; \n  \n  # Format for the table\n  flextable() |&gt; \n  width(width = c(1.8, 1, 1.2, 1)) |&gt; \n  align(align = \"center\", part = \"all\") |&gt; \n  valign(valign = \"bottom\", part = \"header\") |&gt; \n  italic(j = 2, part = \"header\")\n\nPossible compositionpWays to produce dataPlausibility[w w w w]0.0000.00[b w w w]0.2530.15[b b w w]0.5080.40[b b b w]0.7590.45[b b b b]1.0000.00\n\n\nWe just computed the plausibilities, but here’s McElreath’s R code 2.1.\n\nways &lt;- c(0, 3, 8, 9, 0)\n\nways / sum(ways)\n\n[1] 0.00 0.15 0.40 0.45 0.00",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "02.html#building-a-model",
    "href": "02.html#building-a-model",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.2 Building a model",
    "text": "2.2 Building a model\nWe might save our globe-tossing data in a tibble.\n\ntoss_vector &lt;- c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")\n\n(d &lt;- tibble(toss = toss_vector))\n\n# A tibble: 9 × 1\n  toss \n  &lt;chr&gt;\n1 w    \n2 l    \n3 w    \n4 w    \n5 w    \n6 l    \n7 w    \n8 l    \n9 w    \n\n\nSee what I did there with the parentheses? If you assign a value to an object in R (e.g., dog &lt;- 1) and just hit return, nothing will immediately pop up in the console. You have to actually execute dog before R will return 1. But if you wrap the code within parentheses (e.g., (dog &lt;- 1)), R will perform the assignment and return the value as if you had executed dog.\n\n2.2.1 A data story\n\nBayesian data analysis usually means producing a story for how the data came to be. This story may be descriptive, specifying associations that can be used to predict outcomes, given observations. Or it may be causal, a theory of how some events produce other events. Typically, any story you intend to be causal may also be descriptive. But many descriptive stories are hard to interpret causally. But all data stories are complete, in the sense that they are sufficient for specifying an algorithm for simulating new data. (p. 28, emphasis in the original)\n\n\n\n2.2.2 Bayesian updating\nHere we’ll add the cumulative number of trials, n_trials, and the cumulative number of successes, n_successes (i.e., toss == \"w\"), to the data.\n\n(\n  d &lt;- d |&gt; \n  mutate(n_trials  = 1:9,\n         n_success = cumsum(toss == \"w\"))\n  )\n\n# A tibble: 9 × 3\n  toss  n_trials n_success\n  &lt;chr&gt;    &lt;int&gt;     &lt;int&gt;\n1 w            1         1\n2 l            2         1\n3 w            3         2\n4 w            4         3\n5 w            5         4\n6 l            6         4\n7 w            7         5\n8 l            8         5\n9 w            9         6\n\n\nFair warning: We don’t learn the skills for making Figure 2.5 until later in the chapter. So consider the data wrangling steps in this section as something of a preview.\n\nsequence_length &lt;- 50\n\nd &lt;- d |&gt; \n  expand_grid(p_water = seq(from = 0, to = 1, length.out = sequence_length)) |&gt;\n  group_by(p_water) |&gt; \n  # To learn more about lagging, go to:\n  # https://dplyr.tidyverse.org/reference/lead-lag.html\n  mutate(lagged_n_trials  = lag(n_trials, n = 1, default = 0),\n         lagged_n_success = lag(n_success, n = 1, default = 0)) |&gt; \n  ungroup() |&gt; \n  mutate(prior = ifelse(n_trials == 1, 0.5,\n                        dbinom(x = lagged_n_success, \n                               size = lagged_n_trials, \n                               prob = p_water)),\n         likelihood = dbinom(x = n_success, \n                             size = n_trials, \n                             prob = p_water)) |&gt; \n  # The next three lines normalize the prior and the likelihood, \n  # putting them both in a probability metric \n  group_by(n_trials) |&gt; \n  mutate(prior = prior / sum(prior),\n         likelihood = likelihood / sum(likelihood)) |&gt; \n  # For annotation\n  mutate(n = str_c(\"italic(n)==\", n_trials),\n         strip = map_chr(.x = n_trials, .f =~ paste(toss_vector[1:.x], collapse = \"\")))\n\n# Plot!\nd |&gt; \n  ggplot(aes(x = p_water)) +\n  geom_line(aes(y = prior), \n            linetype = 2) +\n  geom_text(data = d |&gt;\n              slice(1),\n            aes(y = Inf, label = n),\n            hjust = 0, parse = TRUE, vjust = 1.5) +\n  geom_line(aes(y = likelihood)) +\n  scale_x_continuous(\"proportion water\", breaks = 0:2 / 2) +\n  scale_y_continuous(\"plausibility\", breaks = NULL) +\n  facet_wrap(~ strip, scales = \"free_y\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIf it wasn’t clear in the code, the dashed curves are normalized prior densities. The solid ones are normalized likelihoods. If you don’t normalize (i.e., divide the density by the sum of the density), their respective heights don’t match up with those in the text. Furthermore, it’s the normalization that makes them directly comparable.\nTo learn more about dplyr::group_by() and its opposite dplyr::ungroup(), check out R4DS, Chapter 5. To learn about tidyr::expand_grid(), go here.\n\n2.2.2.1 Rethinking: Sample size and reliable inference\n\nIt is common to hear that there is a minimum number of observations for a useful statistical estimate. For example, there is a widespread superstition that 30 observations are needed before one can use a Gaussian distribution. Why? In non-Bayesian statistical inference, procedures are often justified by the method’s behavior at very large sample sizes, so-called asymptotic behavior. As a result, performance at small samples sizes is questionable.\nIn contrast, Bayesian estimates are valid for any sample size. This does not mean that more data isn’t helpful–it certainly is. Rather, the estimates have a clear and valid interpretation, no matter the sample size. But the price for this power is dependency upon the initial plausibilities, the prior. If the prior is a bad one, then the resulting inference will be misleading. There’s no free lunch, when it comes to learning about the world. (p. 31, emphasis in the original)\n\n\n\n\n2.2.3 Evaluate\n\nThe Bayesian model learns in a way that is demonstrably optimal, provided that it accurately describes the real, large world. This is to say that your Bayesian machine guarantees perfect inference within the small world. No other way of using the available information, beginning with the same state of information, could do better.\nDon’t get too excited about this logical virtue, however. The calculations may malfunction, so results always have to be checked. And if there are important differences between the model and reality, then there is no logical guarantee of large world performance. And even if the two worlds did match, any particular sample of data could still be misleading. (p. 31)\n\n\n2.2.3.1 Rethinking: Deflationary statistics\n\nIt may be that Bayesian inference is the best general purpose method of inference known. However, Bayesian inference is much less powerful than we’d like it to be. There is no approach to inference that provides universal guarantees. No branch of applied mathematics has unfettered access to reality, because math is not discovered, like the proton. Instead it is invented, like the shovel. (p. 32)\n\nThis stance brushes up against what is sometimes called mathematical platonism, which is a position I suspect is casually held among many scientists and laypersons, alike. For more on the topic, check out Platonism in the philosophy of mathematics (Linnebo, 2018).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "02.html#components-of-the-model",
    "href": "02.html#components-of-the-model",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.3 Components of the model",
    "text": "2.3 Components of the model\nWe can sum up the components of the model as three things:\n\na likelihood function: “the number of ways each conjecture could produce an observation,”\none or more parameters: “the accumulated number of ways each conjecture could produce the entire data,” and\na prior: “the initial plausibility of each conjectured cause of the data” (p. 32).\n\n\n2.3.1 Variables\n\nVariables are just symbols that can take on different values. In a scientific context, variables include things we wish to infer, such as proportions and rates, as well as things we might observe, the data….\nUnobserved variables are usually called parameters. (p. 32, emphasis in the original)\n\n\n\n2.3.2 Definitions\n\nOnce we have the variables listed, we then have to define each of them. In defining each, we build a model that relates the variables to one another. Remember, the goal is to count all the ways the data could arise, given the assumptions. (p. 33)\n\n\n2.3.2.1 Observed variables\n\nSo that we don’t have to literally count, we can use a mathematical function that tells us the right plausibility. In conventional statistics, a distribution function assigned to an observed variable is usually called a likelihood. (p. 33, emphasis in the original)\n\nIf you let the count of water be \\(w\\) and the count of land be \\(l\\), then the binomial likelihood for the globe-tossing data may be expressed as\n\\[\\Pr (w, l \\mid p) = \\frac{(w + l)!}{w!l!} p^w (1 - p)^l.\\]\nAs McElreath wrote, we can read that as: “The counts of ‘water’ W and ‘land’ L are distributed binomially, with probability \\(p\\) of ‘water’ on each toss. (p. 33, emphasis in the original). Given a probability of .5, we can use the dbinom() function to determine the likelihood of 6 out of 9 tosses coming out water.\n\ndbinom(x = 6, size = 9, prob = 0.5)\n\n[1] 0.1640625\n\n\nMcElreath suggested we change the values of prob. Here is a way to do so over the parameter space, \\([0, 1]\\).\n\ntibble(prob = seq(from = 0, to = 1, by = 0.01)) |&gt; \n  ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) +\n  geom_line() +\n  labs(x = \"probability\",\n       y = \"binomial likelihood\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n2.3.2.1.1 Overthinking: Names and probability distributions\n\nThe “d” in dbinom stands for density. Functions named in this way almost always have corresponding partners that begin with “r” for random samples and that begin with “p” for cumulative probabilities. See for example the help ?dbinom. (p. 34, emphasis in the original)\n\n\n\n\n2.3.2.2 Unobserved variables\n\nThe distributions we assign to the observed variables typically have their own variables. In the binomial above, there is $pv, the probability of sampling water. Since \\(p\\) is not observed, we usually call it a parameter. Even though we cannot observe \\(p\\), we still have to define it. (p. 34, emphasis in the original)\n\n\n\n2.3.2.3 Overthinking: Prior as a probability distribution\nMcElreath said that “for a uniform prior from \\(a\\) to \\(b\\), the probability of any point in the interval is \\(1 / (b - a)\\)” (p. 35). Let’s try that out. To keep things simple, we’ll hold \\(a\\) constant while varying the values for \\(b\\).\n\ntibble(a = 0,\n       b = c(1, 1.5, 2, 3, 9)) |&gt; \n  mutate(prob = 1 / (b - a))\n\n# A tibble: 5 × 3\n      a     b  prob\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0   1   1    \n2     0   1.5 0.667\n3     0   2   0.5  \n4     0   3   0.333\n5     0   9   0.111\n\n\nI like to verify things with plots.\n\ntibble(a = 0,\n       b = c(1, 1.5, 2, 3, 9)) |&gt; \n  expand_grid(parameter_space = seq(from = 0, to = 9, length.out = 500)) |&gt; \n  mutate(prob = dunif(parameter_space, a, b),\n         b    = str_c(\"italic(b)==\", b)) |&gt; \n  \n  ggplot(aes(x = parameter_space, y = prob)) +\n  geom_area() +\n  scale_x_continuous(breaks = c(0, 1:3, 9)) +\n  scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1),\n                     labels = c(\"0\", \"1/9\", \"1/3\", \"1/2\", \"2/3\", \"1\")) +\n  facet_wrap(~ b, labeller = label_parsed, ncol = 5) +\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major.x = element_blank())\n\n\n\n\n\n\n\n\nAs we’ll learn much later in the project, the \\(\\operatorname{Uniform}(0, 1)\\) distribution is special in that we can also express it as the beta distribution for which \\(\\alpha = 1 \\text{ and } \\beta = 1\\). E.g.,\n\ntibble(parameter_space = seq(from = 0, to = 1, length.out = 50)) |&gt; \n  mutate(prob = dbeta(parameter_space, 1, 1)) |&gt; \n  \n  ggplot(aes(x = parameter_space, y = prob)) +\n  geom_area() +\n  scale_y_continuous(\"density\", limits = c(0, 2)) +\n  ggtitle(expression(\"This is beta\"*(1*\", \"*1))) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n2.3.2.4 Rethinking: Datum or parameter?\n\nIt is typical to conceive of data and parameters as completely different kinds of entities. Data are measured and known; parameters are unknown and must be estimated from data. Usefully, in the Bayesian framework the distinction between a datum and a parameter is not so fundamental. (p. 35)\n\nFor more in this topic, check out McElreath’s lecture, Understanding Bayesian statistics without frequentist language.\n\n\n\n2.3.3 A model is born\nWe can now describe our observed variables, \\(w\\) and \\(l\\), with parameters within the binomial likelihood, our shorthand notation for which is\n\\[w \\sim \\operatorname{Binomial}(n, p),\\]\nwhere \\(n = w + l\\). Our binomial likelihood contains a parameter for an unobserved variable, \\(p\\). Parameters in Bayesian models are assigned priors and we can report our prior for \\(p\\) as\n\\[p \\sim \\operatorname{Uniform}(0, 1),\\]\nwhich expresses the model assumption that the entire range of possible values for \\(p\\), \\([0, 1\\), are equally plausible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "02.html#making-the-model-go",
    "href": "02.html#making-the-model-go",
    "title": "2  Small Worlds and Large Worlds",
    "section": "2.4 Making the model go",
    "text": "2.4 Making the model go\n\nFor every unique combination of data, likelihood, parameters, and prior, there is a unique posterior distribution. This distribution contains the relative plausibility of different parameter values, conditional on the data and model. The posterior distribution takes the form of the probability of the parameters, conditional on the data. (p. 36, emphasis added)\n\n\n2.4.1 Bayes’ theorem\nWe already know about our values for \\(w\\), \\(l\\), and, by logical necessity, \\(n\\). Bayes’ theorem will allow us to determine the plausibility of various values of \\(p\\), given \\(w\\) and \\(l\\), which we can express formally as \\(\\Pr(p \\mid w, l)\\). Building on some of the earlier equations on page 37, Bayes’ theorem tells us that\n\\[\\Pr(p \\mid w, l) = \\frac{\\Pr(w, l \\mid p) \\Pr(p )}{\\Pr(w, l)}.\\]\n\nAnd this is Bayes’ theorem. It says that the probability of any particular value of \\(p\\), considering the data, is equal to the product of the relative plausibility of the data, conditional on \\(p\\), and the prior plausibility of \\(p\\), divided by this thing \\(\\Pr(W, L)\\), which I’ll call the average probability of the data. (p. 37, emphasis in the original)\n\nWe can express this in words as\n\\[\\text{Posterior} = \\frac{\\text{Probability of the data} \\times \\text{Prior}}{\\text{Average probability of the data}}.\\]\nThe average probability of the data is often called the “evidence” or the “average likelihood” and we’ll get a sense of what that means as we go along. “The key lesson is that the posterior is proportional to the product of the prior and the probability of the data” (p. 37). Figure 2.6 will help us see what this means. Here are the preparatory steps for the data.\n\nsequence_length &lt;- 1e3\n\nd &lt;- tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) |&gt; \n  expand_grid(row = c(\"flat\", \"stepped\", \"Laplace\"))  |&gt; \n  arrange(row, probability) |&gt; \n  mutate(prior = ifelse(row == \"flat\", 1,\n                        ifelse(row == \"stepped\", rep(0:1, each = sequence_length / 2),\n                               exp(-abs(probability - 0.5) / 0.25) / (2 * 0.25))),\n         likelihood = dbinom(x = 6, size = 9, prob = probability)) |&gt; \n  group_by(row) |&gt; \n  mutate(posterior = prior * likelihood / sum(prior * likelihood)) |&gt; \n  pivot_longer(prior:posterior)  |&gt; \n  ungroup() |&gt; \n  mutate(name = factor(name, levels = c(\"prior\", \"likelihood\", \"posterior\")),\n         row  = factor(row, levels = c(\"flat\", \"stepped\", \"Laplace\")))\n\nTo learn more about dplyr::arrange(), check out R4DS, Chapter 5.3.\nIn order to avoid unnecessary facet labels for the rows, it was easier to just make each column of the plot separately. We can then use the elegant and powerful syntax from Thomas Lin Pedersen’s (2022) patchwork package to combine them.\n\np1 &lt;- d |&gt;\n  filter(row == \"flat\") |&gt; \n  ggplot(aes(x = probability, y = value)) +\n  geom_line() +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\np2 &lt;- d |&gt;\n  filter(row == \"stepped\") |&gt; \n  ggplot(aes(x = probability, y = value)) +\n  geom_line() +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\np3 &lt;- d |&gt;\n  filter(row == \"Laplace\") |&gt; \n  ggplot(aes(x = probability, y = value)) +\n  geom_line() +\n  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\n# Combine\nlibrary(patchwork)\np1 / p2 / p3\n\n\n\n\n\n\n\n\nI’m not sure if it’s the same McElreath used in the text, but the formula I used for the triangle-shaped prior is the Laplace distribution with a location of 0.5 and a dispersion of 0.25.\nAlso, to learn all about dplyr::filter(), check out Baert’s Data wrangling part 3: Basic and more advanced ways to filter rows.\n\n\n2.4.2 Motors\n\nVarious numerical techniques are needed to approximate the mathematics that follows from the definition of Bayes’ theorem. In this book, you’ll meet three different conditioning engines, numerical techniques for computing posterior distributions:\n\nGrid approximation\nQuadratic approximation\nMarkov chain Monte Carlo (MCMC)\n\nThere are many other engines, and new ones are being invented all the time. But the three you’ll get to know here are common and widely useful. (p. 39)\n\n⚠️ In this translation of McElreath’s text, we will get a little practice with grid approximation and the quadratic approximation. But since our aim is to practice with brms, we’ll jump rather quickly into MCMC. This will be awkward at times because it will force us to contend with technical issues in earlier problems in the text than McElreath originally did. I’ll do what I can to bridge the pedagogical gaps.\n\n\n2.4.3 Grid approximation\nContinuing on with our globe-tossing example,\n\nat any particular value of a parameter, \\(p'\\) , it’s a simple matter to compute the posterior probability: just multiply the prior probability of \\(p'\\) by the likelihood at \\(p'\\). Repeating this procedure for each value in the grid generates an approximate picture of the exact posterior distribution. This procedure is called grid approximation. (pp. 39–40, emphasis in the original)\n\nWe just employed grid approximation over the last figure. To get nice smooth lines, we computed the posterior over 1,000 evenly-spaced points on the probability space. Here we’ll prepare for Figure 2.7 with 20.\n\nd &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 20),      # Define a grid\n            prior  = 1) |&gt;                                        # Define the prior\n  mutate(likelihood = dbinom(x = 6, size = 9, prob = p_grid)) |&gt;  # Compute the likelihood at each grid point\n  mutate(unstd_posterior = likelihood * prior) |&gt;                 # Compute the product of likelihood and prior\n  mutate(posterior = unstd_posterior / sum(unstd_posterior))      # Normalize the posterior so it sums to 1\n\n# What?\nhead(d)\n\n# A tibble: 6 × 5\n  p_grid prior likelihood unstd_posterior   posterior\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n1 0          1 0               0          0          \n2 0.0526     1 0.00000152      0.00000152 0.000000799\n3 0.105      1 0.0000819       0.0000819  0.0000431  \n4 0.158      1 0.000777        0.000777   0.000409   \n5 0.211      1 0.00360         0.00360    0.00189    \n6 0.263      1 0.0112          0.0112     0.00587    \n\n\nHere’s the code for the right panel of Figure 2.7.\n\np1 &lt;- d |&gt; \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"probability of water\",\n       y = NULL) +\n  facet_wrap(~ \"20 points\") +\n  theme(panel.grid = element_blank())\n\nNow here’s the code for the left hand panel of Figure 2.7.\n\np2 &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = 5),\n             prior  = 1) |&gt;\n  mutate(likelihood = dbinom(x = 6, size = 9, prob = p_grid)) |&gt;\n  mutate(unstd_posterior = likelihood * prior) |&gt;\n  mutate(posterior = unstd_posterior / sum(unstd_posterior)) |&gt;\n  \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"probability of water\",\n       y = \"posterior probability\") +\n  facet_wrap(~ \"5 points\") +\n  theme(panel.grid = element_blank())\n\nHere we combine them and plot!\n\np2 + p1 + plot_annotation(title = \"More grid points make for smoother approximations\")\n\n\n\n\n\n\n\n\nIn his R code 2.5 box, McElreath encouraged us to redo those plots with the two new kinds of priors.\n\nprior &lt;- ifelse( p_grid &lt; 0.5 , 0 , 1 ) \nprior &lt;- exp( -5*abs( p_grid - 0.5 ) )\n\nHere’s a condensed way to make the four plots all at once.\n\n# Make the data\ntibble(n_points = c(5, 20)) |&gt; \n  mutate(p_grid = map(n_points, ~seq(from = 0, to = 1, length.out = .))) |&gt; \n  unnest(p_grid) |&gt; \n  expand_grid(priors = c(\"ifelse(p_grid &lt; 0.5, 0, 1)\", \"exp(-5 * abs(p_grid - 0.5))\")) |&gt; \n  mutate(prior = ifelse(priors == \"ifelse(p_grid &lt; 0.5, 0, 1)\", \n                        ifelse(p_grid &lt; 0.5, 0, 1),\n                        exp(-5 * abs(p_grid - 0.5)))) |&gt; \n  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) |&gt; \n  mutate(posterior = likelihood * prior / sum(likelihood * prior)) |&gt; \n  mutate(n_points = str_c(\"# points = \", n_points),\n         priors   = str_c(\"prior = \", priors)) |&gt; \n  \n  # Plot!\n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"probability of water\",\n       y = \"posterior probability\") +\n  theme(panel.grid = element_blank()) +\n  facet_grid(n_points ~ priors, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\n2.4.4 Quadratic approximation\n\nUnder quite general conditions, the region near the peak of the posterior distribution will be nearly Gaussian–or “normal”–in shape. This means the posterior distribution can be usefully approximated by a Gaussian distribution. A Gaussian distribution is convenient, because it can be completely described by only two numbers: the location of its center (mean) and its spread (variance).\nA Gaussian approximation is called “quadratic approximation” because the logarithm of a Gaussian distribution forms a parabola. And a parabola is a quadratic function. So this approximation essentially represents any log-posterior with a parabola. (p. 42, emphasis added)\n\nThough McElreath will use the quadratic approximation for the first half of the text, we won’t use it much past this chapter. Here, though, we’ll apply the quadratic approximation to the globe tossing data with the rethinking::quap() function.\n\nlibrary(rethinking)\n\nglobe.qa &lt;- quap(\n  data = list(w = 6, \n              l = 3),\n  alist(w ~ dbinom(w + l, p),  # Binomial likelihood \n        p ~ dunif(0, 1))       # Uniform prior \n)\n\n# Display summary of quadratic approximation \nprecis(globe.qa, digits = 3)\n\n       mean        sd      5.5%     94.5%\np 0.6666667 0.1571338 0.4155365 0.9177968\n\n\nIn preparation for Figure 2.8, here’s the model with \\(n = 18\\) and \\(n = 36\\).\n\nglobe.qa.18 &lt;- quap(\n  data = list(w = 6 * 2,       # More data with same proportion\n              l = 3 * 2),\n  alist(w ~ dbinom(w + l, p),  # Same likelihood\n        p ~ dunif(0, 1))       # Same prior\n)\n\nglobe.qa.36 &lt;- quap(\n  data = list(w = 6 * 4, \n              l = 3 * 4),\n  alist(w ~ dbinom(w + l, p),\n        p ~ dunif(0, 1))\n)\n\n# Summarize\nprecis(globe.qa.18)\n\n       mean        sd      5.5%     94.5%\np 0.6666663 0.1111104 0.4890903 0.8442422\n\nprecis(globe.qa.36)\n\n       mean         sd      5.5%     94.5%\np 0.6666665 0.07856691 0.5411014 0.7922316\n\n\nNow make Figure 2.8.\n\nn_grid &lt;- 100\n\n# Wrangle\ntibble(w = c(6, 12, 24),\n       n = c(9, 18, 36),\n       s = c(0.157, 0.111, 0.079)) |&gt; \n  expand_grid(p_grid = seq(from = 0, to = 1, length.out = n_grid)) |&gt; \n  mutate(prior = 1,\n         m     = 0.67)  |&gt;\n  mutate(likelihood = dbinom(w, size = n, prob = p_grid)) |&gt;\n  mutate(unstd_grid_posterior = likelihood * prior,\n         unstd_quad_posterior = dnorm(x = p_grid, mean = m, sd = s)) |&gt;\n  group_by(w) |&gt; \n  mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior),\n         quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior),\n         n              = str_c(\"italic(n)==\", n)) |&gt; \n  mutate(n = factor(n, levels = str_c(\"italic(n)==\", 9 * c(1, 2, 4)))) |&gt; \n  \n  # Plot\n  ggplot(aes(x = p_grid)) +\n  geom_line(aes(y = grid_posterior)) +\n  geom_line(aes(y = quad_posterior),\n            color = \"grey50\") +\n  labs(x = \"proportion water\",\n       y = \"density\") +\n  facet_wrap(~ n, scales = \"free\", labeller = label_parsed) +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\n\n\n\nThis phenomenon, where the quadratic approximation improves with the amount of data, is very common. It’s one of the reasons that so many classical statistical procedures are nervous about small samples: Those procedures use quadratic (or other) approximations that are only known to be safe with infinite data. Often, these approximations are useful with less than infinite data, obviously. But the rate of improvement as sample size increases varies greatly depending upon the details. In some models, the quadratic approximation can remain terrible even with thousands of samples. (p. 44)\n\n\n2.4.4.1 Rethinking: Maximum likelihood estimation\n\nThe quadratic approximation, either with a uniform prior or with a lot of data, is often equivalent to a maximum likelihood estimate (MLE) and its standard error. The MLE is a very common non-Bayesian parameter estimate. This correspondence between a Bayesian approximation and a common non-Bayesian estimator is both a blessing and a curse. It is a blessing, because it allows us to re-interpret a wide range of published non-Bayesian model fits in Bayesian terms. It is a curse, because maximum likelihood estimates have some curious drawbacks, and the quadratic approximation can share them. (p. 44, emphasis, in the original)\n\nTextbooks highlighting the maximum likelihood method for the generalized linear model abound. If this is new to you and you’d like to learn more, perhaps check out Roback and Legler’s (2021) Beyond multiple linear regression: Applied generalized linear models and multilevel models in R, Agresti’s (2015) Foundations of linear and generalized linear models or Dunn and Smyth’s (2018) Generalized linear models with examples in R.\n\n\n\n2.4.5 Markov chain Monte Carlo\n\nThe most popular [alternative to grid approximation and the quadratic approximation] is Markov chain Monte Carlo (MCMC), which is a family of conditioning engines capable of handling highly complex models. It is fair to say that MCMC is largely responsible for the insurgence of Bayesian data analysis that began in the 1990s. While MCMC is older than the 1990s, affordable computer power is not, so we must also thank the engineers. Much later in the book (Chapter 9), you’ll meet simple and precise examples of MCMC model fitting, aimed at helping you understand the technique. (p. 45, emphasis in the original)\n\nThe brms package uses a version of MCMC to fit Bayesian models. Since one of the main goals of this project is to highlight brms, we may as well fit a model. This seems like an appropriately named subsection to do so. First we’ll have to load the package.\n\nlibrary(brms)\n\nIf you haven’t already installed brms, you can find instructions on how to do so here.\nHere we re-fit the last model from above, the one for which \\(w = 24\\) and \\(n = 36\\).\n\nb2.1 &lt;- brm(\n  data = list(w = 24), \n  family = binomial(link = \"identity\"),\n  w | trials(36) ~ 0 + Intercept,\n  prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  seed = 2,\n  file = \"fits/b02.01\")\n\nThe model output from brms looks like so.\n\nprint(b2.1)\n\n Family: binomial \n  Links: mu = identity \nFormula: w | trials(36) ~ 0 + Intercept \n   Data: list(w = 24) (Number of observations: 1) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.66      0.07     0.51     0.80 1.00     1395     1769\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere’s a lot going on in that output, which we’ll start to clarify in Chapter 4. For now, focus on the ‘Intercept’ line. As we’ll also learn in Chapter 4, the intercept of a typical regression model with no predictors is the same as its mean. In the special case of a model using the binomial likelihood, the mean is the probability of a 1 in a given trial, \\(\\theta\\).\nAlso, with brms, there are many ways to summarize the results of a model. The brms::posterior_summary() function is an analogue to rethinking::precis(). We will, however, need to use round() to reduce the output to a reasonable number of decimal places.\n\nposterior_summary(b2.1) |&gt; \n  round(digits = 2)\n\n            Estimate Est.Error  Q2.5 Q97.5\nb_Intercept     0.66      0.07  0.51  0.80\nlprior          0.00      0.00  0.00  0.00\nlp__           -3.94      0.67 -5.91 -3.46\n\n\nThe b_Intercept row is the probability. Don’t worry about the remaining lines, for now. We’ll cover the details of brms model fitting in later chapters. To finish up, why not plot the results of our model and compare them with those from rethinking::quap(), above?\n\nas_draws_df(b2.1) |&gt; \n  ggplot(aes(x = b_Intercept)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion water\", limits = 0:1) +\n  facet_wrap(~ \"italic(n)==36\", labeller = label_parsed) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIf you’re still confused, cool. This is just a preview. We’ll start walking through fitting models with brms in Chapter 4 and we’ll learn a lot about regression with the binomial likelihood in Chapter 11.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "02.html#session-info",
    "href": "02.html#session-info",
    "title": "2  Small Worlds and Large Worlds",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] brms_2.23.0          Rcpp_1.1.0           rethinking_2.42      posterior_1.6.1.9000 cmdstanr_0.9.0       patchwork_1.3.2     \n [7] flextable_0.9.10     lubridate_1.9.4      forcats_1.0.1        stringr_1.6.0        dplyr_1.1.4          purrr_1.2.1         \n[13] readr_2.1.5          tidyr_1.3.2          tibble_3.3.1         ggplot2_4.0.1        tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7             magrittr_2.0.4         \n [6] multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          loo_2.9.0.9000          systemfonts_1.3.1      \n[11] vctrs_0.7.0             reshape2_1.4.5          pkgconfig_2.0.3         shape_1.4.6.1           crayon_1.5.3           \n[16] fastmap_1.2.0           backports_1.5.0         labeling_0.4.3          utf8_1.2.6              rmarkdown_2.30         \n[21] tzdb_0.5.0              ps_1.9.1                ragg_1.5.0              xfun_0.55               jsonlite_2.0.0         \n[26] uuid_1.2-1              R6_2.6.1                stringi_1.8.7           RColorBrewer_1.1-3      StanHeaders_2.36.0.9000\n[31] estimability_1.5.1      assertthat_0.2.1        rstan_2.36.0.9000       knitr_1.51              zoo_1.8-14             \n[36] bayesplot_1.15.0.9000   Matrix_1.7-3            splines_4.5.1           timechange_0.3.0        tidyselect_1.2.1       \n[41] rstudioapi_0.17.1       abind_1.4-8             yaml_2.3.12             codetools_0.2-20        curl_7.0.0             \n[46] processx_3.8.6          pkgbuild_1.4.8          plyr_1.8.9              lattice_0.22-7          withr_3.0.2            \n[51] bridgesampling_1.2-1    S7_0.2.1                askpass_1.2.1           coda_0.19-4.1           evaluate_1.0.5         \n[56] survival_3.8-3          RcppParallel_5.1.11-1   zip_2.3.3               xml2_1.4.0              pillar_1.11.1          \n[61] tensorA_0.36.2.1        checkmate_2.3.3         stats4_4.5.1            distributional_0.5.0    generics_0.1.4         \n[66] hms_1.1.4               rstantools_2.5.0.9000   scales_1.4.0            xtable_1.8-4            glue_1.8.0             \n[71] emo_0.0.0.9000          gdtools_0.4.4           emmeans_1.11.2-8        tools_4.5.1             data.table_1.17.8      \n[76] mvtnorm_1.3-3           grid_4.5.1              QuickJSR_1.8.1          nlme_3.1-168            cli_3.6.5              \n[81] textshaping_1.0.4       officer_0.7.2           fontBitstreamVera_0.1.1 Brobdingnag_1.2-9       V8_8.0.1               \n[86] gtable_0.3.6            digest_0.6.39           fontquiver_0.2.1        TH.data_1.1-4           htmlwidgets_1.6.4      \n[91] farver_2.1.2            htmltools_0.5.9         lifecycle_1.0.5         fontLiberation_0.1.0    openssl_2.3.4          \n[96] MASS_7.3-65",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "02.html#comments",
    "href": "02.html#comments",
    "title": "2  Small Worlds and Large Worlds",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nAgresti, A. (2015). Foundations of linear and generalized linear models. John Wiley & Sons. https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034\n\n\nBorges, JL. (1941). El jardin de senderos que se bifurcan. Buenos Aires: Sur. Translated by D. A. Yates (1964). In Labyrinths: Selected Stories & Other Writings (pp. 19–29). New Directions.\n\n\nDunn, P. K., & Smyth, G. K. (2018). Generalized linear models with examples in R. Springer. https://link.springer.com/book/10.1007/978-1-4419-0118-7\n\n\nGelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-Hacking” and the research hypothesis was posited ahead of time. 17. https://stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf\n\n\nGohel, D. (2022). flextable: Functions for tabular reporting [Manual]. https://CRAN.R-project.org/package=flextable\n\n\nGohel, D. (2023). Using the flextable R package. https://ardata-fr.github.io/flextable-book/\n\n\nGrolemund, G., & Wickham, H. (2017). R for data science. O’Reilly. https://r4ds.had.co.nz\n\n\nLinnebo, Ø. (2018). Platonism in the philosophy of mathematics. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philosophy (Spring 2018). Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/spr2018/entries/platonism-mathematics/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMüller, K., & Wickham, H. (2022). tibble: Simple data frames. https://CRAN.R-project.org/package=tibble\n\n\nPedersen, T. L. (2022). patchwork: The composer of plots. https://CRAN.R-project.org/package=patchwork\n\n\nPeng, R. D. (2022). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/\n\n\nPivot data from wide to long — pivot_longer. (2020). https://tidyr.tidyverse.org/reference/pivot_longer.html\n\n\nPivoting. (2020). https://tidyr.tidyverse.org/articles/pivot.html\n\n\nRoback, P., & Legler, J. (2021). Beyond multiple linear regression: Applied generalized linear models and multilevel models in R. CRC Press. https://bookdown.org/roback/bookdown-BeyondMLR/\n\n\nRow-wise operations. (2026). https://dplyr.tidyverse.org/articles/rowwise.html\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2-book.org/\n\n\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., & Dunnington, D. (2022). ggplot2: Create elegant data visualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Small Worlds and Large Worlds</span>"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Sampling the Imaginary",
    "section": "",
    "text": "3.1 Sampling from a grid-approximate posterior\nIf you would like to know the probability someone is a vampire given they test positive to the blood-based vampire test, you compute\n\\[\n\\Pr(\\text{vampire} \\mid \\text{positive}) = \\frac{\\Pr(\\text{positive} \\mid \\text{vampire})\\Pr(\\text{vampire})}{\\Pr(\\text{positive})}.\n\\]\nWe’ll do so within a tibble.\nHere’s the other way of tackling the vampire problem, this time using the frequency format.\nOnce again, here we use grid approximation to generate samples.\n# How many grid points would you like?\nn &lt;- 1000\nn_success &lt;- 6\nn_trials  &lt;- 9\n\nd &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n),\n            # Note we're still using a flat uniform prior\n            prior  = 1) |&gt; \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) |&gt; \n  mutate(posterior = (likelihood * prior) / sum(likelihood * prior))\n\nd\n\n# A tibble: 1,000 × 4\n    p_grid prior likelihood posterior\n     &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 0           1   0         0       \n 2 0.00100     1   8.43e-17  8.43e-19\n 3 0.00200     1   5.38e-15  5.38e-17\n 4 0.00300     1   6.11e-14  6.11e-16\n 5 0.00400     1   3.42e-13  3.42e-15\n 6 0.00501     1   1.30e-12  1.30e-14\n 7 0.00601     1   3.87e-12  3.88e-14\n 8 0.00701     1   9.73e-12  9.74e-14\n 9 0.00801     1   2.16e-11  2.16e-13\n10 0.00901     1   4.37e-11  4.38e-13\n# ℹ 990 more rows\nOur d data contains all the components in McElreath’s R code 3.2 block. Do note we’ve renamed his prob_p and prob_data as prior and likelihood, respectively. Now we’ll use the dplyr::slice_sample() function to sample rows from d, saving them as sample.\n# How many samples would you like?\nn_samples &lt;- 1e4\n\n# Make it reproducible\nset.seed(3)\n\nsamples &lt;- d |&gt; \n  slice_sample(n = n_samples, weight_by = posterior, replace = T)\n\nglimpse(samples)\n\nRows: 10,000\nColumns: 4\n$ p_grid     &lt;dbl&gt; 0.5645646, 0.6516517, 0.5475475, 0.5905906, 0.5955956, 0.7877878, 0.7267267, 0.4914915, 0.7507508, 0.4494494,…\n$ prior      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ likelihood &lt;dbl&gt; 0.224559942, 0.271902722, 0.209666553, 0.244608692, 0.247990921, 0.191887140, 0.252521033, 0.155693321, 0.232…\n$ posterior  &lt;dbl&gt; 2.247847e-03, 2.721749e-03, 2.098764e-03, 2.448535e-03, 2.482392e-03, 1.920792e-03, 2.527738e-03, 1.558492e-0…\nNow we can plot the left panel of Figure 3.1 with geom_point(). But before we do, we’ll need to add a variable numbering the samples.\nsamples |&gt; \n  mutate(sample_number = row_number()) |&gt; \n  \n  ggplot(aes(x = sample_number, y = p_grid)) +\n  geom_point(alpha = 1/10) +\n  scale_y_continuous(\"proportion of water (p)\", limits = c(0, 1)) +\n  xlab(\"sample number\")\nWe’ll make the density in the right panel with geom_density().\nsamples |&gt; \n  ggplot(aes(x = p_grid)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion of water (p)\", limits = c(0, 1))\nThat was based on 1e4 samples. On page 53, McElreath said the density would converge on the idealized shape if we keep increasing the number of samples. Here’s what it looks like with 1e6.\nset.seed(3)\n\nd |&gt; \n  slice_sample(n = 1e6, weight_by = posterior, replace = T) |&gt; \n  ggplot(aes(x = p_grid)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion of water (p)\", limits = c(0, 1))\nYep, that’s more ideal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  },
  {
    "objectID": "03.html#sampling-to-summarize",
    "href": "03.html#sampling-to-summarize",
    "title": "3  Sampling the Imaginary",
    "section": "3.2 Sampling to summarize",
    "text": "3.2 Sampling to summarize\n“Once your model produces a posterior distribution, the model’s work is done. But your work has just begun. It is necessary to summarize and interpret the posterior distribution. Exactly how it is summarized depends upon your purpose” (p. 53).\n\n3.2.1 Intervals of defined boundaries\nTo get the proportion of water less than some value of p_grid within the tidyverse, you might first filter() by that value and then take the sum() within summarise().\n\nd |&gt; \n  filter(p_grid &lt; 0.5) |&gt; \n  summarise(sum = sum(posterior))\n\n# A tibble: 1 × 1\n    sum\n  &lt;dbl&gt;\n1 0.172\n\n\nTo learn more about dplyr::summarise() and related functions, check out Baert’s Data wrangling part 4: Summarizing and slicing your data and Section 5.6 of R4DS (Grolemund & Wickham, 2017).\nIf what you want is a frequency based on filtering by samples, then you might use n() within summarise().\n\nsamples |&gt;\n  filter(p_grid &lt; 0.5) |&gt; \n  summarise(sum = n() / n_samples)\n\n# A tibble: 1 × 1\n    sum\n  &lt;dbl&gt;\n1 0.163\n\n\nA more explicit approach for the same computation is to follow up count() with mutate().\n\nsamples |&gt; \n  count(p_grid &lt; 0.5) |&gt; \n  mutate(probability = n / sum(n))\n\n# A tibble: 2 × 3\n  `p_grid &lt; 0.5`     n probability\n  &lt;lgl&gt;          &lt;int&gt;       &lt;dbl&gt;\n1 FALSE           8371       0.837\n2 TRUE            1629       0.163\n\n\nAn even trickier approach for the same is to insert the logical statement p_grid &lt; 0.5 within the mean() function.\n\nsamples |&gt;\n  summarise(sum = mean(p_grid &lt; 0.5))\n\n# A tibble: 1 × 1\n    sum\n  &lt;dbl&gt;\n1 0.163\n\n\nMuch like McElreath discussed in the Overthinking: Counting with sum box, this works “because R internally converts a logical expression, like samples &lt; 0.5, to a vector of TRUE and FALSE results, one for each element of samples, saying whether or not each element matches the criterion” (p. 54). When we inserted that vector of TRUE and FALSE values within the mean() function, they were then internally converted to a vector of 1’s and 0’s, the mean of which was the probability. Tricky!\nTo determine the posterior probability between 0.5 and 0.75, you can use & within filter().\n\nsamples |&gt; \n  filter(p_grid &gt; 0.5 & p_grid &lt; 0.75) |&gt; \n  summarise(sum = n() / n_samples)\n\n# A tibble: 1 × 1\n    sum\n  &lt;dbl&gt;\n1 0.606\n\n\nJust multiply that value by 100 to get a percent.\n\nsamples |&gt; \n  filter(p_grid &gt; 0.5 & p_grid &lt; 0.75) |&gt; \n  summarise(sum     = n() / n_samples,\n            percent = n() / n_samples * 100)\n\n# A tibble: 1 × 2\n    sum percent\n  &lt;dbl&gt;   &lt;dbl&gt;\n1 0.606    60.6\n\n\nAnd, of course, you can do that with our mean() trick, too.\n\nsamples |&gt;\n  summarise(percent = 100 * mean(p_grid &gt; 0.5 & p_grid &lt; 0.75))\n\n# A tibble: 1 × 1\n  percent\n    &lt;dbl&gt;\n1    60.6\n\n\n\n\n3.2.2 Intervals of defined mass\n\nIt is more common to see scientific journals reporting an interval of defined mass, usually known as a confidence interval. An interval of posterior probability, such as the ones we are working with, may instead be called a credible interval. We’re going to call it a compatibility interval instead, in order to avoid the unwarranted implications of “confidence” and “credibility.” What the interval indicates is a range of parameter values compatible with the model and data. The model and data themselves may not inspire confidence, in which case the interval will not either. (p. 54, emphasis in the original)\n\nAs a part of this block quote, McElreath linked to endnote 54, which reads: “I learned this term from Sander Greenland and his collaborators. See Amrhein et al. (2019) and Gelman and Greenland (2019)” (p. 560).\nWe’ll create the upper two panels for Figure 3.2 with geom_line(), geom_area(), some careful filtering, and a little patchwork syntax.\n\n# Upper left panel\np1 &lt;- d |&gt; \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  geom_area(data = d |&gt; filter(p_grid &lt; 0.5)) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# Upper right panel\np2 &lt;- d |&gt; \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  # note this next line is the only difference in code from the last plot\n  geom_area(data = d |&gt; filter(p_grid &lt; 0.75 & p_grid &gt; 0.5)) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# Combine\nlibrary(patchwork)\np1 + p2\n\n\n\n\n\n\n\n\nWe’ll come back for the lower two panels in a bit.\nSince we saved our p_grid samples within the well-named samples tibble, we’ll have to index with $ within quantile.\n\n(q_80 &lt;- quantile(samples$p_grid, prob = 0.8))\n\n      80% \n0.7627628 \n\n\nThat value will come in handy for the lower left panel of Figure 3.2. For an alternative approach, we could select() the samples vector, extract it from the tibble with pull(), and then pump it into quantile().\n\nsamples |&gt; \n  pull(p_grid) |&gt; \n  quantile(prob = .8)\n\n      80% \n0.7627628 \n\n\nWe might also use quantile() within summarise().\n\nsamples |&gt; \n  summarise(`80th percentile` = quantile(p_grid, p = 0.8))\n\n# A tibble: 1 × 1\n  `80th percentile`\n              &lt;dbl&gt;\n1             0.763\n\n\nHere’s the summarise() approach with two probabilities.\n\nsamples |&gt; \n  summarise(`10th percentile` = quantile(p_grid, p = 0.1),\n            `90th percentile` = quantile(p_grid, p = 0.9))\n\n# A tibble: 1 × 2\n  `10th percentile` `90th percentile`\n              &lt;dbl&gt;             &lt;dbl&gt;\n1             0.451             0.815\n\n\nThe tidyverse approach is nice in that that family of functions typically returns a data frame. But sometimes you just want your values in a numeric vector for the sake of quick indexing. In that case, base R quantile() shines.\n\n(q_10_and_90 &lt;- quantile(samples$p_grid, prob = c(0.1, 0.9)))\n\n      10%       90% \n0.4514515 0.8148148 \n\n\nNow we have our cutoff values saved as q_80 and q_10_and_90, we’re ready to make the bottom panels of Figure 3.2.\n\n# Lower left panel\np1 &lt;- d |&gt; \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  geom_area(data = d |&gt; filter(p_grid &lt; q_80)) +\n  annotate(geom = \"text\",\n           x = 0.25, y = 0.0025,\n           label = \"lower 80%\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# Lower right panel\np2 &lt;- d |&gt; \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  geom_area(data = d |&gt; filter(p_grid &gt; q_10_and_90[1] & p_grid &lt; q_10_and_90[2])) +\n  annotate(geom = \"text\",\n           x = 0.25, y = 0.0025,\n           label = \"middle 80%\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# Combine\np1 + p2\n\n\n\n\n\n\n\n\nNow we follow along with McElreath’s R code 3.11 to compute a highly skewed posterior. We’ve already defined p_grid and prior within d, above. Here we’ll reuse them and update the rest of the columns.\n\n# Here we update the `dbinom()` parameters\nn_success &lt;- 3\nn_trials  &lt;- 3\n\n# Update `d`\nd &lt;- d |&gt; \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) |&gt; \n  mutate(posterior  = (likelihood * prior) / sum(likelihood * prior))\n\n# Make the next part reproducible\nset.seed(3)\n\n# Here's our new samples tibble\n(\n  samples &lt;- d |&gt; \n    slice_sample(n = n_samples, weight_by = posterior, replace = T)\n)\n\n# A tibble: 10,000 × 4\n   p_grid prior likelihood posterior\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1  0.717     1     0.368   0.00147 \n 2  0.652     1     0.277   0.00111 \n 3  0.548     1     0.164   0.000656\n 4  1         1     1       0.00400 \n 5  0.991     1     0.973   0.00389 \n 6  0.788     1     0.489   0.00195 \n 7  0.940     1     0.830   0.00332 \n 8  0.817     1     0.545   0.00218 \n 9  0.955     1     0.871   0.00348 \n10  0.449     1     0.0908  0.000363\n# ℹ 9,990 more rows\n\n\nThe rethinking::PI() function works like a nice shorthand for quantile().\n\nquantile(samples$p_grid, prob = c(0.25, 0.75))\n\n      25%       75% \n0.7087087 0.9349349 \n\nrethinking::PI(samples$p_grid, prob = 0.5)\n\n      25%       75% \n0.7087087 0.9349349 \n\n\nNow’s a good time to introduce Matthew Kay’s (2022) tidybayes package, which offers an array of convenience functions for summarizing Bayesian models of the type we’ll be working with in this project. For all the brms-related deets, see Kay’s (2021) vignette, Extracting and visualizing tidy draws from brms models. Here we start simple.\n\nlibrary(tidybayes)\n\nmedian_qi(samples$p_grid, .width = 0.5)\n\n          y      ymin      ymax .width .point .interval\n1 0.8428428 0.7087087 0.9349349    0.5 median        qi\n\n\nThe tidybayes package contains a family of functions that make it easy to summarize a distribution with a measure of central tendency accompanied by intervals. With median_qi(), we asked for the median and quantile-based intervals–just like we’ve been doing with quantile(). Note how the .width argument within median_qi() worked the same way the prob argument did within rethinking::PI(). With .width = .5, we indicated we wanted a quantile-based 50% interval, which was returned in the ymin and ymax columns. The tidybayes framework makes it easy to request multiple types of intervals. E.g., here we’ll request 50%, 80%, and 99% intervals.\n\nmedian_qi(samples$p_grid, .width = c(0.5, 0.8, 0.99))\n\n          y      ymin      ymax .width .point .interval\n1 0.8428428 0.7087087 0.9349349   0.50 median        qi\n2 0.8428428 0.5705706 0.9749750   0.80 median        qi\n3 0.8428428 0.2562563 0.9989990   0.99 median        qi\n\n\nThe .width column in the output indexed which line presented which interval. The value in the y column remained constant across rows. That’s because that column listed the measure of central tendency, the median in this case.\nNow let’s use the rethinking::HPDI() function to return 50% highest posterior density intervals (HPDIs).\n\nrethinking::HPDI(samples$p_grid, prob = 0.5)\n\n     |0.5      0.5| \n0.8418418 0.9989990 \n\n\nThe reason I introduce tidybayes now is that the functions of the brms package only support percentile-based intervals of the type we computed with quantile() and median_qi(). But tidybayes also supports HPDIs.\n\n# Check later\nmode_hdi(samples$p_grid, .width = 0.5)\n\nThis time we used the mode as the measure of central tendency. With this family of tidybayes functions, you specify the measure of central tendency in the prefix (i.e., mean, median, or mode) and then the type of interval you’d like (i.e., qi or hdi).\nIf all you want are the intervals without the measure of central tendency or all that other technical information, tidybayes also offers the handy qi() and hdi() functions.\n\n# Check later\nqi(samples$p_grid, .width = 0.5)\nhdi(samples$p_grid, .width = 0.5)\n\nThese are nice in that they return simple numeric vectors, making them particularly useful to use as references within ggplot2 plots. Now we have that skill, we can use it to make Figure 3.3.\n\n# Check later\n\n# Lower left panel\np1 &lt;- d |&gt; \n  ggplot(aes(x = p_grid, y = posterior)) +\n  # check out our sweet `qi()` indexing\n  geom_area(data = d |&gt; \n              filter(p_grid &gt; qi(samples$p_grid, .width = 0.5)[1] & \n                       p_grid &lt; qi(samples$p_grid, .width = 0.5)[2]),\n            fill = \"grey75\") +\n  geom_line() +\n  labs(subtitle = \"50% Percentile Interval\",\n       x = \"proportion of water (p)\",\n       y = \"density\")\n\n# Lower right panel\np2 &lt;- d |&gt; \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_area(data = d |&gt;\n              filter(p_grid &gt; hdi(samples$p_grid, .width = 0.5)[1] &\n                       p_grid &lt; hdi(samples$p_grid, .width = 0.5)[2]),\n            fill = \"grey75\") +\n  geom_line() +\n  labs(subtitle = \"50% HPDI\",\n       x = \"proportion of water (p)\",\n       y = \"density\")\n\n# Combine!\np1 | p2\n\n\nSo the HPDI has some advantages over the PI. But in most cases, these two types of interval are very similar. They only look so different in this case because the posterior distribution is highly skewed. If we instead used samples from the posterior distribution for six waters in nine tosses, these intervals would be nearly identical. Try it for yourself, using different probability masses, such as prob=0.8 and prob=0.95. When the posterior is bell shaped, it hardly matters which type of interval you use. (p. 57)\n\nLet’s try it out. First we’ll update the simulation for six waters in nine tosses.\n\n# \"Six waters in nine tosses\"\nn_success &lt;- 6\nn_trials  &lt;- 9\n\nnew_d &lt;- d |&gt; \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) |&gt; \n  mutate(posterior = (likelihood * prior) / sum(posterior))\n\nset.seed(3)\nnew_samples &lt;- new_d |&gt; \n  slice_sample(n = n_samples, weight_by = posterior, replace = T)\n\nHere are the intervals by .width and type of .interval.\n\nbind_rows(mean_hdi(new_samples$p_grid, .width = c(0.8, 0.95)),\n          mean_qi(new_samples$p_grid,  .width = c(0.8, 0.95))) |&gt; \n  select(.width, .interval, ymin:ymax) |&gt; \n  arrange(.width) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n  .width .interval ymin ymax\n1   0.80       hdi 0.48 0.84\n2   0.80        qi 0.45 0.81\n3   0.95       hdi 0.37 0.90\n4   0.95        qi 0.35 0.88\n\n\nWe didn’t need that last mutate_if() line. It just made it easier to compare the ymin and ymax values. Anyway, McElreath was right. This time the differences between the HPDIs and QIs were trivial. Here’s a look at the posterior.\n\nnew_d |&gt; \n  ggplot(aes(x = p_grid)) +\n  geom_line(aes(y = posterior)) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\",\n       subtitle = \"Six waters in nine tosses made\\nfor a more symmetrical posterior\")\n\n\n\n\n\n\n\n\nBe warned:\n\nThe HPDI also has some disadvantages. HPDI is more computationally intensive than PI and suffers from greater simulation variance, which is a fancy way of saying that it is sensitive to how many samples you draw from the posterior. It is also harder to understand and many scientific audiences will not appreciate its features, while they will immediately understand a percentile interval, as ordinary non-Bayesian intervals are typically interpreted (incorrectly) as percentile intervals (pp. 57–58, emphasis in the original)\n\nFor convenience, we’ll primarily stick to the PI-based intervals in this ebook.\n\n3.2.2.1 Rethinking: What do compatibility intervals mean?\nAt the start of this section, McElreath poked a little at frequentist confidence intervals. For an introduction to confidence intervals from the perspective of a frequentist, you might check out Cumming’s (2014) The new statistics: Why and how and the works referenced therein. Though their definition isn’t the most intuitive, I usually use confidence intervals when I wear my frequentist hat.\n\n\n\n3.2.3 Point estimates\nWe’ve been calling point estimates measures of central tendency. If we arrange() our d tibble in descending order by posterior, we’ll see the corresponding p_grid value for its MAP estimate.\n\nd |&gt; \n  arrange(desc(posterior))\n\n# A tibble: 1,000 × 4\n   p_grid prior likelihood posterior\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1  1         1      1       0.00400\n 2  0.999     1      0.997   0.00398\n 3  0.998     1      0.994   0.00397\n 4  0.997     1      0.991   0.00396\n 5  0.996     1      0.988   0.00395\n 6  0.995     1      0.985   0.00394\n 7  0.994     1      0.982   0.00392\n 8  0.993     1      0.979   0.00391\n 9  0.992     1      0.976   0.00390\n10  0.991     1      0.973   0.00389\n# ℹ 990 more rows\n\n\nTo emphasize it, we can use slice() to select the top row.\n\nd |&gt; \n  arrange(desc(posterior)) |&gt; \n  slice(1)\n\n# A tibble: 1 × 4\n  p_grid prior likelihood posterior\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1      1     1          1   0.00400\n\n\nWe can get the mode with mode_hdi() or mode_qi().\n\n# Check later\n\nsamples |&gt; mode_hdi(p_grid)\nsamples |&gt; mode_qi(p_grid)\n\nThose returned a lot of output in addition to the mode. If all you want is the mode itself, you can just use tidybayes::Mode().\n\nMode(samples$p_grid)\n\n[1] 0.9995616\n\n\nMedians and means are typical measures of central tendency, too.\n\nsamples |&gt; \n  summarise(mean   = mean(p_grid),\n            median = median(p_grid))\n\n# A tibble: 1 × 2\n   mean median\n  &lt;dbl&gt;  &lt;dbl&gt;\n1 0.803  0.843\n\n\nWe can inspect the three types of point estimate in the left panel of Figure 3.4. First we’ll bundle the three point estimates together in a tibble.\n\npoint_estimates &lt;- bind_rows(\n  samples |&gt; mean_qi(p_grid),\n  samples |&gt; median_qi(p_grid),\n  samples |&gt; mode_qi(p_grid)) |&gt; \n  select(p_grid, .point) |&gt; \n  # These last two columns will help us annotate  \n  mutate(x = p_grid + c(-0.03, 0.03, -0.03),\n         y = c(0.0005, 0.0012, 0.002))\n\npoint_estimates\n\n# A tibble: 3 × 4\n  p_grid .point     x      y\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1  0.803 mean   0.773 0.0005\n2  0.843 median 0.873 0.0012\n3  1.000 mode   0.970 0.002 \n\n\nNow plot.\n\nd |&gt; \n  ggplot(aes(x = p_grid)) +\n  geom_area(aes(y = posterior),\n            fill = \"grey75\") +\n  geom_vline(xintercept = point_estimates$p_grid) +\n  geom_text(data = point_estimates,\n            aes(x = x, y = y, label = .point),\n            angle = 90) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAs it turns out “different loss functions imply different point estimates” (p. 59, emphasis in the original).\nLet \\(p\\) be the proportion of the Earth covered by water and \\(d\\) be our guess. If McElreath pays us $100 if we guess exactly right but subtracts money from the prize proportional to how far off we are, then our loss is proportional to \\(d - p\\). If we decide \\(d = .5\\), we can compute our expected loss.\n\nd |&gt; \n  summarise(`expected loss` = sum(posterior * abs(0.5 - p_grid)))\n\n# A tibble: 1 × 1\n  `expected loss`\n            &lt;dbl&gt;\n1           0.313\n\n\nWhat McElreath did with sapply(), we’ll do with purrr::map(). If you haven’t used it, map() is part of a family of similarly-named functions (e.g., map2()) from the purrr package (Henry & Wickham, 2020), which is itself part of the tidyverse. The map() family is the tidyverse alternative to the family of apply() functions from the base R framework. You can learn more about how to use the map() family here or here or here.\n\nmake_loss &lt;- function(our_d) {\n  d |&gt; \n    mutate(loss = posterior * abs(our_d - p_grid)) |&gt; \n    summarise(weighted_average_loss = sum(loss))\n}\n\n(\n  l &lt;- d |&gt; \n  select(p_grid) |&gt; \n  rename(decision = p_grid) |&gt; \n  mutate(weighted_average_loss = purrr::map(decision, make_loss)) |&gt; \n  unnest(weighted_average_loss) \n)\n\n# A tibble: 1,000 × 2\n   decision weighted_average_loss\n      &lt;dbl&gt;                 &lt;dbl&gt;\n 1  0                       0.800\n 2  0.00100                 0.799\n 3  0.00200                 0.798\n 4  0.00300                 0.797\n 5  0.00400                 0.796\n 6  0.00501                 0.795\n 7  0.00601                 0.794\n 8  0.00701                 0.793\n 9  0.00801                 0.792\n10  0.00901                 0.791\n# ℹ 990 more rows\n\n\nNow we’re ready for the right panel of Figure 3.4.\n\n# This will help us find the x and y coordinates for the minimum value\nmin_loss &lt;- l |&gt; \n  slice_min(weighted_average_loss) |&gt; \n  as.numeric()\n\n# The plot\nl |&gt;   \n  ggplot(aes(x = decision, y = weighted_average_loss)) +\n  geom_area(fill = \"grey75\") +\n  geom_vline(xintercept = min_loss[1], color = \"white\", linetype = 3) +\n  geom_hline(yintercept = min_loss[2], color = \"white\", linetype = 3) +\n  ylab(\"expected proportional loss\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWe used the slice_min() to compute, and then save the exact minimum value as min_loss[1], which is 0.8408408. Within sampling error, this is the posterior median as depicted by our samples.\n\nsamples |&gt; \n  summarise(posterior_median = median(p_grid))\n\n# A tibble: 1 × 1\n  posterior_median\n             &lt;dbl&gt;\n1            0.843\n\n\nThe quadratic loss \\((d - p)^2\\) suggests we should use the mean instead. Let’s investigate.\n\n# Amend our loss function\nmake_loss &lt;- function(our_d) {\n  d |&gt; \n    mutate(loss = posterior * (our_d - p_grid)^2) |&gt; \n    summarise(weighted_average_loss = sum(loss))\n}\n\n# Remake our `l` data\nl &lt;- d |&gt; \n  select(p_grid) |&gt; \n  rename(decision = p_grid) |&gt; \n  mutate(weighted_average_loss = purrr::map(decision, make_loss)) |&gt; \n  unnest(weighted_average_loss)\n\n# Update to the new minimum loss coordinates\nmin_loss &lt;- l |&gt; \n  slice_min(weighted_average_loss) |&gt; \n  as.numeric()\n\n# Update the plot\nl |&gt;   \n  ggplot(aes(x = decision, y = weighted_average_loss)) +\n  geom_area(fill = \"grey75\") +\n  geom_vline(xintercept = min_loss[1], color = \"white\", linetype = 3) +\n  geom_hline(yintercept = min_loss[2], color = \"white\", linetype = 3) +\n  ylab(\"expected proportional loss\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nBased on quadratic loss \\((d - p)^2\\), the exact minimum value is 0.8008008. Within sampling error, this is the posterior mean of our samples.\n\nsamples |&gt; \n  summarise(posterior_meaan = mean(p_grid))\n\n# A tibble: 1 × 1\n  posterior_meaan\n            &lt;dbl&gt;\n1           0.803\n\n\n\nUsually, research scientists don’t think about loss functions. And so any point estimate like the mean or MAP that they may report isn’t intended to support any particular decision, but rather to describe the shape of the posterior. You might argue that the decision to make is whether or not to accept an hypothesis. But the challenge then is to say what the relevant costs and benefits would be, in terms of the knowledge gained or lost. Usually it’s better to communicate as much as you can about the posterior distribution, as well as the data and the model itself, so that others can build upon your work. Premature decisions to accept or reject hypotheses can cost lives. (p. 61)\n\nIn the endnote (62) linked to the end of that quote in the text, McElreath wrote: “See Hauer (2004) for three tales from transportation safety in which testing resulted in premature incorrect decisions and a demonstrable and continuing loss of human life” (p. 561).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  },
  {
    "objectID": "03.html#sampling-to-simulate-prediction",
    "href": "03.html#sampling-to-simulate-prediction",
    "title": "3  Sampling the Imaginary",
    "section": "3.3 Sampling to simulate prediction",
    "text": "3.3 Sampling to simulate prediction\nMcElreath’s five good reasons for simulation were\n\nmodel design\nmodel checking,\nsoftware validation,\nresearch design, and\nforecasting.\n\n\n3.3.1 Dummy data\nDummy data for the globe tossing model arise from the binomial likelihood. If you let \\(w\\) be a count of water and \\(n\\) be the number of tosses, the binomial likelihood is\n\\[\\operatorname{Pr} (w \\mid n, p) = \\frac{n!}{w!(n - w)!} p^w (1 - p)^{n - w}.\\]\nLetting \\(n = 2\\), \\(p(w) = 0.7\\), and \\(w_\\text{observed} = 0 \\text{ through }2\\), the densities are:\n\ntibble(n      = 2,\n       `p(w)` = 0.7,\n       w      = 0:2) |&gt; \n  mutate(density = dbinom(w, size = n, prob = `p(w)`))\n\n# A tibble: 3 × 4\n      n `p(w)`     w density\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1     2    0.7     0    0.09\n2     2    0.7     1    0.42\n3     2    0.7     2    0.49\n\n\nIf we’re going to simulate, we should probably set our seed. Doing so makes the results reproducible.\n\nset.seed(3)\n\nrbinom(1, size = 2, prob = 0.7)\n\n[1] 2\n\n\nHere are ten reproducible draws.\n\nset.seed(3)\n\nrbinom(10, size = 2, prob = 0.7)\n\n [1] 2 1 2 2 1 1 2 2 1 1\n\n\nNow generate 100,000 (i.e., 1e5) reproducible dummy observations.\n\n# How many would you like?\nn_draws &lt;- 1e5\n\nset.seed(3)\n\nd &lt;- tibble(draws = rbinom(n_draws, size = 2, prob = 0.7))\n\nd |&gt; \n  count(draws) |&gt; \n  mutate(proportion = n / nrow(d))\n\n# A tibble: 3 × 3\n  draws     n proportion\n  &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;\n1     0  9000      0.09 \n2     1 42051      0.421\n3     2 48949      0.489\n\n\nAs McElreath mused in the text (p. 63), those simulated proportion values are very close to the analytically calculated values in our density column a few code blocks up.\nHere’s the simulation updated so \\(n = 9\\), which we plot in our version of Figure 3.5.\n\nset.seed(3)\nd &lt;- tibble(draws = rbinom(n_draws, size = 9, prob = 0.7))\n\n# The histogram\nd |&gt; \n  ggplot(aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", linewidth = 1/10) +\n  scale_x_continuous(\"dummy water count\", breaks = 0:4 * 2) +\n  ylab(\"frequency\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nMcElreath suggested we play around with different values of size and prob. With the next block of code, we’ll simulate nine conditions.\n\nn_draws &lt;- 1e5\n\nsimulate_binom &lt;- function(n, probability) {\n  set.seed(3)\n  rbinom(n_draws, size = n, prob = probability) \n}\n\nd &lt;- crossing(n           = c(3, 6, 9),\n              probability = c(0.3, 0.6, 0.9)) |&gt; \n  mutate(draws = map2(n, probability, simulate_binom)) |&gt; \n  ungroup() |&gt; \n  mutate(n           = str_c(\"n = \", n),\n         probability = str_c(\"p = \", probability)) |&gt; \n  unnest(draws)\n\nhead(d)\n\n# A tibble: 6 × 3\n  n     probability draws\n  &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;\n1 n = 3 p = 0.3         0\n2 n = 3 p = 0.3         2\n3 n = 3 p = 0.3         1\n4 n = 3 p = 0.3         0\n5 n = 3 p = 0.3         1\n6 n = 3 p = 0.3         1\n\n\nLet’s plot the simulation results.\n\nd |&gt; \n  ggplot(aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", linewidth = 1/10) +\n  scale_x_continuous(\"dummy water count\", breaks = 0:4 * 2) +\n  ylab(\"frequency\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank()) +\n  facet_grid(n ~ probability)\n\n\n\n\n\n\n\n\n\n\n3.3.2 Model checking\nIf you’re new to applied statistics, you might be surprised how often mistakes arise.\n\n3.3.2.1 Did the software work?\nLet this haunt your dreams: “There is no way to really be sure that software works correctly” (p. 64).\nIf you’d like to dive deeper into these dark waters, check out one my favorite talks from StanCon 2018, Esther Williams in the Harold Holt Memorial Swimming Pool, by the ineffable Dan Simpson. If Simpson doesn’t end up drowning you, see Gabry and Simpson’s talk at the Royal Statistical Society 2018, Visualization in Bayesian workflow, a follow-up blog called Maybe it’s time to let the old ways die; or We broke R-hat so now we have to fix it, and that blog’s associated pre-print by Vehtari et al. (2019), Rank-normalization, folding, and localization: An improved \\(\\widehat R\\) for assessing convergence of MCMC.\n\n\n3.3.2.2 Is the model adequate?\n\nThe implied predictions of the model are uncertain in two ways, and it’s important to be aware of both.\nFirst, there is observation uncertainty. For any unique value of the parameter \\(p\\), there is a unique implied pattern of observations that the model expects. These patterns of observations are the same gardens of forking data that you explored in the previous chapter. These patterns are also what you sampled in the previous section. There is uncertainty in the predicted observations, because even if you know \\(p\\) with certainty, you won’t know the next globe toss with certainty (unless \\(p = 0\\) or \\(p = 1\\)).\nSecond, there is uncertainty about \\(p\\). The posterior distribution over \\(p\\) embodies this uncertainty. And since there is uncertainty about \\(p\\), there is uncertainty about everything that depends upon \\(p\\). The uncertainty in \\(p\\) will interact with the sampling variation, when we try to assess what the model tells us about outcomes.\nWe’d like to propagate the parameter uncertainty–carry it forward–as we evaluate the implied predictions. All that is required is averaging over the posterior density for \\(p\\), while computing the predictions. For each possible value of the parameter \\(p\\), there is an implied distribution of outcomes. So if you were to compute the sampling distribution of outcomes at each value of \\(p\\), then you could average all of these prediction distributions together, using the posterior probabilities of each value of \\(p\\), to get a posterior predictive distribution. (pp. 64–65, emphasis in the original)\n\nAll this is depicted in Figure 3.6. To get ready to make our version, let’s first refresh our original grid approximation d.\n\n# How many grid points would you like?\nn &lt;- 1001\nn_success &lt;- 6\nn_trials  &lt;- 9\n\nd &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n),\n            # Note we're still using a flat uniform prior\n            prior  = 1) |&gt; \n  mutate(likelihood = dbinom(n_success, size = n_trials, prob = p_grid)) |&gt; \n  mutate(posterior = (likelihood * prior) / sum(likelihood * prior))\n\nd\n\n# A tibble: 1,001 × 4\n   p_grid prior likelihood posterior\n    &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1  0         1   0         0       \n 2  0.001     1   8.37e-17  8.37e-19\n 3  0.002     1   5.34e-15  5.34e-17\n 4  0.003     1   6.07e-14  6.07e-16\n 5  0.004     1   3.40e-13  3.40e-15\n 6  0.005     1   1.29e-12  1.29e-14\n 7  0.006     1   3.85e-12  3.85e-14\n 8  0.007     1   9.68e-12  9.68e-14\n 9  0.008     1   2.15e-11  2.15e-13\n10  0.009     1   4.34e-11  4.34e-13\n# ℹ 991 more rows\n\n\nWe can make our version of the top of Figure 3.6 with a little tricky filtering.\n\nd |&gt; \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_area(color = \"grey67\", fill = \"grey67\") +\n  geom_segment(data = d |&gt; \n                 filter(p_grid %in% c(seq(from = 0.1, to = 0.9, by = 0.1), 3 / 10)),\n               aes(xend = p_grid, yend = 0, linewidth = posterior),\n               color = \"grey33\", show.legend = F) +\n  geom_point(data = d |&gt;\n               filter(p_grid %in% c(seq(from = 0.1, to = 0.9, by = 0.1), 3 / 10))) +\n  annotate(geom = \"text\", \n           x = 0.08, y = 0.0025,\n           label = \"Posterior probability\") +\n  scale_linewidth_continuous(range = c(0, 1)) +\n  scale_x_continuous(\"probability of water\", breaks = 0:10 / 10) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nNote how we weighted the widths of the vertical lines by the posterior density.\nWe’ll need to do a bit of wrangling before we’re ready to make the plot in the middle panel of Figure 3.6.\n\nn_draws &lt;- 1e5\n\nsimulate_binom &lt;- function(probability) {\n  set.seed(3)\n  rbinom(n_draws, size = 9, prob = probability) \n}\n\nd_small &lt;-\n  tibble(probability = seq(from = 0.1, to = 0.9, by = 0.1)) |&gt; \n  mutate(draws = purrr::map(probability, simulate_binom)) |&gt; \n  unnest(draws) |&gt; \n  mutate(label = str_c(\"italic(p)==\", probability))\n\nhead(d_small)\n\n# A tibble: 6 × 3\n  probability draws label         \n        &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;         \n1         0.1     0 italic(p)==0.1\n2         0.1     2 italic(p)==0.1\n3         0.1     0 italic(p)==0.1\n4         0.1     0 italic(p)==0.1\n5         0.1     1 italic(p)==0.1\n6         0.1     1 italic(p)==0.1\n\n\nNow we’re ready to plot.\n\nd_small |&gt;\n  ggplot(aes(x = draws)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", linewidth = 1/10) +\n  scale_x_continuous(NULL, breaks = 0:3 * 3) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = \"Sampling distributions\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  facet_wrap(~ label, labeller = label_parsed, ncol = 9)  +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTo make the plot at the bottom of Figure 3.6, we’ll redefine our samples, this time including the w variable (see the R code 3.26 block in the text).\n\n# How many samples would you like?\nn_samples &lt;- 1e4\n\n# Make it reproducible\nset.seed(3)\n\nsamples &lt;- d |&gt; \n  slice_sample(n = n_samples, weight_by = posterior, replace = T) |&gt; \n  mutate(w = purrr::map_dbl(p_grid, rbinom, n = 1, size = 9))\n\nglimpse(samples)\n\nRows: 10,000\nColumns: 5\n$ p_grid     &lt;dbl&gt; 0.564, 0.651, 0.487, 0.592, 0.596, 0.787, 0.727, 0.490, 0.751, 0.449, 0.619, 0.260, 0.736, 0.646, 0.639, 0.68…\n$ prior      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ likelihood &lt;dbl&gt; 0.224085305, 0.271795022, 0.151288232, 0.245578315, 0.248256678, 0.192870804, 0.252332792, 0.154229089, 0.232…\n$ posterior  &lt;dbl&gt; 2.240853e-03, 2.717950e-03, 1.512882e-03, 2.455783e-03, 2.482567e-03, 1.928708e-03, 2.523328e-03, 1.542291e-0…\n$ w          &lt;dbl&gt; 4, 7, 3, 3, 7, 6, 8, 2, 6, 4, 5, 5, 8, 6, 4, 6, 8, 2, 6, 9, 9, 7, 4, 8, 9, 8, 6, 6, 7, 5, 3, 3, 7, 7, 7, 3, 5…\n\n\nHere’s our histogram.\n\nsamples |&gt; \n  ggplot(aes(x = w)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", linewidth = 1/10) +\n  scale_x_continuous(\"number of water samples\",\n                     breaks = 0:3 * 3) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"Posterior predictive distribution\") +\n  coord_cartesian(xlim = c(0, 9),\n                  ylim = c(0, 3000)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIn Figure 3.7, McElreath considered the longest sequence of the sample values. We’ve been using rbinom() with the size parameter set to 9 for our simulations. E.g.,\n\nrbinom(10, size = 9, prob = 0.6)\n\n [1] 7 5 6 8 7 5 6 3 3 4\n\n\nNotice this collapsed (i.e., aggregated) over the sequences within the individual sets of 9. What we need is to simulate nine individual trials many times over. For example, this\n\nrbinom(9, size = 1, prob = 0.6)\n\n[1] 0 1 1 1 0 0 0 0 0\n\n\nwould be the disaggregated version of just one of the numerals returned by rbinom() when size = 9. So let’s try simulating again with un-aggregated samples. We’ll keep adding to our samples tibble. In addition to the disaggregated draws based on the \\(p\\) values listed in p_grid, we’ll also want to add a row index for each of those p_grid values–it’ll come in handy when we plot.\n\n# Make it reproducible\nset.seed(3)\n\nsamples &lt;- samples |&gt; \n  mutate(iter  = 1:n(),\n         draws = purrr::map(p_grid, rbinom, n = 9, size = 1)) |&gt; \n  unnest(draws)\n\nglimpse(samples)\n\nRows: 90,000\nColumns: 7\n$ p_grid     &lt;dbl&gt; 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.564, 0.651, 0.651, 0.651, 0.651, 0.651, 0.651, 0.65…\n$ prior      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ likelihood &lt;dbl&gt; 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2240853, 0.2717950,…\n$ posterior  &lt;dbl&gt; 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002240853, 0.002…\n$ w          &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7…\n$ iter       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5…\n$ draws      &lt;int&gt; 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0…\n\n\nThe main action is in the draws column.\nNow we have to count the longest sequences. The base R rle() function will help with that. Consider McElreath’s sequence of tosses.\n\ntosses &lt;- c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")\n\nYou can plug that into rle().\n\nrle(tosses)\n\nRun Length Encoding\n  lengths: int [1:7] 1 1 3 1 1 1 1\n  values : chr [1:7] \"w\" \"l\" \"w\" \"l\" \"w\" \"l\" \"w\"\n\n\nFor our purposes, we’re interested in lengths. That tells us the length of each sequences of the same value. The 3 corresponds to our run of three ws. The max() function will help us confirm it’s the largest value.\n\nrle(tosses)$lengths |&gt; max()\n\n[1] 3\n\n\nNow let’s apply our method to the data and plot.\n\nsamples |&gt; \n  group_by(iter) |&gt; \n  summarise(longest_run_length = rle(draws)$lengths |&gt; max()) |&gt; \n  \n  ggplot(aes(x = longest_run_length)) +\n  geom_histogram(aes(fill = longest_run_length == 3),\n                 binwidth = 1, center = 0,\n                 color = \"grey92\", linewidth = 1/10) +\n  scale_fill_viridis_d(option = \"D\", end = 0.9) +\n  scale_x_continuous(\"longest run length\", breaks = 0:3 * 3) +\n  ylab(\"frequency\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nLet’s look at rle() again.\n\nrle(tosses)\n\nRun Length Encoding\n  lengths: int [1:7] 1 1 3 1 1 1 1\n  values : chr [1:7] \"w\" \"l\" \"w\" \"l\" \"w\" \"l\" \"w\"\n\n\nWe can use the length of the output (i.e., 7 in this example) as the numbers of switches from, in this case, “w” and “l”.\n\nrle(tosses)$lengths |&gt; length()\n\n[1] 7\n\n\nWith that new trick, we’re ready to make the right panel of Figure 3.7.\n\nsamples |&gt; \n  group_by(iter) |&gt; \n  summarise(longest_run_length = rle(draws)$lengths |&gt; length()) |&gt; \n  \n  ggplot(aes(x = longest_run_length)) +\n  geom_histogram(aes(fill = longest_run_length == 7),\n                 binwidth = 1, center = 0,\n                 color = \"grey92\", linewidth = 1/10) +\n  scale_fill_viridis_d(option = \"D\", end = .9) +\n  scale_x_continuous(\"number of switches\", breaks = 0:3 * 3) +\n  ylab(\"frequency\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  },
  {
    "objectID": "03.html#summary-lets-practice-with-brms",
    "href": "03.html#summary-lets-practice-with-brms",
    "title": "3  Sampling the Imaginary",
    "section": "3.4 Summary Let’s practice with brms",
    "text": "3.4 Summary Let’s practice with brms\nOpen brms.\n\nlibrary(brms)\n\nWith brms, we’ll fit the primary model of \\(w = 6\\) and \\(n = 9\\) much like we did in Section 2.4.5.\n\nb3.1 &lt;- brm(\n  data = list(w = 6), \n  family = binomial(link = \"identity\"),\n  w | trials(9) ~ 0 + Intercept,\n  # This is a flat prior\n  prior(beta(1, 1), class = b, lb = 0, ub = 1),\n  iter = 5000, warmup = 1000,\n  seed = 3,\n  file = \"fits/b03.01\")\n\nWe’ll learn more about the beta distribution in Chapter 12. But for now, here’s the posterior summary for b_Intercept, the probability of a “w”.\n\nposterior_summary(b3.1)[\"b_Intercept\", ] |&gt; \n  round(digits = 2)\n\n Estimate Est.Error      Q2.5     Q97.5 \n     0.64      0.14      0.34      0.88 \n\n\nAs we’ll fully cover in the next chapter, Estimate is the posterior mean, the two Q columns are the quantile-based 95% intervals, and Est.Error is the posterior standard deviation.\nMuch like the way we used the samples() function to simulate probability values, above, we can do so with the brms::fitted() function. But we will have to specify scale = \"linear\" in order to return results in the probability metric. By default, brms::fitted() will return summary information. Since we want actual simulation draws, we’ll specify summary = F.\n\nf &lt;- fitted(b3.1, \n            summary = F,\n            scale = \"linear\") |&gt; \n  data.frame() |&gt; \n  set_names(\"p\")\n\nglimpse(f)\n\nRows: 16,000\nColumns: 1\n$ p &lt;dbl&gt; 0.4634040, 0.5294094, 0.5294094, 0.5763292, 0.7893283, 0.7413062, 0.9078231, 0.8610761, 0.4912234, 0.5605412, 0.563002…\n\n\nBy default, we have a generically-named vector of 4,000 samples. We’ll explain the defaults in later chapters. For now, notice we can view these in a density.\n\nf |&gt; \n  ggplot(aes(x = p)) +\n  geom_density(fill = \"grey50\", color = \"grey50\") +\n  annotate(geom = \"text\", x = 0.08, y = 2.5,\n           label = \"Posterior probability\") +\n  scale_x_continuous(\"probability of water\",\n                     breaks = c(0, 0.5, 1),\n                     limits = 0:1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nLooks a lot like the posterior probability density at the top of Figure 3.6, doesn’t it? Much like we did with samples, we can use this distribution of probabilities to predict histograms of w counts. With those in hand, we can make an analogue to the histogram in the bottom panel of Figure 3.6.\n\n# The simulation\nset.seed(3)\n\nf &lt;- f |&gt; \n  mutate(w = rbinom(n = n(), size = n_trials, prob = p))\n\n# The plot\nf |&gt; \n  ggplot(aes(x = w)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"grey92\", linewidth = 1/10) +\n  scale_x_continuous(\"number of water samples\", breaks = 0:3 * 3) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 5000)) +\n  ggtitle(\"Posterior predictive distribution\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAs you might imagine, we can use the output from fitted() to return disaggregated batches of 0’s and 1’s, too. And we could even use those disaggregated 0’s and 1’s to examine longest run lengths and numbers of switches as in the analyses for Figure 3.7. I’ll leave those as exercises for the interested reader.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  },
  {
    "objectID": "03.html#session-info",
    "href": "03.html#session-info",
    "title": "3  Sampling the Imaginary",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] brms_2.23.0     Rcpp_1.1.0      tidybayes_3.0.7 patchwork_1.3.2 lubridate_1.9.4 forcats_1.0.1   stringr_1.6.0  \n [8] dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.8            tidyselect_1.2.1        viridisLite_0.4.2       farver_2.1.2            loo_2.9.0.9000         \n [6] S7_0.2.1                fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39          \n[11] timechange_0.3.0        estimability_1.5.1      lifecycle_1.0.5         StanHeaders_2.36.0.9000 survival_3.8-3         \n[16] processx_3.8.6          magrittr_2.0.4          posterior_1.6.1.9000    compiler_4.5.1          rlang_1.1.7            \n[21] tools_4.5.1             utf8_1.2.6              yaml_2.3.12             knitr_1.51              labeling_0.4.3         \n[26] bridgesampling_1.2-1    htmlwidgets_1.6.4       curl_7.0.0              pkgbuild_1.4.8          RColorBrewer_1.1-3     \n[31] cmdstanr_0.9.0          abind_1.4-8             multcomp_1.4-29         withr_3.0.2             stats4_4.5.1           \n[36] grid_4.5.1              inline_0.3.21           xtable_1.8-4            emmeans_1.11.2-8        scales_1.4.0           \n[41] rethinking_2.42         MASS_7.3-65             cli_3.6.5               mvtnorm_1.3-3           rmarkdown_2.30         \n[46] generics_0.1.4          RcppParallel_5.1.11-1   rstudioapi_0.17.1       tzdb_0.5.0              rstan_2.36.0.9000      \n[51] splines_4.5.1           bayesplot_1.15.0.9000   parallel_4.5.1          matrixStats_1.5.0       vctrs_0.7.0            \n[56] V8_8.0.1                Matrix_1.7-3            sandwich_3.1-1          jsonlite_2.0.0          hms_1.1.4              \n[61] arrayhelpers_1.1-0      ggdist_3.3.3            glue_1.8.0              codetools_0.2-20        ps_1.9.1               \n[66] distributional_0.5.0    stringi_1.8.7           shape_1.4.6.1           gtable_0.3.6            QuickJSR_1.8.1         \n[71] pillar_1.11.1           htmltools_0.5.9         Brobdingnag_1.2-9       R6_2.6.1                evaluate_1.0.5         \n[76] lattice_0.22-7          backports_1.5.0         rstantools_2.5.0.9000   gridExtra_2.3           coda_0.19-4.1          \n[81] nlme_3.1-168            checkmate_2.3.3         xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  },
  {
    "objectID": "03.html#comments",
    "href": "03.html#comments",
    "title": "3  Sampling the Imaginary",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Scientists rise up against statistical significance. Nature, 567(7748), 305–307. https://doi.org/10.1038/d41586-019-00857-9\n\n\nCumming, G. (2014). The new statistics: Why and how. Psychological Science, 25(1), 7–29. https://doi.org/10.1177/0956797613504966\n\n\nGelman, A., & Greenland, S. (2019). Are confidence intervals better termed “uncertainty intervals”? BMJ, l5381. https://doi.org/10.1136/bmj.l5381\n\n\nGrolemund, G., & Wickham, H. (2017). R for data science. O’Reilly. https://r4ds.had.co.nz\n\n\nHauer, E. (2004). The harm done by tests of significance. Accident Analysis & Prevention, 36(3), 495–500. https://doi.org/10.1016/S0001-4575(03)00036-8\n\n\nHenry, L., & Wickham, H. (2020). purrr: Functional programming tools. https://CRAN.R-project.org/package=purrr\n\n\nKay, M. (2021). Extracting and visualizing tidy draws from brms models. https://mjskay.github.io/tidybayes/articles/tidy-brms.html\n\n\nKay, M. (2022). tidybayes: Tidy data and ’geoms’ for Bayesian models. https://CRAN.R-project.org/package=tidybayes\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved for assessing convergence of MCMC. https://arxiv.org/abs/1903.08008?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sampling the Imaginary</span>"
    ]
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "4  Geocentric Models",
    "section": "",
    "text": "4.1 Why normal distributions are normal\nAfter laying out his soccer field coin toss shuffle premise, McElreath wrote:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "04.html#why-normal-distributions-are-normal",
    "href": "04.html#why-normal-distributions-are-normal",
    "title": "4  Geocentric Models",
    "section": "",
    "text": "It’s hard to say where any individual person will end up, but you can say with great confidence what the collection of positions will be. The distances will be distributed in approximately normal, or Gaussian, fashion. This is true even though the underlying distribution is binomial. It does this because there are so many more possible ways to realize a sequence of left-right steps that sums to zero. There are slightly fewer ways to realize a sequence that ends up one step left or right of zero, and so on, with the number of possible sequences declining in the characteristic bell curve of the normal distribution. (p. 72)\n\n\n4.1.1 Normal by addition\nHere’s a way to do the simulation necessary for the plot in the top panel of Figure 4.2.\n\nlibrary(tidyverse)\n\n# We set the seed to make the results of `runif()` reproducible.\nset.seed(4)\n\n# Make data with 100 people, 16 steps each with a starting point of `step == 0` (17 rows per person)\npos &lt;- crossing(person = 1:100,\n                step   = 0:16) |&gt; \n  # Simulate a `deviation` when `step &gt; 0`\n  mutate(deviation = map_dbl(.x = step, .f = \\(step) if_else(step == 0, 0, runif(n = 1, min = -1, max = 1)))) |&gt; \n  # After grouping by `person`, compute the cumulative sum of the deviations, then `ungroup()`\n  group_by(person) |&gt;\n  mutate(position = cumsum(deviation)) |&gt; \n  ungroup() \n\nThat map_dbl() code within the first mutate() line might look odd. Go here to learn more about iterating with purrr::map_dbl().\nWe might glimpse() at the data.\n\nglimpse(pos)\n\nRows: 1,700\nColumns: 4\n$ person    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3,…\n$ step      &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ deviation &lt;dbl&gt; 0.00000000, -0.98210841, -0.41252078, -0.44525008, 0.62714843, -0.47914446, 0.44881179, 0.81218430, 0.89808044…\n$ position  &lt;dbl&gt; 0.0000000, -0.9821084, -1.3946292, -1.8398793, -1.2127308, -1.6918753, -1.2430635, -0.4308792, 0.4672012, -0.3…\n\n\nHere’s the code to make the top panel of Figure 4.2.\n\nggplot(data = pos, \n       aes(x = step, y = position, group = person)) +\n  geom_vline(xintercept = c(4, 8, 16), linetype = 2) +\n  geom_line(aes(color = person &lt; 2, alpha  = person &lt; 2)) +\n  scale_x_continuous(\"step number\", breaks = 0:4 * 4) +\n  scale_color_manual(values = c(\"skyblue4\", \"black\")) +\n  scale_alpha_manual(values = c(1/5, 1)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nHere’s the code for the bottom three plots of Figure 4.2.\n\n# Figure 4.2.a.\np1 &lt;- pos |&gt;\n  filter(step == 4) |&gt;\n  ggplot(aes(x = position)) +\n  geom_line(stat = \"density\", color = \"dodgerblue1\") +\n  facet_wrap(~ \"4 steps\")\n\n# Figure 4.2.b.\np2 &lt;- pos |&gt;\n  filter(step == 8) |&gt;\n  ggplot(aes(x = position)) +\n  geom_density(color = \"dodgerblue2\", outline.type = \"full\") +\n  facet_wrap(~ \"8 steps\")\n\n# This is an intermediary step to get an SD value\nsd &lt;- pos |&gt;\n  filter(step == 16) |&gt;\n  summarise(sd = sd(position)) |&gt; \n  pull(sd)\n\n# Figure 4.2.c.\np3 &lt;- pos |&gt;\n  filter(step == 16) |&gt;\n  ggplot(aes(x = position)) +\n  stat_function(fun = dnorm, \n                args = list(mean = 0, sd = sd),\n                linetype = 2) +\n  geom_density(color = \"transparent\", fill = \"dodgerblue3\", alpha = 1/2) +\n  ylab(\"density\") +\n  facet_wrap(~ \"16 steps\")\n\n# Combine the ggplots\nlibrary(patchwork)\n\n(p1 | p2 | p3) & coord_cartesian(xlim = c(-6, 6))\n\n\n\n\n\n\n\n\nWhile we were at it, we explored a few ways to express densities. The main action was with the geom_line(), geom_density(), and stat_function() functions, respectively.\n\nAny process that adds together random values from the same distribution converges to a normal. But it’s not easy to grasp why addition should result in a bell curve of sums. Here’s a conceptual way to think of the process. Whatever the average value of the source distribution, each sample from it can be thought of as a fluctuation from that average value. When we begin to add these fluctuations together, they also begin to cancel one another out. A large positive fluctuation will cancel a large negative one. (p. 73)\n\n\n\n4.1.2 Normal by multiplication\nHere’s McElreath’s simple random growth rate.\n\nset.seed(4)\n\nprod(1 + runif(12, min = 0, max = 0.1))\n\n[1] 1.774719\n\n\nIn the runif() part of that code, we generated 12 random draws from the uniform distribution with bounds \\([0, 0.1]\\). Within the prod() function, we first added 1 to each of those values, and then computed their product. Consider a more explicit variant of the code.\n\nset.seed(4)\n\ntibble(a = 1,\n       b = runif(12, min = 0, max = 0.1)) |&gt; \n  mutate(c = a + b) |&gt; \n  summarise(p = prod(c))\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1  1.77\n\n\nSame result. Rather than using base R replicate() to do this many times, let’s practice with purrr::map_dbl() like before.\n\n# How many iterations?\nn_iter &lt;- 10000\n\nd &lt;- tibble(iteration = 1:n_iter) |&gt; \n  mutate(growth = map_dbl(.x = iteration, .f = function(x) {\n    set.seed(x)\n    prod(1 + runif(n = 12, min = 0, max = 0.1))\n  }))\n\nglimpse(d)\n\nRows: 10,000\nColumns: 2\n$ iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,…\n$ growth    &lt;dbl&gt; 1.769489, 1.837733, 1.715074, 1.774719, 1.797436, 2.125435, 1.615300, 1.850101, 1.446821, 1.637938, 1.448587, …\n\n\nThis time we used the iteration column in two ways. It’s first function was as a simple row index, which in this context might seem like overkill. However, it’s second function was the value we serially fed into the set.seed() function within map_dbl(). This might also be overkill for such a simple example, but it’s a nice strategy to have in your skillset for when you move on to more complicated simulations. I use it all the time.\nAs to the growth column, when we first used map_dbl() in Section 4.1.1, we defined a custom anonomous function on the fly with the syntax of .f = \\(x) some_function(x). In that syntax, the \\() portion was a shorthand for the function() function. The \\() shortand is generally preferred when defining a simple one-line function. However, it’s generally a good idea to use the more explicit syntax of function() { my() |&gt; complicated_function() } for longer multi-line functions. In our case directly above, we specified set.seed(x) in the first line of the anonomous function, and then prod(1 + runif(n = 12, min = 0, max = 0.1)) in the second line. Thus the more explicit function() syntax was preferred.1 Now we plot the results of our handiwork.\n1 See here.\nd |&gt; \n  ggplot(aes(x = growth)) +\n  geom_density(color = \"transparent\", fill = \"dodgerblue3\", alpha = 1/2) +\n  stat_function(fun = dnorm, \n                args = list(mean = mean(d$growth), sd = sd(d$growth)),\n                linetype = 2)\n\n\n\n\n\n\n\n\nThe stat_function() portion of our code accomplished what McElreath did with dens(norm.comp = TRUE).\nBefore we move on, we could have also accomplished our simulation with much simpler tidyverse-style code like this:\n\nset.seed(4)\nd &lt;- tibble(growth = map_dbl(1:10000, ~ prod(1 + runif(12, min = 0, max = 0.1))))\n\nThe reason I show the more verbose workflow was to make the simulation steps more explicit, and to give newer programers some of the skills they might want for later.\nReturning to the text: “The smaller the effect of each locus, the better this additive approximation will be” (p. 74). Let’s compare big and small. Rather tham making two free-floating vectors, big and small, like McElreath’s R code 4.4, we’ll see how to generalize our approach from map_dbl() above to map2_dbl(), all within a single data frame.\n\nd &lt;- tibble(size = c(\"big\", \"small\"),\n            max = c(0.5, 0.01)) |&gt; \n  expand_grid(iteration = 1:n_iter) |&gt;\n  mutate(growth = map2_dbl(.x = iteration, .y = max, .f = function(x, y) {\n    set.seed(x)\n    prod(1 + runif(n = 12, min = 0, max = y))\n  })) \n\nglimpse(d)\n\nRows: 20,000\nColumns: 4\n$ size      &lt;chr&gt; \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\"…\n$ max       &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, …\n$ iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,…\n$ growth    &lt;dbl&gt; 12.775203, 15.121361, 11.631403, 12.843077, 13.700677, 27.489340, 8.616126, 15.731934, 5.386494, 9.578998, 5.3…\n\n\nNow we can display the simulations in a faceted plot like so.\n\nd |&gt; \n  ggplot(aes(x = growth)) +\n  geom_density(fill = \"dodgerblue3\", linewidth = 0) +\n  facet_wrap(~ size, scales = \"free\") \n\n\n\n\n\n\n\n\nYep, the small samples were more Gaussian. “The interacting growth deviations, as long as they are sufficiently small, converge to a Gaussian distribution. In this way, the range of causal forces that tend towards Gaussian distributions extends well beyond purely additive interactions” (p. 74).\n\n\n4.1.3 Normal by log-multiplication\nSince we saved the big and small versions of the last simulations in the same data frame, we use filter() to isolate the big ones.\n\nd |&gt; \n  filter(size == \"big\") |&gt; \n  \n  ggplot(aes(x = log(growth))) +\n  geom_density(fill = \"gray33\") +\n  xlab('the log of the \"big\" growth')\n\n\n\n\n\n\n\n\n\nYet another Gaussian distribution. We get the Gaussian distribution back, because adding logs is equivalent to multiplying the original numbers. So even multiplicative interactions of large deviations can produce Gaussian distributions, once we measure the outcomes on the log scale. (p. 75)\n\n\n\n4.1.4 Using Gaussian distributions\n“The justifications for using the Gaussian distribution fall into two broad categories: (1) ontological and (2) epistemological” (p. 75). I’m a fan of the justifications to follow.\n\n4.1.4.1 Ontological justification\nThe Gaussian is\n\na widespread pattern, appearing again and again at different scales and in different domains. Measurement errors, variations in growth, and the velocities of molecules all tend towards Gaussian distributions. These processes do this because at their heart, these processes add together fluctuations. And repeatedly adding finite fluctuations results in a distribution of sums that have shed all information about the underlying process, aside from mean and spread.\n[However,] one consequence of this is that statistical models based on Gaussian distributions cannot reliably identify micro-process. (p. 75)\n\nBut like Ptolemy’s circles within circles, the Gaussian can still be useful even if it sheds information.\n\n\n4.1.4.2 Epistemological justification\n\nBy the epistemological justification, the Gaussian represents a particular state of ignorance. When all we know or are willing to say about a distribution of measures (measures are continuous values on the real number line) is their mean and variance, then the Gaussian distribution arises as the most consistent with our assumptions.\nThat is to say that the Gaussian distribution is the most natural expression of our state of ignorance, because if all we are willing to assume is that a measure has finite variance, the Gaussian distribution is the shape that can be realized in the largest number of ways and does not introduce any new assumptions. It is the least surprising and least informative assumption to make. In this way, the Gaussian is the distribution most consistent with our assumptions. Or rather, it is the most consistent with our golem’s assumptions. If you don’t think the distribution should be Gaussian, then that implies that you know something else that you should tell your golem about, something that would improve inference. (p. 75)\n\nWe’ll dive deeper into why the Gaussian is such a natural expression of ignorance in these contexts when we cover maximum entropy in Chapter 7.\n\n\n4.1.4.3 Rethinking: Heavy tails\n\nThe Gaussian distribution is common in nature and has some nice properties. But there are some risks in using it as a default data model. The extreme ends of a distribution are known as its tails. And the Gaussian distribution has some very thin tails–there is very little probability in them. Instead most of the mass in the Gaussian lies within one standard deviation of the mean. Many natural (and unnatural) processes have much heavier tails. (p. 76)\n\nYou have no idea how excited I am that we’ll be covering some of these heavy-tailed alternatives!\n\n\n4.1.4.4 Overthinking: Gaussian distribution\nIn this section McElreath gave the formula for the Gaussian probability density function. Let \\(y\\) be the criterion, \\(\\mu\\) be the mean, and \\(\\sigma\\) be the standard deviation. Then the probability density of some Gaussian value \\(y\\) is\n\\[p(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left (- \\frac{(y - \\mu)^2}{2 \\sigma^2} \\right).\\]\nMcElreath’s right. “This looks monstrous” (p. 76). Why not demystify that monster with a little R code? For simplicity, we’ll compute \\(p(y \\mid \\mu, \\sigma)\\) for a series of \\(y\\)-values ranging from -1 to 1, holding \\(\\mu = 0\\) and \\(\\sigma = 0.1\\). Then we’ll plot.\n\ntibble(y     = seq(from = -1, to = 1, by = 0.01),\n       mu    = 0,\n       sigma = 0.1) |&gt; \n  # Compute p(y) with a hand-made Gaussian probability density function\n  mutate(p = (1 / sqrt(2 * pi * sigma^2)) * exp(-((y - mu)^2 / (2 * sigma^2)))) |&gt; \n  \n  ggplot(aes(x = y, y = p)) +\n  geom_line() + \n  ylab(expression(italic(p)(italic(\"y | \")*mu==0*\",\"~sigma==0.1)))\n\n\n\n\n\n\n\n\nNotice how \\(p(y \\mid \\mu, \\sigma)\\) peaks around 4 when \\(y = 0\\). We can also get that value with the dnorm() function, which will return the \\(p(y)\\) value for a given combination of \\(y\\), \\(\\mu\\), and \\(\\sigma\\).\n\ndnorm(0, mean = 0, sd = 0.1)\n\n[1] 3.989423\n\n\n\nThe answer, about 4, is no mistake. Probability density is the rate of change in cumulative probability. So where cumulative probability is increasing rapidly, density can easily exceed 1. But if we calculate the area under the density function, it will never exceed 1. Such areas are also called probability mass. You can usually ignore these density/mass details while doing computational work. But it’s good to be aware of the distinction. (p. 76, emphasis in the original)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "04.html#a-language-for-describing-models",
    "href": "04.html#a-language-for-describing-models",
    "title": "4  Geocentric Models",
    "section": "4.2 A language for describing models",
    "text": "4.2 A language for describing models\nOur mathy ways of summarizing models will be something like\n\\[\\begin{align*}\n\\text{criterion}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  & = \\beta \\times \\text{predictor}_i \\\\\n\\beta  & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1) \\\\\nx_i    & \\sim \\operatorname{Normal}(0, 1).\n\\end{align*}\\]\n“If that doesn’t make much sense, good. That indicates that you are holding the right textbook” (p. 77). Welcome applied statistics! 🤓\n\n4.2.1 Re-describing the globe tossing model\nFor the globe tossing model, the probability \\(p\\) of a count of water \\(w\\) based on \\(n\\) trials was\n\\[\\begin{align*}\nw & \\sim \\operatorname{Binomial}(n, p) \\\\\np & \\sim \\operatorname{Uniform}(0, 1),\n\\end{align*}\\]\nwhere the top line indicates we’re using the Binomial likelihood function to model \\(w\\) given unique combinations of \\(n\\) and \\(p\\). In our example, \\(w\\) and \\(n\\) were already defined in the data, so all we need to do is compute \\(p\\). Since \\(p\\) is the only parameter, it’s the only element that gets a prior, which is what that second line was.\n\n4.2.1.1 Overthinking: From model definition to Bayes’ theorem\nWe can use grid approximation to work through our globe tossing model.\n\n# How many `p_grid` points would you like?\nn_points &lt;- 100\n\nd &lt;- tibble(p_grid = seq(from = 0, to = 1, length.out = n_points),\n            w = 6, \n            n = 9) |&gt; \n  mutate(prior      = dunif(p_grid, min = 0, max = 1),\n         likelihood = dbinom(w, size = n, prob = p_grid)) |&gt;\n  mutate(posterior = likelihood * prior / sum(likelihood * prior))\n\nhead(d)\n\n# A tibble: 6 × 6\n  p_grid     w     n prior likelihood posterior\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 0          6     9     1   0         0       \n2 0.0101     6     9     1   8.65e-11  8.74e-12\n3 0.0202     6     9     1   5.37e- 9  5.43e-10\n4 0.0303     6     9     1   5.93e- 8  5.99e- 9\n5 0.0404     6     9     1   3.23e- 7  3.26e- 8\n6 0.0505     6     9     1   1.19e- 6  1.21e- 7\n\n\nIn case you were curious, here’s what they look like.\n\nd |&gt; \n  pivot_longer(prior:posterior) |&gt; \n  # This line allows us to dictate the order in which the panels will appear\n  mutate(name = factor(name, levels = c(\"prior\", \"likelihood\", \"posterior\"))) |&gt; \n  \n  ggplot(aes(x = p_grid, y = value, fill = name)) +\n  geom_area() +\n  scale_fill_manual(values = c(\"blue\", \"red\", \"purple\")) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\nThe posterior is a combination of the prior and the likelihood. When the prior is flat across the parameter space, the posterior is just the likelihood re-expressed as a probability. As we go along, you’ll see we almost never use flat priors in practice. Be warned that eschewing flat priors is a recent development, however. You only have to look at the literature from a couple decades ago to see mounds and mounds of flat priors.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "04.html#a-gaussian-model-of-height",
    "href": "04.html#a-gaussian-model-of-height",
    "title": "4  Geocentric Models",
    "section": "4.3 A Gaussian model of height",
    "text": "4.3 A Gaussian model of height\n\nThere are an infinite number of possible Gaussian distributions. Some have small means. Others have large means. Some are wide, with a large \\(\\sigma\\). Others are narrow. We want our Bayesian machine to consider every possible distribution, each defined by a combination of \\(\\mu\\) and \\(\\sigma\\), and rank them by posterior plausibility. Posterior plausibility provides a measure of the logical compatibility of each possible distribution with the data and model. (p. 79)\n\n\n4.3.1 The data\nLet’s load the Howell (2001, 2010) data from McElreath’s (2020a) rethinking package.\n\nlibrary(rethinking)\n\nWarning: package 'cmdstanr' was built under R version 4.5.2\n\n\nWarning: package 'posterior' was built under R version 4.5.2\n\ndata(Howell1)\nd &lt;- Howell1\n\nHere we open our focal statistical package, Bürkner’s brms. But before we do, we’ll want to detach the rethinking package. R will not allow us to use a function from one package that shares the same name as a different function from another package if both packages are open at the same time. The rethinking and brms packages are designed for similar purposes and, unsurprisingly, overlap in some of their function names. To prevent problems, we will always make sure rethinking is detached before using brms. To learn more on the topic, see this R-bloggers post.\n\nrm(Howell1)\ndetach(package:rethinking, unload = T)\nlibrary(brms)\n\nGo ahead and investigate the data with str(), the tidyverse analogue for which is glimpse().\n\nd |&gt;\n  str()\n\n'data.frame':   544 obs. of  4 variables:\n $ height: num  152 140 137 157 145 ...\n $ weight: num  47.8 36.5 31.9 53 41.3 ...\n $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n\n\nThe brms package does not have a function that works like rethinking::precis() for providing numeric and graphical summaries of variables, as see in McElreath’s R code 4.9. We can get some of that information with summary().\n\nd |&gt;\n  summary()\n\n     height           weight            age             male       \n Min.   : 53.98   Min.   : 4.252   Min.   : 0.00   Min.   :0.0000  \n 1st Qu.:125.09   1st Qu.:22.008   1st Qu.:12.00   1st Qu.:0.0000  \n Median :148.59   Median :40.058   Median :27.00   Median :0.0000  \n Mean   :138.26   Mean   :35.611   Mean   :29.34   Mean   :0.4724  \n 3rd Qu.:157.48   3rd Qu.:47.209   3rd Qu.:43.00   3rd Qu.:1.0000  \n Max.   :179.07   Max.   :62.993   Max.   :88.00   Max.   :1.0000  \n\n\nWe might make the histograms like this.\n\nd |&gt;\n  pivot_longer(everything()) |&gt; \n  mutate(name = factor(name, levels = c(\"height\", \"weight\", \"age\", \"male\"))) |&gt; \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 10) +\n  facet_wrap(~ name, scales = \"free\", ncol = 1)\n\n\n\n\n\n\n\n\nIf you’re curious, McElreath made those tiny histograms with help from Wickham’s histospark() function. Here’s the code.\n\nsparks &lt;- c(\"\\u2581\", \"\\u2582\", \"\\u2583\", \"\\u2585\", \"\\u2587\")\n\nhistospark &lt;- function(x, width = 10) {\n  bins &lt;- graphics::hist(x, breaks = width, plot = FALSE)\n\n  factor &lt;- cut(\n    bins$counts / max(bins$counts),\n    breaks = seq(0, 1, length = length(sparks) + 1),\n    labels = sparks,\n    include.lowest = TRUE\n  )\n\n  paste0(factor, collapse = \"\")\n}\n\nHere’s how it works.\n\nhistospark(d$weight)\n\n[1] \"▁▂▃▂▂▂▂▅▇▇▃▂▁\"\n\n\nOne of the neat things about the histospark() function is you can insert the output right in your R Markdown prose. For example, we can use it to casually show how left skewed the our height variable is: ▁▁▁▁▁▁▁▂▁▇▇▅▁. But it’s time to get back on track. You can isolate height values with the dplyr::select() function.\n\nd |&gt;\n  select(height) |&gt; \n  glimpse()\n\nRows: 544\nColumns: 1\n$ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 147.9550, 165.1000, 154.3050, 151…\n\n\nIf you want the values in a numeric vector rather than in a data fame, try pull(d, height).\nWe can use the dplyr::filter() function to make an adults-only data frame.\n\nd2 &lt;- d |&gt;\n  filter(age &gt;= 18)\n\nOur reduced d2 does indeed have \\(n = 352\\) cases.\n\nd2 |&gt; \n  count()\n\n    n\n1 352\n\n\n\n4.3.1.1 Overthinking: Data frames and indexes\nFor more on indexing, check out Chapter 9 of Peng’s (2022) R programming for data science, or even the Subsetting subsection in R4DS.\nThis probably reflects my training history, but the structure of a data frame seems natural and inherently appealing. If you’re in the other camp, do check out either of these two data wrangling talks (here and here) by the ineffable Jenny Bryan.\n\n\n\n4.3.2 The model\nPlease heed McElreath’s warnings to\n\nbe careful about choosing the Gaussian distribution only when the plotted outcome variable looks Gaussian to you. Gawking at the raw data, to try to decide how to model them, is usually not a good idea. The data could be a mixture of different Gaussian distributions, for example, and in that case you won’t be able to detect the underlying normality just by eyeballing the outcome distribution. Furthermore, as mentioned earlier in this chapter, the empirical distribution needn’t be actually Gaussian in order to justify using a Gaussian probability distribution. (p. 81)\n\nAnyway, the likelihood for our model is\n\\[\\text{heights}_i \\sim \\operatorname{Normal}(\\mu, \\sigma),\\]\nwhere the \\(i\\) subscript indexes the individual cases in the data. Our two parameters are \\(\\mu\\) and \\(\\sigma\\), which we will estimate using Bayes’ formula. Our prior for \\(\\mu\\) will be\n\\[\\mu \\sim \\operatorname{Normal}(178, 20)\\]\nand our prior for \\(\\sigma\\) will be\n\\[\\sigma \\sim \\operatorname{Uniform}(0, 50).\\]\nHere’s the shape of the prior for \\(\\mu\\), \\(\\mathcal N(178, 20)\\).\n\np1 &lt;- tibble(x = seq(from = 100, to = 250, by = 0.1)) |&gt; \n  ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(from = 100, to = 250, by = 75)) +\n  labs(y = \"density\",\n       title = \"mu ~ dnorm(178, 20)\")\n\np1\n\n\n\n\n\n\n\n\nAnd here’s the ggplot2 code for our prior for \\(\\sigma\\), a uniform distribution with a minimum value of 0 and a maximum value of 50. We don’t really need the \\(y\\)-axis when looking at the shapes of a density, so we’ll just remove it with scale_y_continuous().\n\np2 &lt;- tibble(x = seq(from = -10, to = 60, by = 0.1)) |&gt;\n  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +\n  geom_line() +\n  scale_x_continuous(breaks = c(0, 50)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"sigma ~ dunif(0, 50)\")\n\np2\n\n\n\n\n\n\n\n\nWe can simulate from both priors at once to get a prior probability distribution of heights.\n\nn &lt;- 1e4\n\nset.seed(4)\n\nsim &lt;- tibble(sample_mu    = rnorm(n, mean = 178, sd  = 20),\n              sample_sigma = runif(n, min = 0, max = 50)) |&gt; \n  mutate(height = rnorm(n, mean = sample_mu, sd = sample_sigma))\n  \np3 &lt;- sim |&gt; \n  ggplot(aes(x = height)) +\n  geom_density(fill = \"grey33\") +\n  scale_x_continuous(breaks = c(0, 73, 178, 283)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"height ~ dnorm(mu, sigma)\") +\n  theme(panel.grid = element_blank())\n\np3\n\n\n\n\n\n\n\n\nIf you look at the \\(x\\)-axis breaks on the plot in McElreath’s lower left panel in Figure 4.3, you’ll notice they’re intentional. To compute the mean and 3 standard deviations above and below, you might do this.\n\nsim |&gt; \n  summarise(ll   = mean(height) - sd(height) * 3,\n            mean = mean(height),\n            ul   = mean(height) + sd(height) * 3) |&gt; \n  mutate_all(round, digits = 1)\n\n# A tibble: 1 × 3\n     ll  mean    ul\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  73.9  177.  281.\n\n\nOur values are very close to his, but are off by just a bit due to simulation variation.\nHere’s the work to make the lower right panel of Figure 4.3. Watch out; we’re starting to get fancy.\n\nc(mean(sim$height) - 3 * sd(sim$height), 0, mean(sim$height), mean(sim$height) + 3 * sd(sim$height))\n\n[1]  73.94624   0.00000 177.41305 280.87986\n\nm_height &lt;- mean(sim$height)\ns_height &lt;- sd(sim$height)\n\nc(m_height - 3 * s_height, 0, m_height, m_height + 3 * s_height)\n\n[1]  73.94624   0.00000 177.41305 280.87986\n\n\n\n# Simulate\nset.seed(4)\n\nsim &lt;- tibble(sample_mu    = rnorm(n, mean = 178, sd = 100),\n              sample_sigma = runif(n, min = 0, max = 50)) |&gt; \n  mutate(height = rnorm(n, mean = sample_mu, sd = sample_sigma))\n\n# Compute the values we'll use to break on our x axis\nm_height &lt;- mean(sim$height)\ns_height &lt;- sd(sim$height)\n\nbreaks &lt;- c(m_height - 3 * s_height, 0, m_height, m_height + 3 * s_height) |&gt; \n  round(digits = 0)\n\n# This is just for aesthetics\ntext &lt;- tibble(height = 272 - 25,\n               y      = 0.0013,\n               label  = \"tallest man\",\n               angle  = 90)\n\n# Plot\np4 &lt;- sim |&gt; \n  ggplot(aes(x = height)) +\n  geom_density(fill = \"black\", linewidth = 0) +\n  geom_vline(xintercept = 0, color = \"grey92\") +\n  geom_vline(xintercept = 272, color = \"grey92\", linetype = 3) +\n  geom_text(data = text,\n            aes(y = y, label = label, angle = angle),\n            color = \"grey92\") +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"height ~ dnorm(mu, sigma)\\nmu ~ dnorm(178, 100)\") +\n  theme(panel.grid = element_blank())\n\np4\n\n\n\n\n\n\n\n\nYou may have noticed how we were saving each of the four last plots as p1 through p4. Let’s combine the four to make our version of McElreath’s Figure 4.3.\n\n(p1 + xlab(\"mu\") | p2 + xlab(\"sigma\")) / (p3 | p4)\n\n\n\n\n\n\n\n\nOn page 84, McElreath said his prior simulation indicated 4% of the heights would be below zero. Here’s how we might determe that percentage for our simulation.\n\nsim |&gt; \n  count(height &lt; 0) |&gt; \n  mutate(percent = 100 * n / sum(n))\n\n# A tibble: 2 × 3\n  `height &lt; 0`     n percent\n  &lt;lgl&gt;        &lt;int&gt;   &lt;dbl&gt;\n1 FALSE         9571   95.7 \n2 TRUE           429    4.29\n\n\nHere’s the break down compared to the tallest man on record, Robert Pershing Wadlow (1918–1940).\n\nsim |&gt; \n  count(height &lt; 272) |&gt; \n  mutate(percent = 100 * n / sum(n))\n\n# A tibble: 2 × 3\n  `height &lt; 272`     n percent\n  &lt;lgl&gt;          &lt;int&gt;   &lt;dbl&gt;\n1 FALSE           1761    17.6\n2 TRUE            8239    82.4\n\n\n\nDoes this matter? In this case, we have so much data that the silly prior is harmless. But that won’t always be the case. There are plenty of inference problems for which the data alone are not sufficient, no matter how numerous. Bayes lets us proceed in these cases. But only if we use our scientific knowledge to construct sensible priors. Using scientific knowledge to build priors is not cheating. The important thing is that your prior not be based on the values in the data, but only on what you know about the data before you see it. (p. 84)\n\n\n4.3.2.1 Rethinking: A farewell to epsilon\n\nSome readers will have already met an alternative notation for a Gaussian linear model:\n\\[\\begin{align*}\nh_i & = \\mu + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma)\n\\end{align*}\\]\nThis is equivalent to the \\(h_i \\sim \\operatorname{Normal}(\\mu, \\sigma)\\) form, with the \\(\\epsilon\\) standing in for the Gaussian density. But this \\(\\epsilon\\) form is poor form. The reason is that it does not usually generalize to other types of models. This means it won’t be possible to express non-Gaussian models using tricks like \\(\\epsilon\\). Better to learn one system that does generalize. (p. 84)\n\nAgreed.\n\n\n\n4.3.3 Grid approximation of the posterior distribution\nAs McElreath explained, you’ll never use this for practical data analysis. But I found this helped me better understanding what exactly we’re doing with Bayesian estimation. So let’s play along.\n\nn &lt;- 200\n\n# We'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`\nd_grid &lt;- crossing(mu    = seq(from = 140, to = 160, length.out = n),\n                   sigma = seq(from = 4, to = 9, length.out = n))\n\nglimpse(d_grid)\n\nRows: 40,000\nColumns: 2\n$ mu    &lt;dbl&gt; 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140,…\n$ sigma &lt;dbl&gt; 4.000000, 4.025126, 4.050251, 4.075377, 4.100503, 4.125628, 4.150754, 4.175879, 4.201005, 4.226131, 4.251256, 4.27…\n\n\nd_grid contains every combination of mu and sigma across their specified values. Instead of base R sapply(), we’ll do the computations by making a custom function which we’ll then plug into purrr::map2().\n\ngrid_function &lt;- function(mu, sigma) {\n  dnorm(d2$height, mean = mu, sd = sigma, log = T) |&gt; \n    sum()\n}\n\nNow we’re ready to complete the tibble.\n\nd_grid &lt;- d_grid |&gt; \n  mutate(log_likelihood = map2(mu, sigma, grid_function)) |&gt;\n  unnest(log_likelihood) |&gt; \n  mutate(prior_mu    = dnorm(mu, mean = 178, sd = 20, log = T),\n         prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) |&gt; \n  mutate(product = log_likelihood + prior_mu + prior_sigma) |&gt; \n  mutate(probability = exp(product - max(product)))\n  \nhead(d_grid)\n\n# A tibble: 6 × 7\n     mu sigma log_likelihood prior_mu prior_sigma product probability\n  &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1   140  4            -3813.    -5.72       -3.91  -3822.           0\n2   140  4.03         -3778.    -5.72       -3.91  -3787.           0\n3   140  4.05         -3743.    -5.72       -3.91  -3753.           0\n4   140  4.08         -3709.    -5.72       -3.91  -3719.           0\n5   140  4.10         -3676.    -5.72       -3.91  -3686.           0\n6   140  4.13         -3644.    -5.72       -3.91  -3653.           0\n\n\nIn the final d_grid, the probability vector contains the posterior probabilities across values of mu and sigma. We can make a contour plot with geom_contour().\n\nd_grid |&gt; \n  ggplot(aes(x = mu, y = sigma, z = probability)) + \n  geom_contour() +\n  labs(x = expression(mu),\n       y = expression(sigma)) +\n  coord_cartesian(xlim = range(d_grid$mu),\n                  ylim = range(d_grid$sigma)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWe’ll make our heat map with geom_raster().\n\nd_grid |&gt; \n  ggplot(aes(x = mu, y = sigma, fill = probability)) + \n  geom_raster(interpolate = T) +\n  scale_fill_viridis_c(option = \"B\") +\n  labs(x = expression(mu),\n       y = expression(sigma)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n4.3.4 Sampling from the posterior\nWe can use dplyr::sample_n() to sample rows, with replacement, from d_grid.\n\nset.seed(4)\n\nd_grid_samples &lt;- d_grid |&gt; \n  sample_n(size = 1e4, replace = T, weight = probability)\n\nd_grid_samples |&gt; \n  ggplot(aes(x = mu, y = sigma)) + \n  geom_point(size = 0.9, alpha = 1/15) +\n  scale_fill_viridis_c() +\n  labs(x = expression(mu[samples]),\n       y = expression(sigma[samples])) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWe can use pivot_longer() and then facet_wrap() to plot the densities for both mu and sigma at once.\n\nd_grid_samples |&gt; \n  pivot_longer(mu:sigma) |&gt; \n\n  ggplot(aes(x = value)) + \n  geom_density(fill = \"grey33\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\", labeller = label_parsed)\n\n\n\n\n\n\n\n\nWe’ll use the tidybayes package to compute their posterior modes and 95% HDIs.\n\nlibrary(tidybayes)\n\nd_grid_samples |&gt; \n  pivot_longer(mu:sigma) |&gt; \n  group_by(name) |&gt; \n  mode_hdi(value)\n\n# A tibble: 2 × 7\n  name   value .lower .upper .width .point .interval\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 mu    155.   154.   155.     0.95 mode   hdi      \n2 sigma   7.82   7.19   8.35   0.95 mode   hdi      \n\n\nLet’s say you wanted their posterior medians and 50% quantile-based intervals, instead. Just switch out the last line for median_qi(value, .width = 0.5).\n\n4.3.4.1 Overthinking: Sample size and the normality of \\(\\sigma\\)’s posterior\nSince we’ll be fitting models with brms almost exclusively from here on out, this section is largely mute. But we’ll do it anyway for the sake of practice. I’m going to break the steps up like before rather than compress the code together. Here’s d3.\n\nset.seed(4)\n(d3 &lt;- sample(d2$height, size = 20))\n\n [1] 147.3200 154.9400 168.9100 156.8450 165.7350 151.7650 165.7350 156.2100 144.7800 154.9400 151.1300 147.9550 149.8600 162.5600\n[15] 161.9250 164.4650 160.9852 151.7650 163.8300 149.8600\n\n\nFor our first step using d3, we’ll redefine d_grid.\n\nn &lt;- 200\n\n# Note we've redefined the ranges of `mu` and `sigma`\nd_grid &lt;- crossing(mu    = seq(from = 150, to = 170, length.out = n),\n                   sigma = seq(from = 4, to = 20, length.out = n))\n\nSecond, we’ll redefine our custom grid_function() function to operate over the height values of d3.\n\ngrid_function &lt;- function(mu, sigma) {\n  dnorm(d3, mean = mu, sd = sigma, log = T) |&gt; \n    sum()\n}\n\nNow we’ll use the amended grid_function() to make the posterior.\n\nd_grid &lt;- d_grid |&gt; \n  mutate(log_likelihood = map2_dbl(mu, sigma, grid_function)) |&gt; \n  mutate(prior_mu    = dnorm(mu, mean = 178, sd = 20, log = T),\n         prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) |&gt; \n  mutate(product = log_likelihood + prior_mu + prior_sigma) |&gt; \n  mutate(probability = exp(product - max(product)))\n\nDid you catch our use of purrr::map2_dbl(), there, in place of purrr::map2()? It turns out that purrr::map() and purrr::map2() always return a list (see here and here). But we can add the _dbl suffix to those functions, which will instruct the purrr package to return a double vector (i.e., a common kind of numeric vector). The advantage of that approach is we no longer need to follow our map() or map2() lines with unnest(). To learn more about the ins and outs of the map() family, check out this section from R4DS or Jenny Bryan’s purrr tutorial.\nNext we’ll sample_n() and plot.\n\nset.seed(4)\n\nd_grid_samples &lt;- d_grid |&gt; \n  sample_n(size = 1e4, replace = T, weight = probability)\n\nd_grid_samples |&gt; \n  ggplot(aes(x = mu, y = sigma)) + \n  geom_point(alpha = 1/15, size = 0.9) +\n  labs(x = expression(mu[samples]),\n       y = expression(sigma[samples])) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nBehold the updated densities.\n\nd_grid_samples |&gt; \n  pivot_longer(mu:sigma) |&gt; \n\n  ggplot(aes(x = value)) + \n  geom_density(fill = \"grey33\", linewidth = 0) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(NULL) +\n  facet_wrap(~ name, labeller = label_parsed, scales = \"free\") +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThat labeller = label_parsed bit in the facet_wrap() function is what converted our subplot strip labels into Greek. You can learn more about labeller here. Anyway, our posterior for \\(\\sigma\\) isn’t so Gaussian with that small \\(n\\).\nThis is the point in the project where we hop off the grid-approximation train. On the one hand, I think this is a great idea. Most of y’all reading this will never use grid approximation in a real-world applied data analysis. On the other hand, there is some pedagogical utility in practicing with it. It can help you grasp what it is we’re doing when we apply Bayes’ theorem. If you’d like more practice, check out the first several chapters in John Kruschke’s (2015) textbook and the corresponding chapters in my (2026) ebook translating it into brms and tidyverse.\n\n\n\n4.3.5 Finding the posterior distribution with quap brm()\nHere we rewrite the statistical model, this time using font color to help differentiate the likelihood from the prior(s).\n\\[\\begin{align*}\n\\color{red}{\\text{heights}_i} & \\color{red}\\sim \\color{red}{\\operatorname{Normal}(\\mu, \\sigma)} && \\color{red}{\\text{likelihood}} \\\\\n\\color{blue}\\mu & \\color{blue}\\sim \\color{blue}{\\operatorname{Normal}(178, 20)} && \\color{blue}{\\text{prior}} \\\\\n\\color{blue}\\sigma & \\color{blue}\\sim \\color{blue}{\\operatorname{Uniform}(0, 50)}\n\\end{align*}\\]\nWe won’t actually use rethinking::quap(). It’s time to jump straight to the primary brms modeling function, brm(). In the text, McElreath indexed his models with names like m4.1. I will largely follow that convention, but will replace the m with a b to stand for the brms package. Here’s how to fit the first model for this chapter.\n\nb4.1 &lt;- brm(\n  data = d2, \n  family = gaussian,\n  height ~ 1,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.01\")\n\nNote our use of the ub parameter for the uniform prior on \\(\\sigma\\). If you want to use an upper-bound prior on \\(\\sigma\\) with a brm() model, the up setting will help out a lot. We’ll start to get a sense of why when we cover Hamiltonian Monte Carlo (HMC) in Chapter 9. This leads to an important point. After running a model fit with HMC, it’s a good idea to inspect the chains. As we’ll see, McElreath covered visual chain diagnostics in Chapter 9. Here’s a typical way to do so with brms.\n\nplot(b4.1)\n\n\n\n\n\n\n\n\nIf you want detailed diagnostics for the HMC chains, call launch_shinystan(b4.1). That’ll keep you busy for a while. But anyway, the chains look good. We can reasonably trust the results. Here’s how to get the model summary of our brm() object.\n\nprint(b4.1)\n\n Family: gaussian \n  Links: mu = identity \nFormula: height ~ 1 \n   Data: d2 (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.61      0.40   153.79   155.41 1.00     3654     2340\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.77      0.29     7.23     8.38 1.00     3901     3112\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe summary() function works in a similar way. You can also get a Stan-like summary [see the RStan: the R interface to Stan vignette; Stan Development Team (2023)] with a little indexing.\n\nb4.1$fit\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n                mean se_mean   sd     2.5%      25%      50%      75%    97.5% n_eff Rhat\nb_Intercept   154.61    0.01 0.40   153.79   154.34   154.61   154.87   155.41  3633    1\nsigma           7.77    0.00 0.29     7.23     7.56     7.76     7.96     8.38  3936    1\nIntercept     154.61    0.01 0.40   153.79   154.34   154.61   154.87   155.41  3633    1\nlprior         -8.51    0.00 0.02    -8.56    -8.53    -8.51    -8.50    -8.46  3636    1\nlp__        -1227.02    0.02 0.97 -1229.56 -1227.38 -1226.72 -1226.30 -1226.06  1941    1\n\nSamples were drawn using NUTS(diag_e) at Mon Jan  5 21:53:25 2026.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nWhereas rethinking defaults to 89% intervals, using print() or summary() with brms models defaults to 95% intervals. Unless otherwise specified, I will stick with 95% intervals throughout. However, if you really want those 89% intervals, an easy way is with the prob argument within brms::summary() or brms::print().\n\nsummary(b4.1, prob = 0.89)\n\n Family: gaussian \n  Links: mu = identity \nFormula: height ~ 1 \n   Data: d2 (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\nIntercept   154.61      0.40   153.96   155.26 1.00     3654     2340\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\nsigma     7.77      0.29     7.31     8.25 1.00     3901     3112\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAnyways, here’s the brms::brm() code for the model with the very-narrow-\\(\\mu\\)-prior corresponding to the rethinking::quap() code in McElreath’s R code 4.31.\n\nb4.2 &lt;- brm(\n  data = d2, \n  family = gaussian,\n  height ~ 1,\n  prior = c(prior(normal(178, 0.1), class = Intercept),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.02\")\n\n\nplot(b4.2, widths = c(1, 2))\n\n\n\n\n\n\n\n\nThe chains look great. Here’s the model summary().\n\nsummary(b4.2)\n\n Family: gaussian \n  Links: mu = identity \nFormula: height ~ 1 \n   Data: d2 (Number of observations: 352) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   177.86      0.10   177.66   178.07 1.00     2982     2248\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    24.61      0.94    22.82    26.59 1.00     3163     2678\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSubsetting the summary() output with $fixed provides a convenient way to compare the Intercept summaries between b4.1_hc and b4.2.\n\nrbind(summary(b4.1)$fixed,\n      summary(b4.2)$fixed)\n\n           Estimate Est.Error l-95% CI u-95% CI     Rhat Bulk_ESS Tail_ESS\nIntercept  154.6054 0.4035752 153.7868 155.4105 1.001695 3654.491 2340.199\nIntercept1 177.8645 0.1022954 177.6620 178.0650 1.000020 2981.815 2247.967\n\n\n\n\n4.3.6 Sampling from a quap() brm() fit\nbrms doesn’t seem to have a convenience function that works the way vcov() does for rethinking. For example:\n\nvcov(b4.1)\n\n          Intercept\nIntercept 0.1628729\n\n\nThis only returns the first element in the matrix it did for rethinking. That is, it appears brms::vcov() only returns the variance/covariance matrix for the single-level \\(\\beta\\) parameters. However, if you really wanted this information, you could get it after putting the HMC chains in a data frame. We do that with the as_draws_df() function, which we’ll be using a lot of as we go along.\n\npost &lt;- as_draws_df(b4.1)\n\nhead(post)\n\n# A draws_df: 6 iterations, 1 chains, and 5 variables\n  b_Intercept sigma Intercept lprior  lp__\n1         154   7.8       154   -8.5 -1226\n2         154   7.7       154   -8.5 -1226\n3         155   8.5       155   -8.5 -1229\n4         155   7.9       155   -8.5 -1226\n5         154   8.1       154   -8.5 -1227\n6         155   8.0       155   -8.5 -1226\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nNow select() the columns containing the draws from the desired parameters and feed them into cov().\n\nselect(post, b_Intercept:sigma) |&gt; \n  cov()\n\n              b_Intercept         sigma\nb_Intercept  0.1628729253 -0.0002503749\nsigma       -0.0002503749  0.0869613252\n\n\nThat was “(1) a vector of variances for the parameters and (2) a correlation matrix” for them (p. 90). Here are just the variances (i.e., the diagonal elements) and the correlation matrix.\n\n# Variances\nselect(post, b_Intercept:sigma) |&gt;\n  cov() |&gt;\n  diag()\n\nb_Intercept       sigma \n 0.16287293  0.08696133 \n\n# Correlation\npost |&gt;\n  select(b_Intercept, sigma) |&gt;\n  cor()\n\n             b_Intercept        sigma\nb_Intercept  1.000000000 -0.002103794\nsigma       -0.002103794  1.000000000\n\n\nWith our post &lt;- as_draws_df(b4.1) code from a few lines above, we’ve already produced the brms version of what McElreath achieved with extract.samples() on page 90. However, what happened under the hood was different. Whereas rethinking used the mvnorm() function from the MASS package (Ripley, 2022; Venables & Ripley, 2002), with brms we just extracted the iterations of the HMC chains and put them in a data frame. It’s also noteworthy that the as_draws_df() is part of a larger class of as_draws() functions brms currently imports from the posterior package (Bürkner et al., 2022).\n\nstr(post)\n\ndraws_df [4,000 × 8] (S3: draws_df/draws/tbl_df/tbl/data.frame)\n $ b_Intercept: num [1:4000] 154 154 155 155 154 ...\n $ sigma      : num [1:4000] 7.76 7.7 8.52 7.95 8.05 ...\n $ Intercept  : num [1:4000] 154 154 155 155 154 ...\n $ lprior     : num [1:4000] -8.53 -8.52 -8.51 -8.5 -8.54 ...\n $ lp__       : num [1:4000] -1226 -1226 -1229 -1226 -1227 ...\n $ .chain     : int [1:4000] 1 1 1 1 1 1 1 1 1 1 ...\n $ .iteration : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...\n $ .draw      : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...\n\n\nThus, our post object is not just a data frame, but also of class draws_df, which means it contains three metadata variables–.chain, .iteration, and .draw–which will are often hidden from view, but are there in the background when needed. As you’ll see, we’ll make good use of the .draw variable in the future. Notice how our post data frame also includes vectors named lprior and lp__. That’s the log prior, adn the log posterior. For details, see the brms reference manual (Bürkner, 2022a), the “The Log-Posterior (function and gradient)” section of the Stan Development Team’s (2023) vignette, RStan: the R interface to Stan, or Stephen Martin’s nice explanation of the log posterior on the Stan Forums. The log prior and log posterior will largely be outside of our focus in this ebook.\nThe summary() function doesn’t work for brms posterior data frames quite the way precis() does for posterior data frames from the rethinking package. Behold the results.\n\nsummary(post[, 1:2])\n\n  b_Intercept        sigma      \n Min.   :153.1   Min.   :6.927  \n 1st Qu.:154.3   1st Qu.:7.564  \n Median :154.6   Median :7.762  \n Mean   :154.6   Mean   :7.768  \n 3rd Qu.:154.9   3rd Qu.:7.961  \n Max.   :156.1   Max.   :8.920  \n\n\nHere’s one option using the transpose of a quantile() call nested within apply(), which is a very general function you can learn more about here or here.\n\nt(apply(post[, 1:2], 2, quantile, probs = c(0.5, 0.025, 0.75)))\n\n                   50%       2.5%        75%\nb_Intercept 154.611940 153.786780 154.865745\nsigma         7.761514   7.225498   7.960993\n\n\nThe base R code is compact, but somewhat opaque. Here’s how to do something similar with more explicit tidyverse code.\n\npost |&gt;\n  pivot_longer(b_Intercept:sigma) |&gt; \n  group_by(name) |&gt;\n  summarise(mean = mean(value),\n            sd   = sd(value),\n            `2.5%`  = quantile(value, probs = 0.025),\n            `97.5%` = quantile(value, probs = 0.975)) |&gt;\n  mutate_if(is.numeric, round, digits = 2)\n\n# A tibble: 2 × 5\n  name          mean    sd `2.5%` `97.5%`\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 b_Intercept 155.    0.4  154.    155.  \n2 sigma         7.77  0.29   7.23    8.38\n\n\nYou can always get pretty similar information by just putting the brm() fit object into posterior_summary().\n\nposterior_summary(b4.1)\n\n                Estimate  Est.Error         Q2.5        Q97.5\nb_Intercept   154.605420 0.40357518   153.786780   155.410455\nsigma           7.768158 0.29489206     7.225498     8.377223\nIntercept     154.605420 0.40357518   153.786780   155.410455\nlprior         -8.511030 0.02360932    -8.559544    -8.464553\nlp__        -1227.016052 0.97349118 -1229.561352 -1226.061946\n\n\nAnd if you’re willing to drop the posterior \\(\\textit{SD}\\)s, you can use tidybayes::mean_hdi(), too.\n\npost |&gt; \n  pivot_longer(b_Intercept:sigma) |&gt; \n  group_by(name) |&gt;\n  mean_qi(value)\n\n# A tibble: 2 × 7\n  name         value .lower .upper .width .point .interval\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 b_Intercept 155.   154.   155.     0.95 mean   qi       \n2 sigma         7.77   7.23   8.38   0.95 mean   qi       \n\n\nThough none of these solutions get you those sweet little histograms, you can always make those for your HMC models by inserting the desired posterior draws into histospark().\n\nrbind(histospark(post$b_Intercept),\n      histospark(post$sigma))\n\n     [,1]         \n[1,] \"▁▁▅▇▂▁▁\"    \n[2,] \"▁▁▂▅▇▇▃▂▁▁▁\"\n\n\nHell, you can even tack those onto the output from our verbose tidyverse code from a few blocks up.\n\npost |&gt;\n  pivot_longer(b_Intercept:sigma) |&gt; \n  group_by(name) |&gt;\n  summarise(mean = mean(value),\n            sd   = sd(value),\n            `2.5%`  = quantile(value, probs = 0.025),\n            `97.5%` = quantile(value, probs = 0.975)) |&gt;\n  mutate_if(is.numeric, round, digits = 2) |&gt; \n  mutate(histospark = c(histospark(post$b_Intercept), histospark(post$sigma)))\n\n# A tibble: 2 × 6\n  name          mean    sd `2.5%` `97.5%` histospark \n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      \n1 b_Intercept 155.    0.4  154.    155.   ▁▁▅▇▂▁▁    \n2 sigma         7.77  0.29   7.23    8.38 ▁▁▂▅▇▇▃▂▁▁▁\n\n\n\n4.3.6.1 Overthinking: Start values for quap() brm()\nWe won’t be emphasizing start values in this ebook. But, yes, you can set start values for the HMC chains from brms, too. Within the brm() function, you do so with the init argument. From the brm section within the brms reference manual, we read:\n\nInitial values for the sampler. If NULL (the default) or \"random\", Stan will randomly generate initial values for parameters in a reasonable range. If 0, all parameters are initialized to zero on the unconstrained space. This option is sometimes useful for certain families, as it happens that default random initial values cause draws to be essentially constant. Generally, setting init = 0 is worth a try, if chains do not initialize or behave well. Alternatively, init can be a list of lists containing the initial values, or a function (or function name) generating initial values. The latter options are mainly implemented for internal testing but are available to users if necessary. If specifying initial values using a list or a function then currently the parameter names must correspond to the names used in the generated Stan code (not the names used in R).\n\n\n\n4.3.6.2 Overthinking: Under the hood with multivariate sampling\nAgain, brms::as_draws_df() is not the same as rethinking::extract.samples(). Rather than use the MASS::mvnorm(), brms takes the draws from the HMC chains. McElreath covered all of this in Chapter 9 and we will too. You might also look at the brms reference manual or GitHub page for details. To get documentation in a hurry, you could also just execute ?as_draws_df.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "04.html#linear-prediction",
    "href": "04.html#linear-prediction",
    "title": "4  Geocentric Models",
    "section": "4.4 Linear prediction",
    "text": "4.4 Linear prediction\nHere’s our scatter plot of weight and height.\n\nggplot(data = d2, \n       aes(x = weight, y = height)) +\n  geom_point(shape = 1, size = 2) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nThere’s obviously a relationship: Knowing a person’s weight helps you predict height.\nTo make this vague observation into a more precise quantitative model that relates values of weight to plausible values of height, we need some more technology. How do we take our Gaussian model from the previous section and incorporate predictor variables? (p. 92)\n\n\n4.4.1 The linear model strategy\n\nThe strategy is to make the parameter for the mean of a Gaussian distribution, \\(\\mu\\), into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the linear model. The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship. (p. 92, emphasis in the original)\n\nLike we did for our first model without a predictor, we’ll use font color to help differentiate between the likelihood and prior(s) of our new univariable model,\n\\[\\begin{align*}\n\\color{red}{\\text{height}_i} & \\color{red}\\sim \\color{red}{\\operatorname{Normal}(\\mu_i, \\sigma)} && \\color{red}{\\text{likelihood}} \\\\\n\\color{red}{\\mu_i} & \\color{red}= \\color{red}{\\alpha + \\beta (\\text{weight}_i - \\overline{\\text{weight}})}  && \\color{red}{\\text{\\{the linear model is just a special part of the likelihood\\}} } \\\\\n\\color{blue}\\alpha & \\color{blue}\\sim \\color{blue}{\\operatorname{Normal}(178, 20)} && \\color{blue}{\\text{prior(s)}} \\\\\n\\color{blue}\\beta  & \\color{blue}\\sim \\color{blue}{\\operatorname{Normal}(0, 10)} \\\\\n\\color{blue}\\sigma & \\color{blue}\\sim \\color{blue}{\\operatorname{Uniform}(0, 50)}.\n\\end{align*}\\]\nDo note that \\((\\text{weight}_i - \\overline{\\text{weight}})\\) part. As we’ll see, it’s often advantageous to mean center our predictors.\n\n4.4.1.1 Probability of the data\n\nLet’s begin with just the probability of the observed height, the first line of the model. This is nearly identical to before, except now there is a little index \\(i\\) on the \\(\\mu\\) as well as the [\\(\\text{height}\\)]. You can read [\\(\\text{height}_i\\)] as “each [\\(\\text{height}\\)]” and \\(\\mu_i\\) as “each \\(\\mu\\).” The mean \\(\\mu\\) now depends upon unique values on each row \\(i\\). So the little \\(i\\) on \\(\\mu_i\\) indicates that the mean depends upon the row. (p. 93, emphasis in the original)\n\n\n\n4.4.1.2 Linear model\n\nThe mean \\(\\mu\\) is no longer a parameter to be estimated. Rather, as seen in the second line of the model, \\(\\mu_i\\) is constructed from other parameters, \\(\\alpha\\) and \\(\\beta\\), and the observed variable [\\(\\text{weight}\\)]. This line is not a stochastic relationship–there is no \\(\\sim\\) in it, but rather an \\(=\\) in it–because the definition of \\(\\mu_i\\) is deterministic. That is to say that, once we know \\(\\alpha\\) and \\(\\beta\\) and [\\(\\text{weight}_i\\)], we know \\(\\mu_i\\) with certainty.\nThe value [\\(\\text{weight}_i\\)] is just the weight value on row \\(i\\). It refers to the same individual as the height value, [\\(\\text{height}_i\\)], on the same row. The parameters \\(\\alpha\\) and \\(\\beta\\) are more mysterious. Where did they come from? We made them up….\nYou’ll be making up all manner of parameters as your skills improve. (p. 93)\n\n\n4.4.1.2.1 Rethinking: Nothing special or natural about linear models\n\nNote that there’s nothing special about the linear model, really. You can choose a different relationship between \\(\\alpha\\) and \\(\\beta\\) and \\(\\mu\\). For example, the following is a perfectly legitimate definition for \\(\\mu_i\\):\n\\[\\mu_i = \\alpha \\exp(- \\beta x_i)\\]\nThis does not define a linear regression, but it does define a regression model. The linear relationship we are using instead is conventional, but nothing requires that you use it. (p. 94)\n\n\n\n\n4.4.1.3 Priors\n\nThe remaining lines in the model define distributions for the unobserved variables. These variables are commonly known as parameters, and their distributions as priors. There are three parameters: \\(\\alpha\\), \\(\\beta\\), and \\(\\sigma\\). You’ve seen priors for \\(\\alpha\\) and \\(\\sigma\\) before, although \\(\\alpha\\) was called \\(\\mu\\) back then.\nThe prior for \\(\\beta\\) deserves explanation. Why have a Gaussian prior with mean zero? (p. 94)\n\nWe’ll simulate to find out. Instead of using a loop to make our data for Figure 4.5, we’ll stay within the tidyverse.\n\nset.seed(2971)\n# How many lines would you like?\nn_lines &lt;- 100\n\nlines &lt;- tibble(n = 1:n_lines,\n                a = rnorm(n_lines, mean = 178, sd = 20),\n                b = rnorm(n_lines, mean = 0, sd = 10)) |&gt; \n  expand_grid(weight = range(d2$weight)) |&gt; \n  mutate(height = a + b * (weight - mean(d2$weight)))\n\nhead(lines)\n\n# A tibble: 6 × 5\n      n     a      b weight height\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1  191. -7.06    31.1  289. \n2     1  191. -7.06    63.0   63.5\n3     2  199.  0.839   31.1  187. \n4     2  199.  0.839   63.0  214. \n5     3  202.  3.93    31.1  147. \n6     3  202.  3.93    63.0  272. \n\n\nNow we’ll plot the left panel from Figure 4.5.\n\nlines |&gt; \n  ggplot(aes(x = weight, y = height, group = n)) +\n  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +\n  geom_line(alpha = 1/10) +\n  coord_cartesian(ylim = c(-100, 400)) +\n  ggtitle(\"b ~ dnorm(0, 10)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nThe pattern doesn’t look like any human population at all. It essentially says that the relationship between weight and height could be absurdly positive or negative. Before we’ve even seen the data, this is a bad model. Can we do better?\nWe can do better immediately. (pp. 95–96)\n\nOne thing we know from the outset is that the correlation between human height and weight is positive. We might not be sure of the magnitude, but it’s definitely the case that, on average, taller people are heavier people. Within the univariable regression context, this implies that the regression coefficient for weight predicting height will be positive. It might be unclear how large that coefficient will be, but it will certainly be above zero. One way we might encode this information in our data is by using the log-normal distribution for our \\(\\beta\\) prior. Here’s what \\(\\operatorname{Log-Normal}(0, 1)\\) looks like.\n\nset.seed(4)\n\ntibble(b = rlnorm(1e4, mean = 0, sd = 1)) |&gt; \n  ggplot(aes(x = b)) +\n  geom_density(fill = \"grey92\") +\n  coord_cartesian(xlim = c(0, 5)) +\n  theme_classic()\n\n\n\n\n\n\n\n\nIf you’re unfamiliar with the log-normal distribution, it is the distribution whose logarithm is normally distributed. For example, here’s what happens when we compare \\(\\operatorname{Normal}(0, 1)\\) with \\(\\log \\big ( \\operatorname{Log-Normal}(0, 1) \\big)\\).\n\nset.seed(4)\n\ntibble(rnorm           = rnorm(1e5, mean = 0, sd = 1),\n       `log(rlognorm)` = log(rlnorm(1e5, mean = 0, sd = 1))) |&gt; \n  pivot_longer(everything()) |&gt; \n\n  ggplot(aes(x = value)) +\n  geom_density(fill = \"grey92\") +\n  coord_cartesian(xlim = c(-3, 3)) +\n  theme_classic() +\n  facet_wrap(~ name, nrow = 2)\n\n\n\n\n\n\n\n\nThey are the same within simulation variance. Also, did you notice how we simulated those log-normal data with mean = 0, sd = 1? Those values are what the mean and standard deviation of the output from the rlnorm() function after they are log transformed. The formulas for the actual mean and standard deviation for the log-normal distribution itself are complicated (see here). They are\n\\[\\begin{align*}\n\\text{mean}               & = \\exp \\left (\\mu + \\frac{\\sigma^2}{2} \\right) & \\text{and} \\\\\n\\text{standard deviation} & = \\sqrt{[\\exp(\\sigma ^{2})-1] \\; \\exp(2\\mu +\\sigma ^{2})}.\n\\end{align*}\\]\nLet’s try our hand at those formulas and compute the mean and standard deviation for \\(\\operatorname{Log-Normal}(0, 1)\\).\n\nmu    &lt;- 0\nsigma &lt;- 1\n\n# Mean\nexp(mu + (sigma^2) / 2)\n\n[1] 1.648721\n\n# SD\nsqrt((exp(sigma^2) - 1) * exp(2 * mu + sigma^2))\n\n[1] 2.161197\n\n\nLet’s confirm with simulated draws from rlnorm().\n\nset.seed(4)\n\ntibble(x = rlnorm(1e7, mean = 0, sd = 1)) |&gt; \n  summarise(mean = mean(x),\n            sd   = sd(x))\n\n# A tibble: 1 × 2\n   mean    sd\n  &lt;dbl&gt; &lt;dbl&gt;\n1  1.65  2.17\n\n\nBut okay, “so what [do all these complications] earn us? Do the prior predictive simulation again, now with the Log-Normal prior:” (p. 96).\n\n# Make a tibble to annotate the plot\ntext &lt;- tibble(weight = c(34, 43),\n               height = c(0 - 25, 272 + 25),\n               label  = c(\"Embryo\", \"World's tallest person (272 cm)\"))\n\n# Simulate\nset.seed(2971)\n\ntibble(n = 1:n_lines,\n       a = rnorm(n_lines, mean = 178, sd = 20),\n       b = rlnorm(n_lines, mean = 0, sd = 1)) |&gt; \n  expand_grid(weight = range(d2$weight)) |&gt; \n  mutate(height = a + b * (weight - mean(d2$weight))) |&gt;\n  \n  # Plot\n  ggplot(aes(x = weight, y = height, group = n)) +\n  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +\n  geom_line(alpha = 1/10) +\n  geom_text(data = text,\n            aes(label = label),\n            size = 3) +\n  coord_cartesian(ylim = c(-100, 400)) +\n  ggtitle(\"log(b) ~ dnorm(0, 1)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n“This is much more sensible. There is still a rare impossible relationship. But nearly all lines in the joint prior for \\(\\alpha\\) and \\(\\beta\\) are now within human reason” (p. 96)\n\n4.4.1.3.1 Rethinking: What’s the correct prior?\nGood luck with that question. Hang around on academic Twitter long enough and you’ll see folks debating this.\n\nThis is a mistake. There is no more a uniquely correct prior than there is a uniquely correct likelihood. Statistical models are machines for inference. Many machines will work, but some work better than others. Priors can be wrong, but only in the same sense that a kind of hammer can be wrong for building a table. (p. 96)\n\n\n\n4.4.1.3.2 Rethinking: Prior predictive simulation and \\(p\\)-hacking\n“We don’t pay any attention to \\(p\\)-values in this book” (p. 97). Off hand, I’m not sure of the exact origin of the term \\(p\\)-hacking. But the paper by Simmons, Nelson and Simonsohn (2011), False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant, is often cited as an introduction to the problem.\n\n\n\n\n4.4.2 Finding the posterior distribution\nUnlike with McElreath’s quap() formula syntax, I’m not aware that you can just specify something like weight – xbar in the formula argument in brm(). However, the alternative is easy: Just make a new variable in the data that is equivalent to weight – mean(weight). We’ll call it weight_c.\n\nd2 &lt;- d2 |&gt; \n  mutate(weight_c = weight - mean(weight))\n\nUnlike with McElreath’s rethinking package, the conventional brms::brm() syntax doesn’t mirror the statistical notation. But here are the analogues to the exposition at the bottom of page 97:\n\n\\(\\text{height}_i \\sim \\operatorname{Normal}(\\mu_i, \\sigma)\\): family = gaussian,\n\\(\\mu_i = \\alpha + \\beta \\text{weight}_i\\): height ~ 1 + weight_c,\n\\(\\alpha \\sim \\operatorname{Normal}(178, 20)\\): prior(normal(178, 20), class = Intercept,\n\\(\\beta \\sim \\operatorname{Log-Normal}(0, 1)\\): prior(lognormal(0, 1), class = b), and\n\\(\\sigma \\sim \\operatorname{Uniform}(0, 50)\\): prior(uniform(0, 50), class = sigma).\n\nThus, to add a predictor you just the + operator in the model formula.\n\nb4.3 &lt;- brm(\n  data = d2, \n  family = gaussian,\n  height ~ 1 + weight_c,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(lognormal(0, 1), class = b),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.03\")\n\nHere are the trace plots.\n\nplot(b4.3, widths = c(1, 2))\n\n\n\n\n\n\n\n\n\n4.4.2.1 Overthinking: Logs and exps, oh my\nbrms does not allow users to insert coefficients into functions like \\(\\exp()\\) within the conventional formula syntax. We can fit a brms model like McElreath’s m4.3b if we adopt what’s called the non-linear syntax (Bürkner, 2022b). The non-linear syntax is a lot like the syntax McElreath uses in rethinking in that it typically includes both predictor and variable names in the formula. Since this is so early in the book and we’re just working through a problem in an Overthinking tangent, I won’t go into a full-blown explanation, here. There will be many more opportunities to practice with the non-linear syntax in the chapters to come (e.g., Section 5.3.2, Section 6.2.1). For now, here’s how we might fit the model.\n\nb4.3b &lt;- brm(\n  data = d2, \n  family = gaussian,\n  bf(height ~ a + exp(lb) * weight_c,\n     a ~ 1,\n     lb ~ 1,\n     nl = TRUE),\n  prior = c(prior(normal(178, 20), class = b, nlpar = a),\n            prior(normal(0, 1), class = b, nlpar = lb),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.03b\")\n\nIf you execute summary(b4.3b), you’ll see the intercept and \\(\\sigma\\) summaries for this model are about the same as those for b4.3, above. The difference is for the \\(\\beta\\) parameter, which we called lb in the b4.3b model. If we term that parameter from b4.3 as \\(\\beta^\\text{b4.3}\\) and the one from our new model \\(\\beta^\\text{b4.3b}\\), it turns out that \\(\\beta^\\text{b4.3} = \\exp \\left (\\beta^\\text{b4.3b} \\right )\\).\n\nfixef(b4.3)[\"weight_c\", \"Estimate\"]\n\n[1] 0.9044572\n\nfixef(b4.3b)[\"lb_Intercept\", \"Estimate\"] |&gt; exp()\n\n[1] 0.9028642\n\n\nThey’re the same within simulation variance.\n\n\n\n4.4.3 Interpreting the posterior distribution\n“One trouble with statistical models is that they are hard to understand” (p. 99). Welcome to the world of applied statistics, friends. 😅\n\n4.4.3.0.1 Rethinking: What do parameters mean?\n\nA basic issue with interpreting model-based estimates is in knowing the meaning of parameters. There is no consensus about what a parameter means, however, because different people take different philosophical stances towards models, probability, and prediction. The perspective in this book is a common Bayesian perspective: Posterior probabilities of parameter values describe the relative compatibility of different states of the world with the data, according to the model. (p. 99, emphasis in the original)\n\n\n\n4.4.3.1 Tables of marginal distributions\nWith a little [] subsetting we can exclude the log posterior from our summary for b4.3.\n\nposterior_summary(b4.3)[1:3, ] |&gt; \n  round(digits = 2)\n\n            Estimate Est.Error   Q2.5  Q97.5\nb_Intercept   154.60      0.27 154.06 155.12\nb_weight_c      0.90      0.04   0.82   0.99\nsigma           5.11      0.20   4.73   5.53\n\n\nIf we put our brms fit into the vcov() function, we’ll get the variance/covariance matrix of the intercept and weight_c coefficient.\n\nvcov(b4.3) |&gt; \n  round(3)\n\n          Intercept weight_c\nIntercept     0.075    0.000\nweight_c      0.000    0.002\n\n\nNo \\(\\sigma\\), however. To get that, we’ll have to extract the posterior draws and use the cov() function, instead.\n\nas_draws_df(b4.3) |&gt;\n  select(b_Intercept:sigma) |&gt;\n  cov() |&gt;\n  round(digits = 3)\n\n            b_Intercept b_weight_c sigma\nb_Intercept       0.075      0.000 0.000\nb_weight_c        0.000      0.002 0.000\nsigma             0.000      0.000 0.039\n\n\nThe pairs() function will work for a brms fit much like it would one from rethinking. It will show “both the marginal posteriors and the covariance” (p. 100).\n\npairs(b4.3)\n\n\n\n\n\n\n\n\n\n\n4.4.3.2 Plotting posterior inference against the data\n“It’s almost always much more useful to plot the posterior inference against the data. Not only does plotting help in interpreting the posterior, but it also provides an informal check on model assumptions” (p. 100).\nHere is the code for Figure 4.6. Note our use of the fixef() function within geom_abline().\n\nd2 |&gt;\n  ggplot(aes(x = weight_c, y = height)) +\n  geom_abline(intercept = fixef(b4.3)[1], \n              slope     = fixef(b4.3)[2]) +\n  geom_point(color = \"royalblue\", shape = 1, size = 2) +\n  theme_classic()\n\n\n\n\n\n\n\n\nNote how the breaks on our \\(x\\)-axis look off. That’s because we fit the model with weight_c and we plotted the points in that metric, too. Since we computed weight_c by subtracting the mean of weight from the data, we can adjust the \\(x\\)-axis break point labels by simply adding that value back.\n\nlabels &lt;- c(-10, 0, 10) + mean(d2$weight) |&gt; \n  round(digits = 0)\n\nd2 |&gt;\n  ggplot(aes(x = weight_c, y = height)) +\n  geom_abline(intercept = fixef(b4.3)[1], \n              slope     = fixef(b4.3)[2]) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  scale_x_continuous(\"weight\",\n                     breaks = c(-10, 0, 10),\n                     labels = labels) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n4.4.3.3 Adding uncertainty around the mean\nBe default, we extract all the posterior draws with as_draws_df().\n\npost &lt;- as_draws_df(b4.3)\n\npost |&gt;\n  slice(1:5)  # This serves a similar function as `head()`\n\n# A draws_df: 5 iterations, 1 chains, and 6 variables\n  b_Intercept b_weight_c sigma Intercept lprior  lp__\n1         155       0.88   5.2       155   -9.3 -1079\n2         154       0.96   5.3       154   -9.4 -1080\n3         155       0.85   4.9       155   -9.3 -1080\n4         155       0.98   5.2       155   -9.4 -1081\n5         154       0.99   5.6       154   -9.5 -1087\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nHere are the four models leading up to McElreath’s Figure 4.7.\n\nN &lt;- 10\n\nb4.3_010 &lt;- brm(\n  data = d2 |&gt;\n    slice(1:N),  # Note our tricky use of `N` and `slice()`\n  family = gaussian,\n  height ~ 1 + weight_c,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(lognormal(0, 1), class = b),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.03_010\")\n\nN &lt;- 50\n\nb4.3_050 &lt;- brm(\n  data = d2 |&gt;\n    slice(1:N), \n  family = gaussian,\n  height ~ 1 + weight_c,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(lognormal(0, 1), class = b),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.03_050\")\n\nN &lt;- 150\n\nb4.3_150 &lt;- brm(\n  data = d2 |&gt;\n    slice(1:N), \n  family = gaussian,\n  height ~ 1 + weight_c,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(lognormal(0, 1), class = b),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.03_150\")\n\nN &lt;- 352\n\nb4.3_352 &lt;- brm(\n  data = d2 |&gt;\n    slice(1:N), \n  family = gaussian,\n  height ~ 1 + weight_c,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(lognormal(0, 1), class = b),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.03_352\")\n\nI’m not going to clutter up the document with all the trace plots and coefficient summaries from these four models. But here’s how to get that information.\n\nplot(b4.3_010)\nprint(b4.3_010)\n\nplot(b4.3_050)\nprint(b4.3_050)\n\nplot(b4.3_150)\nprint(b4.3_150)\n\nplot(b4.3_352)\nprint(b4.3_352)\n\nWe’ll need to put the chains of each model into data frames.\n\npost010 &lt;- as_draws_df(b4.3_010)\npost050 &lt;- as_draws_df(b4.3_050)\npost150 &lt;- as_draws_df(b4.3_150)\npost352 &lt;- as_draws_df(b4.3_352)\n\nHere is the code for the four individual plots.\n\np1 &lt;- ggplot(data =  d2[1:10, ], \n             aes(x = weight_c, y = height)) +\n  geom_abline(data = post010 |&gt; slice(1:20),\n              aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = 0.3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2$weight_c),\n                  ylim = range(d2$height)) +\n  labs(subtitle = \"N = 10\")\n\np2 &lt;- ggplot(data =  d2[1:50, ], \n             aes(x = weight_c, y = height)) +\n  geom_abline(data = post050 |&gt; slice(1:20),\n              aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = 0.3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2$weight_c),\n                  ylim = range(d2$height)) +\n  labs(subtitle = \"N = 50\")\n\np3 &lt;- ggplot(data =  d2[1:150, ], \n             aes(x = weight_c, y = height)) +\n  geom_abline(data = post150 |&gt; slice(1:20),\n              aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = 0.3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2$weight_c),\n                  ylim = range(d2$height)) +\n  labs(subtitle = \"N = 150\")\n\np4 &lt;- ggplot(data =  d2[1:352, ], \n             aes(x = weight_c, y = height)) +\n  geom_abline(data = post352 |&gt; slice(1:20),\n              aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = 0.3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2$weight_c),\n                  ylim = range(d2$height)) +\n  labs(subtitle = \"N = 352\")\n\nNow we can combine the ggplots with patchwork syntax to make the full version of Figure 4.7.\n\n(p1 + p2 + p3 + p4) &\n  scale_x_continuous(\"weight\",\n                     breaks = c(-10, 0, 10),\n                     labels = labels) &\n  theme_classic()\n\n\n\n\n\n\n\n\n“Notice that the cloud of regression lines grows more compact as the sample size increases. This is a result of the model growing more confident about the location of the mean” (p. 102).\n\n\n4.4.3.4 Plotting regression intervals and contours\nSince we used weight_c to fit our model, we might first want to understand what exactly the mean value is for weight.\n\nmean(d2$weight)\n\n[1] 44.99049\n\n\nJust a hair under 45. If we’re interested in \\(\\mu\\) at weight = 50, that implies we’re also interested in \\(\\mu\\) at weight_c + 5.01. Within the context of our model, we compute this with \\(\\alpha + \\beta \\cdot 5.01\\). Here’s what that looks like with post.\n\nmu_at_50 &lt;- post |&gt; \n  transmute(mu_at_50 = b_Intercept + b_weight_c * 5.01)\n \nhead(mu_at_50)\n\n# A tibble: 6 × 1\n  mu_at_50\n     &lt;dbl&gt;\n1     159.\n2     159.\n3     159.\n4     160.\n5     159.\n6     159.\n\n\nAnd here is a version McElreath’s Figure 4.8 density plot.\n\nmu_at_50 |&gt;\n  ggplot(aes(x = mu_at_50)) +\n  geom_density(linewidth = 0, fill = \"royalblue\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(mu[\"height | weight = 50\"])) +\n  theme_classic()\n\n\n\n\n\n\n\n\nWe’ll use mean_hdi() to get both 89% and 95% HPDIs along with the mean.\n\nmean_hdi(mu_at_50[, 1], .width = c(0.89, 0.95))\n\n# A tibble: 2 × 6\n  mu_at_50 .lower .upper .width .point .interval\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1     159.   159.   160.   0.89 mean   hdi      \n2     159.   158.   160.   0.95 mean   hdi      \n\n\nIf you wanted to express those sweet 95% HPDIs on your density plot, you might use tidybayes::stat_halfeye(). Since stat_halfeye() also returns a point estimate, we’ll throw in the mode.\n\nmu_at_50 |&gt;\n  ggplot(aes(x = mu_at_50, y = 0)) +\n  stat_halfeye(point_interval = mode_hdi, .width = 0.95,\n               fill = \"royalblue\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(mu[\"height | weight = 50\"])) +\n  theme_classic()\n\n\n\n\n\n\n\n\nWith brms, you would use fitted() to do what McElreath accomplished with link().\n\nmu &lt;- fitted(b4.3, summary = F)\n\nstr(mu)\n\n num [1:4000, 1:352] 157 157 157 157 157 ...\n\n\nWhen you specify summary = F, fitted() returns a matrix of values with as many rows as there were post-warmup draws across your HMC chains and as many columns as there were cases in your analysis. Because we had 4,000 post-warmup draws and \\(n = 352\\), fitted() returned a matrix of 4,000 rows and 352 vectors. If you omitted the summary = F argument, the default is TRUE and fitted() will return summary information instead.\nMuch like rethinking’s link(), brms::fitted() can accommodate custom predictor values with its newdata argument.\n\nweight_seq &lt;- tibble(weight = 25:70) |&gt; \n  mutate(weight_c = weight - mean(d2$weight))\n\nmu &lt;- fitted(b4.3,\n             summary = F,\n             newdata = weight_seq) |&gt;\n  data.frame() |&gt;\n  # Here we name the columns after the `weight` values from which they were computed\n  set_names(25:70) |&gt; \n  mutate(iter = row_number())\n\nAnticipating ggplot2, we went ahead and converted the output to a data frame. But we might do a little more data processing with the aid of tidyr::pivot_longer(), which will convert the data from the wide format to the long format. If you are new to the distinction between wide and long data, you can learn more from the Pivot data from wide to long vignette from the tidyverse team (2020); Simon Ejdemyr’s blog post, Wide & long data; or Karen Grace-Martin’s blog post, The wide and long data format for repeated measures data.\n\nmu &lt;- mu |&gt;\n  pivot_longer(-iter,\n               names_to = \"weight\",\n               values_to = \"height\") |&gt; \n  # We might reformat `weight` to numerals\n  mutate(weight = as.numeric(weight))\n\nhead(mu)\n\n# A tibble: 6 × 3\n   iter weight height\n  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1     25   137.\n2     1     26   138.\n3     1     27   139.\n4     1     28   140.\n5     1     29   141.\n6     1     30   142.\n\n\nNow our data processing is done, here we reproduce McElreath’s Figure 4.9.a.\n\nd2 |&gt;\n  ggplot(aes(x = weight, y = height)) +\n  geom_point(data = mu |&gt; filter(iter &lt; 101), \n             alpha = 0.05, color = \"navyblue\") +\n  coord_cartesian(xlim = c(30, 65)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWith fitted(), it’s quite easy to plot a regression line and its intervals. Just omit the summary = T argument.\n\nmu_summary &lt;- fitted(b4.3, newdata = weight_seq) |&gt;\n  data.frame() |&gt;\n  bind_cols(weight_seq)\n\nhead(mu_summary)\n\n  Estimate Est.Error     Q2.5    Q97.5 weight  weight_c\n1 136.5198 0.8960037 134.7233 138.2592     25 -19.99049\n2 137.4242 0.8557114 135.7132 139.0705     26 -18.99049\n3 138.3287 0.8156337 136.6969 139.8883     27 -17.99049\n4 139.2331 0.7758038 137.6715 140.7152     28 -16.99049\n5 140.1376 0.7362619 138.6654 141.5488     29 -15.99049\n6 141.0421 0.6970569 139.6356 142.3799     30 -14.99049\n\n\nHere it is, our analogue to Figure 4.9.b.\n\nd2 |&gt;\n  ggplot(aes(x = weight, y = height)) +\n  geom_smooth(data = mu_summary,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(color = \"navyblue\", shape = 1, linewidth = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2$weight)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIf you wanted to use intervals other than the default 95% ones, you’d include the probs argument like this: fitted(b4.3, newdata = weight.seq, probs = c(0.25, 0.75)). The resulting third and fourth vectors from the fitted() object would be named Q25 and Q75 instead of the default Q2.5 and Q97.5. The Q prefix stands for quantile.\n\n4.4.3.4.1 Rethinking: Overconfident intervals\n\nThe compatibility interval for the regression line in Figure 4.9 clings tightly to the MAP line. Thus there is very little uncertainty about the average height as a function of average weight. But you have to keep in mind that these inferences are always conditional on the model. Even a very bad model can have very tight compatibility intervals. It may help if you think of the regression line in Figure 4.9 as saying: Conditional on the assumption that height and weight are related by a straight line, then this is the most plausible line, and these are its plausible bounds. (p. 107, emphasis in the original)\n\n\n\n4.4.3.4.2 Overthinking: How link fitted() works\nSimilar to rethinking::link(), brms::fitted() uses the formula from your model to compute the model expectations for a given set of predictor values. I use it a lot in this project. If you follow along, you’ll get a good handle on it. But to dive deeper, you can go here for the documentation. Though we won’t be using it in this project, brms users might want to know that fitted() is also an alias for the posterior_epred() function, about which you might learn more here. Users can always learn more about them and other functions in the brms reference manual.\n\n\n\n4.4.3.5 Prediction intervals\nEven though our statistical model (omitting priors for the sake of simplicity) is\n\\[\\text{height}_i \\sim \\operatorname{Normal}(\\mu_i = \\alpha + \\beta x_, \\sigma),\\]\nwe’ve only been plotting the \\(\\mu\\) part. In order to bring in the variability expressed by \\(\\sigma\\), we’ll have to switch to the predict() function. Much as brms::fitted() was our analogue to rethinking::link(), brms::predict() is our analogue to rethinking::sim().\nWe can reuse our weight_seq data from before. But in case you forgot, here’s that code again.\n\nweight_seq &lt;- tibble(weight = 25:70) |&gt; \n  mutate(weight_c = weight - mean(d2$weight))\n\nThe predict() code looks a lot like what we used for fitted().\n\npred_height &lt;- predict(b4.3, newdata = weight_seq) |&gt;\n  data.frame() |&gt;\n  bind_cols(weight_seq)\n  \npred_height |&gt;\n  slice(1:6)\n\n  Estimate Est.Error     Q2.5    Q97.5 weight  weight_c\n1 136.5864  5.183359 126.5446 146.9381     25 -19.99049\n2 137.2358  5.143941 127.0908 147.1680     26 -18.99049\n3 138.3768  5.251728 128.1949 148.9343     27 -17.99049\n4 139.2185  5.142894 129.2181 149.4509     28 -16.99049\n5 139.9605  5.166576 129.9273 150.0306     29 -15.99049\n6 140.9982  5.089318 130.7314 151.1074     30 -14.99049\n\n\nThis time the summary information in our data frame is for, as McElreath put it, “simulated heights, not distributions of plausible average height, \\(\\mu\\)” (p. 108). Another way of saying that is that these simulations are the joint consequence of both \\(\\mu\\) and \\(\\sigma\\), unlike the results of fitted(), which only reflect \\(\\mu\\). Figure 4.10 shows how you might visualize them.\n\nd2 |&gt;\n  ggplot(aes(x = weight)) +\n  geom_ribbon(data = pred_height, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = mu_summary,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2$weight),\n                  ylim = range(d2$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nNotice that the outline for the wide shaded interval is a little rough. This is the simulation variance in the tails of the sampled Gaussian values. If it really bothers you, increase the number of samples you take from the posterior distribution. (p. 109)\n\nWith our brms model fitting approach, that would mean we’d have to refit b4.3 after specifying a larger number of post-warmup iterations with alterations to the iter and warmup parameters.\n\n4.4.3.5.1 Overthinking: Rolling your own sim predict()\nHere we follow McElreath’s example and do our model-based predictions by hand. Instead of relying on base R apply() and sapply(), here the main action is in expand_grid(), the second mutate() line and the group_by() + summarise() combination.\n\n# `predict()` by hand\nset.seed(4)\n\npost |&gt; \n  expand_grid(weight = 25:70) |&gt; \n  mutate(weight_c = weight - mean(d2$weight)) |&gt; \n  mutate(sim_height = rnorm(n(),\n                            mean = b_Intercept + b_weight_c * weight_c,\n                            sd   = sigma)) |&gt; \n  group_by(weight) |&gt; \n  summarise(mean = mean(sim_height),\n            ll   = quantile(sim_height, prob = 0.025),\n            ul   = quantile(sim_height, prob = 0.975)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = weight)) +\n  geom_smooth(aes(y = mean, ymin = ll, ymax = ul),\n              stat = \"identity\",\n              fill = \"grey83\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(data = d2,\n             aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2$weight),\n                  ylim = range(d2$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nWe specifically left out the fitted() intervals to make it more apparent what we were simulating. You might also note that we could have easily replaced that three-line summarise() code with a single line of tidybayes::mean_qi(sim_height), or whatever combination of central tendency and interval type you wanted (e.g., mode_hdi(sim_height, .width = 0.89)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "04.html#curves-from-lines",
    "href": "04.html#curves-from-lines",
    "title": "4  Geocentric Models",
    "section": "4.5 Curves from lines",
    "text": "4.5 Curves from lines\n“The models so far all assume that a straight line describes the relationship. But there’s nothing special about straight lines, aside from their simplicity.” (p. 110).\n\n4.5.1 Polynomial regression\nRemember d?\n\nd |&gt;\n  glimpse()\n\nRows: 544\nColumns: 4\n$ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 147.9550, 165.1000, 154.3050, 151…\n$ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, 34.86988, 54.48774, 49.89512, 41.…\n$ age    &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20.0, 65.3, 36.0, 44.0, 31.0, 12.0,…\n$ male   &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,…\n\n\nMcElreath suggested we plot height against weight using the full sample.\n\nd |&gt; \n  ggplot(aes(x = weight, y = height)) +\n  geom_point(color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  annotate(geom = \"text\",\n           x = 42, y = 115,\n           label = \"This relation is\\nvisibly curved.\",\n           family = \"Times\") +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThose variables two appear to follow an orderly relation, but whatever it is, it’s clearly not a simple straight line. The quadratic model is probably the most commonly used polynomial regression model. It follows the generic form\n\\[\\mu = \\alpha + \\beta_1 x_i + \\color{navy}{\\beta_2 x_i^2}.\\]\nMcElreath warned: “Fitting these models to data is easy. Interpreting them can be hard” (p. 111). Standardizing will help brm() fit the model. We might standardize our weight variable like so.\n\nd &lt;- d |&gt;\n  mutate(weight_s = (weight - mean(weight)) / sd(weight)) |&gt; \n  mutate(weight_s2 = weight_s^2)\n\nWhile we were at it, we just went ahead and computed the weight_s2 variable. We can express our statistical model as\n\\[\\begin{align*}\n\\text{height}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\text{weight_s}_i + \\color{navy}{\\beta_2 \\text{weight_s}^2_i} \\\\\n\\alpha  & \\sim \\operatorname{Normal}(178, 20) \\\\\n\\beta_1 & \\sim \\operatorname{Log-Normal}(0, 1) \\\\\n\\color{navy}{\\beta_2} & \\color{navy}\\sim \\color{navy}{\\operatorname{Normal}(0, 1)} \\\\\n\\sigma  & \\sim \\operatorname{Uniform}(0, 50).\n\\end{align*}\\]\nHere’s how we might fit the quadratic model with brms.\n\nb4.5 &lt;- brm(\n  data = d, \n  family = gaussian,\n  height ~ 1 + weight_s + weight_s2,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n            prior(normal(0, 1), class = b, coef = \"weight_s2\"),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.05\")\n\nNote our use of the coef argument within our prior statements. Since \\(\\beta_1\\) and \\(\\beta_2\\) are both parameters of class = b within the brms set-up, we need to use the coef argument when we want their priors to differ.\n\nplot(b4.5, widths = c(1, 2))\n\n\n\n\n\n\n\nprint(b4.5)\n\n Family: gaussian \n  Links: mu = identity \nFormula: height ~ 1 + weight_s + weight_s2 \n   Data: d (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   146.05      0.37   145.32   146.76 1.00     3313     3090\nweight_s     21.73      0.29    21.15    22.32 1.00     2859     2957\nweight_s2    -7.80      0.28    -8.34    -7.24 1.00     3017     3089\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.80      0.18     5.45     6.15 1.00     3416     2744\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur quadratic plot requires new fitted()- and predict()-oriented wrangling.\n\nweight_seq &lt;- tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) |&gt; \n  mutate(weight_s2 = weight_s^2)\n\nfitd_quad &lt;- fitted(b4.5, newdata = weight_seq) |&gt;\n  data.frame() |&gt;\n  bind_cols(weight_seq)\n\npred_quad &lt;- predict(b4.5, newdata = weight_seq) |&gt;\n  data.frame() |&gt;\n  bind_cols(weight_seq)  \n\nBehold the code for our version of Figure 4.11.b.\n\np2 &lt;- ggplot(data = d, \n             aes(x = weight_s)) +\n  geom_ribbon(data = pred_quad, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_quad,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  labs(y = \"height\",\n       subtitle = \"quadratic\") +\n  coord_cartesian(xlim = range(d$weight_s),\n                  ylim = range(d$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\np2\n\n\n\n\n\n\n\n\nFrom a formula perspective, the cubic model is a simple extension of the quadratic:\n\\[\\mu = \\alpha + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3.\\]\nBefore we fit the model, we need to wrangle the data again.\n\nd &lt;- d |&gt; \n  mutate(weight_s3 = weight_s^3)\n\nNow fit the model like so.\n\nb4.6 &lt;- brm(\n  data = d, \n  family = gaussian,\n  height ~ 1 + weight_s + weight_s2 + weight_s3,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n            prior(normal(0, 1), class = b, coef = \"weight_s2\"),\n            prior(normal(0, 1), class = b, coef = \"weight_s3\"),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.06\")\n\nAnd now we’ll fit the good old linear model.\n\nb4.7 &lt;- brm(\n  data = d, \n  family = gaussian,\n  height ~ 1 + weight_s,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.07\")\n\nHere’s the fitted(), predict(), and ggplot2 code for Figure 4.11.c, the cubic model.\n\nweight_seq &lt;- weight_seq |&gt; \n  mutate(weight_s3 = weight_s^3)\n\nfitd_cub &lt;- fitted(b4.6, newdata = weight_seq) |&gt;\n  data.frame() |&gt;\n  bind_cols(weight_seq)\n\npred_cub &lt;- predict(b4.6, newdata = weight_seq) |&gt;\n  data.frame() |&gt;\n  bind_cols(weight_seq) \n\np3 &lt;- ggplot(data = d, \n             aes(x = weight_s)) +\n  geom_ribbon(data = pred_cub, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_cub,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/4) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  labs(y = \"height\",\n       subtitle = \"cubic\") +\n  coord_cartesian(xlim = range(d$weight_s),\n                  ylim = range(d$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\np3\n\n\n\n\n\n\n\n\nAnd here’s the fitted(), predict(), and ggplot2 code for Figure 4.11.a, the linear model.\n\nfitd_line &lt;- fitted(b4.7, newdata = weight_seq) |&gt;\n  data.frame() |&gt;\n  bind_cols(weight_seq)\n\npred_line &lt;- predict(b4.7, newdata = weight_seq) |&gt;\n  data.frame() |&gt;\n  bind_cols(weight_seq) \n\np1 &lt;- ggplot(data = d, \n             aes(x = weight_s)) +\n  geom_ribbon(data = pred_line, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_line,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/4) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  labs(y = \"height\",\n       subtitle = \"linear\") +\n  coord_cartesian(xlim = range(d$weight_s),\n                  ylim = range(d$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\np1\n\n\n\n\n\n\n\n\nDid you notice how we labeled each of the past three plots as p1, p2, and p3? Here we use those names to plot them all together with patchwork syntax.\n\np1 | p2 | p3\n\n\n\n\n\n\n\n\nAs fun as this all has been,\n\nit’s not clear that any of these models make a lot of sense. They are good geocentric descriptions of the sample, yes. But there are two problems. First, a better fit to the sample might not actually be a better model. That’s the subject of Chapter 7. Second, the model contains no biological information. We aren’t learning any causal relationship between height and weight. We’ll deal with this second problem much later, in Chapter 16. (p. 113)\n\n\n4.5.1.0.1 Overthinking: Converting back to natural scale\nYou can apply McElreath’s conversion trick within the ggplot2 environment, too. Here it is with the cubic model.\n\nat &lt;- c(-2, -1, 0, 1, 2)\n\nggplot(data = d, \n       aes(x = weight_s)) +\n  geom_ribbon(data = pred_cub, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_point(aes(y = height),\n             alpha = 1/3, color = \"navyblue\", shape = 1, size = 1.5) +\n  coord_cartesian(xlim = range(d$weight_s)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank()) +\n  # Here it is!\n  scale_x_continuous(\"standardized weight converted back\",\n                     breaks = at,\n                     labels = round(at*sd(d$weight) + mean(d$weight), 1))\n\n\n\n\n\n\n\n\n\n\n\n4.5.2 Splines\nLoad the cherry_blossoms data (Aono, 2012; Aono & Kazui, 2008; Aono & Saito, 2010).\n\nlibrary(rethinking)\n\ndata(cherry_blossoms)\nd &lt;- cherry_blossoms\nrm(cherry_blossoms)\ndetach(package:rethinking, unload = T)\n\nMinus the mini histograms, here is our ground-up tidyverse way to summarize our new d data the way McElreath did with his precis().\n\nd |&gt; \n  pivot_longer(everything())  |&gt; \n  group_by(name) |&gt; \n  summarise(mean = mean(value, na.rm = T),\n            sd   = sd(value, na.rm = T),\n            ll   = quantile(value, prob = 0.055, na.rm = T),\n            ul   = quantile(value, prob = 0.945, na.rm = T)) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 5 × 5\n  name          mean     sd     ll      ul\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 doy         105.     6.41  94.4   115   \n2 temp          6.14   0.66   5.15    7.29\n3 temp_lower    5.1    0.85   3.79    6.37\n4 temp_upper    7.19   0.99   5.9     8.9 \n5 year       1408    351.   868.   1948.  \n\n\nMcElreath encouraged us to plot doy against year.\n\nd |&gt; \n  ggplot(aes(x = year, y = doy)) +\n  # Color from here: https://www.colorhexa.com/ffb7c5\n  geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        # Color from here: https://www.colordic.org/w/, inspired by https://chichacha.netlify.com/2018/11/29/plotting-traditional-colours-of-japan/\n        panel.background = element_rect(fill = \"#4f455c\"))\n\n\n\n\n\n\n\n\nIt looks like there are some wiggly trends, but it’s hard to tell with a scatter plot.\n\nOur goal is to approximate the blossom trend with a wiggly function. With B-splines, just like with polynomial regression, we do this by generating new predictor variables and using those in the linear model, \\(\\mu_i\\). Unlike polynomial regression, B-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a basis function. The linear model ends up looking very familiar:\n\\[\\mu_i = \\alpha + w_1 B_{i, 1} + w_2 B_{i, 2} + w_3 B_{i, 3} + \\dots\\]\nwhere \\(B_{i,n}\\) is the \\(n\\)-th basis function’s value on row \\(i\\), and the \\(w\\) parameters are corresponding weights for each. The parameters act like slopes, adjusting the influence of each basis function on the mean \\(\\mu_i\\). So really this is just another linear regression, but with some fancy, synthetic predictor variables. (p. 115, emphasis in the original)\n\nIt turns out there are cases with missing data for the doy variable.\n\nd |&gt; \n  count(is.na(doy)) |&gt; \n  mutate(percent = 100 * n / sum(n))\n\n  is.na(doy)   n  percent\n1      FALSE 827 68.06584\n2       TRUE 388 31.93416\n\n\nLet’s follow McElreath and make a subset of the data that excludes cases with missing data in doy. Within the tidyverse, we might do so with the tidyr::drop_na() function.\n\nd2 &lt;- d |&gt; \n  drop_na(doy)\n\nOn page 117 in the text, McElreath indirectly explained how to make Figure 4.12 by walking through the workflow for making Figure 4.13. Here we mimic that ordering.\n\nFirst, we choose the knots. Remember, the knots are just values of year that serve as pivots for our spline. Where should the knots go? There are different ways to answer this question. You can, in principle, put the knots wherever you like. Their locations are part of the model, and you are responsible for them. Let’s do what we did in the simple example above, place the knots at different evenlyspaced quantiles of the predictor variable. This gives you more knots where there are more observations. We used only 5 knots in the first example. Now let’s go for 15:\n\n\nnum_knots &lt;- 15\nknot_list &lt;- quantile(d2$year, probs = seq(from = 0, to = 1, length.out = num_knots))\n\nOur knot_list contains 15 year values.\n\nknot_list\n\n       0% 7.142857% 14.28571% 21.42857% 28.57143% 35.71429% 42.85714%       50% 57.14286% 64.28571% 71.42857% 78.57143% 85.71429% \n      812      1036      1174      1269      1377      1454      1518      1583      1650      1714      1774      1833      1893 \n92.85714%      100% \n     1956      2015 \n\n\nHere’s what it looks like if we use those knot_list values to chop up our year/doy scatter plot, from above.\n\nd |&gt; \n  ggplot(aes(x = year, y = doy)) +\n  geom_vline(xintercept = knot_list, alpha = 1/2, color = \"white\") +\n  geom_point(alpha = 1/2, color = \"#ffb7c5\") +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nThe next choice is polynomial degree. This determines how basis functions combine, which determines how the parameters interact to produce the spline. For degree 1, as in Figure 4.12, two basis functions combine at each point. For degree 2, three functions combine at each point. For degree 3, four combine. R already has a nice function that will build basis functions for any list of knots and degree. This code will construct the necessary basis functions for a degree 3 (cubic) spline: (p. 117)\n\n\nlibrary(splines)\n\nB &lt;- bs(d2$year,\n        knots = knot_list[-c(1, num_knots)], \n        degree = 3, \n        intercept = TRUE)\n\nLook closely at McElreath’s tricky knot_list[-c(1, num_knots)] code. Whereas knot_list contains 15 ordered year values, McElreath shaved off the first and last year values with knot_list[-c(1, num_knots)], leaving 13. This is because, by default, the bs() function places knots at the boundaries. Since the first and 15th values in knot_list were boundary values for year, we removed them to avoid redundancies. We can confirm this with the code, below.\n\nB |&gt; str()\n\n 'bs' num [1:827, 1:17] 1 0.96 0.767 0.563 0.545 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr [1:17] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"degree\")= int 3\n - attr(*, \"knots\")= Named num [1:13] 1036 1174 1269 1377 1454 ...\n  ..- attr(*, \"names\")= chr [1:13] \"7.142857%\" \"14.28571%\" \"21.42857%\" \"28.57143%\" ...\n - attr(*, \"Boundary.knots\")= int [1:2] 812 2015\n - attr(*, \"intercept\")= logi TRUE\n\n\nLook at the second to last line, - attr(*, \"Boundary.knots\")= int [1:2] 812 2015. Those default \"Boundary.knots\" are the same as knot_list[c(1, num_knots)]. Let’s confirm.\n\nknot_list[c(1, num_knots)]\n\n  0% 100% \n 812 2015 \n\n\nBy the degree = 3 argument, we indicated we wanted a cubic spline. McElreath used degree = 1 for Figure 4.12. For reasons I’m not prepared to get into, here, splines don’t always include intercept parameters. Indeed, the bs() default is intercept = FALSE. McElreath’s code indicated he wanted to fit a B-spline that included an intercept. Thus: intercept = TRUE.\nHere’s how we might make our version of the top panel of Figure 4.13.\n\n# Wrangle a bit\nb &lt;- B |&gt; \n  data.frame() |&gt; \n  set_names(str_c(0, 1:9), 10:17) |&gt;  \n  bind_cols(select(d2, year)) |&gt; \n  pivot_longer(-year,\n               names_to = \"bias_function\",\n               values_to = \"bias\")\n\n# Plot\nb |&gt; \n  ggplot(aes(x = year, y = bias, group = bias_function)) +\n  geom_vline(xintercept = knot_list, alpha = 1/2, color = \"white\") +\n  geom_line(alpha = 1/2, color = \"#ffb7c5\", linewidth = 1.5) +\n  ylab(\"bias value\") +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nTo elucidate what’s going on in that plot, we might break it up with facet_wrap().\n\nb |&gt; \n  mutate(bias_function = str_c(\"bias function \", bias_function)) |&gt; \n  \n  ggplot(aes(x = year, y = bias)) +\n  geom_vline(xintercept = knot_list, alpha = 1/2, color = \"white\") +\n  geom_line(color = \"#ffb7c5\", linewidth = 1.5) +\n  ylab(\"bias value\") +\n  facet_wrap(~ bias_function, ncol = 1) +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = scales::alpha(\"#ffb7c5\", .25), color = \"transparent\"),\n        strip.text = element_text(size = 8, margin = margin(0.1, 0, 0.1, 0, \"cm\")))\n\n\n\n\n\n\n\n\n\nNow to get the parameter weights for each basis function, we need to actually define the model and make it run. The model is just a linear regression. The synthetic basis functions do all the work. We’ll use each column of the matrix B as a variable. We’ll also have an intercept to capture the average blossom day. This will make it easier to define priors on the basis weights, because then we can just conceive of each as a deviation from the intercept. (p. 117)\n\nThat last line is another indication for why we set intercept = TRUE. Our model will follow the form\n\\[\\begin{align*}\n\\text{day_in_year}_i & \\sim \\operatorname{Normal} (\\mu_i, \\sigma) \\\\\n\\mu_i  & = \\alpha + \\color{#4f455c}{\\sum_{k=1}^K w_k B_{k, i}} \\\\\n\\alpha & \\sim \\operatorname{Normal}(100, 10) \\\\\n\\color{#4f455c}{w_j} & \\color{#4f455c}\\sim \\color{#4f455c}{\\operatorname{Normal}(0, 10)} \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1),\n\\end{align*}\\]\nwhere \\(\\alpha\\) is the intercept, \\(B_{k, i}\\) is the value of the \\(k^\\text{th}\\) bias function on the \\(i^\\text{th}\\) row of the data, and \\(w_k\\) is the estimated regression weight for the corresponding \\(k^\\text{th}\\) bias function.\nAs for the new parameter type for \\(\\sigma\\), the exponential distribution is controlled by a single parameter, \\(\\lambda\\), which is also called the rate. As it turns out, the mean of the exponential distribution is the inverse of the rate, \\(1 / \\lambda\\). Here we use the dexp() function to get a sense of what that prior looks like.\n\ntibble(x = seq(from = 0, to = 10, by = 0.1)) |&gt; \n  mutate(d = dexp(x, rate = 1)) |&gt; \n  \n  ggplot(aes(x = x, y = d)) +\n  geom_area(fill = \"#ffb7c5\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n“We’ll use exponential priors for the rest of the book, in place of uniform priors. It is much more common to have a sense of the average deviation than of the maximum” (p. 119). 🎉\nAcknowledgment: The workflow to follow is heavily influenced by the helpful contributions from Stephen Wild. My first pass through the material in this section was a mess. Wild’s insights knocked it out of the park and I couldn’t be more grateful. 🍻\nBefore fitting this model in brms, well take a minor detour on the data structure. In his R code 4.76, McElreath defined his data in a list, list( D=d2$doy , B=B ). Our approach will be a little different. Here, we’ll add the B matrix to our d2 data frame and name the results as d3.\n\nd3 &lt;- d2 |&gt; \n  mutate(B = B) \n\n# Take a look at the structure of `d3\nd3 |&gt; glimpse()\n\nRows: 827\nColumns: 6\n$ year       &lt;int&gt; 812, 815, 831, 851, 853, 864, 866, 869, 889, 891, 892, 894, 895, 896, 902, 908, 912, 913, 917, 923, 926, 930,…\n$ doy        &lt;int&gt; 92, 105, 96, 108, 104, 100, 106, 95, 104, 109, 108, 106, 104, 104, 102, 98, 95, 110, 95, 104, 98, 97, 106, 10…\n$ temp       &lt;dbl&gt; NA, NA, NA, 7.38, NA, 6.42, 6.44, NA, 6.83, 6.98, 7.11, 6.98, 7.08, 7.20, 7.50, 7.26, 6.78, 6.61, 6.48, 6.76,…\n$ temp_upper &lt;dbl&gt; NA, NA, NA, 12.10, NA, 8.69, 8.11, NA, 8.48, 8.96, 9.11, 8.40, 8.57, 8.69, 8.95, 8.89, 8.83, 8.59, 10.21, 8.5…\n$ temp_lower &lt;dbl&gt; NA, NA, NA, 2.66, NA, 4.14, 4.77, NA, 5.19, 5.00, 5.11, 5.55, 5.58, 5.72, 6.06, 5.62, 4.74, 4.63, 2.76, 5.01,…\n$ B          &lt;bs[,17]&gt; &lt;bs[43 x 17]&gt;\n\n\nIn our d3 data, columns year through temp_lower are all standard data columns. The B column is a matrix column, which contains the same number of rows as the others, but also smuggled in 17 columns within that column. Each of those 17 columns corresponds to one of our synthetic \\(B_k\\) variables. The advantage of such a data structure is we can simply define our formula argument as doy ~ 1 + B, where B is a stand-in for B.1 + B.2 + ... + B.17.\nHere’s how to fit the model.\n\nb4.8 &lt;- brm(\n  data = d3,\n  family = gaussian,\n  doy ~ 1 + B,\n  prior = c(prior(normal(100, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.08\")\n\nHere’s the model summary.\n\nprint(b4.8)\n\n Family: gaussian \n  Links: mu = identity \nFormula: doy ~ 1 + B \n   Data: d3 (Number of observations: 827) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   103.45      2.34    98.73   107.85 1.01      837     1168\nB1           -3.03      3.80   -10.38     4.58 1.00     1766     2523\nB2           -0.97      3.82    -8.40     6.33 1.00     1648     2555\nB3           -1.10      3.60    -8.09     5.95 1.00     1549     2428\nB4            4.75      2.84    -0.75    10.37 1.00     1094     1771\nB5           -0.95      2.83    -6.48     4.62 1.00     1175     1797\nB6            4.19      2.90    -1.41     9.86 1.00     1158     1618\nB7           -5.38      2.80   -10.88     0.13 1.00     1177     2079\nB8            7.72      2.78     2.32    13.33 1.00     1136     1623\nB9           -1.09      2.88    -6.86     4.63 1.00     1141     1926\nB10           2.93      2.84    -2.38     8.53 1.00     1187     1735\nB11           4.53      2.88    -0.93    10.22 1.00     1112     1921\nB12          -0.21      2.80    -5.60     5.32 1.00     1148     1801\nB13           5.44      2.86    -0.09    11.00 1.00     1121     1880\nB14           0.62      2.96    -5.07     6.51 1.00     1283     2202\nB15          -0.91      3.28    -7.32     5.60 1.00     1273     2061\nB16          -7.09      3.35   -13.64    -0.58 1.00     1597     2440\nB17          -7.73      3.23   -13.83    -1.37 1.00     1333     2187\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.94      0.15     5.68     6.24 1.00     4396     2969\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLook at that. Each of the 17 columns in our B matrix was assigned its own parameter. If you fit this model using McElreath’s rethinking code, you’ll see the results are very similar. Anyway, McElreath’s comments are in line with the general consensus on spline modes: the parameter estimates are very difficult to interpret directly. It’s often easier to just plot the results. First we’ll use as_draws_df().\n\npost &lt;- as_draws_df(b4.8)\n\nglimpse(post)\n\nRows: 4,000\nColumns: 25\n$ b_Intercept &lt;dbl&gt; 103.6326, 102.7768, 101.9956, 102.1028, 103.7816, 103.7781, 103.8310, 103.1931, 103.8764, 103.9462, 104.3867…\n$ b_B1        &lt;dbl&gt; -2.515909346, -6.396048025, -1.757507933, -6.086758705, 1.851128494, -6.273272280, -1.211818462, -2.21875941…\n$ b_B2        &lt;dbl&gt; -2.4055377, 2.2012721, 4.5596578, 5.2578616, -4.5009004, -0.3570559, -1.0549341, -1.6574169, 4.1933156, 3.82…\n$ b_B3        &lt;dbl&gt; -0.40024205, -5.50521001, -2.06977008, -4.03419467, -1.28138806, -0.16702822, -3.51982460, -0.09964411, -3.2…\n$ b_B4        &lt;dbl&gt; 5.3729937, 7.7509294, 6.0321454, 8.0430014, 4.4963196, 2.6613670, 4.0942312, 4.4357873, 7.2099729, 5.2449064…\n$ b_B5        &lt;dbl&gt; -1.47992029, -1.27770929, -0.23437035, -0.79954431, -1.53183469, -0.04868177, -0.85925281, 0.17160434, -3.63…\n$ b_B6        &lt;dbl&gt; 3.8597801, 3.3251367, 6.9683564, 6.8291320, 2.6671390, 2.5246753, 2.3791679, 4.6908659, 3.4279361, 3.3564081…\n$ b_B7        &lt;dbl&gt; -7.481753, -5.007213, -3.709432, -5.683608, -6.016329, -6.134282, -3.815637, -8.509985, -4.773250, -4.470728…\n$ b_B8        &lt;dbl&gt; 8.864320, 8.082351, 5.834810, 9.806924, 7.020597, 4.419259, 6.922874, 10.679370, 5.877684, 6.100913, 4.84762…\n$ b_B9        &lt;dbl&gt; -3.88164332, 0.01321532, 2.65382118, 0.99134144, -1.22568314, 1.23484623, -0.87732817, -2.23629567, -1.70963…\n$ b_B10       &lt;dbl&gt; 5.7781130, 0.6891200, 6.0833892, 3.4788663, 3.2301990, 1.3230426, 0.6162188, 4.0433546, 2.7836713, 1.5450822…\n$ b_B11       &lt;dbl&gt; 2.4830288, 8.7237955, 5.9560858, 9.2057690, 4.9453953, 3.9439998, 3.7495933, 7.1866252, 1.2143245, 2.2764813…\n$ b_B12       &lt;dbl&gt; 2.06819542, -0.84632502, -0.38432707, -1.08361338, -0.96560203, 1.34327745, -0.02355109, -1.90679848, 1.8162…\n$ b_B13       &lt;dbl&gt; 3.8899015, 7.5382242, 8.4717539, 6.9990679, 5.4494915, 2.3500843, 3.2880073, 7.0499862, 5.5464476, 5.3166678…\n$ b_B14       &lt;dbl&gt; 0.77105989, -2.67141190, 0.37607076, 4.11485691, -1.35618392, 0.85713697, 3.73958110, -0.20379750, 0.5848544…\n$ b_B15       &lt;dbl&gt; -3.9644199, 3.6552835, 2.1946976, -3.3631965, -2.0234479, -2.7736611, -4.6967781, 2.2346964, 3.5460820, 3.20…\n$ b_B16       &lt;dbl&gt; -2.406126, -10.441109, -8.766467, -3.526741, -2.991139, -5.873885, -8.684549, -8.930950, -9.812842, -9.82334…\n$ b_B17       &lt;dbl&gt; -9.9622206, -2.6057366, -7.8648369, -9.0768901, -10.4270187, -4.8647174, -6.5644432, -7.2663082, -4.8501700,…\n$ sigma       &lt;dbl&gt; 6.054284, 5.985893, 5.961017, 5.893135, 6.102580, 5.749992, 5.874712, 6.000066, 6.038545, 5.998703, 5.895834…\n$ Intercept   &lt;dbl&gt; 104.5259, 104.2515, 104.4118, 104.5854, 104.4360, 104.2760, 104.3511, 104.6222, 104.9202, 104.8300, 104.3120…\n$ lprior      &lt;dbl&gt; -66.08493, -66.61194, -66.32614, -66.94808, -65.84397, -64.84325, -65.36278, -66.59327, -65.94448, -65.69334…\n$ lp__        &lt;dbl&gt; -2711.772, -2715.877, -2716.059, -2712.243, -2712.028, -2715.740, -2712.254, -2711.983, -2721.070, -2715.237…\n$ .chain      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ .iteration  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3…\n$ .draw       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3…\n\n\nWith a little wrangling, we can use summary information from post to make our version of the middle panel of Figure 4.13.\n\npost |&gt; \n  select(b_B1:b_B17) |&gt; \n  set_names(c(str_c(0, 1:9), 10:17)) |&gt; \n  pivot_longer(everything(), names_to = \"bias_function\") |&gt; \n  group_by(bias_function) |&gt; \n  summarise(weight = mean(value)) |&gt; \n  full_join(b, by = \"bias_function\") |&gt; \n  \n  # Plot\n  ggplot(aes(x = year, y = bias * weight, group = bias_function)) +\n  geom_vline(xintercept = knot_list, alpha = 1/2, color = \"white\") +\n  geom_line(alpha = 1/2, color = \"#ffb7c5\", linewidth = 1.5) +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank()) \n\n\n\n\n\n\n\n\nIn case you missed it, the main action in the ggplot2 code was y = bias * weight, where we defined the \\(y\\)-axis as the product of bias and weight. This is fulfillment of the \\(w_k B_{k, i}\\) parts of the model. Now here’s how we might use brms::fitted() to make the lower plot of Figure 4.13.\n\nf &lt;- fitted(b4.8)\n\nf |&gt; \n  data.frame() |&gt; \n  bind_cols(d2) |&gt; \n  \n  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + \n  geom_vline(xintercept = knot_list, alpha = 1/2, color = \"white\") +\n  geom_hline(yintercept = fixef(b4.8)[1, 1], color = \"white\", linetype = 2) +\n  geom_point(alpha = 1/2, color = \"#ffb7c5\") +\n  geom_ribbon(alpha = 2/3, fill = \"white\") +\n  labs(x = \"year\",\n       y = \"day in year\") +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIf it wasn’t clear, the dashed horizontal line intersecting a little above 100 on the \\(y\\)-axis is the posterior mean for the intercept. Now let’s use our skills to remake the simpler model expressed in Figure 4.12. This model, recall, is based on 5 knots.\n\n# Redo the `B` splines\nnum_knots &lt;- 5\nknot_list &lt;- quantile(d2$year, probs = seq(from = 0, to = 1, length.out = num_knots))\n\nB &lt;- bs(d2$year,\n        knots = knot_list[-c(1, num_knots)], \n        # this makes the splines liner rater than cubic\n        degree = 1, \n        intercept = TRUE)\n\n# Define a new `d4` data\nd4 &lt;- d2 |&gt; \n  mutate(B = B)\n\nb4.9 &lt;- brm(\n  data = d4,\n  family = gaussian,\n  formula = doy ~ 1 + B,\n  prior = c(prior(normal(100, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.09\")\n\nReview the new model summary.\n\nprint(b4.9)\n\n Family: gaussian \n  Links: mu = identity \nFormula: doy ~ 1 + B \n   Data: d4 (Number of observations: 827) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   103.47      4.51    94.58   112.60 1.00      772      795\nB1           -0.35      4.61    -9.56     8.89 1.00      803      878\nB2            1.50      4.53    -7.54    10.45 1.00      784      778\nB3            1.56      4.54    -7.65    10.50 1.00      784      931\nB4            3.99      4.52    -5.06    12.98 1.00      783      818\nB5           -5.59      4.58   -14.88     3.46 1.00      786      834\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     6.10      0.15     5.80     6.41 1.00     2244     2058\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we do all the work in bulk to make and save the three subplots for Figure 4.12.\n\n## Top\n# Wrangle a bit\nb &lt;- invoke(data.frame, d4) |&gt; \n  pivot_longer(starts_with(\"B\"),\n               names_to = \"bias_function\",\n               values_to = \"bias\")\n\n# Plot\np1 &lt;- b |&gt; \n  ggplot(aes(x = year, y = bias, group = bias_function)) +\n  geom_vline(xintercept = knot_list, alpha = 1/2, color = \"white\") +\n  geom_line(alpha = 1/2, color = \"#ffb7c5\", linewidth = 1.5) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  ylab(\"bias value\")\n\n## Middle\n# Wrangle\np2 &lt;- as_draws_df(b4.9) |&gt; \n  select(b_B1:b_B5) |&gt; \n  set_names(str_c(\"B.\", 1:5)) |&gt; \n  pivot_longer(everything(), names_to = \"bias_function\") |&gt; \n  group_by(bias_function) |&gt; \n  summarise(weight = mean(value)) |&gt; \n  full_join(b, by = \"bias_function\") |&gt; \n  \n  # Plot\n  ggplot(aes(x = year, y = bias * weight, group = bias_function)) +\n  geom_vline(xintercept = knot_list, alpha = 1/2, color = \"white\") +\n  geom_line(alpha = 1/2, color = \"#ffb7c5\", linewidth = 1.5) +\n  scale_x_continuous(NULL, breaks = NULL)\n\n## Bottom\n# Wrangle\nf &lt;- fitted(b4.9)\n\np3 &lt;- f |&gt; \n  data.frame() |&gt; \n  bind_cols(d2) |&gt; \n  \n  # Plot\n  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + \n  geom_vline(xintercept = knot_list, alpha = 1/2, color = \"white\") +\n  geom_hline(yintercept = fixef(b4.9)[1, 1], color = \"white\", linetype = 2) +\n  geom_point(alpha = 1/2, color = \"#ffb7c5\") +\n  geom_ribbon(alpha = 2/3, fill = \"white\") +\n  labs(x = \"year\",\n       y = \"day in year\")\n\nNow combine the subplots with patchwork syntax and behold their glory.\n\n(p1 / p2 / p3) &\n  theme_bw() &\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n\n4.5.3 Smooth functions for a rough world\n\nThe splines in the previous section are just the beginning. A entire class of models, generalized additive models (GAMs), focuses on predicting an outcome variable using smooth functions of some predictor variables. The topic is deep enough to deserve its own book. (p. 120, emphasis in the original)\n\nMcElreath ended that block quote with a reference to his endnote #78. On page 562, we read: “A very popular and comprehensive text is Wood (2017b).”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "04.html#sec-smooth-functions-with-s",
    "href": "04.html#sec-smooth-functions-with-s",
    "title": "4  Geocentric Models",
    "section": "4.6 Summary First bonus: Smooth functions with brms::s()",
    "text": "4.6 Summary First bonus: Smooth functions with brms::s()\nIt’s convenient for us how McElreath ended that last section with a reference to Simon Wood’s work because brms allows for a variety of non-linear models by borrowing functions from Woods’s mgcv package (Wood, 2003, 2004, 2011, 2017a, 2022; Wood et al., 2016). The two smooth functions brms imports from mgcv are s() and t2(). We’ll be exploring s(). We might use the brms::get_prior() function to get a sense of how to set up the priors when using s().\n\nget_prior(data = d2,\n          family = gaussian,\n          doy ~ 1 + s(year))\n\n                  prior     class    coef group resp dpar nlpar lb ub tag       source\n                 (flat)         b                                              default\n                 (flat)         b syear_1                                 (vectorized)\n student_t(3, 105, 5.9) Intercept                                              default\n   student_t(3, 0, 5.9)       sds                                0             default\n   student_t(3, 0, 5.9)       sds s(year)                        0        (vectorized)\n   student_t(3, 0, 5.9)     sigma                                0             default\n\n\nWe have an overall intercept (class = Intercept), a single \\(\\beta\\) parameter for year (class = b), a \\(\\sigma\\) parameter (class = sigma), and an unfamiliar parameter of class = sds. I’m not going to go into that last parameter in any detail, here. We’ll need to work our way up through Chapter 13 and the multilevel model to get a full picture of what it means. The important thing to note here is that the priors for our s()-based alternative to the B-spline models, above, are going to look a little different. Here’s how we might fit an alternative to b4.8\n\nb4.10 &lt;- brm(\n  data = d2,\n  family = gaussian,\n  doy ~ 1 + s(year),\n  prior = c(prior(normal(100, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(student_t(3, 0, 5.9), class = sds),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/b04.10\")\n\nCheck out the model summary.\n\nprint(b4.10)\n\n Family: gaussian \n  Links: mu = identity \nFormula: doy ~ 1 + s(year) \n   Data: d2 (Number of observations: 827) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)    22.38      7.21    11.84    40.41 1.00     1032     1765\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   104.53      0.21   104.11   104.94 1.00     3988     2627\nsyear_1      -9.74      9.12   -27.80     8.48 1.00     3512     2713\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     6.04      0.15     5.76     6.34 1.00     4243     2972\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur intercept and \\(\\sigma\\) summaries are similar to those we got from b4.8. The rest looks different and maybe a little disorienting. Here’s what happens when we use brms::fitted() to plot the implications of the model.\n\nfitted(b4.10) |&gt; \n  data.frame() |&gt; \n  bind_cols(select(d2, year, doy)) |&gt; \n  \n  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = fixef(b4.10)[1, 1], color = \"white\", linetype = 2) +\n  geom_point(alpha = 1/2, color = \"#ffb7c5\") +\n  geom_ribbon(alpha = 2/3, fill = \"white\") +\n  labs(y = \"day in year\",\n       subtitle = \"b4.7 using s(year)\") +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"), \n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThat smooth doesn’t look quite the same. Hopefully this isn’t terribly surprising. We used a function from a different package and ended up with a different underlying statistical model. In fact, we didn’t even use a B-spline. The default for s() is to use what’s called a thin plate regression spline. If we’d like to fit a B-spline, we have to set bs = \"bs\". Here’s an example.\n\nb4.11 &lt;- brm(\n  data = d2,\n  family = gaussian,\n  doy ~ 1 + s(year, bs = \"bs\", k = 19),\n  prior = c(prior(normal(100, 10), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(student_t(3, 0, 5.9), class = sds),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/b04.11\")\n\n\nprint(b4.11)\n\n Family: gaussian \n  Links: mu = identity \nFormula: doy ~ 1 + s(year, bs = \"bs\", k = 19) \n   Data: d2 (Number of observations: 827) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nSmoothing Spline Hyperparameters:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsds(syear_1)     1.25      0.58     0.49     2.69 1.00      689     1503\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   104.54      0.21   104.13   104.94 1.00     5154     3008\nsyear_1      -0.10      0.32    -0.73     0.51 1.00     1812     1983\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.99      0.15     5.72     6.29 1.00     4966     3031\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow here’s the depiction of our s()-based B-spline model.\n\nfitted(b4.11) |&gt; \n  data.frame() |&gt; \n  bind_cols(select(d2, year, doy)) |&gt; \n  \n  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = fixef(b4.11)[1, 1], color = \"white\", linetype = 2) +\n  geom_point(alpha = 1/2, color = \"#ffb7c5\") +\n  geom_ribbon(alpha = 2/3, fill = \"white\") +\n  labs(y = \"day in year\",\n       subtitle = 'b4.7_bs using s(year, bs = \"bs\")') +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"), \n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThere are still other important differences between the underlying statistical model for b4.11 and the earlier b4.8 that I’m just not going to go into, here.\nFor more on the B-splines and smooths, more generally, check out the blog post by the great Gavin Simpson, Extrapolating with B splines and GAMs. For a high-level introduction to the models you can fit with mgcv, check out the nice talk by Noam Ross, Nonlinear models in R: The wonderful world of mgcv, or the equally-nice presentation by Simpson, Introduction to generalized additive models with R and mgcv. Ross offers a free online course covering mgcv, called GAMS in R, and he maintains a GitHub repo cataloguing other GAM-related resources, called Resources for learning about and using GAMs in R. For specific examples of fitting various GAMS with brms, check out Simpson’s blog post, Fitting GAMs with brms: part 1. Finally, Tristan Mahr has a nice blog post called Random effects and penalized splines are the same thing, where he outlined the connections between penalized smooths, such as you might fit with mgcv, with the multilevel model, which we’ll learn all about starting in Chapter 13, which helps explain what’s going on with the s() function in our last two models, b4.10 and b4.11.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "04.html#sec-Group-predictors-with-matrix-columns",
    "href": "04.html#sec-Group-predictors-with-matrix-columns",
    "title": "4  Geocentric Models",
    "section": "4.7 Second bonus: Group predictors with matrix columns",
    "text": "4.7 Second bonus: Group predictors with matrix columns\nWhen we fit b4.8, our direct brms analogue to McElreath’s m4.7,2 we used a compact syntax to pass a matrix column of predictors into the formula. If memory serves, this is one of the only places in the text where we see this. It would be easy for the casual reader to think this was only appropriate for something like a spline model. But that’s not the case. One could use the matrix-column trick as a general approach. In this bonus section, we’ll explore how.\n2 I know. I’m sorry our model numbering diverged from the numbering in the text. The problem popped up in Section 4.5.1 where McElreath fit a linear model with the weight_s predictor, but didn’t show the code in the text and, as a consequence, didn’t assign that model a number. Since we explicitly fit that linear model, we assigned it the next number in the sequence. So it goes. But yes, I am correct in comparing our b4.8 to McElreath’s m4.7.In Section 11.2.6 of their (2020) text, Gelman, Hill, and Vehtari worked through an example of a multiple regression model,\n\\[\n\\begin{align*}\ny_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + \\theta z_i + \\sum_{k = 1}^K b_k x_{k, i} \\\\\n& \\text{&lt;priors&gt;},\n\\end{align*}\n\\]\nwhere \\(y_i\\) was some continuous variable collected across participants, \\(i\\). The \\(\\alpha\\) term was the intercept and the \\(\\theta\\) term was the regression slope for a binary variable \\(z\\)–we’ll practice with binary predictors in Section 5.3. More to our interest, the last portion of the equation is a compact way to convey there are \\(K\\) additional predictors and their associated regression coefficients, which we might more explicitly express as \\(\\beta_1 x_{1, i} + \\cdots + \\beta_k x_{k, i}\\), where \\(K \\geq 1\\). In this particular example, \\(K = 10\\), meaning there were ten \\(x_{k, i}\\) predictors, making this an example of a model with 11 total predictor variables.\nRiffing off of Gelman and colleagues, here’s how you might simulate data of this kind.\n\n# How many cases would you like?\nn &lt;- 100\n\n# How many continuous x predictor variables would you like?\nk &lt;- 10\n\n# Simulate a dichotomous dummy variable for `z`\n# Simulate an `n` by `k` array for `X`\nset.seed(4)\n\nd &lt;- tibble(z = sample(0:1, size = n, replace = T),\n            X = array(runif(n * k, min = 0, max = 1), dim = c(n, k)))\n\n# Set the data-generating parameter values\na     &lt;- 1\ntheta &lt;- 5\nb     &lt;- 1:k\nsigma &lt;- 2\n\n# Simulate the criterion\nd &lt;- d |&gt; \n  mutate(y = as.vector(a + X %*% b + theta * z + rnorm(n, mean = 0, sd = sigma)))\n\n# Check the data structure\nd |&gt; \n  glimpse()\n\nRows: 100\nColumns: 3\n$ z &lt;int&gt; 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0…\n$ X &lt;dbl[,10]&gt; &lt;matrix[43 x 10]&gt;\n$ y &lt;dbl&gt; 19.51231, 26.75217, 28.92495, 20.63144, 29.18219, 43.67198, 36.98107, 36.68241, 28.67696, 24.39985, 24.66649, 31.…\n\n\nAlthough our d tibble has only three columns, the X column is a matrix column into which we’ve smuggled ten columns more. Here’s how we might access them more directly.\n\nd |&gt; \n  pull(X) |&gt; \n  glimpse()\n\n num [1:100, 1:10] 0.253 0.63 0.266 0.532 0.468 ...\n\n\nSee? There’s an \\(100 \\times 10\\) data matrix in there. Tricky. Here’s how to fit the full model with brms where we use the compact matrix-column syntax in the formula argument.\n\nb4.12 &lt;- brm(\n  data = d,\n  family = gaussian,\n  y ~ 1 + z + X,\n  prior = c(prior(normal(0, 2), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 4,\n  file = \"fits/b04.12\")\n\nCheck the parameter summary.\n\nprint(b4.12)\n\n Family: gaussian \n  Links: mu = identity \nFormula: y ~ 1 + z + X \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.95      1.19    -1.30     3.25 1.00     6754     3448\nz             4.75      0.42     3.92     5.54 1.00     8441     3485\nX1            0.57      0.74    -0.91     2.04 1.00     6140     3288\nX2            0.89      0.69    -0.54     2.22 1.00     6425     3219\nX3            3.41      0.72     1.98     4.79 1.00     6276     3505\nX4            2.81      0.73     1.42     4.25 1.00     6995     3266\nX5            5.75      0.72     4.37     7.13 1.00     5745     3119\nX6            6.40      0.75     4.94     7.87 1.00     6894     3199\nX7            8.51      0.73     7.03     9.95 1.00     6379     3214\nX8            8.39      0.72     6.97     9.77 1.00     6943     3015\nX9            8.84      0.83     7.23    10.45 1.00     7032     2997\nX10           9.30      0.70     7.93    10.67 1.00     5953     2927\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.99      0.15     1.71     2.31 1.00     5710     3133\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nbrms automatically numbered our \\(K = 10\\) X variables as X1 through X10. As far as applications go, I’m not sure where I’d use this way of storing and modeling data in real life. But maybe some of y’all work in domains where this is just the right way to approach your data needs. If so, good luck and happy modeling.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "04.html#session-info",
    "href": "04.html#session-info",
    "title": "4  Geocentric Models",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] splines   parallel  stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.7      brms_2.23.0          Rcpp_1.1.0           posterior_1.6.1.9000 cmdstanr_0.9.0       patchwork_1.3.2     \n [7] lubridate_1.9.4      forcats_1.0.1        stringr_1.6.0        dplyr_1.1.4          purrr_1.2.1          readr_2.1.5         \n[13] tidyr_1.3.2          tibble_3.3.1         ggplot2_4.0.1        tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.8            tidyselect_1.2.1        viridisLite_0.4.2       farver_2.1.2            loo_2.9.0.9000         \n [6] S7_0.2.1                fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39          \n[11] timechange_0.3.0        estimability_1.5.1      lifecycle_1.0.5         StanHeaders_2.36.0.9000 survival_3.8-3         \n[16] processx_3.8.6          magrittr_2.0.4          compiler_4.5.1          rlang_1.1.7             tools_4.5.1            \n[21] utf8_1.2.6              yaml_2.3.12             knitr_1.51              emo_0.0.0.9000          labeling_0.4.3         \n[26] bridgesampling_1.2-1    htmlwidgets_1.6.4       curl_7.0.0              pkgbuild_1.4.8          plyr_1.8.9             \n[31] RColorBrewer_1.1-3      abind_1.4-8             multcomp_1.4-29         withr_3.0.2             stats4_4.5.1           \n[36] grid_4.5.1              inline_0.3.21           xtable_1.8-4            emmeans_1.11.2-8        scales_1.4.0           \n[41] MASS_7.3-65             isoband_0.3.0           cli_3.6.5               mvtnorm_1.3-3           rmarkdown_2.30         \n[46] crayon_1.5.3            generics_0.1.4          RcppParallel_5.1.11-1   rstudioapi_0.17.1       reshape2_1.4.5         \n[51] tzdb_0.5.0              rstan_2.36.0.9000       bayesplot_1.15.0.9000   assertthat_0.2.1        matrixStats_1.5.0      \n[56] vctrs_0.7.0             V8_8.0.1                Matrix_1.7-3            sandwich_3.1-1          jsonlite_2.0.0         \n[61] hms_1.1.4               arrayhelpers_1.1-0      ggdist_3.3.3            glue_1.8.0              codetools_0.2-20       \n[66] ps_1.9.1                distributional_0.5.0    stringi_1.8.7           shape_1.4.6.1           gtable_0.3.6           \n[71] QuickJSR_1.8.1          pillar_1.11.1           htmltools_0.5.9         Brobdingnag_1.2-9       R6_2.6.1               \n[76] evaluate_1.0.5          lattice_0.22-7          backports_1.5.0         rstantools_2.5.0.9000   gridExtra_2.3          \n[81] coda_0.19-4.1           nlme_3.1-168            checkmate_2.3.3         mgcv_1.9-3              xfun_0.55              \n[86] zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "04.html#comments",
    "href": "04.html#comments",
    "title": "4  Geocentric Models",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nAono, Y. (2012). Long-term change in climate and floral phenophase. Chikyu Kankyo (Global Environment), 17. http://atmenv.envi.osakafu-u.ac.jp/aono/kyophenotemp4/\n\n\nAono, Y., & Kazui, K. (2008). Phenological data series of cherry tree flowering in Kyoto, Japan, and its application to reconstruction of springtime temperatures since the 9th century. International Journal of Climatology, 28(7), 905–914. https://doi.org/10.1002/joc.1594\n\n\nAono, Y., & Saito, S. (2010). Clarifying springtime temperature reconstructions of the medieval period by gap-filling the cherry blossom phenological data series at Kyoto, Japan. International Journal of Biometeorology, 54(2), 211–219. https://doi.org/10.1007/s00484-009-0272-x\n\n\nBürkner, P.-C. (2022a). brms reference manual, Version 2.18.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2022b). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html\n\n\nBürkner, P.-C., Gabry, J., Kay, M., & Vehtari, A. (2022). posterior: Tools for working with posterior distributions. https://CRAN.R-project.org/package=posterior\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nHowell, N. (2001). Demography of the dobe! Kung (2nd Edition). Routledge. https://www.routledge.com/Demography-of-the-Dobe-Kung/Howell/p/book/9780202306490\n\n\nHowell, N. (2010). Life histories of the Dobe! Kung: Food, fatness, and well-being over the life span (Vol. 4). Univ of California Press. https://www.ucpress.edu/book/9780520262348/life-histories-of-the-dobe-kung\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKurz, A. S. (2026). Doing Bayesian data analysis in brms and the tidyverse (Version 1.3.0). https://solomon.quarto.pub/dbda2/\n\n\nMcElreath, R. (2020a). rethinking R package. https://xcelab.net/rm/software/\n\n\nMcElreath, R. (2020b). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nPeng, R. D. (2022). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/\n\n\nPivot data from wide to long — pivot_longer. (2020). https://tidyr.tidyverse.org/reference/pivot_longer.html\n\n\nRipley, B. (2022). MASS: Support functions and datasets for venables and Ripley’s MASS. https://CRAN.R-project.org/package=MASS\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nStan Development Team. (2023). RStan: The R Interface to Stan. https://CRAN.R-project.org/package=rstan/vignettes/rstan.html\n\n\nVenables, W. N., & Ripley, B. D. (2002). Modern applied statistics with S (Fourth Edition). Springer. http://www.stats.ox.ac.uk/pub/MASS4\n\n\nWood, S. N. (2003). Thin-plate regression splines. Journal of the Royal Statistical Society (B), 65(1), 95–114. https://doi.org/10.1111/1467-9868.00374\n\n\nWood, S. N. (2004). Stable and efficient multiple smoothing parameter estimation for generalized additive models. Journal of the American Statistical Association, 99(467), 673–686. https://doi.org/10.1198/016214504000000980\n\n\nWood, S. N. (2011). Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. Journal of the Royal Statistical Society (B), 73(1), 3–36. https://doi.org/10.1111/j.1467-9868.2010.00749.x\n\n\nWood, S. N. (2017a). Generalized additive models: An introduction with R (2nd ed.). Chapman and Hall/CRC. https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331?utm_source=crcpress.com&utm_medium=referral\n\n\nWood, S. N. (2017b). Generalized additive models: An introduction with R (Second Edition). CRC Press. https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331\n\n\nWood, S. N. (2022). mgcv: Mixed GAM computation vehicle with automatic smoothness estimation. https://CRAN.R-project.org/package=mgcv\n\n\nWood, S. N., Pya, N., & Säfken, B. (2016). Smoothing parameter and model selection for general smooth models (with discussion). Journal of the American Statistical Association, 111, 1548–1575. https://doi.org/10.1080/01621459.2016.1180986",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Geocentric Models</span>"
    ]
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "5  The Many Variables & The Spurious Waffles",
    "section": "",
    "text": "5.0.0.1 Rethinking: Causal inference\nIn his endnote #80 (p. 562), McElreath wrote: “See Meehl (1990), in particular the ‘crud factor’ described on page 204.” For a fun look at some dubious correlations, check out the examples at https://www.tylervigen.com/spurious-correlations.\nBut back to the text, McElreath’s listed reasons for multivariable regression include:\nWe’ll approach the first two in this chapter. Interactions are reserved for Chapter 7.\n“Despite its central importance, there is no unified approach to causal inference yet in the sciences” (p. 124). To dip into the topic, you might check out the recent blog post by Finn Lattimore and David Rohde, Causal inference with Bayes rule; McElreath blog series on causal inference, starting with Regression, Fire, and Dangerous Things (1/3); or McElreath’s epic 3-hour introductory lecture on causal inference called Science Before Statistics: Causal Inference.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Many Variables & The Spurious Waffles</span>"
    ]
  },
  {
    "objectID": "05.html#sec-Spurious-associations",
    "href": "05.html#sec-Spurious-associations",
    "title": "5  The Many Variables & The Spurious Waffles",
    "section": "5.1 Spurious associations",
    "text": "5.1 Spurious associations\nLoad the Waffle House data.\n\nlibrary(tidyverse)\n\ndata(WaffleDivorce, package = \"rethinking\")\nd &lt;- WaffleDivorce\n\nDid you notice how we used the package argument within the data() function, there? That allowed us to load the WaffleDivorce without actually loading the rethinking package. Since we generally don’t want to have both rethinking and brms loaded up at the same time, using the package argument will save us a line of code.\nNow standardize the focal variables with the rethinking::standardize() function.\n\nd &lt;- d |&gt; \n  mutate(d = rethinking::standardize(Divorce),\n         m = rethinking::standardize(Marriage),\n         a = rethinking::standardize(MedianAgeMarriage))\n\nBecause we avoided directly loading the rethinking package, we did not have immediate access to McElreath’s handy standardize() function. If you want to use a function from a package without loading that package, you can use the double colon operator ::. You can learn more about the double colon operator here. Now load brms.\n\nrm(WaffleDivorce)\nlibrary(brms)\n\nI’m not going to show the output, but you might go ahead and investigate the data with the typical functions. E.g.,\n\nhead(d)\nglimpse(d)\n\nNow we have our data, we can reproduce Figure 5.1. One convenient way to get the handful of sate labels into the plot was with the geom_text_repel() function from the ggrepel package (Slowikowski, 2022). But first, we spent the last few chapters warming up with ggplot2. Going forward, each chapter will have its own plot theme. In this chapter, we’ll characterize the plots with theme_bw() + theme(panel.grid = element_rect()) and coloring based off of \"firebrick\".\n\nlibrary(ggrepel)\n\nd |&gt;\n  ggplot(aes(x = WaffleHouses/Population, y = Divorce)) +\n  stat_smooth(method = \"lm\", fullrange = T, linewidth = 1/2,\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick\") +\n  geom_point(alpha = 1/2, color = \"firebrick4\", size = 1.5) +\n  geom_text_repel(data = d |&gt; filter(Loc %in% c(\"ME\", \"OK\", \"AR\", \"AL\", \"GA\", \"SC\", \"NJ\")),  \n                  aes(label = Loc), \n                  size = 3, seed = 1042) +  # This makes it reproducible\n  scale_x_continuous(\"Waffle Houses per million\", limits = c(0, 55)) +\n  ylab(\"Divorce rate\") +\n  coord_cartesian(xlim = c(0, 50), ylim = c(5, 15)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nSince these are geographically-based data, we might plot our three major variables in a map format. The tigris package (Walker, 2022) provides functions for retrieving latitude and longitude data for the 50 states and we can plot then with the ggplot2::geom_sf() function. We’ll use the right_join() function to combine those data with our primary data d.1\n1 Note, there is a small difficulty with this tigris-oriented workflow. The issue is that the data frame is of class sf, and objects of class sf don’t pivot nicely with the pivot_longer() function unless you specifically refer to the columns within quotes, as in pivot_longer(cols = c(\"d\", \"m\", \"a\")). The details are in GitHub issue #1171 for the tidyr package.\nlibrary(tigris)\n\n# Get the map data\nd_states &lt;- states(cb = TRUE, resolution = \"20m\") |&gt;\n  shift_geometry() |&gt; \n  # Add the primary data\n  right_join(d |&gt; \n               mutate(NAME = Location |&gt; as.character()) |&gt; \n               select(d:a, NAME),\n             by = \"NAME\") |&gt; \n  # Convert to the long format for faceting\n  pivot_longer(cols = c(\"d\", \"m\", \"a\"), names_to = \"variable\")\n\nNow plot.\n\nd_states |&gt;\n  ggplot() +\n  geom_sf(aes(fill = value, geometry = geometry),\n          size = 0) +\n  scale_fill_gradient(low = \"#f8eaea\", high = \"firebrick4\") +\n  facet_wrap(~ variable, labeller = label_both)  +\n  theme_void() +\n  theme(legend.position = \"none\",\n        strip.text = element_text(margin = margin(0, 0, .5, 0)))\n\n\n\n\n\n\n\n\nOne of the advantages of this visualization method is it just became clear that Nevada is missing from the WaffleDivorce data. Execute d |&gt; distinct(Location) to see for yourself and click here to find out why it’s missing. Those missing data should motivate the skills we’ll cover in Chapter 15. But let’s get back on track.\nHere’s the standard deviation for MedianAgeMarriage in its current metric.\n\nsd(d$MedianAgeMarriage)\n\n[1] 1.24363\n\n\nOur first statistical model follows the form\n\\[\\begin{align*}\n\\text{divorce\\_std}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\text{median\\_age\\_at\\_marriage\\_std}_i \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1),\n\\end{align*}\\]\nwhere the _std suffix indicates the variables are standardized (i.e., zero centered, with a standard deviation of one). Let’s fit the first univariable model.\n\nb5.1 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + a,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  sample_prior = T,\n  file = \"fits/b05.01\")\n\nDid you notice the sample_prior = T line? This told brms to take draws from both the posterior distribution (as usual) and from the prior predictive distribution. If you look at McElreath’s R code 5.4, you’ll see he plotted 50 draws from the prior predictive distribution of his m5.1. For our brms workflow, our first step is the extract our prior draws with the well-named prior_draws() function.\n\nprior &lt;- prior_draws(b5.1)\n\nprior |&gt; glimpse()\n\nRows: 4,000\nColumns: 3\n$ Intercept &lt;dbl&gt; 0.3001143533, -0.1229098276, -0.0427254747, 0.3320661822, 0.2263362881, -0.0968778653, 0.0080745020, 0.0006555…\n$ b         &lt;dbl&gt; -0.01732627, 0.71738711, 0.39840681, 0.41297729, 0.23628334, -0.67776143, -0.62601426, -0.04947224, 1.26867052…\n$ sigma     &lt;dbl&gt; 0.198153043, 0.048476578, 0.511016687, 0.156677917, 0.470371634, 0.514275098, 0.819989393, 0.532679079, 0.2754…\n\n\nWe ended up with 4,000 draws from the prior predictive distribution, much like as_draws_df() would return 4,000 draws from the posterior. Next we’ll use slice_sample() to take a random sample from our prior object. After just a little more wrangling, we’ll be in good shape to plot our version of Figure 5.3.\n\nset.seed(5)\n\nprior |&gt; \n  slice_sample(n = 50) |&gt; \n  rownames_to_column(\"draw\") |&gt; \n  expand_grid(a = c(-2, 2)) |&gt; \n  mutate(d = Intercept + b * a) |&gt; \n  \n  ggplot(aes(x = a, y = d)) +\n  geom_line(aes(group = draw),\n            alpha = 0.4, color = \"firebrick\") +\n  labs(x = \"Median age marriage (std)\",\n       y = \"Divorce rate (std)\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\n\n\nTo get the posterior predictions from our brms model, we’ll use fitted() in place of link().\n\n# Determine the range of `a` values we'd like to feed into `fitted()`\nnd &lt;- tibble(a = seq(from = -3, to = 3.2, length.out = 30))\n\n# Now use `fitted()` to get the model-implied trajectories\nfitted(b5.1, newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  \n  # Plot\n  ggplot(aes(x = a)) +\n  geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick\", linewidth = 1/4) +\n  geom_point(data = d, \n             aes(y = d), \n             color = \"firebrick4\", size = 2) +\n  labs(x = \"Median age marriage (std)\",\n       y = \"Divorce rate (std)\") +\n  coord_cartesian(xlim = range(d$a), \n                  ylim = range(d$d)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\n\n\nThat’ll serve as our version of the right panel of Figure 5.2. To paraphrase McElreath, “if you inspect the [print()] output, you’ll see that posterior for \\([\\beta_\\text{a}]\\) is reliably negative” (p. 127). Let’s see.\n\nprint(b5.1)\n\n Family: gaussian \n  Links: mu = identity \nFormula: d ~ 1 + a \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.10    -0.20     0.19 1.00     3550     2466\na            -0.57      0.11    -0.79    -0.34 1.00     3389     2920\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.82      0.08     0.68     1.00 1.00     3766     3142\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOn the standardized scale, -0.57 95% CI [-0.79, -0.34] is pretty negative, indeed.\nWe’re ready to fit our second univariable model.\n\nb5.2 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + m,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.02\")\n\nThe summary suggests \\(\\beta_\\text{m}\\) is of a smaller magnitude.\n\nprint(b5.2)\n\n Family: gaussian \n  Links: mu = identity \nFormula: d ~ 1 + m \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.11    -0.23     0.22 1.00     3970     2951\nm             0.35      0.13     0.10     0.61 1.00     3756     2821\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.95      0.10     0.78     1.16 1.00     3948     2934\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we’ll wangle and plot our version of the left panel in Figure 5.2.\n\nnd &lt;- tibble(m = seq(from = -2.5, to = 3.5, length.out = 30))\n\nfitted(b5.2, newdata = nd) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = m)) +\n  geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick\", linewidth = 1/4) +\n  geom_point(data = d, \n             aes(y = d), \n             color = \"firebrick4\", size = 2) +\n  labs(x = \"Marriage rate (std)\",\n       y = \"Divorce rate (std)\") +\n  coord_cartesian(xlim = range(d$m), \n                  ylim = range(d$d)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())                   \n\n\n\n\n\n\n\n\n\nBut merely comparing parameter means between different bivariate regressions is no way to decide which predictor is better. Both of these predictors could provide independent value, or they could be redundant, or one could eliminate the value of the other.\nTo make sense of this, we’re going to have to think causally. And then, only after we’ve done some thinking, a bigger regression model that includes both age at marriage and marriage rate will help us. (pp. 127–128)\n\n\n5.1.1 Think before you regress\n\nIt is helpful to introduce a particular type of causal graph known as a DAG, short for directed acyclic graph. Graph means it is nodes and connections. Directed means the connections have arrows that indicate directions of causal influence. And acyclic means that causes do not eventually flow back on themselves. A DAG is a way of describing qualitative causal relationships among variables. It isn’t as detailed as a full model description, but it contains information that a purely statistical model does not. Unlike a statistical model, a DAG will tell you the consequences of intervening to change a variable. But only if the DAG is correct. There is no inference without assumption. (p. 128, emphasis in the original)\n\nIf you’re interested in making directed acyclic graphs (DAG) in R, the dagitty (Textor et al., 2016, 2021) and ggdag (Barrett, 2022a) packages are handy. Our approach will focus on ggdag.\n\nlibrary(ggdag)\n\nIf all you want is a quick and dirty DAG for our three variables, you might execute something like this.\n\nset.seed(5)\n\ndagify(M ~ A,\n       D ~ A + M) |&gt;\n  ggdag(node_size = 8)\n\n\n\n\n\n\n\n\nWe can pretty it up a little, too.\n\ndag_coords &lt;- tibble(\n  name = c(\"A\", \"M\", \"D\"),\n  x    = c(1, 3, 2),\n  y    = c(2, 2, 1))\n\np1 &lt;- dagify(\n  M ~ A,\n  D ~ A + M,\n  coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(alpha = 1/4, color = \"firebrick\", size = 10) +\n  geom_dag_text(color = \"firebrick\") +\n  geom_dag_edges(edge_color = \"firebrick\") +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\np1\n\n\n\n\n\n\n\n\nWe could have left out the coords argument and let the dagify() function set the layout of the nodes on its own. But since we were picky and wanted to ape McElreath, we first specified our coordinates in a tibble and then included that tibble in the coords argument. For more on the topic, check out the Barrett’s (2022b) vignette, An introduction to ggdag.\nBuy anyway, our DAG\n\nrepresents a heuristic causal model. Like other models, it is an analytical assumption. The symbols \\(A\\), \\(M\\), and \\(D\\) are our observed variables. The arrows show directions of influence. What this DAG says is:\n\n\\(A\\) directly influences \\(D\\)\n\\(M\\) directly influences \\(D\\)\n\\(A\\) directly influences \\(M\\)\n\nThese statements can then have further implications. In this case, age of marriage influences divorce in two ways. First it has a direct effect, \\(A \\rightarrow D\\). Perhaps a direct effect would arise because younger people change faster than older people and are therefore more likely to grow incompatible with a partner. Second, it has an indirect effect by influencing the marriage rate, which then influences divorce, \\(A \\rightarrow M \\rightarrow D\\). If people get married earlier, then the marriage rate may rise, because there are more young people. (p. 128)\n\nConsidering alternative models, “It could be that the association between \\(M\\) and \\(D\\) arises entirely from \\(A\\)’s influence on both \\(M\\) and \\(D\\). Like this:” (p. 129)\n\np2 &lt;- dagify(\n  M ~ A,\n  D ~ A,\n  coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(alpha = 1/4, color = \"firebrick\", size = 10) +\n  geom_dag_text(color = \"firebrick\") +\n  geom_dag_edges(edge_color = \"firebrick\") +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\np2\n\n\n\n\n\n\n\n\n\nThis DAG is also consistent with the posterior distributions of models [b5.1] and [b5.2]. Why? Because both \\(M\\) and \\(D\\) “listen” to \\(A\\). They have information from \\(A\\). So when you inspect the association between \\(D\\) and \\(M\\), you pick up that common information that they both got from listening to \\(A\\). You’ll see a more formal way to deduce this, in the next chapter.\nSo which is it? Is there a direct effect of marriage rate, or rather is age at marriage just driving both, creating a spurious correlation between marriage rate and divorce rate? To find out, we need to consider carefully what each DAG implies. That’s what’s next. (p. 129)\n\n\n5.1.1.1 Rethinking: What’s a cause?\n\nQuestions of causation can become bogged down in philosophical debates. These debates are worth having. But they don’t usually intersect with statistical concerns. Knowing a cause in statistics means being able to correctly predict the consequences of an intervention. There are contexts in which even this is complicated. (p. 129)\n\n\n\n\n5.1.2 Testable implications\nSo far, we have entertained two DAGs. Here we use patchwork to combine them into one plot.\n\nlibrary(patchwork)\n\np1 | p2\n\n\n\n\n\n\n\n\nMcElreath encouraged us to examine the correlations among these three variables with cor().\n\nd |&gt; \n  select(d:a) |&gt; \n  cor()\n\n           d          m          a\nd  1.0000000  0.3737314 -0.5972392\nm  0.3737314  1.0000000 -0.7210960\na -0.5972392 -0.7210960  1.0000000\n\n\nIf you just want the lower triangle, you can use the lowerCor() function from the psych package (Revelle, 2022).\n\nlibrary(psych)\n\nd |&gt; \n  select(d:a) |&gt; \n  lowerCor(digits = 3)\n\n  d      m      a     \nd  1.000              \nm  0.374  1.000       \na -0.597 -0.721  1.000\n\n\nOur second DAG, above, suggests “that \\(D\\) is independent of \\(M\\), conditional on \\(A\\)” (p. 130). We can use the dagitty::impliedConditionalIndependencies() function to express that conditional independence in formal notation.\n\nlibrary(dagitty)\n\ndagitty('dag{ D &lt;- A -&gt; M }') |&gt; \n  impliedConditionalIndependencies()\n\nD _||_ M | A\n\n\nThe lack of conditional dependencies in the first DAG may be expressed this way.\n\ndagitty('dag{D &lt;- A -&gt; M -&gt; D}') |&gt; \n  impliedConditionalIndependencies()\n\nOkay, that was a bit of a tease. “There are no conditional independencies, so there is no output to display” (p. 131). To close out this section,\n\nonce you fit a multiple regression to predict divorce using both marriage rate and age at marriage, the model addresses the questions:\n\nAfter I already know marriage rate, what additional value is there in also knowing age at marriage?\nAfter I already know age at marriage, what additional value is there in also knowing marriage rate?\n\nThe parameter estimates corresponding to each predictor are the (often opaque) answers to these questions. The questions above are descriptive, and the answers are also descriptive. It is only the derivation of the testable implications above that gives these descriptive results a causal meaning. But that meaning is still dependent upon believing the DAG. (p. 131)\n\n\n\n5.1.3 Multiple regression notation\nWe can write the statistical formula for our first multivariable model as\n\\[\\begin{align*}\n\\text{divorce\\_std}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\text{Marriage\\_std}_i + \\beta_2 \\text{MedianAgeMarriage\\_std}_i \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\n\n\n5.1.4 Approximating the posterior\nMuch like we used the + operator to add single predictors to the intercept, we just use more + operators in the formula argument to add more predictors. Also notice we’re using the same prior prior(normal(0, 1), class = b) for both predictors. Within the brms framework, they are both of class = b. But if we wanted their priors to differ, we’d make two prior() statements and differentiate them with the coef argument. You’ll see examples of that later on.\n\nb5.3 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + m + a,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.03\")\n\nBehold the summary.\n\nprint(b5.3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: d ~ 1 + m + a \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.10    -0.20     0.19 1.00     3732     2350\nm            -0.06      0.16    -0.38     0.25 1.00     2934     2880\na            -0.61      0.16    -0.91    -0.29 1.00     2978     2674\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.09     0.68     1.01 1.00     3582     2816\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe brms package doesn’t have a convenience function like rethinking::coeftab(). However, we can make something similar with a little deft wrangling and ggplot2 code.\n\n# First, extract and rename the necessary posterior parameters\nbind_cols(\n  as_draws_df(b5.1) |&gt; \n    transmute(`b5.1_beta[A]` = b_a),\n  as_draws_df(b5.2) |&gt; \n    transmute(`b5.2_beta[M]` = b_m),\n  as_draws_df(b5.3) |&gt; \n    transmute(`b5.3_beta[M]` = b_m,\n              `b5.3_beta[A]` = b_a)\n  ) |&gt; \n  # Convert them to the long format, group, and get the posterior summaries\n  pivot_longer(everything()) |&gt; \n  group_by(name) |&gt; \n  summarise(mean = mean(value),\n            ll   = quantile(value, prob = 0.025),\n            ul   = quantile(value, prob = 0.975)) |&gt; \n  # Since the `name` variable is really two variables in one, here we split them up\n  separate(col = name, into = c(\"fit\", \"parameter\"), sep = \"_\") |&gt; \n  \n  # Plot!\n  ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) +\n  geom_vline(xintercept = 0, alpha = 1/5, color = \"firebrick\") +\n  geom_pointrange(color = \"firebrick\") +\n  labs(x = \"posterior\", y = NULL) +\n  facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        strip.background = element_rect(fill = \"transparent\", color = \"transparent\"))\n\n\n\n\n\n\n\n\nDon’t worry, coefficient plots won’t always be this complicated. We’ll walk out simpler ones toward the end of the chapter.\nThe substantive interpretation of all those coefficients is: “Once we know median age at marriage for a State, there is little or no additional predictive power in also knowing the rate of marriage in that State” (p. 134, emphasis in the original). This coheres well with one of our impliedConditionalIndependencies() statements, from above.\n\ndagitty('dag{ D &lt;- A -&gt; M }') |&gt; \n  impliedConditionalIndependencies()\n\nD _||_ M | A\n\n\n\n5.1.4.1 Overthinking: Simulating the divorce example\nOkay, let’s simulate our divorce data in a tidyverse sort of way.\n\n# How many states would you like?\nn &lt;- 50 \n\nset.seed(5)\nsim_d &lt;- tibble(age = rnorm(n, mean = 0, sd = 1)) |&gt;  # Simulate A \n  mutate(mar = rnorm(n, mean = -age, sd = 1),         # Simulate A -&gt; M \n         div = rnorm(n, mean =  age, sd = 1))         # Simulate A -&gt; D\n\nhead(sim_d)\n\n# A tibble: 6 × 3\n      age    mar    div\n    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 -0.841   2.30  -2.84 \n2  1.38   -1.20   2.52 \n3 -1.26    2.28  -0.580\n4  0.0701 -0.662  0.279\n5  1.71   -1.82   1.65 \n6 -0.603  -0.322  0.291\n\n\nWe simulated those data based on this formulation.\n\ndagitty('dag{divorce &lt;- age -&gt; marriage}') |&gt; \n  impliedConditionalIndependencies()\n\ndvrc _||_ mrrg | age\n\n\nHere are the quick pairs() plots.\n\npairs(sim_d, col = \"firebrick4\")\n\n\n\n\n\n\n\n\nIf we use the update() function, we can refit the last models in haste.\n\nb5.1_sim &lt;- update(\n  b5.1, \n  newdata = sim_d, \n  formula = div ~ 1 + age,\n  seed = 5,\n  file = \"fits/b05.01_sim\")\n\nb5.2_sim &lt;- update(\n  b5.2, \n  newdata = sim_d, \n  formula = div ~ 1 + mar,\n  seed = 5,\n  file = \"fits/b05.02_sim\")\n\nb5.3_sim &lt;- update(\n  b5.3, \n  newdata = sim_d,\n  formula = div ~ 1 + mar + age,\n  seed = 5,\n  file = \"fits/b05.03_sim\")\n\nThe steps for our homemade coefplot() plot are basically the same. Just switch out some of the names.\n\nbind_cols(\n  as_draws_df(b5.1_sim) |&gt; \n    transmute(`b5.1_beta[A]` = b_age),\n  as_draws_df(b5.2_sim) |&gt; \n    transmute(`b5.2_beta[M]` = b_mar),\n  as_draws_df(b5.3_sim) |&gt; \n    transmute(`b5.3_beta[M]` = b_mar,\n              `b5.3_beta[A]` = b_age)\n  ) |&gt; \n  pivot_longer(everything()) |&gt; \n  group_by(name) |&gt; \n  summarise(mean = mean(value),\n            ll   = quantile(value, prob = 0.025),\n            ul   = quantile(value, prob = 0.975)) |&gt; \n  # Since the `name` variable is really two variables in one, here we split them up\n  separate(name, into = c(\"fit\", \"parameter\"), sep = \"_\") |&gt; \n  \n  # Plot!\n  ggplot(aes(x = mean, xmin = ll, xmax = ul, y = fit)) +\n  geom_vline(xintercept = 0, alpha = 1/5, color = \"firebrick\") +\n  geom_pointrange(color = \"firebrick\") +\n  labs(x = \"posterior\", y = NULL) +\n  facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank())\n\n\n\n\n\n\n\n\nWell, okay. This is the same basic pattern, but with the signs switched and with a little simulation variability thrown in. But you get the picture.\n\n\n\n5.1.5 Plotting multivariate posteriors\n“Let’s pause for a moment, before moving on. There are a lot of moving parts here: three variables, some strange DAGs, and three models. If you feel at all confused, it is only because you are paying attention” (p. 133).\nPreach, brother.\nDown a little further, McElreath gave us this deflationary delight: “There is a huge literature detailing a variety of plotting techniques that all attempt to help one understand multiple linear regression. None of these techniques is suitable for all jobs, and most do not generalize beyond linear regression” (pp. 134–135). Now you’re inspired, let’s learn three:\n\npredictor residual plots\nposterior prediction plots\ncounterfactual plots\n\n\n5.1.5.1 Predictor residual plots\nTo get ready to make our residual plots, we’ll predict one predictor, m, with another one, a.\n\nb5.4 &lt;- brm(\n  data = d, \n  family = gaussian,\n  m ~ 1 + a,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.04\")\n\n\nprint(b5.4)\n\n Family: gaussian \n  Links: mu = identity \nFormula: m ~ 1 + a \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.09    -0.18     0.17 1.00     3983     2778\na            -0.69      0.10    -0.89    -0.48 1.00     3433     2824\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.71      0.07     0.59     0.87 1.00     3286     3104\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWith fitted(), we compute the expected values for each state (with the exception of Nevada). Since the a values for each state are in the date we used to fit the model, we’ll omit the newdata argument.\n\nf &lt;- fitted(b5.4) |&gt;\n  data.frame() |&gt;\n  bind_cols(d)\n\nglimpse(f)\n\nRows: 50\nColumns: 20\n$ Estimate          &lt;dbl&gt; 0.41591236, 0.47144026, 0.13827289, 0.97119131, -0.41700607, 0.19380078, -0.86122923, -0.30595027, -2.…\n$ Est.Error         &lt;dbl&gt; 0.10780326, 0.11290737, 0.08983048, 0.17067138, 0.10662354, 0.09220902, 0.15490264, 0.09797500, 0.3152…\n$ Q2.5              &lt;dbl&gt; 0.200196249, 0.246803760, -0.041147635, 0.635469955, -0.626303532, 0.009984499, -1.171619601, -0.49634…\n$ Q97.5             &lt;dbl&gt; 0.62694222, 0.69138758, 0.31516259, 1.29727991, -0.20590984, 0.37618024, -0.55917700, -0.11350942, -1.…\n$ Location          &lt;fct&gt; Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, District of Columbia,…\n$ Loc               &lt;fct&gt; AL, AK, AZ, AR, CA, CO, CT, DE, DC, FL, GA, HI, ID, IL, IN, IA, KS, KY, LA, ME, MD, MA, MI, MN, MS, MO…\n$ Population        &lt;dbl&gt; 4.78, 0.71, 6.33, 2.92, 37.25, 5.03, 3.57, 0.90, 0.60, 18.80, 9.69, 1.36, 1.57, 12.83, 6.48, 3.05, 2.8…\n$ MedianAgeMarriage &lt;dbl&gt; 25.3, 25.2, 25.8, 24.3, 26.8, 25.7, 27.6, 26.6, 29.7, 26.4, 25.9, 26.9, 23.2, 27.0, 25.7, 25.4, 25.0, …\n$ Marriage          &lt;dbl&gt; 20.2, 26.0, 20.3, 26.4, 19.1, 23.5, 17.1, 23.1, 17.7, 17.0, 22.1, 24.9, 25.8, 17.9, 19.8, 21.5, 22.1, …\n$ Marriage.SE       &lt;dbl&gt; 1.27, 2.93, 0.98, 1.70, 0.39, 1.24, 1.06, 2.89, 2.53, 0.58, 0.81, 2.54, 1.84, 0.58, 0.81, 1.46, 1.48, …\n$ Divorce           &lt;dbl&gt; 12.7, 12.5, 10.8, 13.5, 8.0, 11.6, 6.7, 8.9, 6.3, 8.5, 11.5, 8.3, 7.7, 8.0, 11.0, 10.2, 10.6, 12.6, 11…\n$ Divorce.SE        &lt;dbl&gt; 0.79, 2.05, 0.74, 1.22, 0.24, 0.94, 0.77, 1.39, 1.89, 0.32, 0.58, 1.27, 1.05, 0.45, 0.63, 0.91, 1.09, …\n$ WaffleHouses      &lt;int&gt; 128, 0, 18, 41, 0, 11, 0, 3, 0, 133, 381, 0, 0, 2, 17, 0, 6, 64, 66, 0, 11, 0, 0, 0, 72, 39, 0, 0, 0, …\n$ South             &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ Slaves1860        &lt;int&gt; 435080, 0, 0, 111115, 0, 0, 0, 1798, 0, 61745, 462198, 0, 0, 0, 0, 0, 2, 225483, 331726, 0, 87189, 0, …\n$ Population1860    &lt;int&gt; 964201, 0, 0, 435450, 379994, 34277, 460147, 112216, 75080, 140424, 1057286, 0, 0, 1711951, 1350428, 6…\n$ PropSlaves1860    &lt;dbl&gt; 4.5e-01, 0.0e+00, 0.0e+00, 2.6e-01, 0.0e+00, 0.0e+00, 0.0e+00, 1.6e-02, 0.0e+00, 4.4e-01, 4.4e-01, 0.0…\n$ d                 &lt;dbl&gt; 1.6542053, 1.5443643, 0.6107159, 2.0935693, -0.9270579, 1.0500799, -1.6410244, -0.4327735, -1.8607064,…\n$ m                 &lt;dbl&gt; 0.022644060, 1.549801620, 0.048974363, 1.655122831, -0.266989270, 0.891544051, -0.793595325, 0.7862228…\n$ a                 &lt;dbl&gt; -0.60628951, -0.68669925, -0.20424076, -1.41038699, 0.59985673, -0.28465051, 1.24313471, 0.43903723, 2…\n\n\nAfter a little data processing, we can make the upper left panel of Figure 5.4.\n\np1 &lt;- f |&gt; \n  ggplot(aes(x = a, y = m)) +\n  geom_point(size = 2, shape = 1, color = \"firebrick4\") +\n  geom_segment(aes(xend = a, yend = Estimate), \n               linewidth = 1/4) +\n  geom_line(aes(y = Estimate), \n            color = \"firebrick4\") +\n  geom_text_repel(data = f |&gt; filter(Loc %in% c(\"WY\", \"ND\", \"ME\", \"HI\", \"DC\")),  \n                  aes(label = Loc), \n                  size = 3, seed = 14) +\n  labs(x = \"Age at marriage (std)\",\n       y = \"Marriage rate (std)\") +\n  coord_cartesian(ylim = range(d$m)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\np1\n\n\n\n\n\n\n\n\nWe get the residuals with the well-named residuals() function. Much like with brms::fitted(), brms::residuals() returns a four-vector matrix with the number of rows equal to the number of observations in the original data (by default, anyway). The vectors have the familiar names: Estimate, Est.Error, Q2.5, and Q97.5. See the brms reference manual (Bürkner, 2022a) for details.\nWith our residuals in hand, we just need a little more data processing to make lower left panel of Figure 5.4.\n\nr &lt;- residuals(b5.4) |&gt;\n  # To use this in ggplot2, we need to make it a tibble or data frame\n  data.frame() |&gt; \n  bind_cols(d)\n\np3 &lt;- r |&gt; \n  ggplot(aes(x = Estimate, y = d)) +\n  stat_smooth(method = \"lm\", fullrange = T,\n              color = \"firebrick4\", fill = \"firebrick4\", \n              alpha = 1/5, linewidth = 1/2) +\n  geom_vline(xintercept = 0, linetype = 2, color = \"grey50\") +\n  geom_point(size = 2, alpha = 2/3, color = \"firebrick4\") +\n  geom_text_repel(data = r |&gt; filter(Loc %in% c(\"WY\", \"ND\", \"ME\", \"HI\", \"DC\")),  \n                  aes(label = Loc), \n                  size = 3, seed = 5) +\n  scale_x_continuous(limits = c(-2, 2)) +\n  coord_cartesian(xlim = range(r$Estimate)) +\n  labs(x = \"Marriage rate residuals\",\n       y = \"Divorce rate (std)\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\np3\n\n\n\n\n\n\n\n\nTo get the MedianAgeMarriage_s residuals, we have to fit the corresponding model where m predicts a.\n\nb5.4b &lt;- brm(\n  data = d, \n  family = gaussian,\n  a ~ 1 + m,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.04b\")\n\nWith b5.4b in hand, we’re ready to make the upper right panel of Figure 5.4.\n\nf &lt;- fitted(b5.4b) |&gt;\n  data.frame() |&gt;\n  bind_cols(d) \n\np2 &lt;- f |&gt; \n  ggplot(aes(x = m, y = a)) +\n  geom_point(size = 2, shape = 1, color = \"firebrick4\") +\n  geom_segment(aes(xend = m, yend = Estimate), \n               linewidth = 1/4) +\n  geom_line(aes(y = Estimate), \n            color = \"firebrick4\") +\n  geom_text_repel(data = f |&gt; filter(Loc %in% c(\"DC\", \"HI\", \"ID\")),  \n                  aes(label = Loc), \n                  size = 3, seed = 5) +\n  labs(x = \"Marriage rate (std)\",\n       y = \"Age at marriage (std)\") +\n  coord_cartesian(ylim = range(d$a)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())   \n\np2\n\n\n\n\n\n\n\n\nAnd now we’ll get the new batch of residuals, do a little data processing, and make a plot corresponding to the final panel of Figure 5.4.\n\nr &lt;- residuals(b5.4b) |&gt;\n  data.frame() |&gt; \n  bind_cols(d)\n\np4 &lt;- r |&gt;\n  ggplot(aes(x = Estimate, y = d)) +\n  stat_smooth(method = \"lm\", fullrange = T,\n              color = \"firebrick4\", fill = \"firebrick4\", \n              alpha = 1/5, linewidth = 1/2) +\n  geom_vline(xintercept = 0, linetype = 2, color = \"grey50\") +\n  geom_point(size = 2, alpha = 2/3, color = \"firebrick4\") +\n  geom_text_repel(data = r |&gt; filter(Loc %in% c(\"ID\", \"HI\", \"DC\")),  \n                  aes(label = Loc), \n                  size = 3, seed = 5) +\n  scale_x_continuous(limits = c(-2, 3)) +\n  coord_cartesian(xlim = range(r$Estimate),\n                  ylim = range(d$d)) +\n  labs(x = \"Age at marriage residuals\",\n       y = \"Divorce rate (std)\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\np4\n\n\n\n\n\n\n\n\nHere we close out the section by combining our four subplots into one glorious whole with a little patchwork syntax.\n\np1 + p2 + p3 + p4 + plot_annotation(title = \"Understanding multiple regression through residuals\")\n\n\n\n\n\n\n\n\n\n5.1.5.1.1 Rethinking: Residuals are parameters, not data\n\nThere is a tradition, especially in parts of biology, of using residuals from one model as data in another model. For example, a biologist might regress brain size on body size and then use the brain size residuals as data in another model. This procedure is always a mistake. Residuals are not known. They are parameters, variables with unobserved values. Treating them as known values throws away uncertainty. (p. 137)\n\nLet’s hammer this point home. Recall how brms::residuals() returns four columns: Estimate, Est.Error, Q2.5, and Q97.5.\n\nr |&gt; \n  glimpse()\n\nRows: 50\nColumns: 20\n$ Estimate          &lt;dbl&gt; -0.59712014, 0.38429399, -0.18435161, -0.28342601, 0.40904528, 0.33531838, 0.68629283, 0.97087466, 2.4…\n$ Est.Error         &lt;dbl&gt; 0.7307434, 0.7448644, 0.7272531, 0.7390290, 0.7272224, 0.7244338, 0.7280102, 0.7251978, 0.7295269, 0.7…\n$ Q2.5              &lt;dbl&gt; -2.06256619, -1.09372587, -1.63936588, -1.72338427, -1.00776785, -1.07197330, -0.76873353, -0.42655509…\n$ Q97.5             &lt;dbl&gt; 0.8363025, 1.8545440, 1.2386482, 1.1609900, 1.8589647, 1.7559819, 2.0839278, 2.3763249, 3.9002979, 1.1…\n$ Location          &lt;fct&gt; Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, District of Columbia,…\n$ Loc               &lt;fct&gt; AL, AK, AZ, AR, CA, CO, CT, DE, DC, FL, GA, HI, ID, IL, IN, IA, KS, KY, LA, ME, MD, MA, MI, MN, MS, MO…\n$ Population        &lt;dbl&gt; 4.78, 0.71, 6.33, 2.92, 37.25, 5.03, 3.57, 0.90, 0.60, 18.80, 9.69, 1.36, 1.57, 12.83, 6.48, 3.05, 2.8…\n$ MedianAgeMarriage &lt;dbl&gt; 25.3, 25.2, 25.8, 24.3, 26.8, 25.7, 27.6, 26.6, 29.7, 26.4, 25.9, 26.9, 23.2, 27.0, 25.7, 25.4, 25.0, …\n$ Marriage          &lt;dbl&gt; 20.2, 26.0, 20.3, 26.4, 19.1, 23.5, 17.1, 23.1, 17.7, 17.0, 22.1, 24.9, 25.8, 17.9, 19.8, 21.5, 22.1, …\n$ Marriage.SE       &lt;dbl&gt; 1.27, 2.93, 0.98, 1.70, 0.39, 1.24, 1.06, 2.89, 2.53, 0.58, 0.81, 2.54, 1.84, 0.58, 0.81, 1.46, 1.48, …\n$ Divorce           &lt;dbl&gt; 12.7, 12.5, 10.8, 13.5, 8.0, 11.6, 6.7, 8.9, 6.3, 8.5, 11.5, 8.3, 7.7, 8.0, 11.0, 10.2, 10.6, 12.6, 11…\n$ Divorce.SE        &lt;dbl&gt; 0.79, 2.05, 0.74, 1.22, 0.24, 0.94, 0.77, 1.39, 1.89, 0.32, 0.58, 1.27, 1.05, 0.45, 0.63, 0.91, 1.09, …\n$ WaffleHouses      &lt;int&gt; 128, 0, 18, 41, 0, 11, 0, 3, 0, 133, 381, 0, 0, 2, 17, 0, 6, 64, 66, 0, 11, 0, 0, 0, 72, 39, 0, 0, 0, …\n$ South             &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ Slaves1860        &lt;int&gt; 435080, 0, 0, 111115, 0, 0, 0, 1798, 0, 61745, 462198, 0, 0, 0, 0, 0, 2, 225483, 331726, 0, 87189, 0, …\n$ Population1860    &lt;int&gt; 964201, 0, 0, 435450, 379994, 34277, 460147, 112216, 75080, 140424, 1057286, 0, 0, 1711951, 1350428, 6…\n$ PropSlaves1860    &lt;dbl&gt; 4.5e-01, 0.0e+00, 0.0e+00, 2.6e-01, 0.0e+00, 0.0e+00, 0.0e+00, 1.6e-02, 0.0e+00, 4.4e-01, 4.4e-01, 0.0…\n$ d                 &lt;dbl&gt; 1.6542053, 1.5443643, 0.6107159, 2.0935693, -0.9270579, 1.0500799, -1.6410244, -0.4327735, -1.8607064,…\n$ m                 &lt;dbl&gt; 0.022644060, 1.549801620, 0.048974363, 1.655122831, -0.266989270, 0.891544051, -0.793595325, 0.7862228…\n$ a                 &lt;dbl&gt; -0.60628951, -0.68669925, -0.20424076, -1.41038699, 0.59985673, -0.28465051, 1.24313471, 0.43903723, 2…\n\n\nIn the residual plots from the lower two panels of Figure 5.4, we focused on the means of the residuals (i.e., Estimate). However, we can express the uncertainty in the residuals by including error bars for the 95% intervals. Here’s what that might look like with a slight reworking of the lower right panel of Figure 5.4.\n\nr |&gt;\n  ggplot(aes(x = Estimate, y = d)) +\n  stat_smooth(method = \"lm\", fullrange = T,\n              color = \"firebrick4\", fill = \"firebrick4\", \n              alpha = 1/5, linewidth = 1/2) +\n  geom_vline(xintercept = 0, linetype = 2, color = \"grey50\") +\n  # The only change is here\n  geom_pointrange(aes(xmin = Q2.5, xmax = Q97.5),\n                  alpha = 2/3, color = \"firebrick4\") +\n  geom_text_repel(data = r |&gt; filter(Loc %in% c(\"ID\", \"HI\", \"DC\")),  \n                  aes(label = Loc), \n                  size = 3, seed = 5) +\n  scale_x_continuous(limits = c(-2, 3)) +\n  coord_cartesian(xlim = range(r$Estimate),\n                  ylim = range(d$d)) +\n  labs(x = \"Age at marriage residuals\",\n       y = \"Divorce rate (std)\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nLook at that. If you were to fit a follow-up model based on only the point estimates (posterior means) of those residuals, you’d be ignoring a lot of uncertainty. For more on the topic of residuals, see Freckleton (2002), On the misuse of residuals in ecology: regression of residuals vs. multiple regression.\n\n\n5.1.5.1.2 Bonus: Rethinking residuals\nMcElreath was a little inconsistent with how he talked about residuals in the text. The typical definition of residuals nowadays is\n\\[r_i = y_i - \\hat y_i.\\]\nThat is, residuals are the observed values \\((y_i)\\) minus the predicted values \\((\\hat y_i)\\). This is the opposite of what McElreath wrote at the bottom of page 135: “We compute the residuals by subtracting the observed marriage rate in each State from the predicted rate, based upon the model.” But to McElreath’s credit, in page 9 of McCullagh & Nelder (1989), we read: “Gauss defined the residuals with opposite sign to that in current use, i.e. by \\(X \\beta - y\\),” which we might express in the notation above as\n\\[r_{i \\text{, old}} = \\hat y_i - y_i\\]\nwhich is a formal expression of what we read in McElreath’s quote. The confusion continues because in McElreath’s version of Figure 5.4 in the text (as well as our version), it appears he used the contemporary definition of residuals \\((r_i)\\), but when he introduced his R2_is_bad() function in Section 7.1.1, he used the older Gaussian definition \\((r_{i \\text{, old}})\\). I don’t think it matters at all for the R2_is_bad() function, and it matters very little for residual plots like this in this chapter. But it would have been nice if the text was more consistent, and I can empathize with any confused readers.\n\n\n\n5.1.5.2 Posterior prediction plots\n“It’s important to check the model’s implied predictions against the observed data” (p. 137). For more on the topic, check out Gabry and colleagues’ (2019) Visualization in Bayesian workflow or Simpson’s related blog post, Touch me, I want to feel your data.\nThe code below will make our version of Figure 5.5.\n\nf &lt;- fitted(b5.3) |&gt;\n  data.frame() |&gt;\n  # Un-standardize the model predictions\n  mutate(across(.cols = -Est.Error, .fns = \\(x) x * sd(d$Divorce) + mean(d$Divorce))) |&gt; \n  bind_cols(d) \n\nf |&gt;\n  ggplot(aes(x = Divorce, y = Estimate)) +\n  geom_abline(linetype = 2, color = \"grey50\", linewidth = 0.5) +\n  geom_point(size = 1.5, color = \"firebrick4\", alpha = 3/4) +\n  geom_linerange(aes(ymin = Q2.5, ymax = Q97.5),\n                 linewidth = 1/4, color = \"firebrick4\") +\n  geom_text(data = f |&gt; filter(Loc %in% c(\"ID\", \"UT\", \"RI\", \"ME\")),\n            aes(label = Loc), \n            hjust = 1, nudge_x = - 0.25) +\n  labs(x = \"Observed divorce\", y = \"Predicted divorce\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nIt’s easy to see from this arrangement of the simulations that the model under-predicts for States with very high divorce rates while it over-predicts for States with very low divorce rates. That’s normal. This is what regression does–it is skeptical of extreme values, so it expects regression towards the mean. But beyond this general regression to the mean, some States are very frustrating to the model, lying very far from the diagonal. (p. 139)\n\n\n5.1.5.2.1 Rethinking: Stats, huh, yeah what is it good for?\n\nOften people want statistical modeling to do things that statistical modeling cannot do. For example, we’d like to know whether an effect is “real” or rather spurious. Unfortunately, modeling merely quantifies uncertainty in the precise way that the model understands the problem. Usually answers to large world questions about truth and causation depend upon information not included in the model. For example, any observed correlation between an outcome and predictor could be eliminated or reversed once another predictor is added to the model. But if we cannot think of the right variable, we might never notice. Therefore all statistical models are vulnerable to and demand critique, regardless of the precision of their estimates and apparent accuracy of their predictions. (p. 139)\n\n\n\n5.1.5.2.2 Overthinking: Simulating spurious association\n\nn &lt;- 100                      # Number of cases\n\nset.seed(5)                   # Setting the seed makes the results reproducible\nd_spur &lt;- tibble(\n  x_real = rnorm(n),          # `x_real` as Gaussian with mean 0 and SD 1 (i.e., the defaults)\n  x_spur = rnorm(n, x_real),  # `x_spur` as Gaussian with mean = `x_real`\n  y =      rnorm(n, x_real))  # `y` as Gaussian with mean = `x_real`\n\nHere are the quick pairs() plots.\n\npairs(d_spur, col = \"firebrick4\")\n\n\n\n\n\n\n\n\nWe may as well fit and evaluate a model.\n\nb5.0_spur &lt;- brm(\n  data = d_spur, \n  family = gaussian,\n  y ~ 1 + x_real + x_spur,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.00_spur\")\n\n\nfixef(b5.0_spur) |&gt; \n  round(digits = 2)\n\n          Estimate Est.Error  Q2.5 Q97.5\nIntercept    -0.01      0.09 -0.19  0.17\nx_real        0.92      0.15  0.64  1.21\nx_spur        0.09      0.09 -0.10  0.27\n\n\nIf we let “r” stand for x_rel and “s” stand for x_spur, here’s how we might depict that our simulation in a DAG.\n\ndag_coords &lt;- tibble(\n  name = c(\"r\", \"s\", \"y\"),\n  x    = c(1, 3, 2),\n  y    = c(2, 2, 1))\n\ndagify(s ~ r,\n       y ~ r,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(alpha = 1/4, color = \"firebrick\", size = 10) +\n  geom_dag_text(color = \"firebrick\") +\n  geom_dag_edges(edge_color = \"firebrick\") +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\n\n\n\n\n\n5.1.5.3 Counterfactual plots\n\nA second sort of inferential plot displays the causal implications of the model. I call these plots counterfactual, because they can be produced for any values of the predictor variables you like, even unobserved combinations like very high median age of marriage and very high marriage rate. There are no States with this combination, but in a counterfactual plot, you can ask the model for a prediction for such a State. (p. 140, emphasis in the original)\n\nTake another look at one of the DAGs from back in Section 5.1.2.\n\ndag_coords &lt;- tibble(\n  name = c(\"A\", \"M\", \"D\"),\n  x    = c(1, 3, 2),\n  y    = c(2, 2, 1))\n\ndagify(M ~ A,\n       D ~ A + M,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(alpha = 1/4, color = \"firebrick\", size = 10) +\n  geom_dag_text(color = \"firebrick\") +\n  geom_dag_edges(edge_color = \"firebrick\") +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nThe full statistical model implied in this DAG requires we have two criterion variables, \\(D\\) and \\(M\\). To simultaneously model the effects of \\(A\\) on \\(M\\) and \\(D\\) AND the effects of \\(A\\) on \\(M\\) with brms, we’ll need to invoke the multivariate syntax. There are several ways to do this with brms, which Bürkner outlines in his (2022b) vignette, Estimating multivariate models with brms. At this point, it’s important to recognize we have two regression models. As a first step, we might specify each model separately in a bf() function and save them as objects.\n\nd_model &lt;- bf(d ~ 1 + a + m)\nm_model &lt;- bf(m ~ 1 + a)\n\nNext we will combine our bf() objects with the + operator within the brm() function. For a model like this, we also specify set_rescor(FALSE) to prevent brms from adding a residual correlation between d and m. Also, notice how each prior statement includes a resp argument. This clarifies which sub-model the prior refers to.\n\nb5.3_A &lt;- brm(\n  data = d, \n  family = gaussian,\n  d_model + m_model + set_rescor(FALSE),\n  prior = c(prior(normal(0, 0.2), class = Intercept, resp = d),\n            prior(normal(0, 0.5), class = b, resp = d),\n            prior(exponential(1), class = sigma, resp = d),\n            \n            prior(normal(0, 0.2), class = Intercept, resp = m),\n            prior(normal(0, 0.5), class = b, resp = m),\n            prior(exponential(1), class = sigma, resp = m)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.03_A\")\n\nLook at the summary.\n\nprint(b5.3_A)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity\n         mu = identity \nFormula: d ~ 1 + a + m \n         m ~ 1 + a \n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nd_Intercept     0.00      0.10    -0.20     0.20 1.00     5975     3081\nm_Intercept    -0.00      0.09    -0.17     0.17 1.00     5417     3322\nd_a            -0.60      0.16    -0.92    -0.29 1.00     3482     3063\nd_m            -0.06      0.16    -0.37     0.26 1.00     3646     3168\nm_a            -0.69      0.10    -0.89    -0.50 1.00     5139     2718\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_d     0.83      0.09     0.68     1.02 1.00     4973     2767\nsigma_m     0.71      0.07     0.58     0.87 1.00     5213     2967\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote our parameters now all have either a d_ or an m_ prefix to help clarify which sub-model they were for. The m_a row shows how strongly and negatively associated a is to m. Here’s how we might use predict() to make our version of the counterfactual plot in the left panel of Figure 5.6.\n\nnd &lt;- tibble(a = seq(from = -2, to = 2, length.out = 30),\n             m = 0)\n\np1 &lt;- predict(b5.3_A, resp = \"d\", newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = a, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_smooth(stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick\", linewidth = 1/4) +\n  labs(x = \"manipulated A\",\n       y = \"counterfactual D\",\n       subtitle = \"Total counterfactual effect of A on D\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\np1\n\n\n\n\n\n\n\n\nBecause the plot is based on a multivariate model, we used the resp argument within predict() to tell brms which of our two criterion variables (d or m) we were interested in. Unlike McElreath’s R code 5.20, we included predictor values for both a and m. This is because brms requires we provide values for all predictors in a model when using predict(). Even though we set all the m values to 0 for the counterfactual, it was necessary to tell predict() that’s exactly what we wanted.\nLet’s do that all again, this time making the counterfactual for d. While we’re at it, we’ll combine this subplot with the last one to make the full version of Figure 5.6.\n\nnd &lt;- tibble(a = seq(from = -2, to = 2, length.out = 30))\n\np2 &lt;- predict(b5.3_A, resp = \"m\", newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = a, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_smooth(stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick\", linewidth = 1/4) +\n  labs(x = \"manipulated A\",\n       y = \"counterfactual M\",\n       subtitle = \"Counterfactual effect of A on M\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\np1 + p2 + plot_annotation(title = \"Counterfactual plots for the multivariate divorce model\")\n\n\n\n\n\n\n\n\nWith our brms + tidyverse paradigm, we might compute “the expected causal effect of increasing median age at marriage from 20 to 30” (p. 142) like this.\n\n# New data frame, standardized to mean 26.1 and std dev 1.24 \nnd &lt;- tibble(a = (c(20, 30) - 26.1) / 1.24,\n             m = 0)\n\npredict(b5.3_A,\n        resp = \"d\",\n        newdata = nd,\n        summary = F) |&gt; \n  data.frame() |&gt; \n  set_names(\"a20\", \"a30\") |&gt; \n  mutate(difference = a30 - a20) |&gt; \n  summarise(mean = mean(difference))\n\n       mean\n1 -4.861794\n\n\n\nThe trick with simulating counterfactuals is to realize that when we manipulate some variable \\(X\\), we break the causal influence of other variables on \\(X\\). This is the same as saying we modify the DAG so that no arrows enter \\(X\\). Suppose for example that we now simulate the effect of manipulating \\(M.\\) (p. 143)\n\nHere’s how to plot that DAG.\n\ndag_coords &lt;- tibble(\n  name = c(\"A\", \"M\", \"D\"),\n  x    = c(1, 3, 2),\n  y    = c(2, 2, 1))\n\ndagify(D ~ A + M,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(alpha = 1/4, color = \"firebrick\", size = 10) +\n  geom_dag_text(color = \"firebrick\") +\n  geom_dag_edges(edge_color = \"firebrick\") +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\n\n\nHere’s the new counterfactual plot focusing on \\(M \\rightarrow D\\), holding \\(A = 0\\), Figure 5.7.\n\nnd &lt;- tibble(m = seq(from = -2, to = 2, length.out = 30),\n             a = 0)\n\npredict(b5.3_A,\n        resp = \"d\",\n        newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = m, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_smooth(stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick\", linewidth = 1/4) +\n  labs(x = \"manipulated M\",\n       y = \"counterfactual D\",\n       subtitle = \"Total counterfactual effect of M on D\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\n\n\n\n5.1.5.3.1 Overthinking: Simulating counterfactuals\nJust like McElreath showed how to compute the counterfactuals without his sim() function, we can make ours without brms::predict(). First we’ll start out extracting the posterior draws.\n\npost &lt;- as_draws_df(b5.3_A)\n\nHere we use expand_grid() to elongate the output from above by a factor of thirty, each time corresponding to one of the levels of a = seq(from = -2, to = 2, length.out = 30). In the two mutate() lines that follow, we plug the model formulas into the rnorm() function to take random draws from posterior predictive distribution. The rest is just wrangling and summarizing.\n\nset.seed(5)\n\npost &lt;- post |&gt; \n  expand_grid(a = seq(from = -2, to = 2, length.out = 30)) |&gt; \n  mutate(m_sim = rnorm(n(), mean = b_m_Intercept + b_m_a * a, sd = sigma_m)) |&gt; \n  mutate(d_sim = rnorm(n(), mean = b_d_Intercept + b_d_a * a + b_d_m * m_sim, sd = sigma_d)) |&gt; \n  pivot_longer(ends_with(\"sim\")) |&gt; \n  group_by(a, name) |&gt; \n  summarise(mean = mean(value),\n            ll = quantile(value, prob = 0.025),\n            ul = quantile(value, prob = 0.975))\n\n# What did we do?\nhead(post)\n\n# A tibble: 6 × 5\n# Groups:   a [3]\n      a name   mean      ll    ul\n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 -2    d_sim 1.15  -0.637   2.88\n2 -2    m_sim 1.38  -0.0588  2.85\n3 -1.86 d_sim 1.03  -0.707   2.68\n4 -1.86 m_sim 1.29  -0.198   2.73\n5 -1.72 d_sim 0.979 -0.751   2.70\n6 -1.72 m_sim 1.17  -0.272   2.66\n\n\nNow we plot.\n\npost |&gt; \n  mutate(dv = if_else(name == \"d_sim\", \"predictions for D\", \"predictions for M\")) |&gt; \n  \n  ggplot(aes(x = a, y = mean, ymin = ll, ymax = ul)) +\n  geom_smooth(stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick\", linewidth = 1/4) +\n  labs(x = \"manipulated A\",\n       y = \"counterfactual\",\n       title = \"Hand-made counterfactual plots for the multivariate divorce model\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  facet_wrap(~ dv) +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Many Variables & The Spurious Waffles</span>"
    ]
  },
  {
    "objectID": "05.html#sec-Masked-relationship",
    "href": "05.html#sec-Masked-relationship",
    "title": "5  The Many Variables & The Spurious Waffles",
    "section": "5.2 Masked relationship",
    "text": "5.2 Masked relationship\n\nA second reason to use more than one predictor variable is to measure the direct influences of multiple factors on an outcome, when none of those influences is apparent from bivariate relationships. This kind of problem tends to arise when there are two predictor variables that are correlated with one another. However, one of these is positively correlated with the outcome and the other is negatively correlated with it. (p. 144)\n\nLet’s load the Hinde & Milligan (2011) milk data.\n\ndata(milk, package = \"rethinking\")\nd &lt;- milk\nrm(milk)\n\nglimpse(d)\n\nRows: 29\nColumns: 8\n$ clade          &lt;fct&gt; Strepsirrhine, Strepsirrhine, Strepsirrhine, Strepsirrhine, Strepsirrhine, New World Monkey, New World Mo…\n$ species        &lt;fct&gt; Eulemur fulvus, E macaco, E mongoz, E rubriventer, Lemur catta, Alouatta seniculus, A palliata, Cebus ape…\n$ kcal.per.g     &lt;dbl&gt; 0.49, 0.51, 0.46, 0.48, 0.60, 0.47, 0.56, 0.89, 0.91, 0.92, 0.80, 0.46, 0.71, 0.71, 0.73, 0.68, 0.72, 0.9…\n$ perc.fat       &lt;dbl&gt; 16.60, 19.27, 14.11, 14.91, 27.28, 21.22, 29.66, 53.41, 46.08, 50.58, 41.35, 3.93, 38.38, 36.90, 39.17, 4…\n$ perc.protein   &lt;dbl&gt; 15.42, 16.91, 16.85, 13.18, 19.50, 23.58, 23.46, 15.80, 23.34, 22.33, 20.85, 25.30, 20.09, 21.27, 14.65, …\n$ perc.lactose   &lt;dbl&gt; 67.98, 63.82, 69.04, 71.91, 53.22, 55.20, 46.88, 30.79, 30.58, 27.09, 37.80, 70.77, 41.53, 41.83, 46.18, …\n$ mass           &lt;dbl&gt; 1.95, 2.09, 2.51, 1.62, 2.19, 5.25, 5.37, 2.51, 0.71, 0.68, 0.12, 0.47, 0.32, 0.60, 3.47, 1.55, 7.08, 3.2…\n$ neocortex.perc &lt;dbl&gt; 55.16, NA, NA, NA, NA, 64.54, 64.54, 67.64, NA, 68.85, 58.85, 61.69, 60.32, NA, NA, 69.97, NA, 70.41, NA,…\n\n\nYou might inspect the primary variables in the data with the pairs() function.\n\nd |&gt; \n  select(kcal.per.g, mass, neocortex.perc) |&gt; \n  pairs(col = \"firebrick4\")\n\n\n\n\n\n\n\n\nBy just looking at that mess, do you think you could describe the associations of mass and neocortex.perc with the criterion, kcal.per.g? I couldn’t. It’s a good thing we have math.\nLet’s standardize our variables by hand.\n\nd &lt;- d |&gt; \n  mutate(kcal.per.g_s     = (kcal.per.g - mean(kcal.per.g)) / sd(kcal.per.g), \n         log_mass_s       = (log(mass) - mean(log(mass))) / sd(log(mass)), \n         neocortex.perc_s = (neocortex.perc - mean(neocortex.perc, na.rm = T)) / sd(neocortex.perc, na.rm = T))\n\nMcElreath has us starting off our first milk model with more permissive priors than we’ve used in the past. Although we should note that from a historical perspective, these priors are pretty informative. Times keep changing.\n\nb5.5_draft &lt;- brm(\n  data = d, \n  family = gaussian,\n  kcal.per.g_s ~ 1 + neocortex.perc_s,\n  prior = c(prior(normal(0, 1), class = Intercept),\n            prior(normal(0, 1), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  sample_prior = T,\n  file = \"fits/b05.05_draft\")\n\nSimilar to the rethinking example in the text, brms warned that “Rows containing NAs were excluded from the model.” This isn’t necessarily a problem; the model fit just fine. But we should be ashamed of ourselves and look eagerly forward to Chapter 15 where we’ll learn how to do better.\nTo compliment how McElreath removed cases with missing values on our variables of interest with base R complete.cases(), here we’ll do so with tidyr::drop_na() and a little help with ends_with().\n\ndcc &lt;- d |&gt;\n  drop_na(ends_with(\"_s\"))\n\n# How many rows did we drop?\nnrow(d) - nrow(dcc)\n\n[1] 12\n\n\nWe’ll use update() to refit the model with the altered data.\n\nb5.5_draft_cc &lt;- update(\n  b5.5_draft,\n  newdata = dcc,\n  seed = 5,\n  file = \"fits/b05.05_draft_cc\")\n\n“Before considering the posterior predictions, let’s consider those priors. As in many simple linear regression problems, these priors are harmless. But are they reasonable?” (p. 146). Let’s find out with our version of Figure 5.8.a.\n\nset.seed(5)\n\nprior_draws(b5.5_draft_cc) |&gt; \n  slice_sample(n = 50) |&gt; \n  rownames_to_column() |&gt; \n  expand_grid(neocortex.perc_s = c(-2, 2)) |&gt; \n  mutate(kcal.per.g_s = Intercept + b * neocortex.perc_s) |&gt; \n  \n  ggplot(aes(x = neocortex.perc_s, y = kcal.per.g_s)) +\n  geom_line(aes(group = rowname),\n            alpha = 0.4, color = \"firebrick\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  labs(x = \"neocortex percent (std)\",\n       y = \"kilocal per g (std)\",\n       subtitle = \"Intercept ~ dnorm(0, 1)\\nb ~ dnorm(0, 1)\") +\n  theme_bw() +\n  theme(panel.grid = element_blank()) \n\n\n\n\n\n\n\n\nThat’s a mess. How’d the posterior turn out?\n\nprint(b5.5_draft_cc)\n\n Family: gaussian \n  Links: mu = identity \nFormula: kcal.per.g_s ~ 1 + neocortex.perc_s \n   Data: dcc (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.10      0.26    -0.43     0.63 1.00     3601     2880\nneocortex.perc_s     0.15      0.28    -0.41     0.71 1.00     3194     2307\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.13      0.21     0.81     1.64 1.00     2903     2805\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLet’s tighten up our priors and fit b5.5.\n\nb5.5 &lt;- brm(\n  data = dcc, \n  family = gaussian,\n  kcal.per.g_s ~ 1 + neocortex.perc_s,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  sample_prior = T,\n  file = \"fits/b05.05\")\n\nNow make our version of Figure 5.8.b.\n\nset.seed(5)\nprior_draws(b5.5) |&gt; \n  slice_sample(n = 50) |&gt; \n  rownames_to_column() |&gt; \n  expand_grid(neocortex.perc_s = c(-2, 2)) |&gt; \n  mutate(kcal.per.g_s = Intercept + b * neocortex.perc_s) |&gt; \n  \n  ggplot(aes(x = neocortex.perc_s, y = kcal.per.g_s, group = rowname)) +\n  geom_line(alpha = 0.4, color = \"firebrick\") +\n  coord_cartesian(ylim = c(-2, 2)) +\n  labs(x = \"neocortex percent (std)\",\n       y = \"kilocal per g (std)\",\n       subtitle = \"Intercept ~ dnorm(0, 0.2)\\nb ~ dnorm(0, 0.5)\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nLook at the posterior summary.\n\nprint(b5.5)\n\n Family: gaussian \n  Links: mu = identity \nFormula: kcal.per.g_s ~ 1 + neocortex.perc_s \n   Data: dcc (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.04      0.16    -0.28     0.37 1.00     3830     2660\nneocortex.perc_s     0.13      0.24    -0.34     0.61 1.00     3534     2530\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.11      0.21     0.79     1.62 1.00     3473     2571\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe results are very similar to those returned earlier from print(b5.5_draft). It’s not in the text, but let’s compare the parameter estimates between the two models with another version of our homemade coeftab() plot.\n\nall.equal(\n  bind_rows(\n    as_draws_df(b5.5_draft_cc) |&gt; select(b_Intercept:sigma),\n    as_draws_df(b5.5) |&gt; select(b_Intercept:sigma)),\n  bind_rows(as_draws_df(b5.5_draft_cc), as_draws_df(b5.5)) |&gt; \n    select(b_Intercept:sigma)\n)\n\n[1] TRUE\n\n\n\n# Wrangle\nbind_rows(as_draws_df(b5.5_draft_cc), as_draws_df(b5.5)) |&gt; \n  select(b_Intercept:sigma) |&gt; \n  mutate(fit = rep(c(\"b5.5_draft\", \"b5.5\"), each = n() / 2)) |&gt; \n  pivot_longer(-fit) |&gt; \n  group_by(name, fit) |&gt; \n  summarise(mean = mean(value),\n            ll = quantile(value, prob = 0.025),\n            ul = quantile(value, prob = 0.975)) |&gt; \n  mutate(fit = factor(fit, levels = c(\"b5.5_draft\", \"b5.5\"))) |&gt; \n  \n  # Plot\n  ggplot(aes(x = mean, y = fit, xmin = ll, xmax = ul)) +\n  geom_pointrange(color = \"firebrick\") +\n  geom_hline(yintercept = 0, alpha = 1/5, color = \"firebrick\") +\n  labs(x = \"posterior\", \n       y = NULL) +\n  facet_wrap(~ name, ncol = 1) +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        strip.background = element_blank())\n\n\n\n\n\n\n\n\nThe results were quite similar, but the posteriors from b5.5 are more precise. Let’s get back on track with the text and make the top left panel of Figure 5.9. Just for kicks, we’ll superimpose 50% intervals atop 95% intervals for the next few plots. Here’s Figure 5.9, top left.\n\nnd &lt;- tibble(neocortex.perc_s = seq(from = -2.5, to = 2, length.out = 30))\n\nfitted(b5.5, \n       newdata = nd,\n       probs = c(0.025, 0.975, 0.25, 0.75)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = neocortex.perc_s, y = Estimate)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/5, fill = \"firebrick\") +\n  geom_smooth(aes(ymin = Q25, ymax = Q75),\n              stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick4\", linewidth = 1/2) +\n  geom_point(data = dcc, \n             aes(x = neocortex.perc_s, y = kcal.per.g_s),\n             color = \"firebrick4\", size = 2) +\n  coord_cartesian(xlim = range(dcc$neocortex.perc_s), \n                  ylim = range(dcc$kcal.per.g_s)) +\n  labs(x = \"neocortex percent (std)\",\n       y = \"kilocal per g (std)\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nDo note the probs argument in the fitted() code, above.\nNow we use log_mass_s as the new sole predictor.\n\nb5.6 &lt;- brm(\n  data = dcc, \n  family = gaussian,\n  kcal.per.g_s ~ 1 + log_mass_s,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  sample_prior = T,\n  file = \"fits/b05.06\")\n\n\nprint(b5.6)\n\n Family: gaussian \n  Links: mu = identity \nFormula: kcal.per.g_s ~ 1 + log_mass_s \n   Data: dcc (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      0.05      0.16    -0.27     0.36 1.00     3959     3206\nlog_mass_s    -0.27      0.21    -0.69     0.16 1.00     2774     2485\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.20     0.76     1.53 1.00     3339     2400\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nMake Figure 5.9, top right.\n\nnd &lt;- tibble(log_mass_s = seq(from = -2.5, to = 2.5, length.out = 30))\n\nfitted(b5.6, \n       newdata = nd,\n       probs = c(0.025, 0.975, 0.25, 0.75)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = log_mass_s, y = Estimate)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/5, fill = \"firebrick\") +\n  geom_smooth(aes(ymin = Q25, ymax = Q75),\n              stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick4\", linewidth = 1/2) +\n  geom_point(data = dcc, \n             aes(y = kcal.per.g_s),\n             color = \"firebrick4\", size = 2) +\n  coord_cartesian(xlim = range(dcc$log_mass_s), \n                  ylim = range(dcc$kcal.per.g_s)) +\n  labs(x = \"log body mass (std)\",\n       y = \"kilocal per g (std)\") +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nFinally, we’re ready to fit with both predictors included in a multivariable model. The statistical formula is\n\\[\\begin{align*}\n\\text{kcal.per.g\\_s}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + \\beta_1 \\text{neocortex.perc\\_s}_i + \\beta_2 \\text{log\\_mass\\_s} \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nFit the model.\n\nb5.7 &lt;- brm(\n  data = dcc, \n  family = gaussian,\n  kcal.per.g_s ~ 1 + neocortex.perc_s + log_mass_s,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.07\")\n\n\nprint(b5.7)\n\n Family: gaussian \n  Links: mu = identity \nFormula: kcal.per.g_s ~ 1 + neocortex.perc_s + log_mass_s \n   Data: dcc (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            0.07      0.14    -0.23     0.34 1.00     3322     2728\nneocortex.perc_s     0.60      0.28     0.03     1.13 1.00     2110     2101\nlog_mass_s          -0.63      0.25    -1.09    -0.12 1.00     2097     2199\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.87      0.18     0.59     1.30 1.00     2423     2151\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOnce again, let’s roll out our homemade coefplot() plot code.\n\nbind_cols(\n  as_draws_df(b5.5) |&gt; \n    transmute(`b5.5_beta[N]` = b_neocortex.perc_s),\n  as_draws_df(b5.6) |&gt; \n    transmute(`b5.6_beta[M]` = b_log_mass_s),\n  as_draws_df(b5.7) |&gt; \n    transmute(`b5.7_beta[N]` = b_neocortex.perc_s,\n              `b5.7_beta[M]` = b_log_mass_s)\n  ) |&gt; \n  pivot_longer(everything()) |&gt; \n  group_by(name) |&gt; \n  summarise(mean = mean(value),\n            ll   = quantile(value, prob = 0.025),\n            ul   = quantile(value, prob = 0.975)) |&gt; \n  separate(name, into = c(\"fit\", \"parameter\"), sep = \"_\") |&gt; \n  \n  ggplot(aes(x = mean, y = fit, xmin = ll, xmax = ul)) +\n  geom_pointrange(color = \"firebrick\") +\n  geom_hline(yintercept = 0, alpha = 1/5, color = \"firebrick\") +\n  ylab(NULL) +\n  facet_wrap(~ parameter, ncol = 1, labeller = label_parsed) +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        strip.background = element_rect(fill = \"transparent\", color = \"transparent\"))\n\n\n\n\n\n\n\n\nOn page 151, McElreath suggested we look at a pairs plot to get a sense of the zero-order correlations. We did that once with the raw data. Here it is, again, but with the transformed variables.\n\ndcc |&gt; \n  select(ends_with(\"_s\")) |&gt; \n  pairs(col = \"firebrick4\")\n\n\n\n\n\n\n\n\nHave you noticed how un-tidyverse-like those pairs() plots are? I have. Within the tidyverse, you can make custom pairs plots with the GGally package (Schloerke et al., 2021), which will also compute the point estimates for the bivariate correlations. Here’s a default-style plot.\n\nlibrary(GGally)\n\ndcc |&gt; \n  select(ends_with(\"_s\")) |&gt; \n  ggpairs()\n\n\n\n\n\n\n\n\nBut you can customize these, too. E.g.,\n\n# Define custom functions\nmy_diag &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_density(fill = \"firebrick4\", linewidth = 0)\n}\n\nmy_lower &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_smooth(method = \"lm\", color = \"firebrick4\", linewidth = 1, \n                se = F) +\n    geom_point(color = \"firebrick\", alpha = 0.8, size = 1/3)\n  }\n\n# Plot\ndcc |&gt; \n  select(ends_with(\"_s\")) |&gt; \n  ggpairs(upper = list(continuous = wrap(\"cor\", family = \"sans\", color = \"black\")),\n          # Plug those custom functions into `ggpairs()`\n          diag  = list(continuous = my_diag),\n          lower = list(continuous = my_lower)) + \n  theme_bw() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        strip.background = element_rect(color = \"white\", fill = \"white\"))\n\n\n\n\n\n\n\n\n\nWhat the regression model does is ask if species that have high neocortex percent for their body mass have higher milk energy. Likewise, the model asks if species with high body mass for their neocortex percent have higher milk energy. Bigger species, like apes, have milk with less energy. But species with more neocortex tend to have richer milk. The fact that these two variables, body size and neocortex, are correlated across species makes it hard to see these relationships, unless we account for both.\nSome DAGs will help. (p. 148, emphasis in the original)\n\nHere are three. I’m not aware we can facet dagify() objects. But we can take cues from Chapter 4 to link our three DAGs like McElreath did his. first, we’ll recognize the ggplot2 code will be nearly identical for each DAG. So we can just wrap the ggplot2 code into a compact function, like so.\n\ngg_dag &lt;- function(d) {\n  d |&gt; \n    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(alpha = 1/4, color = \"firebrick\", size = 10) +\n    geom_dag_text(color = \"firebrick\") +\n    geom_dag_edges(edge_color = \"firebrick\") +\n    scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n    scale_y_continuous(NULL, breaks = NULL, expand = c(0.2, 0.2)) +\n    theme_bw() +\n    theme(panel.grid = element_blank())\n}\n\nNow we’ll make the three individual DAGs, saving each.\n\n# Left DAG\ndag_coords &lt;- tibble(\n  name = c(\"M\", \"N\", \"K\"),\n  x    = c(1, 3, 2),\n  y    = c(2, 2, 1))\n\np1 &lt;- dagify(\n  N ~ M,\n  K ~ M + N,\n  coords = dag_coords) |&gt;\n  gg_dag()\n\n# Middle DAG\np2 &lt;- dagify(\n  M ~ N,\n  K ~ M + N,\n  coords = dag_coords) |&gt;\n  gg_dag()\n\n# Right DAG\ndag_coords &lt;- tibble(\n  name = c(\"M\", \"N\", \"K\", \"U\"),\n  x    = c(1, 3, 2, 2),\n  y    = c(2, 2, 1, 2))\np3 &lt;- dagify(\n  M ~ U,\n  N ~ U,\n  K ~ M + N,\n  coords = dag_coords) |&gt;\n  gg_dag() +\n  geom_point(x = 2, y = 2,\n             color = \"firebrick4\", shape = 1, size = 10, stroke = 1.25)\n\nNow we combine our gg_dag() plots together with patchwork syntax.\n\np1 + p2 + p3\n\n\n\n\n\n\n\n\n\nWhich of these graphs is right? We can’t tell from the data alone, because these graphs imply the same set of conditional independencies. In this case, there are no conditional independencies–each DAG above implies that all pairs of variables are associated, regardless of what we condition on. A set of DAGs with the same conditional independencies is known as a Markov equivalence set. (p. 151, emphasis in the original).\n\nLet’s make the counterfactual plots at the bottom of Figure 5.9. Here’s the one on the left.\n\nnd &lt;- tibble(neocortex.perc_s = seq(from = -2.5, to = 2, length.out = 30),\n             log_mass_s       = 0)\n\np1 &lt;- fitted(\n  b5.7, \n  newdata = nd,\n  probs = c(0.025, 0.975, 0.25, 0.75)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = neocortex.perc_s, y = Estimate)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/5, fill = \"firebrick\") +\n  geom_smooth(aes(ymin = Q25, ymax = Q75),\n              stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick4\", linewidth = 1/2) +\n  coord_cartesian(xlim = range(dcc$neocortex.perc_s), \n                  ylim = range(dcc$kcal.per.g_s)) +\n  labs(x = \"neocortex percent (std)\",\n       y = \"kilocal per g (std)\",\n       subtitle = \"Counterfactual holding M = 0\")\n\nNow make Figure 5.9, bottom right, and combine the two.\n\nnd &lt;- tibble(log_mass_s       = seq(from = -2.5, to = 2.5, length.out = 30),\n             neocortex.perc_s = 0)\n\np2 &lt;- fitted(\n  b5.7, \n  newdata = nd,\n  probs = c(0.025, 0.975, 0.25, 0.75)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = log_mass_s, y = Estimate)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/5, fill = \"firebrick\") +\n  geom_smooth(aes(ymin = Q25, ymax = Q75),\n              stat = \"identity\",\n              alpha = 1/5, color = \"firebrick4\", fill = \"firebrick4\", linewidth = 1/2) +\n  coord_cartesian(xlim = range(dcc$log_mass_s), \n                  ylim = range(dcc$kcal.per.g_s)) +\n  labs(x = \"log body mass (std)\",\n       y = \"kilocal per g (std)\",\n       subtitle = \"Counterfactual holding N = 0\")\n\n# Combine\np1 + p2 + \n  plot_annotation(title = \"Figure 5.9 [bottom row]. Milk energy and neocortex among primates.\") &\n  theme_bw() &\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\n5.2.0.1 Overthinking: Simulating a masking relationship\nAs a refresher, here’s our focal DAG.\n\ndag_coords &lt;- tibble(\n  name = c(\"M\", \"N\", \"K\"),\n  x    = c(1, 3, 2),\n  y    = c(2, 2, 1))\n\ndagify(N ~ M,\n       K ~ M + N,\n       coords = dag_coords) |&gt;\n  gg_dag()\n\n\n\n\n\n\n\n\nNow simulate data consistent with that DAG.\n\n# How many cases would you like?\nn &lt;- 100\n\nset.seed(5)\nd_sim &lt;- tibble(m = rnorm(n, mean = 0, sd = 1)) |&gt; \n  mutate(n = rnorm(n, mean = m, sd = 1)) |&gt; \n  mutate(k = rnorm(n, mean = n - m, sd = 1))\n\nUse ggpairs() to get a sense of what we just simulated.\n\nd_sim |&gt; \n  ggpairs(upper = list(continuous = wrap(\"cor\", family = \"sans\", color = \"firebrick4\")),\n          diag  = list(continuous = my_diag),\n          lower = list(continuous = my_lower)) + \n  theme_bw() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid = element_blank(),\n        strip.background = element_rect(color = \"white\", fill = \"white\"))\n\n\n\n\n\n\n\n\nHere we fit the simulation models with a little help from the update() function.\n\nb5.7_sim &lt;- update(\n  b5.7,\n  newdata = d_sim,\n  formula = k ~ 1 + n + m,\n  seed = 5,\n  file = \"fits/b05.07_sim\")\n\nb5.5_sim &lt;- update(\n  b5.7_sim,\n  formula = k ~ 1 + n,\n  seed = 5,\n  file = \"fits/b05.05_sim\")\n\nb5.6_sim &lt;- update(\n  b5.7_sim,\n  formula = k ~ 1 + m,\n  seed = 5,\n  file = \"fits/b05.06_sim\")\n\nCompare the coefficients.\n\nfixef(b5.5_sim) |&gt; round(digits = 2)\n\n          Estimate Est.Error  Q2.5 Q97.5\nIntercept    -0.02      0.10 -0.22  0.19\nn             0.58      0.08  0.43  0.74\n\nfixef(b5.6_sim) |&gt; round(digits = 2)\n\n          Estimate Est.Error  Q2.5 Q97.5\nIntercept     0.00      0.12 -0.23  0.23\nm             0.18      0.15 -0.12  0.47\n\nfixef(b5.7_sim) |&gt; round(digits = 2)\n\n          Estimate Est.Error  Q2.5 Q97.5\nIntercept    -0.01      0.09 -0.18  0.17\nn             0.98      0.09  0.79  1.15\nm            -0.88      0.15 -1.17 -0.60\n\n\nDue to space considerations, I’m not going to show the code corresponding to the other two DAGs from the R code 5.43 block. Rather, I’ll leave that as an exercise for the interested reader.\nLet’s do the preliminary work to making our DAGs.\n\ndag5.7 &lt;- dagitty(\"dag{ M -&gt; K &lt;- N M -&gt; N }\" )\n\ncoordinates(dag5.7) &lt;- list(x = c(M = 0, K = 1, N = 2), \n                            y = c(M = 0.5, K = 1, N = 0.5)) \n\nIf you just want a quick default plot, ggdag::ggdag_equivalent_dags() is the way to go.\n\nggdag_equivalent_dags(dag5.7)\n\n\n\n\n\n\n\n\nHowever, if you’d like to customize your DAGs, start with the ggdag::node_equivalent_dags() function and build from there.\n\ndag5.7 |&gt; \n  node_equivalent_dags() |&gt; \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) + \n  geom_dag_point(alpha = 1/4, color = \"firebrick\", size = 10) +\n  geom_dag_text(color = \"firebrick\") +\n  geom_dag_edges(edge_color = \"firebrick\") +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  facet_wrap(~ dag, labeller = label_both) +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank())\n\n\n\n\n\n\n\n\nThese all demonstrate Markov equivalence. I should note that I got help from the great Malcolm Barrett on how to make this plot with ggdag.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Many Variables & The Spurious Waffles</span>"
    ]
  },
  {
    "objectID": "05.html#sec-Categorical-variables",
    "href": "05.html#sec-Categorical-variables",
    "title": "5  The Many Variables & The Spurious Waffles",
    "section": "5.3 Categorical variables",
    "text": "5.3 Categorical variables\n\nMany readers will already know that variables like this, routinely called factors, can easily be included in linear models. But what is not widely understood is how these variables are represented in a model… Knowing how the machine (golem) works both helps you interpret the posterior distribution and gives you additional power in building the model. (p. 153, emphasis in the original)\n\n\n5.3.1 Binary categories\nReload the Howell1 data.\n\ndata(Howell1, package = \"rethinking\")\nd &lt;- Howell1\nrm(Howell1)\n\nIf you forgot what these data were like, take a glimpse().\n\nd |&gt;\n  glimpse()\n\nRows: 544\nColumns: 4\n$ height &lt;dbl&gt; 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149.2250, 168.9100, 147.9550, 165.1000, 154.3050, 151…\n$ weight &lt;dbl&gt; 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.24348, 55.47997, 34.86988, 54.48774, 49.89512, 41.…\n$ age    &lt;dbl&gt; 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.0, 66.0, 73.0, 20.0, 65.3, 36.0, 44.0, 31.0, 12.0,…\n$ male   &lt;int&gt; 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,…\n\n\n\nThe male variable is our new predictor, an example of a indicator variable. Indicator variables—sometimes also called “dummy” variables–are devices for encoding unordered categories into quantitative models. There is no sense here in which “male” is one more than “female.” The purpose of the male variable is to indicate when a person in the sample is “male.” So it takes the value 1 whenever the person is male, but it takes the value 0 when the person belongs to any other category. It doesn’t matter which category is indicated by the 1. The model won’t care. But correctly interpreting the model demands that you remember, so it’s a good idea to name the variable after the category assigned the 1 value. (p. 154, emphasis in the original)\n\nThe statistical model including a male dummy might follow the form\n\\[\\begin{align*}\n\\text{height}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\text{male}_i \\\\\n\\alpha  & \\sim \\operatorname{Normal}(178, 20) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma  & \\sim \\operatorname{Uniform}(0, 50),\n\\end{align*}\\]\nwhere \\(\\beta_1\\) is the expected (i.e., average) difference between males and females for height. Here we simulate from our priors and summarise() the results.\n\nset.seed(5)\n\nprior &lt;- tibble(mu_female = rnorm(1e4, mean = 178, sd = 20)) |&gt; \n  mutate(mu_male = mu_female + rnorm(1e4, mean = 0, sd = 10))\n\nprior |&gt; \n  pivot_longer(everything()) |&gt; \n  group_by(name) |&gt; \n  summarise(mean = mean(value),\n            sd   = sd(value),\n            ll   = quantile(value, prob = 0.025),\n            ul   = quantile(value, prob = 0.975)) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 2 × 5\n  name       mean    sd    ll    ul\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 mu_female  178.  20.2  138.  219.\n2 mu_male    178.  22.5  133.  222.\n\n\nWe might visualize the two prior predictive distributions as overlapping densities.\n\nprior |&gt; \n  pivot_longer(everything()) |&gt; \n  ggplot(aes(x = value, fill = name, color = name)) +\n  geom_density(alpha = 2/3, linewidth = 2/3) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  scale_fill_manual(NULL, values = c(\"firebrick4\", \"black\")) +\n  scale_color_manual(NULL, values = c(\"firebrick4\", \"black\")) +\n  xlab(\"prior predictive distribution for our dummy groups\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        legend.position = c(0.82, 0.83))\n\n\n\n\n\n\n\n\nYep, this parameterization makes \\(\\alpha + \\beta_1\\) more uncertain than \\(\\alpha\\). A nice alternative is to make an index variable. We’ll call it sex, for which 1 = female and 2 = male. “No order is implied. These are just labels” (p. 155).\n\nd &lt;- d |&gt; \n  mutate(sex = ifelse(male == 1, 2, 1))\n\nhead(d)\n\n   height   weight age male sex\n1 151.765 47.82561  63    1   2\n2 139.700 36.48581  63    0   1\n3 136.525 31.86484  65    0   1\n4 156.845 53.04191  41    1   2\n5 145.415 41.27687  51    0   1\n6 163.830 62.99259  35    1   2\n\n\nWe can update our statistical model to include sex with the formula\n\\[\\begin{align*}\n\\text{height}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i    & = \\alpha_{\\text{sex}[i]} \\\\\n\\alpha_j & \\sim \\operatorname{Normal}(178, 20) & \\text{for } j = 1 \\; \\& \\; 2 \\\\\n\\sigma   & \\sim \\operatorname{Uniform}(0, 50),\n\\end{align*}\\]\nwhere now we have rows indexed by \\(i\\) and two levels of sex indexed by \\(j\\). Again, for our version of this model, we will continue using the simple \\(\\lambda = 1\\) exponential prior on \\(\\sigma\\), rather than the uniform. The exponential is just much easier on Stan than the uniform. But if you prefer to go uniform, have at it.\nOne more thing before we fit our model: Notice McElreath’s a[sex] notation in his R code 5.48. I’m not aware that brms will accommodate this notation. The fix is easy. Just save sex as a factor.\n\nd &lt;- d |&gt; \n  mutate(sex = factor(sex))\n\nWe’re ready to fit the model.\n\nb5.8 &lt;- brm(\n  data = d, \n  family = gaussian,\n  height ~ 0 + sex,\n  prior = c(prior(normal(178, 20), class = b),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.08\")\n\nBehold the summary.\n\nprint(b5.8)\n\n Family: gaussian \n  Links: mu = identity \nFormula: height ~ 0 + sex \n   Data: d (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsex1   134.90      1.58   131.80   138.05 1.00     3561     2838\nsex2   142.59      1.72   139.28   145.99 1.00     3675     2853\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    27.42      0.82    25.85    29.07 1.00     3711     2548\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that for us, there was no depth=2 argument to get all the model output. When you fit a model with brms that excludes the typical intercept parameter–when you use the 0 + ... syntax–, you’ll get a separate intercept for each of your factor variables. The brm() function noticed there were two levels for our sex factor, and therefore gave use two intercepts: sex1 and sex2. Here’s how you might compute the difference score.\n\nlibrary(tidybayes)\n\nas_draws_df(b5.8) |&gt; \n  mutate(diff_fm = b_sex1 - b_sex2) |&gt; \n  pivot_longer(cols = c(b_sex1:sigma, diff_fm)) |&gt; \n  group_by(name) |&gt; \n  mean_qi(value, .width = 0.89)\n\n# A tibble: 4 × 7\n  name     value .lower .upper .width .point .interval\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 b_sex1  135.    132.  137.     0.89 mean   qi       \n2 b_sex2  143.    140.  145.     0.89 mean   qi       \n3 diff_fm  -7.69  -11.4  -3.78   0.89 mean   qi       \n4 sigma    27.4    26.1  28.8    0.89 mean   qi       \n\n\nNote how we used tidybayes::mean_qi() to summarize our difference variable, diff_fm. Anyway, “this kind of calculation is called a contrast. No matter how many categories you have, you can use samples from the posterior to compute the contrast between any two” (p. 156, emphasis in the original).\n\n\n5.3.2 Many categories\n\nBinary categories are easy, whether you use an indicator variable or instead an index variable. But when there are more than two categories, the indicator variable approach explodes. You’ll need a new indicator variable for each new category. If you have \\(k\\) unique categories, you need \\(k - 1\\) indicator variables. Automated tools like R’s lm do in fact go this route, constructing \\(k - 1\\) indicator variables for you and returning \\(k - 1\\) parameters (in addition to the intercept).\nBut we’ll instead stick with the index variable approach. It does not change at all when you add more categories. You do get more parameters, of course, just as many as in the indicator variable approach. But the model specification looks just like it does in the binary case. (p. 156)\n\nWe’ll practice with milk.\n\ndata(milk, package = \"rethinking\")\nd &lt;- milk\nrm(milk)\n\nWith the tidyverse, we can peek at clade with distinct() in the place of base R unique().\n\nd |&gt;\n  distinct(clade)\n\n             clade\n1    Strepsirrhine\n2 New World Monkey\n3 Old World Monkey\n4              Ape\n\n\nRather than make the clade_id index variable, like McElreath did in the text, we’ll just use the clade factor. It will actually work easier within the brms framework. We will, however, standardize the kcal.per.g variable, again.\n\nd &lt;- d |&gt; \n  mutate(kcal.per.g_s = (kcal.per.g - mean(kcal.per.g)) / sd(kcal.per.g))\n\nOur statistical model follows the form\n\\[\\begin{align*}\n\\text{kcal.per.g\\_s}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i    & = \\alpha_{\\text{clade}[i]} \\\\\n\\alpha_j & \\sim \\operatorname{Normal}(0, 0.5), & \\text{for } j = 1, \\dots, 4 \\\\\n\\sigma   & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nNow fit that model.\n\nb5.9 &lt;- brm(\n  data = d, \n  family = gaussian,\n  kcal.per.g_s ~ 0 + clade,\n  prior = c(prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.09\")\n\n\nprint(b5.9)\n\n Family: gaussian \n  Links: mu = identity \nFormula: kcal.per.g_s ~ 0 + clade \n   Data: d (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ncladeApe               -0.47      0.24    -0.93     0.02 1.00     4792     2721\ncladeNewWorldMonkey     0.35      0.24    -0.14     0.82 1.00     4649     2788\ncladeOldWorldMonkey     0.64      0.28     0.07     1.18 1.00     4617     2872\ncladeStrepsirrhine     -0.55      0.29    -1.11     0.05 1.00     4794     2925\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.80      0.12     0.61     1.07 1.00     4082     3186\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nUp to this point, all of our coefficient plots have been of a rather complicated type. We tried to mimic McElreath’s coeftab() plots without the aid of the rethinking convenience function. But now the coefficient plot from McElreath’s R code 5.42 is of a much simpler type. We can finally take it easy and use some of the convenience functions available to us within our framework.\nThe mcmc_plot() function is an easy way to get a default coefficient plot. You just put the brms fit object into the function.\n\nmcmc_plot(b5.9, variable = \"^b_\", regex = TRUE)\n\n\n\n\n\n\n\n\nThere are numerous ways to make a coefficient plot. Another is with the mcmc_intervals() function from the bayesplot package (Gabry et al., 2019; Gabry & Mahr, 2022). A nice feature of the bayesplot package is its convenient way to alter the color scheme with the color_scheme_set() function. Here, for example, we’ll make the theme red. But note how the mcmc_intervals() function requires you to work with the as_draws_df() instead of the brmsfit object.\n\nlibrary(bayesplot)\n\ncolor_scheme_set(\"red\")\n\npost &lt;- as_draws_df(b5.9)\n\npost |&gt; \n  select(starts_with(\"b_\")) |&gt; \n  mcmc_intervals(prob = 0.5,\n                 point_est = \"median\") +\n  labs(title = \"My fancy bayesplot-based coefficient plot\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nBecause bayesplot returns a ggplot2 object, the plot was adjustable with familiar ggplot2 syntax. For more ideas, check out Gabry’s (2022) vignette, Plotting MCMC draws using the bayesplot package.\nThe tidybayes::stat_pointinterval() function offers a third way, this time with a more ground-up ggplot2 workflow.\n\nlibrary(tidybayes)\n\npost |&gt; \n  select(starts_with(\"b\")) |&gt; \n  set_names(distinct(d, clade) |&gt; arrange(clade) |&gt; pull()) |&gt; \n  pivot_longer(everything()) |&gt; \n  \n  ggplot(aes(x = value, y = reorder(name, value))) +  # Note how we used `reorder()` to arrange the coefficients\n  geom_vline(xintercept = 0, alpha = 1/10, color = \"firebrick4\") +\n  stat_pointinterval(point_interval = mode_hdi, .width = 0.89, \n                     color = \"firebrick4\", size = 1) +\n  labs(x = \"expected kcal (std)\", \n       y = NULL,\n       title = \"My tidybayes-based coefficient plot\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nOkay, let’s simulate some “made up categories” (p. 157). We’ll use names rather than numeric indices.\n\nhouses &lt;- c(\"Gryffindor\", \"Hufflepuff\", \"Ravenclaw\", \"Slytherin\")\n\nset.seed(63)\nd &lt;- d |&gt; \n  mutate(house = sample(rep(houses, each = 8), size = n()))\n\nHere we attempt to fit the model with brms using the naïve approach. ⚠️\n\nb5.10 &lt;- brm(\n  data = d, \n  family = gaussian,\n  kcal.per.g_s ~ 0 + clade + house,\n  prior = c(prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.10\")\n\nYep, the parameter summary suggests Slytherin stood out.\n\nprint(b5.10)\n\n Family: gaussian \n  Links: mu = identity \nFormula: kcal.per.g_s ~ 0 + clade + house \n   Data: d (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ncladeApe               -0.43      0.26    -0.94     0.08 1.00     3985     3336\ncladeNewWorldMonkey     0.34      0.26    -0.19     0.85 1.00     4362     2761\ncladeOldWorldMonkey     0.50      0.30    -0.07     1.09 1.00     4823     3279\ncladeStrepsirrhine     -0.51      0.29    -1.07     0.08 1.00     4811     3126\nhouseHufflepuff        -0.17      0.28    -0.73     0.38 1.00     4524     3425\nhouseRavenclaw         -0.13      0.27    -0.65     0.43 1.00     4056     3170\nhouseSlytherin          0.49      0.30    -0.13     1.07 1.00     4056     2894\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.77      0.12     0.58     1.04 1.00     3803     3079\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nEven though that last model fit well and is a fine and valid way to examine the relevant variables, it’s a bit misleading. If you look closely at the output, you’ll see there are rows for all four of the clade levels, but only rows for three of the four levels of house. This is easy to miss because McElreath didn’t show the precis() output from his m5.10, but if you run and inspect his code for yourself you’ll see it returns all rows for all four of the house variable.\nWhat gives?\nbrms syntax is flexible in that it provided multiple ways to fit and post-process models. The formula syntax we use most of the type with brms is called the design formula syntax. In the first edition of his text (2015, pp. 159–161), McElreath contrasted design formula syntax, which is widely used in base R lm(), the lme4 package, and brms, with the more explicit syntax he uses in his rethinking package.\nI bring this all up because the rethinking package has no problem handling multiple index variables. Just use McElreath’s slick bracket code like he did with his m5.10 (i.e., mu &lt;- a[clade_id] + h[house]). In such a model, there is no default intercept. That is, McElreath’s m5.10 followed the statistical formula of\n\\[\\begin{align*}\n\\text{kcal.per.g\\_s}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i    & = \\alpha_{\\color{#8B1A1A}{\\text{clade}[i]}} + \\alpha_{\\color{#8B1A1A}{\\text{house}[i]}} \\\\\n\\alpha_{\\color{#8B1A1A}{\\text{clade}, j}} & \\sim \\operatorname{Normal}(0, 0.5), && \\color{#8B1A1A}{\\text{for } j = 1, \\dots, 4} \\\\\n\\alpha_{\\color{#8B1A1A}{\\text{house}, k}} & \\sim \\operatorname{Normal}(0, 0.5), && \\color{#8B1A1A}{\\text{for } k = 1, \\dots, 4} \\\\\n\\sigma   & \\sim \\operatorname{Exponential}(1),\n\\end{align*}\\]\nwhere there is an \\(\\alpha_\\text{clade}\\) “intercept” for each of the four levels of clade and an \\(\\alpha_\\text{house}\\) “intercept” for each of the four levels of house. But there is no overall intercept, \\(\\alpha\\), that stands for the expected value when all the predictors are set to 0.\nWhen we use the typical formula syntax with brms, we can suppress the overall intercept when for a single index variable with the &lt;criterion&gt; ~ 0 + &lt;index variable&gt; syntax. That’s exactly what we did with our b5.9 model. The catch is this approach only works with one index variable within brms. Even though we suppressed the default intercept with our b5.10 formula, kcal.per.g_s ~ 0 + clade + house, we ended up loosing the first category of the second variable, house.\nFortunately, we have a fix, which I learned about from one of Bürkner’s answers to Paul Dong’s thread in the Stan Forums, Indexing approach to predictors. The solution is the use the non-linear syntax. If you recall, our first encounter with the brms non-linear syntax was with model b4.3b in Section 4.4.2.1 where we used it to model the exponent of a parameter. Here we’ll use it to model our two index variables while excluding an overall or reference-category intercept. I’m still going to wait until Section 6.2.1 before walking out the non-linear syntax in detail. But if you’re burning with curiosity, check out Bürkner’s (2022c) vignette, Estimating non-linear models with brms. For now, we focus on how to faithfully replicate McElreath’s m510 with brms::brm(). In the code below, we defined our two model parameters, a and h, in the first bf() line, connected each to a index variable in the next two lines, and closed out the bf() statement with nl = TRUE to alert brm() we were using the non-linear syntax.\n\nb5.11 &lt;- brm(\n  data = d, \n  family = gaussian,\n  bf(kcal.per.g_s ~ 0 + a + h, \n     a ~ 0 + clade, \n     h ~ 0 + house,\n     nl = TRUE),\n  prior = c(prior(normal(0, 0.5), nlpar = a),\n            prior(normal(0, 0.5), nlpar = h),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.11\")\n\nHere’s the summary of the correct model.\n\nprint(b5.11)\n\n Family: gaussian \n  Links: mu = identity \nFormula: kcal.per.g_s ~ 0 + a + h \n         a ~ 0 + clade\n         h ~ 0 + house\n   Data: d (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cladeApe               -0.39      0.28    -0.95     0.17 1.00     3148     3274\na_cladeNewWorldMonkey     0.37      0.29    -0.21     0.94 1.00     2769     2875\na_cladeOldWorldMonkey     0.53      0.31    -0.09     1.14 1.00     3790     2890\na_cladeStrepsirrhine     -0.46      0.32    -1.06     0.17 1.00     3061     2891\nh_houseGryffindor        -0.10      0.29    -0.66     0.48 1.00     3185     2968\nh_houseHufflepuff        -0.20      0.29    -0.78     0.37 1.00     3195     3163\nh_houseRavenclaw         -0.17      0.29    -0.73     0.41 1.00     2977     2701\nh_houseSlytherin          0.46      0.31    -0.13     1.04 1.00     3513     2985\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.78      0.12     0.59     1.05 1.00     3156     2859\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFinally, here’s how we might visualize the results in a faceted coefficient plot.\n\nas_draws_df(b5.11) |&gt; \n  pivot_longer(starts_with(\"b_\")) |&gt; \n  mutate(name = str_remove(name, \"b_\") |&gt; \n           str_remove(\"clade\") |&gt; \n           str_remove(\"house\") |&gt; \n           str_replace(\"World\", \" World \")) \n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n# A tibble: 32,000 × 8\n   sigma lprior  lp__ .chain .iteration .draw name                  value\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1 0.906  -8.13 -42.0      1          1     1 a_Ape              -0.975  \n 2 0.906  -8.13 -42.0      1          1     1 a_New World Monkey -0.316  \n 3 0.906  -8.13 -42.0      1          1     1 a_Old World Monkey  0.181  \n 4 0.906  -8.13 -42.0      1          1     1 a_Strepsirrhine    -0.539  \n 5 0.906  -8.13 -42.0      1          1     1 h_Gryffindor        0.143  \n 6 0.906  -8.13 -42.0      1          1     1 h_Hufflepuff        0.456  \n 7 0.906  -8.13 -42.0      1          1     1 h_Ravenclaw         0.00633\n 8 0.906  -8.13 -42.0      1          1     1 h_Slytherin         1.05   \n 9 0.621  -6.22 -38.1      1          2     2 a_Ape              -0.371  \n10 0.621  -6.22 -38.1      1          2     2 a_New World Monkey  0.465  \n# ℹ 31,990 more rows\n\n\n\nas_draws_df(b5.11) |&gt; \n  pivot_longer(starts_with(\"b_\")) |&gt; \n  mutate(name = str_remove(name, \"b_\") |&gt; \n           str_remove(\"clade\") |&gt; \n           str_remove(\"house\") |&gt; \n           str_replace(\"World\", \" World \")) |&gt; \n  separate(name, into = c(\"predictor\", \"level\"), sep = \"_\") |&gt; \n  mutate(predictor = if_else(predictor == \"a\", \"predictor: clade\", \"predictor: house\")) |&gt; \n  \n  ggplot(aes(x = value, y = reorder(level, value))) +  # Note how we used `reorder()` to arrange the coefficients\n  geom_vline(xintercept = 0, alpha = 1/10, color = \"firebrick4\") +\n  stat_pointinterval(point_interval = mode_hdi, .width = 0.89, \n                     color = \"firebrick4\", size = 1) +\n  labs(x = \"expected kcal (std)\", \n       y = NULL) +\n  facet_wrap(~ predictor, scales = \"free_y\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank(),\n        strip.background = element_blank())\n\n\n\n\n\n\n\n\nJust for fun, we used 89% intervals.\n\n5.3.2.1 Rethinking: Differences and statistical significance\n\nA common error in interpretation of parameter estimates is to suppose that because one parameter is sufficiently far from zero–is “significant”–and another parameter is not–is “not significant”–that the difference between the parameters is also significant. This is not necessarily so. This isn’t just an issue for non-Bayesian analysis: If you want to know the distribution of a difference, then you must compute that difference, a contrast. (p. 158, emphasis in the original)\n\nThis reminds me of a paper by Gelman and Stern (2006), The difference between “significant” and “not significant” is not itself statistically significant.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Many Variables & The Spurious Waffles</span>"
    ]
  },
  {
    "objectID": "05.html#summary-bonus-we-can-model-categorical-variables-in-more-ways-than-one",
    "href": "05.html#summary-bonus-we-can-model-categorical-variables-in-more-ways-than-one",
    "title": "5  The Many Variables & The Spurious Waffles",
    "section": "5.4 Summary Bonus: We can model categorical variables in more ways than one",
    "text": "5.4 Summary Bonus: We can model categorical variables in more ways than one\nOne of the noteworthy changes in the second edition of Statistical rethinking is McElreath’s consistent use of the index variable approach. It’s a fine approach and I’m glad he emphasized it. But as McElreath alluded to in the early part of Section 5.3.1, we have other modeling options at our disposal. I’d like to briefly walk through three more.\n\n5.4.1 You can use a dummy\nThe first alternative is the one McElreath mentioned directly at the top of page 154. If you have \\(K\\) categories in a nominal variable, you can depict the categories in the model with \\(K - 1\\) dummy variables. To start simple, consider the case of the \\(K = 2\\) variable sex we used in b5.8.\n\nb5.8$data |&gt; \n  head()\n\n   height sex\n1 151.765   2\n2 139.700   1\n3 136.525   1\n4 156.845   2\n5 145.415   1\n6 163.830   2\n\n\nWe can convert our index variable sex into two dummy variables, which we might call male and female.\n\nd &lt;- b5.8$data |&gt; \n  mutate(male   = if_else(sex == \"1\", 1, 0),\n         female = if_else(sex == \"2\", 1, 0))\n\nhead(d)\n\n   height sex male female\n1 151.765   2    0      1\n2 139.700   1    1      0\n3 136.525   1    1      0\n4 156.845   2    0      1\n5 145.415   1    1      0\n6 163.830   2    0      1\n\n\nAs with the index variable sex, no order is implied by the 0’s and 1’s in our male and female dummies. But remember, we only need \\(K - 1\\) dummies, not both. If we wanted to fit the model using the female dummy variable, we might express the statistical model as\n\\[\\begin{align*}\n\\text{height}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\text{female}_i \\\\\n\\alpha  & \\sim \\operatorname{Normal}(178, 20) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma  & \\sim \\operatorname{Uniform}(0, 50),\n\\end{align*}\\]\nwhere \\(\\alpha\\) would be the intercept (i.e., the mean) for males and \\(\\beta_1\\) would be the expected change in the mean for females, relative to males. This parameterization makes males the reference category. Here’s how to fit the model with brms.\n\nb5.8b &lt;- brm(\n  data = d, \n  family = gaussian,\n  height ~ 1 + female,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.08b\")\n\n\nprint(b5.8b)\n\n Family: gaussian \n  Links: mu = identity \nFormula: height ~ 1 + female \n   Data: d (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   134.97      1.58   131.88   138.09 1.00     3827     2518\nfemale        7.29      2.33     2.72    12.02 1.00     3403     2473\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    27.42      0.82    25.87    29.10 1.00     4314     2791\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nI’m not going to show it, here, but this approach generalizes to instances where you have 3 or more categories. In the case of the monkey model b5.9, we had a \\(K = 4\\) variable clade, which could have been entered in the model with three dummy variables instead. Whichever of the four levels of clade that is not entered into the model as a dummy, that’s the level that would become the reference category.\n\n\n5.4.2 Consider contrast coding\nContrast coding offers something of a blend between the index and dummy approach. Consider out new contrast variable, sex_c.\n\nd &lt;- d |&gt; \n  mutate(sex_c = if_else(sex == \"1\", -0.5, 0.5))\n\nhead(d)\n\n   height sex male female sex_c\n1 151.765   2    0      1   0.5\n2 139.700   1    1      0  -0.5\n3 136.525   1    1      0  -0.5\n4 156.845   2    0      1   0.5\n5 145.415   1    1      0  -0.5\n6 163.830   2    0      1   0.5\n\n\nsex_c == -0.5 for males and sex_c == 0.5 for females. We might express our revised statistical model as\n\\[\\begin{align*}\n\\text{height}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\text{sex\\_c}_i \\\\\n\\alpha  & \\sim \\operatorname{Normal}(178, 20) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma  & \\sim \\operatorname{Uniform}(0, 50),\n\\end{align*}\\]\nwhere \\(\\alpha\\) is now the average of the mean heights of males and females and \\(\\beta_1\\) is the difference when subtracting the mean height of males from the mean height of females. Fit the model.\n\nb5.8c &lt;- brm(\n  data = d, \n  family = gaussian,\n  height ~ 1 + sex_c,\n  prior = c(prior(normal(178, 20), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(uniform(0, 50), class = sigma, ub = 50)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.08c\")\n\nInspect the summary.\n\nprint(b5.8c)\n\n Family: gaussian \n  Links: mu = identity \nFormula: height ~ 1 + sex_c \n   Data: d (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   138.62      1.15   136.36   140.81 1.00     4076     3110\nsex_c         7.29      2.33     2.72    12.02 1.00     3403     2473\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    27.42      0.82    25.87    29.10 1.00     4314     2791\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf you take a look, you’ll discover that posterior summary for \\(\\beta_1\\) is basically the same, here, as it was for b5.8b. The intercept \\(\\alpha\\) is a little more complicated. To build a sense of what it means, first consider the simple mean for height, ignoring sex, computed directly from the data.\n\nd |&gt; \n  summarise(mean_height = mean(height))\n\n  mean_height\n1    138.2636\n\n\nNow here is the average of the mean of height when first computed separately by levels of sex.\n\nd |&gt; \n  group_by(sex) |&gt; \n  summarise(group_mean = mean(height)) |&gt; \n  summarise(average_of_the_group_means_in_height = mean(group_mean))\n\n# A tibble: 1 × 1\n  average_of_the_group_means_in_height\n                                 &lt;dbl&gt;\n1                                 138.\n\n\nOur posterior for \\(\\alpha\\), above, is designed to capture the average_of_the_group_means_in_height, not mean_height. In cases where the sample sizes in the two groups were equal, these two would be same. Since we have different numbers of males and females in our data, the two values differ a bit.\n\nd |&gt; \n  count(sex) |&gt; \n  mutate(percent = 100 * n / sum(n))\n\n  sex   n  percent\n1   1 287 52.75735\n2   2 257 47.24265\n\n\nAnyway, here’s how we might wrangle the posterior samples to make our marginal posteriors for males, females, and their difference.\n\nas_draws_df(b5.8c) |&gt; \n  mutate(male            = b_Intercept - b_sex_c * 0.5,\n         female          = b_Intercept + b_sex_c * 0.5,\n         `female - male` = b_sex_c) |&gt; \n  pivot_longer(male:`female - male`) |&gt; \n  \n  ggplot(aes(x = value, y = 0)) +\n  stat_halfeye(.width = 0.95,\n               fill = \"firebrick4\", normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"height\") +\n  facet_wrap(~ name, scales = \"free\") +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank())\n\n\n\n\n\n\n\n\nThis approach can also be generalized to contexts where you have three or more groups. I’m not going to show you how to do so, here. It’s a deep rabbit hole and I’m just not interested in shoveling that hard, today. For those who want more, I’ll provide some references at the end.\n\n\n5.4.3 There’s always the multilevel ANOVA approach\nIt’s a little absurd I’m covering this, here, because (a) I’m not a fan of the ANOVA framework and (b) McElreath didn’t properly introduce the multilevel framework until Chapter 13. But we’ll throw this in, anyway. For those of you who are new to the multilevel model, make a mental note of this section, back away slowly, and come back after you’ve worked through the later chapters in this text. For those of y’all who have some experience with the multilevel model, perhaps this will offer an application you hadn’t considered.\nFor this approach, let’s consider our \\(K = 4\\) variable clade from back in b5.9. We used clade as an index variable to model four means for standardized kilocalories per gram of milk kcal.per.g_s.\n\nd &lt;- b5.9$data\n\nhead(d)\n\n  kcal.per.g_s            clade\n1   -0.9400408    Strepsirrhine\n2   -0.8161263    Strepsirrhine\n3   -1.1259125    Strepsirrhine\n4   -1.0019980    Strepsirrhine\n5   -0.2585112    Strepsirrhine\n6   -1.0639553 New World Monkey\n\n\nOur multilevel ANOVA approach will give us an overall intercept (mean) for kcal.per.g_s as well as four clade-specific deviations from that overall intercept. One way to express this might be\n\\[\\begin{align*}\n\\text{kcal.per.g\\_s} & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + \\color{#8B1A1A}{u_{j[i]}} \\\\\n\\alpha & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma   & \\sim \\operatorname{Exponential}(1) \\\\\n\\color{#8B1A1A}{u_j} & \\color{#8B1A1A}\\sim \\color{#8B1A1A}{\\operatorname{Normal}(0, \\sigma_\\text{clade})}, & \\color{#8B1A1A}{\\text{for } j = 1, \\dots, 4}  \\\\\n\\color{#8B1A1A}{\\sigma_\\text{clade}} & \\color{#8B1A1A}\\sim \\color{#8B1A1A}{\\operatorname{Exponential}(1)},\n\\end{align*}\\]\nwhere \\(\\alpha\\) is the overall intercept (mean) and the four clade-specific deviations from that mean are captured by the four levels of \\(u_j\\), which are themselves modeled as normally distributed with a mean of zero (because they are deviations, after all) and a standard deviation \\(\\sigma_\\text{clade}\\). Here’s how to fit such a model with brms.\n\nb5.9b &lt;- brm(\n  data = d, \n  family = gaussian,\n  kcal.per.g_s ~ 1 + (1 | clade),\n  prior = c(prior(normal(0, 0.5), class = Intercept),\n            prior(exponential(1), class = sigma),\n            prior(exponential(1), class = sd)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.09b\")\n\nCheck the results.\n\nprint(b5.9b)\n\n Family: gaussian \n  Links: mu = identity \nFormula: kcal.per.g_s ~ 1 + (1 | clade) \n   Data: d (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~clade (Number of levels: 4) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.84      0.42     0.29     1.86 1.01     1009     1155\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.32    -0.63     0.63 1.00     1548     1408\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.80      0.12     0.61     1.06 1.00     2070     2602\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe ‘Group-Level Effects:’ section in that output won’t make much sense until we cover the material in Chapter 13. Here’s how one might wrangle the posterior samples to make a coefficient plot similar to the one we made for the original b5.9.\n\nas_draws_df(b5.9b) |&gt; \n  mutate(Ape                = b_Intercept + `r_clade[Ape,Intercept]`,\n         `New World Monkey` = b_Intercept + `r_clade[New.World.Monkey,Intercept]`,\n         `Old World Monkey` = b_Intercept + `r_clade[Old.World.Monkey,Intercept]`,\n         Strepsirrhine      = b_Intercept + `r_clade[Strepsirrhine,Intercept]`) |&gt; \n  pivot_longer(Ape:Strepsirrhine) |&gt; \n  \n  ggplot(aes(x = value, y = reorder(name, value))) +\n  geom_vline(xintercept = 0, alpha = 1/10, color = \"firebrick4\") +\n  stat_pointinterval(point_interval = mode_hdi, .width = 0.89, \n                     color = \"firebrick4\", size = 1) +\n  labs(x = \"expected kcal (std)\", \n       y = NULL,\n       title = \"My tidybayes-based coefficient plot for b5.9b\") +\n  theme_bw() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nAs we will learn, an advantage of this approach is we get improved estimates due to multilevel partial pooling. Technically, you can apply this approach to cases with as few as \\(K = 2\\) categories. In practice, you’ll probably want to use this only when \\(K \\geq 3\\).\n\n\n5.4.4 Would you like to learn more?\nTo learn more about the dummy coding approach, McElreath covered it in Chapter 5 of his first (2015) edition, Hayes covered it in Chapter 2 of his (2017) text, and it’s been covered in many other texts, such as the modern classic by Cohen et al. (2013). I have ebook translations for the first of those two (Kurz, 2026c, 2026b). If you like learning by video, the great Erin Buchanan has a video or two (e.g., here) on her Statistics of DOOM chanel.\nThe specific type of contrast coding we used is part of a more general approach. You can learn more about that in Cohen et al (2013, Chapter 8) or from this vignette by the good folks at the UCLA Institute for Digital Research and Education (R Library Contrast Coding Systems for Categorical Variables, n.d.).\nIn addition to waiting for Chapter 13 in this ebook, you can learn more about the multilevel ANOVA approach in Chapters 19 and 24 of Kruschke’s (2015), in the corresponding chapters in my (2026a) ebook translation of the same, and in some of Gelman’s peer-reviewed work (Gelman, 2005, 2006).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Many Variables & The Spurious Waffles</span>"
    ]
  },
  {
    "objectID": "05.html#session-info",
    "href": "05.html#session-info",
    "title": "5  The Many Variables & The Spurious Waffles",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] bayesplot_1.15.0.9000 tidybayes_3.0.7       GGally_2.4.0          dagitty_0.3-4         psych_2.5.6          \n [6] patchwork_1.3.2       ggdag_0.2.13          tigris_2.2.1          ggrepel_0.9.6         brms_2.23.0          \n[11] Rcpp_1.1.0            lubridate_1.9.4       forcats_1.0.1         stringr_1.6.0         dplyr_1.1.4          \n[16] purrr_1.2.1           readr_2.1.5           tidyr_1.3.2           tibble_3.3.1          ggplot2_4.0.1        \n[21] tidyverse_2.0.0      \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3      tensorA_0.36.2.1        rstudioapi_0.17.1       jsonlite_2.0.0          shape_1.4.6.1          \n  [6] magrittr_2.0.4          TH.data_1.1-4           estimability_1.5.1      farver_2.1.2            rmarkdown_2.30         \n [11] vctrs_0.7.0             memoise_2.0.1           htmltools_0.5.9         distributional_0.5.0    curl_7.0.0             \n [16] StanHeaders_2.36.0.9000 KernSmooth_2.23-26      htmlwidgets_1.6.4       plyr_1.8.9              sandwich_3.1-1         \n [21] emmeans_1.11.2-8        zoo_1.8-14              cachem_1.1.0            uuid_1.2-1              igraph_2.2.0           \n [26] lifecycle_1.0.5         pkgconfig_2.0.3         Matrix_1.7-3            R6_2.6.1                fastmap_1.2.0          \n [31] digest_0.6.39           ps_1.9.1                labeling_0.4.3          timechange_0.3.0        httr_1.4.7             \n [36] polyclip_1.10-7         abind_1.4-8             mgcv_1.9-3              compiler_4.5.1          proxy_0.4-27           \n [41] withr_3.0.2             S7_0.2.1                backports_1.5.0         inline_0.3.21           rethinking_2.42        \n [46] viridis_0.6.5           DBI_1.2.3               ggstats_0.11.0          QuickJSR_1.8.1          pkgbuild_1.4.8         \n [51] ggforce_0.5.0           MASS_7.3-65             rappdirs_0.3.3          classInt_0.4-11         loo_2.9.0.9000         \n [56] tools_4.5.1             units_1.0-0             glue_1.8.0              nlme_3.1-168            grid_4.5.1             \n [61] sf_1.0-21               cmdstanr_0.9.0          checkmate_2.3.3         reshape2_1.4.5          generics_0.1.4         \n [66] gtable_0.3.6            tzdb_0.5.0              class_7.3-23            hms_1.1.4               tidygraph_1.3.1        \n [71] utf8_1.2.6              pillar_1.11.1           ggdist_3.3.3            posterior_1.6.1.9000    splines_4.5.1          \n [76] tweenr_2.0.3            lattice_0.22-7          survival_3.8-3          tidyselect_1.2.1        knitr_1.51             \n [81] arrayhelpers_1.1-0      gridExtra_2.3           V8_8.0.1                stats4_4.5.1            xfun_0.55              \n [86] graphlayouts_1.2.2      bridgesampling_1.2-1    matrixStats_1.5.0       rstan_2.36.0.9000       stringi_1.8.7          \n [91] yaml_2.3.12             boot_1.3-31             evaluate_1.0.5          codetools_0.2-20        ggraph_2.2.2           \n [96] emo_0.0.0.9000          cli_3.6.5               RcppParallel_5.1.11-1   xtable_1.8-4            processx_3.8.6         \n[101] coda_0.19-4.1           svUnit_1.0.8            parallel_4.5.1          rstantools_2.5.0.9000   assertthat_0.2.1       \n[106] Brobdingnag_1.2-9       viridisLite_0.4.2       mvtnorm_1.3-3           scales_1.4.0            e1071_1.7-16           \n[111] crayon_1.5.3            rlang_1.1.7             multcomp_1.4-29         mnormt_2.1.1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Many Variables & The Spurious Waffles</span>"
    ]
  },
  {
    "objectID": "05.html#comments",
    "href": "05.html#comments",
    "title": "5  The Many Variables & The Spurious Waffles",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBarrett, M. (2022a). ggdag: Analyze and create elegant directed acyclic graphs. https://CRAN.R-project.org/package=ggdag\n\n\nBarrett, M. (2022b). An introduction to ggdag. https://CRAN.R-project.org/package=ggdag/vignettes/intro-to-ggdag.html\n\n\nBürkner, P.-C. (2022a). brms reference manual, Version 2.18.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2022b). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html\n\n\nBürkner, P.-C. (2022c). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2013). Applied multiple regression/correlation analysis for the behavioral sciences (Third Edition). Routledge. https://doi.org/10.4324/9780203774441\n\n\nFreckleton, R. P. (2002). On the misuse of residuals in ecology: Regression of residuals vs. Multiple regression. Journal of Animal Ecology, 71(3), 542–545. https://doi.org/10.1046/j.1365-2656.2002.00618.x\n\n\nGabry, J. (2022). Plotting MCMC draws using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/plotting-mcmc-draws.html\n\n\nGabry, J., & Mahr, T. (2022). bayesplot: Plotting for Bayesian models. https://CRAN.R-project.org/package=bayesplot\n\n\nGabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A. (2019). Visualization in Bayesian workflow. Journal of the Royal Statistical Society: Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378\n\n\nGelman, A. (2005). Analysis of variance–Why it is more important than ever. Annals of Statistics, 33(1), 1–53. https://doi.org/10.1214/009053604000001048\n\n\nGelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Bayesian Analysis, 1(3), 515–534. https://doi.org/10.1214/06-BA117A\n\n\nGelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60(4), 328–331. https://doi.org/10.1198/000313006X152649\n\n\nHayes, A. F. (2017). Introduction to mediation, moderation, and conditional process analysis: A regression-based approach. Guilford publications. https://www.guilford.com/books/Introduction-to-Mediation-Moderation-and-Conditional-Process-Analysis/Andrew-Hayes/9781462534654\n\n\nHinde, K., & Milligan, L. A. (2011). Primate milk: Proximate mechanisms and ultimate perspectives. Evolutionary Anthropology: Issues, News, and Reviews, 20(1), 9–23. https://doi.org/10.1002/evan.20289\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKurz, A. S. (2026a). Doing Bayesian data analysis in brms and the tidyverse (Version 1.3.0). https://solomon.quarto.pub/dbda2/\n\n\nKurz, A. S. (2026b). Recoding Introduction to Mediation, Moderation, and Conditional Process Analysis (version 1.4.0). https://solomon.quarto.pub/immcpa2/\n\n\nKurz, A. S. (2026c). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.4.0). https://solomon.quarto.pub/sr/\n\n\nMcCullagh, P., & Nelder, J. A. (1989). Generalized linear models (Second Edition). Chapman and Hall.\n\n\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMeehl, P. E. (1990). Why summaries of research on psychological theories are often uninterpretable. Psychological Reports, 66(1), 195–244. https://doi.org/10.2466/pr0.1990.66.1.195\n\n\nR Library Contrast Coding Systems for categorical variables. (n.d.). Retrieved October 14, 2020, from https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/\n\n\nRevelle, W. (2022). psych: Procedures for psychological, psychometric, and personality research. https://CRAN.R-project.org/package=psych\n\n\nSchloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen, E., Elberg, A., & Larmarange, J. (2021). GGally: Extension to ’ggplot2’. https://CRAN.R-project.org/package=GGally\n\n\nSlowikowski, K. (2022). ggrepel: Automatically position non-overlapping text labels with ’ggplot2’. https://CRAN.R-project.org/package=ggrepel\n\n\nTextor, J., van der Zander, B., & Ankan, A. (2021). dagitty: Graphical analysis of structural causal models. https://CRAN.R-project.org/package=dagitty\n\n\nTextor, J., van der Zander, B., Gilthorpe, M. S., Liśkiewicz, M., & Ellison, G. T. (2016). Robust causal inference using directed acyclic graphs: The R package ’dagitty’. International Journal of Epidemiology, 45(6), 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nWalker, K. (2022). Tigris: Load census TIGER/Line shapefiles [Manual]. https://github.com/walkerke/tigris",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Many Variables & The Spurious Waffles</span>"
    ]
  },
  {
    "objectID": "06.html",
    "href": "06.html",
    "title": "6  The Haunted DAG & The Causal Terror",
    "section": "",
    "text": "6.0.0.1 Overthinking: Simulated science distortion\nRead this opening and cry:\nFirst let’s run the simulation.\nlibrary(tidyverse)\n\nset.seed(1914)\nn &lt;- 200  # Number of grant proposals\np &lt;- 0.1  # Proportion to select\n\n# Uncorrelated newsworthiness and trustworthiness\nd &lt;- tibble(newsworthiness  = rnorm(n, mean = 0, sd = 1),\n            trustworthiness = rnorm(n, mean = 0, sd = 1)) |&gt; \n  # `total_score`\n  mutate(total_score = newsworthiness + trustworthiness) |&gt; \n  # Select top 10% of combined scores\n  mutate(selected = ifelse(total_score &gt;= quantile(total_score, 1 - p), TRUE, FALSE))\n\nhead(d)\n\n# A tibble: 6 × 4\n  newsworthiness trustworthiness total_score selected\n           &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;   \n1         -0.379         -1.20        -1.58  FALSE   \n2          0.130          0.504        0.634 FALSE   \n3          0.334          0.532        0.866 FALSE   \n4         -1.89          -0.594       -2.48  FALSE   \n5          2.05           0.0672       2.12  TRUE    \n6          2.54           1.02         3.56  TRUE\nHere’s the correlation among those cases for which selected == TRUE.\nd |&gt; \n  filter(selected == TRUE) |&gt; \n  select(newsworthiness, trustworthiness) |&gt; \n  cor()\n\n                newsworthiness trustworthiness\nnewsworthiness       1.0000000      -0.7680083\ntrustworthiness     -0.7680083       1.0000000\nFor the plots in this chapter, we’ll take some aesthetic cues from Aki Vehtari’s great GitHub repo, Bayesian Data Analysis R Demos.\ntheme_set(theme_minimal())\nOkay, let’s make Figure 6.1.\n# We'll need this for the annotation\ntext &lt;- tibble(\n  newsworthiness  = c(2, 1), \n  trustworthiness = c(2.25, -2.5),\n  selected        = c(TRUE, FALSE),\n  label           = c(\"selected\", \"rejected\"))\n\nd |&gt; \n  ggplot(aes(x = newsworthiness, y = trustworthiness, color = selected)) +\n  geom_point(aes(shape = selected), alpha = 3/4) +\n  geom_text(data = text,\n            aes(label = label)) +\n  geom_smooth(data = d |&gt; filter(selected == TRUE),\n              method = \"lm\", fullrange = T, se = F,\n              color = \"lightblue\", linewidth = 1/2) +\n  scale_x_continuous(limits = c(-3, 3.9), expand = c(0, 0)) +\n  scale_color_manual(values = c(\"black\", \"lightblue\")) +\n  scale_shape_manual(values = c(1, 19)) +\n  coord_cartesian(ylim = range(d$trustworthiness)) +\n  theme(legend.position = \"none\")\nWhy might “the most newsworthy studies might be the least trustworthy?” The selection-distortion effect.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG & The Causal Terror</span>"
    ]
  },
  {
    "objectID": "06.html#multicollinearity",
    "href": "06.html#multicollinearity",
    "title": "6  The Haunted DAG & The Causal Terror",
    "section": "6.1 Multicollinearity",
    "text": "6.1 Multicollinearity\n\nMulticollinearity means a very strong association between two or more predictor variables. The raw correlation isn’t what matters. Rather what matters is the association, conditional on the other variables in the model. The consequence of multicollinearity is that the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome, even if all of the variables are in reality strongly associated with the outcome.\n\n\nThis frustrating phenomenon arises from the details of how multiple regression works. In fact, there is nothing wrong with multicollinearity. The model will work fine for prediction. You will just be frustrated trying to understand it. The hope is that once you understand multicollinearity, you will better understand regression models in general. (p. 163, emphasis added)\n\nFor more on this topic, check out Jan VanHove’s interesting blog post, Collinearity isn’t a disease that needs curing.\n\n6.1.1 Multicollinear legs\nLet’s simulate some leg data.\n\nn &lt;- 100\nset.seed(909)\n\nd &lt;- tibble(height   = rnorm(n, mean = 10, sd = 2),\n            leg_prop = runif(n, min = 0.4, max = 0.5)) |&gt; \n  mutate(leg_left  = leg_prop * height + rnorm(n, mean = 0, sd = 0.02),\n         leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02))\n\nAs you might expect in real-life data, the leg_left and leg_right columns are highly correlated.\n\nd |&gt; \n  summarise(r = cor(leg_left, leg_right) |&gt; round(digits = 4))\n\n# A tibble: 1 × 1\n      r\n  &lt;dbl&gt;\n1 1.000\n\n\nHave you ever even seen a \\(\\rho = 0.9997\\) correlation, before? Here it is in a plot.\n\nd |&gt;\n  ggplot(aes(x = leg_left, y = leg_right)) +\n  geom_point(alpha = 1/2, color = \"forestgreen\")\n\n\n\n\n\n\n\n\nLoad brms.\n\nlibrary(brms)\n\nHere’s our attempt to predict height with both legs.\n\nb6.1 &lt;- brm(\n  data = d, \n  family = gaussian,\n  height ~ 1 + leg_left + leg_right,\n  prior = c(prior(normal(10, 100), class = Intercept),\n            prior(normal(2, 10), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.01\")\n\nLet’s inspect the damage.\n\nprint(b6.1)\n\n Family: gaussian \n  Links: mu = identity \nFormula: height ~ 1 + leg_left + leg_right \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.98      0.29     0.39     1.55 1.00     3651     3104\nleg_left      0.33      2.52    -4.44     5.43 1.00     1651     1466\nleg_right     1.66      2.52    -3.46     6.44 1.00     1651     1505\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.63      0.05     0.55     0.73 1.00     1990     1725\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThat ‘Est.Error’ column isn’t looking too good. But it’s easy to miss that, which is why McElreath suggested “a graphical view of the [output] is more useful because it displays the posterior means and [intervals] in a way that allows us with a glance to see that something has gone wrong here” (p. 164).\nHere’s our coefficient plot using brms::mcmc_plot() with a little help from bayesplot::color_scheme_set().\n\nlibrary(bayesplot)\n\ncolor_scheme_set(\"orange\")\n\nmcmc_plot(b6.1, \n          type = \"intervals\", \n          prob = 0.5, \n          prob_outer = 0.95,\n          point_est = \"mean\") +\n  labs(title = \"The coefficient plot for the two-leg model\",\n       subtitle = \"Holy smokes; look at the widths of those betas!\") +\n  theme(axis.text.y = element_text(hjust = 0),\n        panel.grid.minor = element_blank(),\n        strip.text = element_text(hjust = 0)) \n\n\n\n\n\n\n\n\nNow you can use the brms::mcmc_plot() function without explicitly loading the bayesplot package. But loading bayesplot allows you to set the color scheme with color_scheme_set().\nIn the middle of page 164, McElreath suggested we might try this again with different seeds. This might be a good place to practice iteration. Our first step will be to make a custom function that will simulate new data of the same form as above and then immediately fit a model based on b6.1 to those new data. To speed up the process, we’ll use the update() function to avoid recompiling the model. In the final step, the function extracts the posterior draws with as_draws_df(), which is what the function returns. Our custom function, sim_fit_draws(), will take two arguments. The seed argument will allow us to keep the results reproducible by setting a seed for the data simulation. The n argument will allow us, should we wish, to change the sample size.\n\nsim_fit_draws &lt;- function(seed, n = 100) {\n  \n  # Set up the parameters\n  n &lt;- n\n  set.seed(seed)\n  \n  # Simulate the new data\n  d &lt;- tibble(height   = rnorm(n, mean = 10, sd = 2),\n              leg_prop = runif(n, min = 0.4, max = 0.5)) |&gt; \n    mutate(leg_left  = leg_prop * height + rnorm(n, mean = 0, sd = 0.02),\n           leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02))\n  \n  # Update b6.1 to the new data\n  fit &lt;- update(b6.1, newdata = d, seed = 6) \n  \n  # Extract the posterior draws\n  as_draws_df(fit)\n}\n\nNow use sim_fit_draws() to make our simulations which correspond to seed values 1:4. By nesting the seed values and the sim_fit_draws() function within purrr::map(), the results will be saved within our tibble, sim.\n\nsim &lt;- tibble(seed = 1:4) |&gt; \n  mutate(post = map(.x = seed, .f = sim_fit_draws))\n\nTake a look at what we did.\n\nhead(sim)\n\n# A tibble: 4 × 2\n   seed post                   \n  &lt;int&gt; &lt;list&gt;                 \n1     1 &lt;draws_df [4,000 × 10]&gt;\n2     2 &lt;draws_df [4,000 × 10]&gt;\n3     3 &lt;draws_df [4,000 × 10]&gt;\n4     4 &lt;draws_df [4,000 × 10]&gt;\n\n\nWe have a tibble with four rows and two columns. Hopefully it’s unsurprising the first column shows our four seed values. The second column, post, might look odd. Each cell contains the as_draws_df() output for each iteration. That output is currently in a compressed format, often called “nested” in the tidyverse framework. To unpack the contents of those four nested data frames, you simply call unnest(sim, post). In the next code block, we’ll do that within the context of a larger work flow designed to plot the results of each in a faceted coefficient plot. For data of this kind of structure, the tidybayes::stat_pointinterval() function will be particularly useful.\n\nlibrary(tidybayes)\n\nsim |&gt; \n  unnest(post) |&gt; \n  pivot_longer(b_Intercept:sigma, values_to = \"posterior\") |&gt; \n  mutate(seed = factor(seed)) |&gt; \n  \n  ggplot(aes(x = posterior, y = seed)) +\n  stat_pointinterval(.width = 0.95, color = \"forestgreen\") +\n  scale_x_continuous(expand = expansion(mult = 0.2)) +\n  facet_wrap(~ name, nrow = 1, scales = \"free_x\") +\n  theme(panel.border = element_rect(color = \"black\", fill = \"transparent\"),\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nThough the results varied across iterations, the overall pattern was massive uncertainty in the two \\(\\beta\\) parameters. You may be wondering,\n\nDid the posterior approximation work correctly?\nIt did work correctly, and the posterior distribution here is the right answer to the question we asked. The problem is the question. Recall that a multiple linear regression answers the question: What is the value of knowing each predictor, after already knowing all of the other predictors? So in this case, the question becomes: What is the value of knowing each leg’s length, after already knowing the other leg’s length?\nThe answer to this weird question is equally weird, but perfectly logical. (p. 164, emphasis in the original)\n\nTo further answer this weird question, we might start making the panels from Figure 6.2. Starting with the left panel, this is perhaps the simplest way to plot the bivariate posterior of our two predictor coefficients.\n\npairs(b6.1, variable = variables(b6.1)[2:3])\n\n\n\n\n\n\n\n\nIf you’d like a nicer and more focused attempt, you might have to revert to the as_draws_df() function and a little ggplot2 code.\n\npost &lt;- as_draws_df(b6.1)\n  \npost |&gt; \n  ggplot(aes(x = b_leg_left, y = b_leg_right)) +\n  geom_point(color = \"forestgreen\", alpha = 1/10, size = 1/2)\n\n\n\n\n\n\n\n\nWhile we’re at it, you can make a similar plot with the mcmc_scatter() function (see Gabry, 2022, Plotting MCMC draws using the bayesplot package).\n\ncolor_scheme_set(\"green\")\n\npost |&gt; \n  mcmc_scatter(pars = c(\"b_leg_left\", \"b_leg_right\"),\n               size = 1/2, \n               alpha = 1/10)\n\n\n\n\n\n\n\n\nBut wow, those coefficients look about as highly correlated as the predictors, just with the reversed sign.\n\npost |&gt; \n  summarise(rho = cor(b_leg_left, b_leg_right))\n\n# A tibble: 1 × 1\n     rho\n   &lt;dbl&gt;\n1 -1.000\n\n\nOn page 165, McElreath clarified that from the perspective of brms, this model may as well be\n\\[\\begin{align*}\ny_i   & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + (\\beta_1 + \\beta_2) x_i.\n\\end{align*}\\]\nAccordingly, here’s the posterior of the sum of the two regression coefficients, Figure 6.2.b. We’ll use tidybayes::stat_halfeye() to both plot the density and mark off the posterior median and percentile-based 95% probability intervals at its base.\n\npost |&gt; \n  ggplot(aes(x = b_leg_left + b_leg_right)) +\n  stat_halfeye(point_interval = median_qi, .width = 0.95, \n               fill = \"steelblue\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Sum the multicollinear coefficients\",\n       subtitle = \"Marked by the median and 95% PIs\")\n\n\n\n\n\n\n\n\nNow we fit the revised model after ditching one of the leg lengths.\n\nb6.2 &lt;- brm(\n  data = d, \n  family = gaussian,\n  height ~ 1 + leg_left,\n  prior = c(prior(normal(10, 100), class = Intercept),\n            prior(normal(2, 10), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.02\")\n\n\nprint(b6.2)\n\n Family: gaussian \n  Links: mu = identity \nFormula: height ~ 1 + leg_left \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.00      0.29     0.42     1.58 1.00     3886     2706\nleg_left      1.99      0.06     1.87     2.12 1.00     3947     2866\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.63      0.05     0.55     0.73 1.00     3843     2824\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThat posterior \\(\\textit{SD}\\) for leg_left looks much better. Compare this density to the one in Figure 6.2.b.\n\nas_draws_df(b6.2) |&gt; \n  ggplot(aes(x = b_leg_left)) +\n  stat_halfeye(point_interval = median_qi, .width = 0.95, \n               fill = \"steelblue\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"only b_leg_left, this time\",\n       title = \"Just one coefficient needed\",\n       subtitle = \"Marked by the median and 95% PIs\")\n\n\n\n\n\n\n\n\n\nThe basic lesson is only this: When two predictor variables are very strongly correlated (conditional on other variables in the model), including both in a model may lead to confusion. The posterior distribution isn’t wrong, in such cases. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. And if you are just interested in prediction, you’ll find that this leg model makes fine predictions. It just doesn’t make any claims about which leg is more important. (p. 166)\n\n\n\n6.1.2 Multicollinear milk\nMulticollinearity arises in real data, too. Load the milk data.\n\ndata(milk, package = \"rethinking\")\nd &lt;- milk\nrm(milk)\n\nNow use the handy rethinking::standardize() function to standardize our focal variables.\n\nd &lt;- d |&gt; \n  mutate(k = rethinking::standardize(kcal.per.g),\n         f = rethinking::standardize(perc.fat),\n         l = rethinking::standardize(perc.lactose))\n\nWe’ll follow the text and fit the two univariable models, first. Note our use of the update() function.\n\n# `k` regressed on `f`\nb6.3 &lt;- brm(\n  data = d, \n  family = gaussian,\n  k ~ 1 + f,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.03\")\n\n# `k` regressed on `l`\nb6.4 &lt;- update(\n  b6.3,\n  newdata = d,\n  formula = k ~ 1 + l,\n  seed = 6,\n  file = \"fits/b06.04\")\n\n\nposterior_summary(b6.3)[1:3, ] |&gt; round(digits = 2)\n\n            Estimate Est.Error  Q2.5 Q97.5\nb_Intercept     0.00      0.08 -0.16  0.16\nb_f             0.86      0.09  0.68  1.04\nsigma           0.49      0.07  0.37  0.65\n\nposterior_summary(b6.4)[1:3, ] |&gt; round(digits = 2)\n\n            Estimate Est.Error  Q2.5 Q97.5\nb_Intercept     0.00      0.07 -0.14  0.14\nb_l            -0.90      0.08 -1.04 -0.75\nsigma           0.41      0.06  0.32  0.54\n\n\nNow “watch what happens when we place both predictor variables in the same regression model” (p. 167).\n\nb6.5 &lt;- update(\n  b6.4,\n  newdata = d,\n  formula = k ~ 1 + f + l,\n  seed = 6,\n  file = \"fits/b06.05\")\n\nNow the \\(\\beta\\)’s are smaller and less certain.\n\nposterior_summary(b6.5)[1:3, ] |&gt; round(digits = 2)\n\n            Estimate Est.Error  Q2.5 Q97.5\nb_Intercept     0.00      0.07 -0.14  0.15\nb_f             0.25      0.20 -0.13  0.65\nb_l            -0.67      0.20 -1.05 -0.27\n\n\nHere’s a quick paris() plot.\n\nd |&gt; \n  select(kcal.per.g, perc.fat, perc.lactose) |&gt; \n  pairs(col = \"forestgreen\")\n\n\n\n\n\n\n\n\nNow here’s the custom GGally version.\n\nlibrary(GGally)\n\n# Define a couple custom functions\nmy_diag &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_density(color = \"black\", fill = \"steelblue\")\n}\n\nmy_lower &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_smooth(method = \"lm\", color = \"orange\", se = F) +\n    geom_point(alpha = 0.8, color = \"blue\", size = 1/3)\n  }\n\n# Plug those custom functions into `ggpairs()`\nggpairs(data = d, columns = c(3:4, 6),\n        upper = list(continuous = wrap(\"cor\", color = \"black\", family = \"sans\")),\n        diag = list(continuous = my_diag),\n        lower = list(continuous = my_lower))\n\n\n\n\n\n\n\n\nOur two predictor “variables are negatively correlated, and so strongly so that they are nearly redundant. Either helps in predicting kcal.per.g, but neither helps much once you already know the other” (p. 169, emphasis in the original). You can really see that on the lower two scatter plots. You’ll note the ggpairs() plot also returned the Pearson’s correlation coefficients with their \\(p\\)-value stars (\\(^\\star\\)). Later on in the ebook, we’ll practice setting custom correlation functions which avoid displaying that dirty \\(p\\)-value stuff.\nAnyway, making a DAG might help us make sense of this.\n\nlibrary(ggdag)\n\ndag_coords &lt;- tibble(\n  name = c(\"L\", \"D\", \"F\", \"K\"),\n  x    = c(1, 2, 3, 2),\n  y    = c(2, 2, 2, 1))\n\ndagify(L ~ D,\n       F ~ D,\n       K ~ L + F,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"D\"),\n                 alpha = 1/2, size = 6.5, show.legend = F) +\n  geom_point(x = 2, y = 2, \n             color = \"orange\", shape = 1, size = 6.5, stroke = 1) +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges() +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0.1, 0.1)) +\n  scale_color_manual(values = c(\"steelblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\nThe central tradeoff decides how dense, \\(D\\), the milk needs to be. We haven’t observed this variable, so it’s shown circled. Then fat, \\(F\\), and lactose, \\(L\\), are determined. Finally, the composition of \\(F\\) and \\(L\\) determines the kilocalories, \\(K\\). If we could measure \\(D\\), or had an evolutionary and economic model to predict it based upon other aspects of a species, that would be better than stumbling through regressions. (p. 167)\n\n\n6.1.2.1 Rethinking: Identification guaranteed; comprehension up to you\nIf you come from a frequentist background, this may jar you, but\n\ntechnically speaking, identifiability is not a concern for Bayesian models. The reason is that as long as the posterior distribution is proper–which just means that it integrates to 1–then all of the parameters are identified. But this technical fact doesn’t also mean that you can make sense of the posterior distribution. So it’s probably better to speak of weakly identified parameters in a Bayesian context. (pp. 169–170, emphasis in the original)\n\n\n\n6.1.2.2 Overthinking: Simulating collinearity\nFirst we’ll get the data and define the functions. You’ll note I’ve defined my sim_coll() a little differently from sim.coll() in the text. I’ve omitted rep.sim.coll() as an independent function altogether, but computed similar summary information with the summarise() code at the bottom of the block.\n\n# Define a custom function\nsim_coll &lt;- function(seed, rho) {\n  \n  # Simulate the data\n  set.seed(seed)\n  \n  d &lt;- d |&gt; \n    mutate(x = rnorm(n(), \n                     mean = perc.fat * rho,\n                     sd = sqrt((1 - rho^2) * var(perc.fat))))\n    \n  # Fit an OLS model\n  m &lt;- lm(kcal.per.g ~ perc.fat + x, data = d)\n  \n  # Extract the parameter SD\n  sqrt(diag(vcov(m)))[2]\n}\n\n# How many simulations per `rho`-value would you like?\nn_seed &lt;- 100\n# How many `rho`-values from 0 to 0.99 would you like to evaluate the process over?\nn_rho &lt;- 30\n\nd &lt;- crossing(seed = 1:n_seed,\n              rho  = seq(from = 0, to = 0.99, length.out = n_rho))  |&gt; \n  mutate(parameter_sd = purrr::map2_dbl(seed, rho, sim_coll)) |&gt; \n  group_by(rho) |&gt; \n  # We'll `summarise()` our output by the mean and 95% intervals\n  summarise(mean = mean(parameter_sd),\n            ll   = quantile(parameter_sd, prob = 0.025),\n            ul   = quantile(parameter_sd, prob = 0.975))\n\nWe’ve added 95% interval bands to our version of Figure 5.10.\n\nd |&gt; \n  ggplot(aes(x = rho, y = mean)) +\n  geom_smooth(aes(ymin = ll, ymax = ul),\n              stat = \"identity\",\n              alpha = 1/3, color = \"orange\", fill = \"orange\", linewidth = 2/3) +\n  labs(x = expression(rho),\n       y = \"parameter SD\") +\n  coord_cartesian(ylim = c(0, 0.0072))",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG & The Causal Terror</span>"
    ]
  },
  {
    "objectID": "06.html#post-treatment-bias",
    "href": "06.html#post-treatment-bias",
    "title": "6  The Haunted DAG & The Causal Terror",
    "section": "6.2 Post-treatment bias",
    "text": "6.2 Post-treatment bias\nIt helped me understand the next example by mapping out the sequence of events McElreath described in the second paragraph:\n\nseed and sprout plants\nmeasure heights\napply different antifungal soil treatments (i.e., the experimental manipulation)\nmeasure (a) the heights and (b) the presence of fungus\n\nBased on the design, let’s simulate our data.\n\n# How many plants would you like?\nn &lt;- 100\n\nset.seed(71)\nd &lt;- tibble(h0        = rnorm(n, mean = 10, sd = 2), \n            treatment = rep(0:1, each = n / 2),\n            fungus    = rbinom(n, size = 1, prob = 0.5 - treatment * 0.4),\n            h1        = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1))\n\nWe’ll use head() to peek at the data.\n\nd |&gt;\n  head()\n\n# A tibble: 6 × 4\n     h0 treatment fungus    h1\n  &lt;dbl&gt;     &lt;int&gt;  &lt;int&gt; &lt;dbl&gt;\n1  9.14         0      0  14.3\n2  9.11         0      0  15.6\n3  9.04         0      0  14.4\n4 10.8          0      0  15.8\n5  9.16         0      1  11.5\n6  7.63         0      0  11.1\n\n\nAnd here’s a quick summary with tidybayes::mean_qi().\n\nd |&gt; \n  pivot_longer(everything()) |&gt; \n  group_by(name) |&gt; \n  mean_qi(.width = 0.89) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 4 × 7\n  name      value .lower .upper .width .point .interval\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 fungus     0.23   0       1     0.89 mean   qi       \n2 h0         9.96   6.57   13.1   0.89 mean   qi       \n3 h1        14.4   10.6    17.9   0.89 mean   qi       \n4 treatment  0.5    0       1     0.89 mean   qi       \n\n\nIf you want to make those cute mini histograms, go back and check out our custom histospark() code from Chapter 4.\n\n6.2.1 A prior is born\nLet’s take a look at the \\(p \\sim \\operatorname{Log-Normal}(0, 0.25)\\) prior distribution.\n\nset.seed(6)\n\n# Simulate\nsim_p &lt;- tibble(sim_p = rlnorm(1e4, meanlog = 0, sdlog = 0.25)) \n\n# Wrangle\nsim_p |&gt; \n  mutate(`exp(sim_p)` = exp(sim_p)) |&gt;\n  pivot_longer(everything()) |&gt; \n  \n  # Plot\n  ggplot(aes(x = value)) +\n  geom_density(fill = \"steelblue\") +\n  scale_x_continuous(breaks = c(0, 0.5, 1, 1.5, 2, 3, 5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 6)) +\n  facet_wrap(~ name, scale = \"free_y\", ncol = 1) +\n  theme(panel.grid.minor.x = element_blank())\n\n\n\n\n\n\n\n\nSummarize.\n\nsim_p |&gt; \n  mutate(`exp(sim_p)` = exp(sim_p)) |&gt;\n  pivot_longer(everything()) |&gt;\n  group_by(name) |&gt; \n  mean_qi(.width = 0.89) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 2 × 7\n  name       value .lower .upper .width .point .interval\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 exp(sim_p)  2.92   1.96   4.49   0.89 mean   qi       \n2 sim_p       1.03   0.67   1.5    0.89 mean   qi       \n\n\n“This prior expects anything from 40% shrinkage up to 50% growth” (p. 172). So then, our initial statistical model will follow the form\n\\[\\begin{align*}\nh_{1i} & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  & = h_{0i} \\times p \\\\\np      & \\sim \\operatorname{Log-Normal}(0, 0.25) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nLet’s fit that model.\n\nb6.6 &lt;- brm(\n  data = d, \n  family = gaussian,\n  h1 ~ 0 + h0,\n  prior = c(prior(lognormal(0, 0.25), class = b, lb = 0),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.06\")\n\nNote our use of the lb argument to set the lower bound for the prior on \\(p\\). Anyway, behold the summary.\n\nprint(b6.6)\n\n Family: gaussian \n  Links: mu = identity \nFormula: h1 ~ 0 + h0 \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nh0     1.43      0.02     1.39     1.46 1.00     3641     2513\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.82      0.13     1.60     2.09 1.00     2927     2094\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSo then, the expectation is an increase of about 43 percent relative to \\(h_0\\). But this isn’t the best model. We’re leaving important predictors on the table. Our updated model follows the form\n\\[\\begin{align*}\nh_{1i}  & \\sim  \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = h_{0,i} \\times p \\\\\np       & = \\alpha + \\beta_1 \\text{treatment}_i + \\beta_2 \\text{fungus}_i \\\\\n\\alpha  & \\sim \\operatorname{Log-Normal}(0, 0.25) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nThat is, the “proportion of growth \\(p\\) is now a function of the predictor variables” (p. 172). Although we will fit the equivalent of McElreath’s model in brms, I’m not aware that we can translate it directly into conventional brms syntax. But take a look at the critical two lines from above:\n\\[\\begin{align*}\n\\mu_i & = h_{0,i} \\times p \\\\\np     & = \\alpha + \\beta_1 \\text{treatment}_i + \\beta_2 \\text{fungus}_i. \\\\\n\\end{align*}\\]\nWith just a little algebra, we can re-express that as\n\\[\\mu_i = h_{0i} \\times (\\alpha + \\beta_1 \\text{treatment}_i + \\beta_2 \\text{fungus}_i).\\]\nWith brms, we fit models like that using the non-linear syntax (Bürkner, 2022), which we briefly introduced in Section 4.4.2.1 and Section 5.3.2. Yes friends, it’s now time we discuss the non-linear brms syntax in detail. Bur first, here’s what it looks like for b6.7.\n\nb6.7 &lt;- brm(\n  data = d, \n  family = gaussian,\n  bf(h1 ~ h0 * (a + t * treatment + f * fungus),\n     a + t + f ~ 1,\n     nl = TRUE),\n  prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0),\n            prior(normal(0, 0.5), nlpar = t),\n            prior(normal(0, 0.5), nlpar = f),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.07\")\n\nTo explain what’s going on with our formulasyntax, it’s probably best to quote Bürkner’s (2022) vignette at length:\n\nWhen looking at the above code, the first thing that becomes obvious is that we changed the formula syntax to display the non-linear formula including predictors (i.e., [h0, treatment, and fungus]) and parameters (i.e., [a, t, and f]) wrapped in a call to [the bf() function]. This stands in contrast to classical R formulas, where only predictors are given and parameters are implicit. The argument [a + t + f ~ 1] serves two purposes. First, it provides information, which variables in formula are parameters, and second, it specifies the linear predictor terms for each parameter. In fact, we should think of non-linear parameters as placeholders for linear predictor terms rather than as parameters themselves (see also the following examples). In the present case, we have no further variables to predict [a, t, and f] and thus we just fit intercepts that represent our estimates of [\\(\\alpha\\), \\(t\\), and \\(f\\)]. The formula [a + t + f ~ 1] is a short form of[a ~ 1, t ~ 1, f ~ 1] that can be used if multiple non-linear parameters share the same formula. Setting nl = TRUE tells brms that the formula should be treated as non-linear.\n\n\nIn contrast to generalized linear models, priors on population-level parameters (i.e., ‘fixed effects’) are often mandatory to identify a non-linear model. Thus, brms requires the user to explicitly specify these priors. In the present example, we used a [lognormal(0, 0.2) prior on (the population-level intercept of) a, while we used a normal(0, 0.5) prior on both (population-level intercepts of) t and f]. Setting priors is a non-trivial task in all kinds of models, especially in non-linear models, so you should always invest some time to think of appropriate priors. Quite often, you may be forced to change your priors after fitting a non-linear model for the first time, when you observe different MCMC chains converging to different posterior regions. This is a clear sign of an identification problem and one solution is to set stronger (i.e., more narrow) priors. (emphasis in the original)\n\nLet’s see what we’ve done.\n\nprint(b6.7)\n\n Family: gaussian \n  Links: mu = identity \nFormula: h1 ~ h0 * (a + t * treatment + f * fungus) \n         a ~ 1\n         t ~ 1\n         f ~ 1\n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_Intercept     1.48      0.03     1.43     1.53 1.00     1796     2125\nt_Intercept     0.00      0.03    -0.06     0.06 1.00     1944     2242\nf_Intercept    -0.27      0.04    -0.34    -0.19 1.00     2231     2540\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.45      0.11     1.26     1.67 1.00     2552     2377\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAll in all, it looks like we did a good job matching up McElreath’s results. The posterior doesn’t, however, match up well with the way we generated the data…\n\n\n6.2.2 Blocked by consequence\nTo measure the treatment effect properly, we should omit fungus from the model. This leaves us with the equation\n\\[\\begin{align*}\nh_{1i}  & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = h_{0i} \\times (\\alpha + \\beta_1 \\text{treatment}_i) \\\\\n\\alpha  & \\sim \\operatorname{Log-Normal}(0, 0.25) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nFit this model with the non-linear syntax, too.\n\nb6.8 &lt;- brm(\n  data = d,\n  family = gaussian,\n  bf(h1 ~ h0 * (a + t * treatment),\n     a + t ~ 1,\n     nl = TRUE),\n  prior = c(prior(lognormal(0, 0.2), nlpar = a, lb = 0),\n            prior(normal(0, 0.5), nlpar = t),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.08\")\n\nDid we do better?\n\nprint(b6.8)\n\n Family: gaussian \n  Links: mu = identity \nFormula: h1 ~ h0 * (a + t * treatment) \n         a ~ 1\n         t ~ 1\n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_Intercept     1.38      0.03     1.33     1.43 1.00     2004     2254\nt_Intercept     0.09      0.04     0.02     0.15 1.00     2087     2337\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.79      0.13     1.56     2.06 1.00     2234     2394\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nYes, now we have a positive treatment effect.\n\n\n6.2.3 Fungus and \\(d\\)-separation\nLet’s make a DAG.\n\n# Define our coordinates\ndag_coords &lt;- tibble(\n  name = c(\"H0\", \"T\", \"F\", \"H1\"),\n  x    = c(1, 5, 4, 3),\n  y    = c(2, 2, 1.5, 1))\n\n# Save our DAG\ndag &lt;- dagify(\n  F ~ T,\n  H1 ~ H0 + F,\n  coords = dag_coords)\n\n# Plot \ndag |&gt;\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(alpha = 1/2, color = \"steelblue\", size = 6.5) +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges() + \n  theme_dag()\n\n\n\n\n\n\n\n\nWe’ll be making a lot of simple DAGs following this format over this chapter. To streamline out plotting code, let’s make a custom plotting function. I’ll call it gg_simple_dag().\n\ngg_simple_dag &lt;- function(d) {\n  d |&gt; \n    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(alpha = 1/2, color = \"steelblue\", size = 6.5) +\n    geom_dag_text(color = \"black\") +\n    geom_dag_edges() + \n    theme_dag()\n}\n\n# Try it out!\ndag |&gt; \n  gg_simple_dag()\n\n\n\n\n\n\n\n\nAnyway, the DAG clarifies\n\nthat learning the treatment tells us nothing about the outcome, once we know the fungus status.\nAn even more DAG way to say this is that conditioning on \\(F\\) induces d-separation. The “d” stands for directional. D-separation means that some variables on a directed graph are independent of others. There is no path connecting them. In this case, \\(H_1\\) is d-separated from \\(T\\), but only when we condition on \\(F\\). Conditioning on \\(F\\) effectively blocks the directed path \\(T \\rightarrow F \\rightarrow H_1\\), making \\(T\\) and \\(H_1\\) independent (d-separated). (p. 174, emphasis in the original)\n\nNote that our ggdag object, dag, will also work with the dagitty::dseparated() function.\n\nlibrary(dagitty)\n\ndag |&gt; \n  dseparated(\"T\", \"H1\")\n\n[1] FALSE\n\ndag |&gt; \n  dseparated(\"T\", \"H1\", \"F\")\n\n[1] TRUE\n\n\nThe descriptively-named dagitty::mpliedConditionalIndependencies() function will work, too.\n\nimpliedConditionalIndependencies(dag)\n\nF _||_ H0\nH0 _||_ T\nH1 _||_ T | F\n\n\nNow consider a DAG of a different kind of causal structure.\n\n# Define our coordinates\ndag_coords &lt;- tibble(\n  name = c(\"H0\", \"H1\", \"M\", \"F\", \"T\"),\n  x    = c(1, 2, 2.5, 3, 4),\n  y    = c(2, 2, 1, 2, 2))\n\n# Save our DAG\ndag &lt;- dagify(\n  F ~ M + T,\n  H1 ~ H0 + M,\n  coords = dag_coords)\n\n# Plot \ndag |&gt;\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"M\"),\n                 alpha = 1/2, size = 6.5, show.legend = F) +\n  geom_point(x = 2.5, y = 1, \n             color = \"orange\", shape = 1, size = 6.5, stroke = 1) +\n  geom_dag_text(color = \"black\") +\n  geom_dag_edges() + \n  scale_color_manual(values = c(\"steelblue\", \"orange\")) +\n  theme_dag()\n\n\n\n\n\n\n\n\nOur custom gg_simple_dag() was a little too brittle to accommodate DAGs that mark of unobserved variables. Since we’ll be making a few more DAGs of this kind, we’ll make one more custom plotting function. We’ll call this one gg_fancy_dag().\n\ngg_fancy_dag &lt;- function(d, x = 1, y = 1, circle = \"U\") {\n  d |&gt; \n    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n    geom_dag_point(aes(color = name == circle),\n                   alpha = 1/2, size = 6.5, show.legend = F) +\n    geom_point(x = x, y = y, \n               color = \"orange\", shape = 1, size = 6.5, stroke = 1) +\n    geom_dag_text(color = \"black\") +\n    geom_dag_edges() + \n    scale_color_manual(values = c(\"steelblue\", \"orange\")) +\n    theme_dag()\n}\n\n# Check it out\ndag |&gt; \n  gg_fancy_dag(x = 2.5, y = 1, circle = \"M\")\n\n\n\n\n\n\n\n\nBased on McElreath’s R code 6.20, here we simulate some data based on the new DAG.\n\nset.seed(71)\nn &lt;- 1000\n\nd2 &lt;- tibble(\n  h0        = rnorm(n, mean = 10, sd = 2),\n  treatment = rep(0:1, each = n / 2),\n  m         = rbinom(n, size = 1, prob = 0.5),\n  fungus    = rbinom(n, size = 1, prob = 0.5 - treatment * 0.4 + 0.4 * m),\n  h1        = h0 + rnorm(n, mean = 5 + 3 * m, sd = 1))\n\nhead(d2)\n\n# A tibble: 6 × 5\n     h0 treatment     m fungus    h1\n  &lt;dbl&gt;     &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;dbl&gt;\n1  9.14         0     0      0  14.8\n2  9.11         0     0      0  15.3\n3  9.04         0     1      1  16.4\n4 10.8          0     1      1  19.1\n5  9.16         0     1      1  17.2\n6  7.63         0     0      0  13.4\n\n\nUse update() to refit b6.7 and b6.8 to the new data.\n\nb6.7b &lt;- update(\n  b6.7,\n  newdata = d2,\n  seed = 6,\n  file = \"fits/b06.07b\")\n\nb6.8b &lt;- update(\n  b6.8,\n  newdata = d2,\n  seed = 6,\n  file = \"fits/b06.08b\")\n\nCheck the results.\n\nposterior_summary(b6.7b)[1:4, ] |&gt; round(digits = 2)\n\n              Estimate Est.Error Q2.5 Q97.5\nb_a_Intercept     1.52      0.01 1.50  1.55\nb_t_Intercept     0.05      0.01 0.02  0.08\nb_f_Intercept     0.14      0.01 0.12  0.17\nsigma             2.11      0.05 2.02  2.20\n\nposterior_summary(b6.8b)[1:3, ] |&gt; round(digits = 2)\n\n              Estimate Est.Error  Q2.5 Q97.5\nb_a_Intercept     1.62      0.01  1.61  1.64\nb_t_Intercept    -0.01      0.01 -0.04  0.02\nsigma             2.21      0.05  2.12  2.31\n\n\n“Including fungus again confounds inference about the treatment, this time by making it seem like it helped the plants, even though it had no effect” (p. 175).\n\n6.2.3.1 Rethinking: Model selection doesn’t help\n\nIn the next chapter, you’ll learn about model selection using information criteria. Like other model comparison and selection schemes, these criteria help in contrasting and choosing model structure. But such approaches are no help in the example presented just above, since the model that includes fungus both fits the sample better and would make better out-of-sample predictions. Model [b6.7] misleads because it asks the wrong question, not because it would make poor predictions. As argued in Chapter 1, prediction and causal inference are just not the same task. No statistical procedure can substitute for scientific knowledge and attention to it. We need multiple models because they help us understand causal paths, not just so we can choose one or another for prediction. (p. 175)\n\nBrutal.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG & The Causal Terror</span>"
    ]
  },
  {
    "objectID": "06.html#sec-Collider-bias",
    "href": "06.html#sec-Collider-bias",
    "title": "6  The Haunted DAG & The Causal Terror",
    "section": "6.3 Collider bias",
    "text": "6.3 Collider bias\nMake the collider bias DAG of the trustworthiness/newsworthiness example.\n\ndag_coords &lt;- tibble(\n  name = c(\"T\", \"S\", \"N\"),\n  x    = 1:3,\n  y    = 1)\n\ndagify(S ~ T + N,\n       coords = dag_coords) |&gt;\n  gg_simple_dag()\n\n\n\n\n\n\n\n\n\nThe fact that two arrows enter S means it is a collider. The core concept is easy to understand: When you condition on a collider, it creates statistical–but not necessarily causal–associations among its causes. In this case, once you learn that a proposal has been selected (\\(S\\)), then learning its trustworthiness (\\(T\\)) also provides information about its newsworthiness (\\(N\\)). Why? Because if, for example, a selected proposal has low trustworthiness, then it must have high newsworthiness. Otherwise it wouldn’t have been funded. The same works in reverse: If a proposal has low newsworthiness, we’d infer that it must have higher than average trustworthiness. Otherwise it would not have been selected for funding. (p. 176. emphasis in the original)\n\n\n6.3.1 Collider of false sorrow\nAll it takes is a single mutate() line in the dagify() function to amend our previous DAG.\n\ndagify(M ~ H + A,\n       coords = dag_coords |&gt;\n         mutate(name = c(\"H\", \"M\", \"A\"))) |&gt;\n  gg_simple_dag()\n\n\n\n\n\n\n\n\nIn this made-up example,\n\nhappiness (\\(H\\)) and age (\\(A\\)) both cause marriage (\\(M\\)). Marriage is therefore a collider. Even though there is no causal association between happiness and age, if we condition on marriage–which means here, if we include it as a predictor in a regression–then it will induce a statistical association between age and happiness. And this can mislead us to think that happiness changes with age, when in fact it is constant.\nTo convince you of this, let’s do another simulation. (pp. 176–177)\n\nMcElreath simulated the data for this section using his custom rethinking::sim_happiness() function. If you’d like to see the guts of the function, execute rethinking::sim_happiness. Our approach will be to simulate the data from the ground up. The workflow to follow is based on help from the great Randall Pruim; I was initially stumped and he lent a helping hand. The first step is to make a simple new_borns() function, which returns a tibble with n unmarried one-year-old’s who have different levels of happiness. We’ll set the default for n at 20.\n\nnew_borns &lt;- function(n = 20) {\n  tibble(a = 1,                                       # 1 year old\n         m = 0,                                       # Not married\n         h = seq(from = -2, to = 2, length.out = n))  # Range of happiness scores\n}\n\nHere’s how it works.\n\nnew_borns()\n\n# A tibble: 20 × 3\n       a     m      h\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1     0 -2    \n 2     1     0 -1.79 \n 3     1     0 -1.58 \n 4     1     0 -1.37 \n 5     1     0 -1.16 \n 6     1     0 -0.947\n 7     1     0 -0.737\n 8     1     0 -0.526\n 9     1     0 -0.316\n10     1     0 -0.105\n11     1     0  0.105\n12     1     0  0.316\n13     1     0  0.526\n14     1     0  0.737\n15     1     0  0.947\n16     1     0  1.16 \n17     1     0  1.37 \n18     1     0  1.58 \n19     1     0  1.79 \n20     1     0  2    \n\n\nThe second step is to make another custom function, update_population(), which takes the input from new_borns(). This function will age up the simulated one-year-old’s from new_borns(), add another cohort of new_borns(), and append the cohorts. As you iterate, the initial cohort of new_borns() will eventually hit the age of 18, which is also the age they’re first eligible to marry (aom = 18). At that age and up, the happier people are more likely to get married than the less happy folks. You’ll also see that our simulation follows McElreath’s in that we remove people from the population after the age of 65. 🤷\n\nupdate_population &lt;- function(pop, n_births = 20, aom = 18, max_age = 65) {\n  pop |&gt;\n    mutate(a = a + 1,  # Everyone gets one year older\n           # Some people get married\n           m = ifelse(m &gt;= 1, 1, (a &gt;= aom) * rbinom(n(), 1, rethinking::inv_logit(h - 4)))) |&gt;\n    filter(a &lt;= max_age) |&gt;         # Old people die\n    bind_rows(new_borns(n_births))  # New people are born\n}\n\nHere’s what it looks like if we start with an initial new_borns() and pump them into update_population().\n\nnew_borns() |&gt; \n  update_population()\n\n# A tibble: 40 × 3\n       a     m      h\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     2     0 -2    \n 2     2     0 -1.79 \n 3     2     0 -1.58 \n 4     2     0 -1.37 \n 5     2     0 -1.16 \n 6     2     0 -0.947\n 7     2     0 -0.737\n 8     2     0 -0.526\n 9     2     0 -0.316\n10     2     0 -0.105\n# ℹ 30 more rows\n\n\nFor our final step, we run the population simulation for 1,000 years. On my 2-year-old laptop, this took less than a minute.\n\n# This was McElreath's seed\nset.seed(1977)\n\n# Year 1\nd &lt;- new_borns(n = 20)\n\n# Years 2 through 1000\nfor(i in 2:1000) {\n  d &lt;- update_population(d, n_births = 20, aom = 18, max_age = 65)\n}\n\n# Now `rename()`\nd &lt;- d |&gt; \n  rename(age = a, married = m, happiness = h)\n\n# Take a look\nglimpse(d)\n\nRows: 1,300\nColumns: 3\n$ age       &lt;dbl&gt; 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 64, 64, 64, 64, 64, 64, 64…\n$ married   &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,…\n$ happiness &lt;dbl&gt; -2.0000000, -1.7894737, -1.5789474, -1.3684211, -1.1578947, -0.9473684, -0.7368421, -0.5263158, -0.3157895, -0…\n\n\nSummarize the variables.\n\nd |&gt; \n  pivot_longer(everything()) |&gt; \n  group_by(name) |&gt; \n  mean_qi(value) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 3 × 7\n  name      value .lower .upper .width .point .interval\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 age        33        2     64   0.95 mean   qi       \n2 happiness   0       -2      2   0.95 mean   qi       \n3 married     0.3      0      1   0.95 mean   qi       \n\n\nHere’s our version of Figure 6.4.\n\nd |&gt; \n  mutate(married = factor(married, labels = c(\"unmarried\", \"married\"))) |&gt; \n  \n  ggplot(aes(x = age, y = happiness, color = married)) +\n  geom_point(size = 1.75) +\n  scale_color_manual(NULL, values = c(\"grey85\", \"forestgreen\")) +\n  scale_x_continuous(expand = c(0.015, 0.015)) +\n  theme(panel.grid = element_blank())\n\n\n\n\n\n\n\n\nHere’s the likelihood for the simple Gaussian multivariable model predicting happiness:\n\\[\\begin{align*}\n\\text{happiness}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i              & = \\alpha_{\\text{married} [i]} + \\beta_1 \\text{age}_i ,\\\\\n\\end{align*}\\]\nwhere \\(\\text{married}[i]\\) is the marriage status of individual \\(i\\). Here we make d2, the subset of d containing only those 18 and up. We then make a new age variable, a, which is scaled such that \\(18 = 0\\), \\(65 = 1\\), and so on.\n\nd2 &lt;- d |&gt; \n  filter(age &gt; 17) |&gt; \n  mutate(a = (age - 18) / (65 - 18))\n\nhead(d2)\n\n# A tibble: 6 × 4\n    age married happiness     a\n  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1    65       0    -2         1\n2    65       0    -1.79      1\n3    65       0    -1.58      1\n4    65       1    -1.37      1\n5    65       1    -1.16      1\n6    65       0    -0.947     1\n\n\nWith respect to priors,\n\nhappiness is on an arbitrary scale, in these data, from \\(-2\\) to \\(+2\\). So our imaginary strongest relationship, taking happiness from maximum to minimum, has a slope with rise over run of \\((2 - (-2))/1 = 4\\). Remember that 95% of the mass of a normal distribution is contained within 2 standard deviations. So if we set the standard deviation of the prior to half of 4, we are saying that we expect 95% of plausible slopes to be less than maximally strong. That isn’t a very strong prior, but again, it at least helps bound inference to realistic ranges. Now for the intercepts. Each \\(\\alpha\\) is the value of \\(\\mu_i\\) when \\(A_i = 0\\). In this case, that means at age 18. So we need to allow \\(\\alpha\\) to cover the full range of happiness scores. \\(\\operatorname{Normal}(0, 1)\\) will put 95% of the mass in the \\(-2\\) to \\(+2\\) interval. (p. 178)\n\nHere we’ll take one last step before fitting our model with brms. Saving mid as a factor will make it easier to interpret the model results.\nHere we’ll take one last step before fitting our model with brms. Saving the mid index variable as a factor will make it easier to interpret the model results. To see what I mean, skip this step, fit the model, and compare your results with mine, below.\n\nd2 &lt;- d2 |&gt; \n  mutate(mid = factor(married + 1, labels = c(\"single\", \"married\")))\n\nhead(d2)\n\n# A tibble: 6 × 5\n    age married happiness     a mid    \n  &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n1    65       0    -2         1 single \n2    65       0    -1.79      1 single \n3    65       0    -1.58      1 single \n4    65       1    -1.37      1 married\n5    65       1    -1.16      1 married\n6    65       0    -0.947     1 single \n\n\nFit the model.\n\nb6.9 &lt;- brm(\n  data = d2, \n  family = gaussian,\n  happiness ~ 0 + mid + a,\n  prior = c(prior(normal(0, 1), class = b, coef = midmarried),\n            prior(normal(0, 1), class = b, coef = midsingle),\n            prior(normal(0, 2), class = b, coef = a),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.09\")\n\n\nprint(b6.9)\n\n Family: gaussian \n  Links: mu = identity \nFormula: happiness ~ 0 + mid + a \n   Data: d2 (Number of observations: 960) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmidsingle     -0.19      0.07    -0.32    -0.07 1.00     1626     1943\nmidmarried     1.27      0.09     1.09     1.44 1.00     1539     2061\na             -0.80      0.12    -1.03    -0.56 1.00     1371     2069\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.01      0.02     0.97     1.05 1.00     2629     2581\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow drop marriage status, mid.\n\nb6.10 &lt;- brm(\n  data = d2, \n  family = gaussian,\n  happiness ~ 0 + Intercept + a,\n  prior = c(prior(normal(0, 1), class = b, coef = Intercept),\n            prior(normal(0, 2), class = b, coef = a),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.10\")\n\n\nprint(b6.10)\n\n Family: gaussian \n  Links: mu = identity \nFormula: happiness ~ 0 + Intercept + a \n   Data: d2 (Number of observations: 960) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.08    -0.15     0.14 1.00     1545     1686\na             0.01      0.13    -0.25     0.26 1.00     1527     1648\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.21      0.03     1.16     1.27 1.00     2476     2413\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWow. So when we take out mid, the coefficient for a drops to zero.\n\nThe pattern above is exactly what we should expect when we condition on a collider. The collider is marriage status. It is a common consequence of age and happiness. As a result, when we condition on it, we induce a spurious association between the two causes. So it looks like, to model [b6.9], that age is negatively associated with happiness. But this is just a statistical association, not a causal association. Once we know whether someone is married or not, then their age does provide information about how happy they are. (p. 179)\n\nA little further down, McElreath gets rough:\n\nIt’s easy to plead with this example. Shouldn’t marriage also influence happiness? What if happiness does change with age? But this misses the point. If you don’t have a causal model, you can’t make inferences from a multiple regression. And the regression itself does not provide the evidence you need to justify a causal model. Instead, you need some science. (pp. 179–180, emphasis added)\n\n\n\n6.3.2 The haunted DAG\nIt gets worse. “Unmeasured causes can still induce collider bias. So I’m sorry to say that we also have to consider the possibility that our DAG may be haunted” (p. 180).\nHere’s the unhaunted DAG.\n\ndag_coords &lt;- tibble(\n  name = c(\"G\", \"P\", \"C\"),\n  x    = c(1, 2, 2),\n  y    = c(2, 2, 1))\n\ndagify(P ~ G,\n       C ~ P + G,\n       coords = dag_coords) |&gt;\n  gg_simple_dag()\n\n\n\n\n\n\n\n\nNow we add the haunting variable, U.\n\ndag_coords &lt;- tibble(\n  name = c(\"G\", \"P\", \"C\", \"U\"),\n  x    = c(1, 2, 2, 2.5),\n  y    = c(2, 2, 1, 1.5))\n\ndagify(P ~ G + U,\n       C ~ P + G + U,\n       coords = dag_coords) |&gt;\n  gg_fancy_dag(x = 2.5, y = 1.5, circle = \"U\")\n\n\n\n\n\n\n\n\nThis is a mess. Let’s simulate some data.\n\n# How many grandparent-parent-child triads would you like?\nn &lt;- 200 \n\nb_gp &lt;- 1  # Direct effect of G on P\nb_gc &lt;- 0  # Direct effect of G on C\nb_pc &lt;- 1  # Direct effect of P on C\nb_u  &lt;- 2  # Direct effect of U on P and C\n\n# simulate triads\nset.seed(1)\nd &lt;- tibble(u = 2 * rbinom(n, size = 1, prob = 0.5) - 1,\n            g = rnorm(n, mean = 0, sd = 1)) |&gt; \n  mutate(p = rnorm(n, mean = b_gp * g + b_u * u, sd = 1)) |&gt; \n  mutate(c = rnorm(n, mean = b_pc * p + b_gc * g + b_u * u, sd = 1))\n\nhead(d)\n\n# A tibble: 6 × 4\n      u       g     p     c\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    -1 -0.620  -1.73 -3.65\n2    -1  0.0421 -3.01 -5.30\n3     1 -0.911   3.06  3.88\n4     1  0.158   1.77  3.79\n5    -1 -0.655  -1.00 -2.01\n6     1  1.77    5.28  8.87\n\n\nFit the model without u.\n\nb6.11 &lt;- brm(\n  data = d, \n  family = gaussian,\n  c ~ 0 + Intercept + p + g,\n  prior = c(prior(normal(0, 1), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.11\")\n\n\nprint(b6.11)\n\n Family: gaussian \n  Links: mu = identity \nFormula: c ~ 0 + Intercept + p + g \n   Data: d (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.12      0.10    -0.31     0.08 1.00     3931     2937\np             1.79      0.04     1.70     1.87 1.00     3347     2470\ng            -0.84      0.11    -1.04    -0.62 1.00     3356     2895\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.43      0.07     1.30     1.58 1.00     3530     3144\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere’s our version of Figure 6.5.\n\nd_plot &lt;- d |&gt; \n  mutate(centile = ifelse(p &gt;= quantile(p, prob = 0.45) & p &lt;= quantile(p, prob = 0.60), \"a\", \"b\"),\n         u = factor(u))\n\nd_plot |&gt;\n  ggplot(aes(x = g, y = c)) +\n  geom_point(aes(shape = centile, color = u),\n             size = 2.5, stroke = 1/4) +\n  stat_smooth(data = d_plot |&gt; filter(centile == \"a\"),\n              method = \"lm\", se = F, color = \"black\", linewidth = 1/2, fullrange = T) +\n  scale_shape_manual(values = c(19, 1)) +\n  scale_color_manual(values = c(\"black\", \"lightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow fit the model including u.\n\nb6.12 &lt;- update(\n  b6.11,\n  newdata = d,\n  formula = c ~ 0 + Intercept + p + g + u,\n  seed = 6,\n  file = \"fits/b06.12\")\n\nCheck the results.\n\nprint(b6.12)\n\n Family: gaussian \n  Links: mu = identity \nFormula: c ~ Intercept + p + g + u - 1 \n   Data: d (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.12      0.07    -0.26     0.02 1.00     3124     2612\np             1.01      0.07     0.88     1.14 1.00     1802     2011\ng            -0.04      0.10    -0.22     0.15 1.00     2152     2511\nu             2.00      0.15     1.71     2.29 1.00     1953     2359\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.04      0.05     0.94     1.14 1.00     2795     2355\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow the posterior for \\(\\beta_\\text g\\) is hovering around 0, where it belongs.\n\nb_gc\n\n[1] 0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG & The Causal Terror</span>"
    ]
  },
  {
    "objectID": "06.html#confronting-confounding",
    "href": "06.html#confronting-confounding",
    "title": "6  The Haunted DAG & The Causal Terror",
    "section": "6.4 Confronting confounding",
    "text": "6.4 Confronting confounding\n“Let’s define confounding as any context in which the association between an outcome \\(Y\\) and a predictor of interest \\(X\\) is not the same as it would be, if we had experimentally determined the values of \\(X\\)” (p. 183, emphasis in the original).\nHere’s the \\(E\\)-\\(U\\)-\\(W\\) DAG to help us sort this out.\n\ndag_coords &lt;- tibble(\n  name = c(\"E\", \"U\", \"W\"),\n  x    = c(1, 2, 3),\n  y    = c(1, 2, 1))\n\ndagify(E ~ U,\n       W ~ E + U,\n       coords = dag_coords) |&gt;\n  gg_simple_dag()\n\n\n\n\n\n\n\n\nHere’s the alternative if we were to randomly assign \\(E\\).\n\ndagify(W ~ E + U,\n       coords = dag_coords) |&gt;\n  gg_simple_dag()\n\n\n\n\n\n\n\n\n“Manipulation removes the confounding, because it blocks the other path between \\(E\\) and \\(W\\)” (p. 184).\n\n6.4.1 Shutting the backdoor\n\nBlocking confounding paths between some predictor \\(X\\) and some outcome \\(Y\\) is known as shutting the backdoor. We don’t want any spurious association sneaking in through a non-causal path that enters the back of the predictor \\(X\\). In the example above, the path \\(E \\leftarrow U \\rightarrow W\\) is a backdoor path, because it enters $$E with an arrow and also connects \\(E\\) to \\(W\\). This path is non-causal–intervening on \\(E\\) will not cause a change in \\(W\\) through this path–but it still produces an association between \\(E\\) and \\(W\\).\nNow for some good news. Given a causal DAG, it is always possible to say which, if any, variables one must control for in order to shut all the backdoor paths. It is also possible to say which variables one must not control for, in order to avoid making new confounds. And–some more good news–there are only four types of variable relations that combine to form all possible paths. (p. 184, emphasis in the original)\n\nHere are the representations for our four types of variable relations: the fork, pipe, collider, and descendant.\n\nd1 &lt;- dagify(\n  X ~ Z,\n  Y ~ Z,\n  coords = tibble(name = c(\"X\", \"Y\", \"Z\"),\n                  x = c(1, 3, 2),\n                  y = c(2, 2, 1)))\n\nd2 &lt;- dagify(\n  Z ~ X,\n  Y ~ Z,\n  coords = tibble(name = c(\"X\", \"Y\", \"Z\"),\n                  x = c(1, 3, 2),\n                  y = c(2, 1, 1.5)))\n\nd3 &lt;- dagify(\n  Z ~ X + Y,\n  coords = tibble(name = c(\"X\", \"Y\", \"Z\"),\n                  x = c(1, 3, 2),\n                  y = c(1, 1, 2)))\n\nd4 &lt;- dagify(\n  Z ~ X + Y,\n  D ~ Z,\n  coords = tibble(name = c(\"X\", \"Y\", \"Z\", \"D\"),\n                  x = c(1, 3, 2, 2),\n                  y = c(1, 1, 2, 1.05)))\n\np1 &lt;- gg_simple_dag(d1) + labs(subtitle = \"The Fork\")\np2 &lt;- gg_simple_dag(d2) + labs(subtitle = \"The Pipe\")\np3 &lt;- gg_simple_dag(d3) + labs(subtitle = \"The Collider\")\np4 &lt;- gg_simple_dag(d4) + labs(subtitle = \"The Descendant\")\n\nOutside of the DAGs produced by the node_equivalent_dags(), the ggdag package does not simultaneously plot multiple DAGs the way one might with ggplot::facet_wrap(). For details, see GitHub issues #41 and #42. However, a simple way around that is to make each plot separately, save them, and then combine the individual plots with patchwork. Here it is, our DAGgy Figure 6.6.\n\nlibrary(patchwork)\n\n(p1 | p2 | p3 | p4) &\n  theme(plot.subtitle = element_text(hjust = 0.5)) &\n  plot_annotation(title = \"The four elemental confounds\") \n\n\n\n\n\n\n\n\n\nNo matter how complicated a causal DAG appears, it is always built out of these four types of relations. And since you know how to open and close each, you (or your computer) can figure out which variables you need to include or not include. Here’s the recipe:\n\n1 List all of the paths connecting \\(X\\) (the potential cause of interest) and \\(Y\\) (the outcome).\n2 Classify each path by whether it is open or closed. A path is open unless it contains a collider.\n3 Classify each path by whether it is a backdoor path. A backdoor path has an arrow entering \\(X\\).\n4 If there are any open backdoor paths, decide which variable(s) to condition on to close it (if possible). (p. 185)\n\n\n\n\n6.4.2 Two roads\n“The DAG below contains an exposure of interest \\(X\\), an outcome of interest \\(Y\\), an unobserved variable \\(U\\), and three observed covariates (\\(A\\), \\(B\\), and \\(C\\))” (p. 186).\n\ndag_coords &lt;- tibble(\n  name = c(\"A\", \"B\", \"C\", \"U\", \"X\", \"Y\"),\n  x    = c(2, 2, 3, 1, 1, 3),\n  y    = c(4, 2, 3, 3, 1, 1))\n\ndagify(B ~ C + U,\n       C ~ A,\n       U ~ A,\n       X ~ U,\n       Y ~ C + X,\n       coords = dag_coords) |&gt;\n  gg_fancy_dag(x = 1, y = 3, circle = \"U\")\n\n\n\n\n\n\n\n\nIn this DAG, there are two indirect (backdoor) paths from \\(X\\) to \\(Y\\), which are\n\n\\(X \\leftarrow U \\leftarrow A \\rightarrow C \\rightarrow Y\\), which is open; and\n\\(X \\leftarrow U \\rightarrow B \\leftarrow C \\rightarrow Y\\), which is closed.\n\nConditioning on either \\(C\\) or \\(A\\) will close the open backdoor.\n\ndag_6.1 &lt;- dagitty(\n  \"dag {\n    U [unobserved]\n    X -&gt; Y\n    X &lt;- U &lt;- A -&gt; C -&gt; Y \n    U -&gt; B &lt;- C\n    }\"\n)\n\nadjustmentSets(dag_6.1, exposure = \"X\", outcome = \"Y\")\n\n{ C }\n{ A }\n\n\nDo note the adjustmentSets() default is to return only minimal sets of variables on which to condition. If you would like to see other non-minimal adjustment sets, set type = \"all\". Here’s what that looks like for this example.\n\nadjustmentSets(dag_6.1, exposure = \"X\", outcome = \"Y\", type = \"all\")\n\n{ A }\n{ C }\n{ A, C }\n{ B, C }\n{ A, B, C }\n{ U }\n{ A, U }\n{ B, U }\n{ A, B, U }\n{ C, U }\n{ A, C, U }\n{ B, C, U }\n{ A, B, C, U }\n\n\n\n\n6.4.3 Backdoor waffles\n\ndag_coords &lt;- tibble(\n  name = c(\"A\", \"D\", \"M\", \"S\", \"W\"),\n  x    = c(1, 3, 2, 1, 3),\n  y    = c(1, 1, 2, 3, 3))\n\ndagify(A ~ S,\n       D ~ A + M + W,\n       M ~ A + S,\n       W ~ S,\n       coords = dag_coords) |&gt;\n  gg_simple_dag()\n\n\n\n\n\n\n\n\n\nIn this graph, \\(S\\) is whether or not a State is in the southern United States, \\(A\\) is median age at marriage, \\(M\\) is marriage rate, \\(W\\) is number of Waffle Houses, and \\(D\\) is divorce rate. This graph assumes that southern States have lower ages of marriage (\\(S \\rightarrow A\\)), higher rates of marriage both directly (\\(S \\rightarrow M\\)) and mediated through age of marriage (\\(S \\rightarrow A \\rightarrow M\\)), as well as more waffles (\\(S \\rightarrow W\\)). Age of marriage and marriage rate both influence divorce.\nThere are three open backdoor paths between \\(W\\) and \\(D\\). (p. 187)\n\n\ndag_6.2 &lt;- dagitty(\n  \"dag {\n    A -&gt; D\n    A -&gt; M -&gt; D\n    A &lt;- S -&gt; M\n    S -&gt; W -&gt; D\n    }\"\n)\n\nadjustmentSets(dag_6.2, exposure = \"W\", outcome = \"D\")\n\n{ A, M }\n{ S }\n\n\nThe first line of output indicates we’d have to condition on \\(A\\) and \\(M\\) simultaneously. As an alternative, we could just condition on \\(S\\). Here are the conditional independencies implied in that DAG.\n\nimpliedConditionalIndependencies(dag_6.2)\n\nA _||_ W | S\nD _||_ S | A, M, W\nM _||_ W | S",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG & The Causal Terror</span>"
    ]
  },
  {
    "objectID": "06.html#summary-and-a-little-more-practice",
    "href": "06.html#summary-and-a-little-more-practice",
    "title": "6  The Haunted DAG & The Causal Terror",
    "section": "6.5 Summary [and a little more practice]",
    "text": "6.5 Summary [and a little more practice]\n\nMultiple regression is no oracle, but only a golem. It is logical, but the relationships it describes are conditional associations, not causal influences. Therefore additional information, from outside the model, is needed to make sense of it. This chapter presented introductory examples of some common frustrations: multicollinearity, post-treatment bias, and collider bias. Solutions to these frustrations can be organized under a coherent framework in which hypothetical causal relations among variables are analyzed to cope with confounding. (p. 189)\n\nIn the last section, we used a DAG to explore how including/excluding different covariates might influence our estimate of the causal relationship between the number of Waffle Houses and the divorce rate (\\(W \\rightarrow D\\)). To finish that example out, we might explore some of the possible models informed by the DAG. First we load the Waffle House data.\n\ndata(WaffleDivorce, package = \"rethinking\")\nd &lt;- WaffleDivorce\n\n# Standardize the continuous focal variables\nd &lt;- d |&gt; \n  mutate(a = rethinking::standardize(MedianAgeMarriage),\n         d = rethinking::standardize(Divorce),\n         m = rethinking::standardize(Marriage),\n         s = factor(South, levels = 0:1, labels = c(\"North\", \"South\")),\n         w = rethinking::standardize(WaffleHouses))\n\n# tidy up\nrm(WaffleDivorce)\n\nThe only focal variable we did not standardize was South, which is binary. Technically, you can standardize binary variables and not break the regression model. However, my preference it so leave them in their 0/1 metric. To do otherwise would complicate how you’d interpret the result of any model including them. Here is our ggpairs() plot of all five focal variables. Remember, the central issue is the causal relation between w and d.\n\nggpairs(data = d, columns = c(14:16, 18, 17),\n        upper = list(continuous = wrap(\"cor\", family = \"sans\", color = \"black\", size = 3)),\n        diag = list(continuous = my_diag),\n        lower = list(continuous = my_lower),\n        mapping = aes(color = s)) +\n  scale_fill_manual(values = c(\"forestgreen\", \"lightblue\"))\n\n\n\n\n\n\n\n\nLet’s fit a series of models. The priors are based on those we used the last time we saw the WaffleHouses data (Section 5.1).\n\nb6.13 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + w,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.13\")\n\nb6.14 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + w + s,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.14\")\n\nb6.15 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + w + a + m,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.15\")\n\nb6.16 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + w + a,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.16\")\n\nb6.17 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + w + m,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 6,\n  file = \"fits/b06.17\")\n\nIf we extract the posterior draws from each model, the column corresponding to \\(W \\rightarrow D\\) will be b_w. Here we extract those columns, wrangle, and compare the posteriors in a coefficient plot.\n\nformula &lt;- c(\"d ~ 1 + w\", \n             \"d ~ 1 + w + s\", \n             \"d ~ 1 + w + a + m\", \n             \"d ~ 1 + w + a\", \n             \"d ~ 1 + w + m\")\n\ntibble(fit = str_c(\"b6.1\", 3:7)) |&gt; \n  mutate(y    = str_c(fit, \" (\", formula, \")\"),\n         post = purrr::map(.x = fit, .f = function(x) {\n           get(x) |&gt; \n             as_draws_df() |&gt; \n             select(b_w)\n           })) |&gt; \n  unnest(post) |&gt; \n  \n  ggplot(aes(x = b_w, y = y, color = fit %in% c(\"b6.14\", \"b6.15\"))) +\n  stat_pointinterval(.width = 0.95) +\n  scale_color_manual(values = c(\"grey50\", \"forestgreen\")) +\n  labs(x = expression(beta[w]),\n       y = NULL) +\n  coord_cartesian(xlim = c(-0.4, 0.6)) +\n  theme(axis.text.y = element_text(hjust = 0),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe two \\(\\beta_w\\) posteriors corresponding to those endorsed by our DAG are in green. Frustratingly, the results from a real-world data analysis aren’t as conclusive as we may have naïvely supposed. Why? Well, McElreath left us with some words of wisdom a few pages back:\n\nThis DAG is obviously not satisfactory–it assumes there are no unobserved confounds, which is very unlikely for this sort of data. But we can still learn something by analyzing it. While the data cannot tell us whether a graph is correct, it can sometimes suggest how a graph is wrong. (p. 187)\n\nDAGs are nice tool for informing our data analytic strategy. But just as our multivariable predictor models are no fail-safes, our DAGs aren’t either. Models, theories, data-collection procedures–they all work together (or not) to determine the quality of our posteriors. To the extent any of those components are off, watch out.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG & The Causal Terror</span>"
    ]
  },
  {
    "objectID": "06.html#session-info",
    "href": "06.html#session-info",
    "title": "6  The Haunted DAG & The Causal Terror",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] patchwork_1.3.2       dagitty_0.3-4         ggdag_0.2.13          GGally_2.4.0          tidybayes_3.0.7      \n [6] bayesplot_1.15.0.9000 brms_2.23.0           Rcpp_1.1.0            lubridate_1.9.4       forcats_1.0.1        \n[11] stringr_1.6.0         dplyr_1.1.4           purrr_1.2.1           readr_2.1.5           tidyr_1.3.2          \n[16] tibble_3.3.1          ggplot2_4.0.1         tidyverse_2.0.0      \n\nloaded via a namespace (and not attached):\n  [1] gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7             magrittr_2.0.4         \n  [6] multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          mgcv_1.9-3              loo_2.9.0.9000         \n [11] vctrs_0.7.0             reshape2_1.4.5          crayon_1.5.3            pkgconfig_2.0.3         shape_1.4.6.1          \n [16] arrayhelpers_1.1-0      fastmap_1.2.0           backports_1.5.0         labeling_0.4.3          ggraph_2.2.2           \n [21] utf8_1.2.6              cmdstanr_0.9.0          rmarkdown_2.30          tzdb_0.5.0              ps_1.9.1               \n [26] xfun_0.55               cachem_1.1.0            jsonlite_2.0.0          tweenr_2.0.3            parallel_4.5.1         \n [31] R6_2.6.1                stringi_1.8.7           RColorBrewer_1.1-3      StanHeaders_2.36.0.9000 boot_1.3-31            \n [36] estimability_1.5.1      assertthat_0.2.1        rstan_2.36.0.9000       knitr_1.51              zoo_1.8-14             \n [41] Matrix_1.7-3            splines_4.5.1           igraph_2.2.0            timechange_0.3.0        tidyselect_1.2.1       \n [46] viridis_0.6.5           rstudioapi_0.17.1       abind_1.4-8             yaml_2.3.12             codetools_0.2-20       \n [51] rethinking_2.42         curl_7.0.0              processx_3.8.6          pkgbuild_1.4.8          lattice_0.22-7         \n [56] plyr_1.8.9              withr_3.0.2             bridgesampling_1.2-1    S7_0.2.1                posterior_1.6.1.9000   \n [61] coda_0.19-4.1           evaluate_1.0.5          survival_3.8-3          ggstats_0.11.0          RcppParallel_5.1.11-1  \n [66] polyclip_1.10-7         ggdist_3.3.3            pillar_1.11.1           tensorA_0.36.2.1        checkmate_2.3.3        \n [71] stats4_4.5.1            distributional_0.5.0    generics_0.1.4          hms_1.1.4               rstantools_2.5.0.9000  \n [76] scales_1.4.0            xtable_1.8-4            emo_0.0.0.9000          glue_1.8.0              emmeans_1.11.2-8       \n [81] tools_4.5.1             mvtnorm_1.3-3           graphlayouts_1.2.2      tidygraph_1.3.1         grid_4.5.1             \n [86] QuickJSR_1.8.1          nlme_3.1-168            ggforce_0.5.0           cli_3.6.5               svUnit_1.0.8           \n [91] viridisLite_0.4.2       Brobdingnag_1.2-9       V8_8.0.1                gtable_0.3.6            digest_0.6.39          \n [96] ggrepel_0.9.6           TH.data_1.1-4           htmlwidgets_1.6.4       farver_2.1.2            memoise_2.0.1          \n[101] htmltools_0.5.9         lifecycle_1.0.5         MASS_7.3-65",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG & The Causal Terror</span>"
    ]
  },
  {
    "objectID": "06.html#comments",
    "href": "06.html#comments",
    "title": "6  The Haunted DAG & The Causal Terror",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBürkner, P.-C. (2022). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html\n\n\nGabry, J. (2022). Plotting MCMC draws using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/plotting-mcmc-draws.html\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>The Haunted DAG & The Causal Terror</span>"
    ]
  },
  {
    "objectID": "07.html",
    "href": "07.html",
    "title": "7  Ulysses’ Compass",
    "section": "",
    "text": "7.0.0.1 Rethinking stargazing\nIn this chapter we contend with two contrasting kinds of statistical error:\nThere’s a lot going on in this chapter. More more practice with these ideas, check out Yarkoni & Westfall (2017), Choosing prediction over explanation in psychology: Lessons from machine learning.\nMcElreath spent little time discussing \\(p\\)-values and null hypothesis testing in the text. If you’d like to learn more from a Bayesian perspective, you might check out the first several chapters (particularly 10–13) in Kruschke’s (2015) text and my (2026a) ebook translating it to brms and the tidyverse. The great Frank Harrell has complied A Litany of Problems With p-values. Also, don’t miss the statement on \\(p\\)-values released by the American Statistical Association (Wasserstein & Lazar, 2016).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "07.html#the-problem-with-parameters",
    "href": "07.html#the-problem-with-parameters",
    "title": "7  Ulysses’ Compass",
    "section": "7.1 The problem with parameters",
    "text": "7.1 The problem with parameters\nThe \\(R^2\\) is a popular way to measure how well you can retrodict the data. It traditionally follows the form\n\\[R^2 = \\frac{\\text{var(outcome)} - \\text{var(residuals)}}{\\text{var(outcome)}} = 1 - \\frac{\\text{var(residuals)}}{\\text{var(outcome)}}.\\]\nBy \\(\\operatorname{var}()\\), of course, we meant variance (i.e., what you get from the var() function in R). McElreath is not a fan of the \\(R^2\\). But it’s important in my field, so instead of a summary at the end of the chapter, we will cover the Bayesian version of \\(R^2\\) and how to use it in brms.\n\n7.1.1 More parameters (almost) always improve fit\nWe’ll start off by making the data with brain size and body size for seven species.\n\nlibrary(tidyverse)\n\nd &lt;- tibble(species = c(\"afarensis\", \"africanus\", \"habilis\", \"boisei\", \"rudolfensis\", \"ergaster\", \"sapiens\"), \n            brain   = c(438, 452, 612, 521, 752, 871, 1350), \n            mass    = c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5))\n\nd\n\n# A tibble: 7 × 3\n  species     brain  mass\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 afarensis     438  37  \n2 africanus     452  35.5\n3 habilis       612  34.5\n4 boisei        521  41.5\n5 rudolfensis   752  55.5\n6 ergaster      871  61  \n7 sapiens      1350  53.5\n\n\nLet’s get ready for Figure 7.2. The plots in this chapter will be characterized by theme_classic() + theme(text = element_text(family = \"Courier\")). Our color palette will come from the rcartocolor package (Nowosad, 2019), which provides color schemes designed by ‘CARTO’.\n\nlibrary(rcartocolor)\n\nThe specific palette we’ll be using is “BurgYl.” In addition to palettes, the rcartocolor package offers a few convenience functions which make it easier to use their palettes. The carto_pal() function will return the HEX numbers associated with a given palette’s colors and the display_carto_pal() function will display the actual colors.\n\ncarto_pal(7, \"BurgYl\")\n\n[1] \"#FBE6C5\" \"#F5BA98\" \"#EE8A82\" \"#DC7176\" \"#C8586C\" \"#9C3F5D\" \"#70284A\"\n\ndisplay_carto_pal(7, \"BurgYl\")\n\n\n\n\n\n\n\n\nWe’ll be using a diluted version of the third color for the panel background (i.e., theme(panel.background = element_rect(fill = alpha(carto_pal(7, \"BurgYl\")[3], 1/4)))) and the darker purples for other plot elements. Here’s the plot.\n\nlibrary(ggrepel)\n\ntheme_set(\n  theme_classic() +\n    theme(text = element_text(family = \"Courier\"),\n          panel.background = element_rect(fill = alpha(carto_pal(7, \"BurgYl\")[3], 1/4)))\n)\n\nd |&gt;\n  ggplot(aes(x =  mass, y = brain, label = species)) +\n  geom_point(color = carto_pal(7, \"BurgYl\")[5]) +\n  geom_text_repel(size = 3, color = carto_pal(7, \"BurgYl\")[7], family = \"Courier\", seed = 438) +\n  labs(x = \"body mass (kg)\",\n       y = \"brain volume (cc)\",\n       subtitle = \"Average brain volume by body\\nmass for six hominin species\") +\n  xlim(30, 65)\n\n\n\n\n\n\n\n\nBefore fitting our models,\n\nwe want to standardize body mass–give it mean zero and standard deviation one–and rescale the outcome, brain volume, so that the largest observed value is 1. Why not standardize brain volume as well? Because we want to preserve zero as a reference point: No brain at all. You can’t have negative brain. I don’t think. (p. 195)\n\n\nd &lt;- d |&gt; \n  mutate(mass_std  = (mass - mean(mass)) / sd(mass),\n         brain_std = brain / max(brain))\n\nOur first statistical model will follow the form\n\\[\\begin{align*}\n\\text{brain\\_std}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i              & = \\alpha + \\beta \\text{mass\\_std}_i \\\\\n\\alpha             & \\sim \\operatorname{Normal}(0.5, 1) \\\\\n\\beta              & \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma             & \\sim \\operatorname{Log-Normal}(0, 1).\n\\end{align*}\\]\n\nThis simply says that the average brain volume \\(b_i\\) of species \\(i\\) is a linear function of its body mass \\(m_i\\). Now consider what the priors imply. The prior for \\(\\alpha\\) is just centered on the mean brain volume (rescaled) in the data. So it says that the average species with an average body mass has a brain volume with an 89% credible interval from about −1 to 2. That is ridiculously wide and includes impossible (negative) values. The prior for \\(\\beta\\) is very flat and centered on zero. It allows for absurdly large positive and negative relationships. These priors allow for absurd inferences, especially as the model gets more complex. And that’s part of the lesson. (p. 196)\n\nFire up brms.\n\nlibrary(brms)\n\nA careful study of McElreath’s R code 7.3 will show he is modeling log_sigma, rather than \\(\\sigma\\). There are ways to do this with brms (see Bürkner, 2022b), but I’m going to keep things simple, here. Our approach will be to follow the above equation more literally and just slap the \\(\\operatorname{Log-Normal}(0, 1)\\) prior directly onto \\(\\sigma\\).\n\nb7.1 &lt;- brm(\n  data = d, \n  family = gaussian,\n  brain_std ~ 1 + mass_std,\n  prior = c(prior(normal(0.5, 1), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(lognormal(0, 1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/b07.01\")\n\nCheck the model summary.\n\nprint(b7.1)\n\n Family: gaussian \n  Links: mu = identity \nFormula: brain_std ~ 1 + mass_std \n   Data: d (Number of observations: 7) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.53      0.12     0.28     0.75 1.00     2502     1887\nmass_std      0.16      0.13    -0.10     0.42 1.00     2190     1615\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.28      0.13     0.14     0.60 1.00     1327     1587\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs we’ll learn later on, brms already has a convenience function for computing the Bayesian \\(R^2\\). At this point in the chapter, we’ll follow along and make a brms-centric version of McElreath’s R2_is_bad(). But because part of our version of R2_is_bad() will contain the brms::predict() function, I’m going to add a seed argument to make the results more reproducible.1\n1 For more context for my ordering in the residuals portion of the code, see Section 5.1.5.1.2.\nR2_is_bad &lt;- function(brm_fit, seed = 7, ...) {\n  set.seed(seed)\n  p &lt;- predict(brm_fit, summary = F, ...)\n  # Nowadays it's more typical to define residuals as y minus yhat\n  r &lt;- d$brain_std - apply(p, 2, mean)\n  1 - rethinking::var2(r) / rethinking::var2(d$brain_std)\n}\n\nHere’s the estimate for our \\(R^2\\).\n\nR2_is_bad(b7.1)\n\n[1] 0.485846\n\n\nDo note that,\n\nin principle, the Bayesian approach mandates that we do this for each sample from the posterior. But \\(R^2\\) is traditionally computed only at the mean prediction. So we’ll do that as well here. Later in the chapter you’ll learn a properly Bayesian score that uses the entire posterior distribution. (p. 197)\n\nNow fit the quadratic through the fifth-order polynomial models using update().\n\n# Quadratic\nb7.2 &lt;- update(\n  b7.1,\n  newdata = d, \n  formula = brain_std ~ 1 + mass_std + I(mass_std^2),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/b07.02\")\n\n# Cubic\nb7.3 &lt;- update(\n  b7.1,\n  newdata = d, \n  formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  control = list(adapt_delta = 0.95),\n  file = \"fits/b07.03\")\n\n\n# Fourth-order\nb7.4 &lt;- update(\n  b7.1,\n  newdata = d, \n  formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  control = list(adapt_delta = 0.995, max_treedepth = 11),\n  file = \"fits/b07.04\")\n\n# Fifth-order\nb7.5 &lt;- update(\n  b7.1,\n  newdata = d, \n  formula = brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4) + I(mass_std^5),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  control = list(adapt_delta = 0.99999, max_treedepth = 11),\n  file = \"fits/b07.05\")\n\nYou may have noticed we fiddled with the adapt_delta setting for some of the models. When you try to fit complex models with few data points and wide priors, that can cause difficulties for Stan. I’m not going to get into the details on what adapt_delta does, right now. But it’ll make appearances in later chapters and we’ll more formally introduce it in Section 13.4.2.\nNow returning to the text,\n\nThat last model, m7.6, has one trick in it. The standard deviation is replaced with a constant value 0.001. The model will not work otherwise, for a very important reason that will become clear as we plot these monsters. (p. 198)\n\nBy “last model, m7.6,” McElreath was referring to the sixth-order polynomial, fit on page 199. McElreath’s rethinking package is set up so the syntax is simple to replacing \\(\\sigma\\) with a constant value. We can do this with brms, too, but it’ll take more effort. If we want to fix \\(\\sigma\\) to a constant, we’ll need to define a custom likelihood. Bürkner explained how to do so in his (2022c) vignette, Define custom response distributions with brms. I’m not going to explain this in great detail, here. In brief, first we use the custom_family() function to define the name and parameters of a custom_normal() likelihood that will set \\(\\sigma\\) to a constant value, 0.001. Second, we’ll define some functions for Stan which are not defined in Stan itself and save them as stan_funs. Third, we make a stanvar() statement which will allow us to pass our stan_funs to brm().\n\ncustom_normal &lt;- custom_family(\n  \"custom_normal\", dpars = \"mu\",\n  links = \"identity\",\n  type = \"real\"\n)\n\nstan_funs  &lt;- \"real custom_normal_lpdf(real y, real mu) {\n  return normal_lpdf(y | mu, 0.001);\n}\nreal custom_normal_rng(real mu) {\n  return normal_rng(mu, 0.001);\n}\n\" \n\nstanvars &lt;- stanvar(scode = stan_funs, block = \"functions\")\n\nNow we can fit the model by setting family = custom_normal. Note that since we’ve set \\(\\sigma = 0.001\\), we don’t need to include a prior for \\(\\sigma\\). Also notice our stanvars = stanvars line.\n\nb7.6 &lt;- brm(\n  data = d, \n  family = custom_normal,\n  brain_std ~ 1 + mass_std + I(mass_std^2) + I(mass_std^3) + I(mass_std^4) + I(mass_std^5) + I(mass_std^6),\n  prior = c(prior(normal(0.5, 1), class = Intercept),\n            prior(normal(0, 10), class = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  stanvars = stanvars,\n  file = \"fits/b07.06\")\n\nBefore we can do our variant of Figure 7.3, we’ll need to define a few more custom functions to work with b7.6. The posterior_epred_custom_normal() function is required for brms::fitted() to work with our family = custom_normal brmsfit object. Same thing for posterior_predict_custom_normal() and brms::predict(). Though we won’t need it until Section 7.2.5, we’ll also define log_lik_custom_normal so we can use the log_lik() function for any models fit with family = custom_normal. Before all that, we need to throw in a line with the expose_functions() function. If you want to understand why, read up in Bürkner’s (2022c) vignette. For now, just go with it.\n\nexpose_functions(b7.6, vectorize = TRUE)\n\nposterior_epred_custom_normal &lt;- function(prep) {\n  mu &lt;- prep$dpars$mu\n  mu \n}\n\nposterior_predict_custom_normal &lt;- function(i, prep, ...) {\n  mu &lt;- prep$dpars$mu\n  mu \n  custom_normal_rng(mu)\n}\n\nlog_lik_custom_normal &lt;- function(i, prep) {\n  mu &lt;- prep$dpars$mu\n  y &lt;- prep$data$Y[i]\n  custom_normal_lpdf(y, mu)\n}\n\nOkay, here’s how we might plot the result for the first model, b7.1.\n\nlibrary(tidybayes)\n\nnd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 100))\n\nfitted(b7.1, \n       newdata = nd, \n       probs = c(0.055, 0.945)) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = mass_std, y = Estimate)) +\n  geom_lineribbon(aes(ymin = Q5.5, ymax = Q94.5),\n                  color = carto_pal(7, \"BurgYl\")[7], linewidth = 1/2, \n                  fill = alpha(carto_pal(7, \"BurgYl\")[6], 1/3)) +\n  geom_point(data = d,\n             aes(y = brain_std),\n             color = carto_pal(7, \"BurgYl\")[7]) +\n  labs(x = \"body mass (standardized)\",\n       y = \"brain volume (standardized)\",\n       subtitle = bquote(italic(R)^2==.(round(R2_is_bad(b7.1), digits = 2))),) +\n  coord_cartesian(xlim = range(d$mass_std))\n\n\n\n\n\n\n\n\nTo slightly repurpose a quote from McElreath:\n\nWe’ll want to do this for the next several models, so let’s write a function to make it repeatable. If you find yourself writing code more than once, it is usually saner to write a function and call the function more than once instead. (p. 197)\n\nOur make_figure7.3() function will wrap the simulation, data wrangling, and plotting code all in one. It takes two arguments, the first of which defines which fit object we’d like to plot. If you look closely at Figure 7.3 in the text, you’ll notice that the range of the \\(y\\)-axis changes in the last three plots. Our second argument, ylim, will allow us to vary those parameters across subplots.\n\nmake_figure7.3 &lt;- function(brms_fit, ylim = range(d$brain_std)) {\n  \n  # Compute the R2\n  r2 &lt;- R2_is_bad(brms_fit)\n  \n  # Define the new data \n  nd &lt;- tibble(mass_std = seq(from = -2, to = 2, length.out = 200))\n  \n  # Simulate and wrangle\n  fitted(brms_fit, newdata = nd, probs = c(0.055, 0.945)) |&gt; \n    data.frame() |&gt; \n    bind_cols(nd) |&gt; \n    \n    # Plot!  \n    ggplot(aes(x = mass_std)) +\n    geom_lineribbon(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n                    color = carto_pal(7, \"BurgYl\")[7], linewidth = 1/2, \n                    fill = alpha(carto_pal(7, \"BurgYl\")[6], 1/3)) +\n    geom_point(data = d,\n               aes(y = brain_std),\n               color = carto_pal(7, \"BurgYl\")[7]) +\n    labs(x = \"body mass (std)\",\n         y = \"brain volume (std)\",\n         subtitle = bquote(italic(R)^2==.(round(r2, digits = 2)))) +\n    coord_cartesian(xlim = c(-1.2, 1.5),\n                    ylim = ylim)\n}\n\nHere we make and save the six subplots in bulk.\n\np1 &lt;- make_figure7.3(b7.1)\np2 &lt;- make_figure7.3(b7.2)\np3 &lt;- make_figure7.3(b7.3)\np4 &lt;- make_figure7.3(b7.4, ylim = c(0.25, 1.1))\np5 &lt;- make_figure7.3(b7.5, ylim = c(0.1, 1.4))\np6 &lt;- make_figure7.3(b7.6, ylim = c(-0.25, 1.5)) +\n  geom_hline(yintercept = 0, color = carto_pal(7, \"BurgYl\")[2], linetype = 2) \n\nNow use patchwork syntax to bundle them all together.\n\nlibrary(patchwork)\n\n((p1 | p2) / (p3 | p4) / (p5 | p6)) +\n  plot_annotation(title = \"Figure7.3. Polynomial linear models of increasing\\ndegree for the hominin data.\")\n\n\n\n\n\n\n\n\nIt’s not clear, to me, why our brms-based 89% intervals are so much broader than those in the text. But if you do fit the size models with rethinking::quap() and compare the summaries, you’ll see our brms-based parameters are systemically less certain than those fit with quap(). In you have an answer or perhaps even an alternative workflow to solve the issue, share on GitHub.\nIf you really what the axes scaled in the original metrics of the variables rather than their standardized form, you can use the re-scaling techniques from back in Section 4.5.1.0.1.\n“This is a general phenomenon: If you adopt a model family with enough parameters, you can fit the data exactly. But such a model will make rather absurd predictions for yet-to-be-observed cases” (pp. 199–201).\n\n7.1.1.1 Rethinking: Model fitting as compression\n\nAnother perspective on the absurd model just above is to consider that model fitting can be considered a form of data compression. Parameters summarize relationships among the data. These summaries compress the data into a simpler form, although with loss of information (“lossy” compression) about the sample. The parameters can then be used to generate new data, effectively decompressing the data. (p. 201, emphasis in the original)\n\n\n\n\n7.1.2 Too few parameters hurts, too\n\nThe overfit polynomial models fit the data extremely well, but they suffer for this within-sample accuracy by making nonsensical out-of-sample predictions. In contrast, underfitting produces models that are inaccurate both within and out of sample. They learn too little, failing to recover regular features of the sample. (p. 201, emphasis in the original)\n\nTo explore the distinctions between overfitting and underfitting, we’ll need to refit b7.1 and b7.4 several times after serially dropping one of the rows in the data. You can filter() by row_number() to drop rows in a tidyverse kind of way. For example, we can drop the second row of d like this.\n\nd |&gt;\n  mutate(row = 1:n()) |&gt; \n  filter(row_number() != 2)\n\n# A tibble: 6 × 6\n  species     brain  mass mass_std brain_std   row\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 afarensis     438  37     -0.779     0.324     1\n2 habilis       612  34.5   -1.01      0.453     3\n3 boisei        521  41.5   -0.367     0.386     4\n4 rudolfensis   752  55.5    0.917     0.557     5\n5 ergaster      871  61      1.42      0.645     6\n6 sapiens      1350  53.5    0.734     1         7\n\n\nIn his Overthinking: Dropping rows box (p. 202), McElreath encouraged us to take a look at the brain_loo_plot() function to get a sense of how he made his Figure 7.4. Here it is.\n\nlibrary(rethinking)\nbrain_loo_plot\n\nfunction (fit, atx = c(35, 47, 60), aty = c(450, 900, 1300), \n    xlim, ylim, npts = 100) \n{\n    post &lt;- extract.samples(fit)\n    n &lt;- dim(post$b)[2]\n    if (is.null(n)) \n        n &lt;- 1\n    if (missing(xlim)) \n        xlim &lt;- range(d$mass_std)\n    else xlim &lt;- (xlim - mean(d$mass))/sd(d$mass)\n    if (missing(ylim)) \n        ylim &lt;- range(d$brain_std)\n    else ylim &lt;- ylim/max(d$brain)\n    plot(d$brain_std ~ d$mass_std, xaxt = \"n\", yaxt = \"n\", xlab = \"body mass (kg)\", \n        ylab = \"brain volume (cc)\", col = rangi2, pch = 16, xlim = xlim, \n        ylim = ylim)\n    axis_unscale(1, atx, d$mass)\n    axis_unscale(2, at = aty, factor = max(d$brain))\n    d &lt;- as.data.frame(fit@data)\n    for (i in 1:nrow(d)) {\n        di &lt;- d[-i, ]\n        m_temp &lt;- quap(fit@formula, data = di, start = list(b = rep(0, \n            n)))\n        xseq &lt;- seq(from = xlim[1] - 0.2, to = xlim[2] + 0.2, \n            length.out = npts)\n        l &lt;- link(m_temp, data = list(mass_std = xseq), refresh = 0)\n        mu &lt;- apply(l, 2, mean)\n        lines(xseq, mu, lwd = 2, col = col.alpha(\"black\", 0.3))\n    }\n    model_name &lt;- deparse(match.call()[[2]])\n    mtext(model_name, adj = 0)\n}\n&lt;bytecode: 0x32edf3b68&gt;\n&lt;environment: namespace:rethinking&gt;\n\n\nThough we’ll be taking a slightly different route than the one outlined in McElreath’s brain_loo_plot() function, we can glean some great insights. For example, we’ll be refitting our brms models with update().\n\nb7.1.1 &lt;- update(\n  b7.1,\n  newdata = filter(d, row_number() != 1), \n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/b07.01.1\")\n\nprint(b7.1.1)\n\n Family: gaussian \n  Links: mu = identity \nFormula: brain_std ~ 1 + mass_std \n   Data: filter(d, row_number() != 1) (Number of observations: 6) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.54      0.14     0.26     0.83 1.00     2373     1698\nmass_std      0.15      0.16    -0.17     0.47 1.00     2295     1513\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.32      0.16     0.14     0.77 1.00     1115     1439\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nYou can see by the newdata statement that b7.1.1 is fit on the d data after dropping the first row. Here’s how we might amend our plotting strategy from before to visualize the posterior mean for the model-implied trajectory.\n\nfitted(b7.1.1, newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = mass_std)) +\n  geom_line(aes(y = Estimate),\n            alpha = 1/2, color = carto_pal(7, \"BurgYl\")[7], linewidth = 1/2) +\n  geom_point(data = d,\n             aes(y = brain_std),\n             color = carto_pal(7, \"BurgYl\")[7]) +\n  labs(x = \"body mass (std)\",\n       y = \"brain volume (std)\",\n       subtitle = \"b7.1.1\") +\n  coord_cartesian(xlim = range(d$mass_std),\n                  ylim = range(d$brain_std))\n\n\n\n\n\n\n\n\nNow we’ll make a brms-oriented version of McElreath’s brain_loo_plot() function. Our version brain_loo_lines() will refit the model and extract the lines information in one step. We’ll leave the plotting for a second step.\n\nbrain_loo_lines &lt;- function(brms_fit, row, ...) {\n  \n  # Refit the model\n  new_fit &lt;- update(\n    brms_fit,\n    newdata = filter(d, row_number() != row), \n    iter = 2000, warmup = 1000, chains = 4, cores = 4,\n    seed = 7,\n    refresh = 0,\n    ...)\n  \n  # Pull the lines values\n  fitted(new_fit, newdata = nd) |&gt; \n    data.frame() |&gt; \n    select(Estimate) |&gt; \n    bind_cols(nd)\n}\n\nHere’s how brain_loo_lines() works.\n\nbrain_loo_lines(b7.1, row = 1) |&gt; \n  glimpse()\n\n\n\nRows: 100\nColumns: 2\n$ Estimate &lt;dbl&gt; 0.2432784, 0.2493555, 0.2554325, 0.2615096, 0.2675867, 0.2736638, 0.2797409, 0.2858179, 0.2918950, 0.2979721, 0…\n$ mass_std &lt;dbl&gt; -2.0000000, -1.9595960, -1.9191919, -1.8787879, -1.8383838, -1.7979798, -1.7575758, -1.7171717, -1.6767677, -1.…\n\n\nWorking within the tidyverse paradigm, we’ll make a tibble with the predefined row values. We will then use purrr::map() to plug those row values into brain_loo_lines(), which will return the desired posterior mean values for each corresponding value of mass_std. Here we do that for both b7.1 and b7.4.\n\nb7.1_fits &lt;- tibble(row = 1:7) |&gt; \n  mutate(post = purrr::map(.x = row, \n                           .f = \\(x) brain_loo_lines(brms_fit = b7.1, row = x))) |&gt; \n  unnest(post)\n\nb7.4_fits &lt;- tibble(row = 1:7) |&gt; \n  mutate(post = purrr::map(.x = row, .f = function(x) {\n    brain_loo_lines(brms_fit = b7.4, \n                    row = x,\n                    control = list(adapt_delta = 0.999))\n  })) |&gt; \n  unnest(post)\n\nNow for each, pump those values into ggplot(), customize the settings, and combine the two ggplots to make the full Figure 7.4.\n\n# Left\np1 &lt;- b7.1_fits |&gt;\n  ggplot(aes(x = mass_std)) +\n  geom_line(aes(y = Estimate, group = row),\n            alpha = 1/2, color = carto_pal(7, \"BurgYl\")[7], linewidth = 1/2) +\n  geom_point(data = d,\n             aes(y = brain_std),\n             color = carto_pal(7, \"BurgYl\")[7]) +\n  labs(x = \"body mass (std)\",\n       y = \"brain volume (std)\",\n       subtitle = \"b7.1\") +\n  coord_cartesian(xlim = range(d$mass_std),\n                  ylim = range(d$brain_std))\n\n# Right\np2 &lt;- b7.4_fits |&gt;\n  ggplot(aes(x = mass_std, y = Estimate)) +\n  geom_line(aes(group = row),\n            alpha = 1/2, color = carto_pal(7, \"BurgYl\")[7], linewidth = 1/2) +\n  geom_point(data = d,\n             aes(y = brain_std),\n             color = carto_pal(7, \"BurgYl\")[7]) +\n  labs(x = \"body mass (std)\",\n       y = \"brain volume (std)\",\n       subtitle = \"b7.4\") +\n  coord_cartesian(xlim = range(d$mass_std),\n                  ylim = c(-0.1, 1.4))\n\n# Combine\np1 + p2\n\n\n\n\n\n\n\n\n“Notice that the straight lines hardly vary, while the curves fly about wildly. This is a general contrast between underfit and overfit models: sensitivity to the exact composition of the sample used to fit the model” (p. 201).\n\n7.1.2.1 Rethinking: Bias and variance\n\nThe underfitting/overfitting dichotomy is often described as the bias-variance trade-off. While not exactly the same distinction, the bias-variance trade-off addresses the same problem. “Bias” is related to underfitting, while “variance” is related to overfitting. These terms are confusing, because they are used in many different ways in different contexts, even within statistics. The term “bias” also sounds like a bad thing, even though increasing bias often leads to better predictions. (p. 201, emphasis in the original)\n\nTake a look at Yarkoni & Westfall (2017) for more on the bias-variance trade-off. As McElreath indicated in his endnote #104 (p. 563), Hastie, Tibshirani and Friedman (2009) broadly cover these ideas in their freely-downloadable text, The elements of statistical learning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "07.html#entropy-and-accuracy",
    "href": "07.html#entropy-and-accuracy",
    "title": "7  Ulysses’ Compass",
    "section": "7.2 Entropy and accuracy",
    "text": "7.2 Entropy and accuracy\n\nSo how do we navigate between the hydra of overfitting and the vortex of underfitting? Whether you end up using regularization or information criteria or both, the first thing you must do is pick a criterion of model performance. What do you want the model to do well at? We’ll call this criterion the target, and in this section you’ll see how information theory provides a common and useful target. (p. 202, emphasis in the original)\n\n\n7.2.1 Firing the weatherperson\nIf you let rain = 1 and sun = 0, here’s a way to make a plot of the first table of page 203, the weatherperson’s predictions.\n\nweatherperson &lt;- tibble(\n  day        = 1:10,\n  prediction = rep(c(1, 0.6), times = c(3, 7)),\n  observed   = rep(1:0, times = c(3, 7)))\n\nweatherperson |&gt; \n  pivot_longer(-day) |&gt;\n  \n  ggplot(aes(x = day, y = name, fill = value)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = value, color = value == 0)) +\n  scale_x_continuous(breaks = 1:10, expand = c(0, 0)) +\n  scale_y_discrete(NULL, expand = c(0, 0)) +\n  scale_fill_viridis_c(direction = -1) +\n  scale_color_manual(values = c(\"white\", \"black\")) +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nHere’s how the newcomer fared:\n\nnewcomer &lt;- tibble(\n  day        = 1:10,\n  prediction = 0,\n  observed   = rep(1:0, times = c(3, 7))) \n\nnewcomer |&gt; \n  pivot_longer(-day) |&gt;\n  \n  ggplot(aes(x = day, y = name, fill = value)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = value, color = value == 0)) +\n  scale_x_continuous(breaks = 1:10, expand = c(0, 0)) +\n  scale_y_discrete(NULL, expand = c(0, 0)) +\n  scale_fill_viridis_c(direction = -1) +\n  scale_color_manual(values = c(\"white\", \"black\")) +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nIf we do the math entailed in the tibbles, we’ll see why the newcomer could boast “I’m the best person for the job” (p. 203).\n\nweatherperson |&gt; \n  bind_rows(newcomer) |&gt; \n  mutate(person = rep(c(\"weatherperson\", \"newcomer\"), each = n()/2),\n         hit    = ifelse(prediction == observed, 1, 1 - prediction - observed)) |&gt; \n  group_by(person) |&gt; \n  summarise(hit_rate = mean(hit))\n\n# A tibble: 2 × 2\n  person        hit_rate\n  &lt;chr&gt;            &lt;dbl&gt;\n1 newcomer          0.7 \n2 weatherperson     0.58\n\n\n\n7.2.1.1 Costs and benefits\nOur new points variable doesn’t fit into the nice color-based geom_tile() plots from above, but we can still do the math.\n\nbind_rows(weatherperson, newcomer) |&gt; \n  mutate(person = rep(c(\"weatherperson\", \"newcomer\"), each = n()/2),\n         points = ifelse(observed == 1 & prediction != 1, -5,\n                         ifelse(observed == 1 & prediction == 1, -1,\n                                -1 * prediction))) |&gt; \n  group_by(person) |&gt; \n  summarise(happiness = sum(points))\n\n# A tibble: 2 × 2\n  person        happiness\n  &lt;chr&gt;             &lt;dbl&gt;\n1 newcomer          -15  \n2 weatherperson      -7.2\n\n\n\n\n7.2.1.2 Measuring accuracy\n\nConsider computing the probability of predicting the exact sequence of days. This means computing the probability of a correct prediction for each day. Then multiply all of these probabilities together to get the joint probability of correctly predicting the observed sequence. This is the same thing as the joint likelihood, which you’ve been using up to this point to fit models with Bayes’ theorem. This is the definition of accuracy that is maximized by the correct model.\nIn this light, the newcomer looks even worse. (p. 204)\n\n\nbind_rows(weatherperson, newcomer) |&gt; \n  mutate(person = rep(c(\"weatherperson\", \"newcomer\"), each = n() / 2),\n         hit    = ifelse(prediction == observed, 1, 1 - prediction - observed)) |&gt; \n  count(person, hit) |&gt; \n  mutate(power = hit ^ n,\n         term  = rep(letters[1:2], times = 2)) |&gt; \n  select(person, term, power) |&gt; \n  pivot_wider(names_from = term,\n              values_from = power) |&gt; \n  mutate(probability_correct_sequence = a * b)\n\n# A tibble: 2 × 4\n  person              a     b probability_correct_sequence\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;                        &lt;dbl&gt;\n1 newcomer      0           1                      0      \n2 weatherperson 0.00164     1                      0.00164\n\n\n\n\n\n7.2.2 Information and uncertainty\nWithin the context of information theory (Shannon, 1948; also Cover & Thomas, 2006), information is “the reduction in uncertainty when we learn an outcome” (p. 205). Information entropy is a way of measuring that uncertainty in a way that is (a) continuous, (b) increases as the number of possible events increases, and (c) is additive. The formula for information entropy is:\n\\[H(p) = - \\text E \\log (p_i) = - \\sum_{i = 1}^n p_i \\log (p_i).\\]\nMcElreath put it in words as: “The uncertainty contained in a probability distribution is the average log-probability of an event.” (p. 206). We’ll compute the information entropy for weather at the first unnamed location, which we’ll call McElreath's house, and Abu Dhabi at once.\n\ntibble(place  = c(\"McElreath's house\", \"Abu Dhabi\"),\n       p_rain = c(0.3, 0.01)) |&gt; \n  mutate(p_shine = 1 - p_rain) |&gt; \n  group_by(place) |&gt; \n  mutate(h_p = (p_rain * log(p_rain) + p_shine * log(p_shine)) |&gt; mean() * -1)\n\n# A tibble: 2 × 4\n# Groups:   place [2]\n  place             p_rain p_shine    h_p\n  &lt;chr&gt;              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 McElreath's house   0.3     0.7  0.611 \n2 Abu Dhabi           0.01    0.99 0.0560\n\n\nDid you catch how we used the equation \\(H(p) = - \\sum_{i = 1}^n p_i \\log (p_i)\\) in our mutate() code, there? Our computation indicated the uncertainty is less in Abu Dhabi because it rarely rains, there. If you have sun, rain and snow, the entropy for weather is:\n\np &lt;- c(0.7, 0.15, 0.15)\n-sum(p * log(p))\n\n[1] 0.8188085\n\n\n“These entropy values by themselves don’t mean much to us, though. Instead we can use them to build a measure of accuracy. That comes next” (p. 206).\n\n\n7.2.3 From entropy to accuracy\n\nHow can we use information entropy to say how far a model is from the target? The key lies in divergence:\n\nDivergence: The additional uncertainty induced by using probabilities from one distribution to describe another distribution.\n\nThis is often known as Kullback-Leibler divergence or simply KL divergence. (p. 207, emphasis in the original, see Kullback & Leibler, 1951)\n\nThe formula for the KL divergence is\n\\[D_\\text{KL} (p, q) = \\sum_i p_i \\big ( \\log (p_i) - \\log (q_i) \\big ) = \\sum_i p_i \\log \\left ( \\frac{p_i}{q_i} \\right ),\\]\nwhich is what McElreath described in plainer language as “the average difference in log probability between the target (\\(p\\)) and model (\\(q\\))” (p. 207).\nIn McElreath’s initial example\n\n\\(p_1 = 0.3\\),\n\\(p_2 = 0.7\\),\n\\(q_1 = 0.25\\), and\n\\(q_2 = 0.75\\).\n\nWith those values, we can compute \\(D_\\text{KL} (p, q)\\) within a tibble like so:\n\ntibble(p_1 = 0.3,\n       p_2 = 0.7,\n       q_1 = 0.25,\n       q_2 = 0.75) |&gt;\n  mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2)))\n\n# A tibble: 1 × 5\n    p_1   p_2   q_1   q_2    d_kl\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1   0.3   0.7  0.25  0.75 0.00640\n\n\nOur systems in this section are binary (e.g., \\(q = \\{ q_i, q_2 \\}\\)). Thus if you know \\(q_1 = .3\\) you know of a necessity \\(q_2 = 1 - q_1\\). Therefore we can code the tibble for the next example, for when \\(p = q\\), like this.\n\ntibble(p_1 = 0.3) |&gt; \n  mutate(p_2 = 1 - p_1,\n         q_1 = p_1) |&gt; \n  mutate(q_2 = 1 - q_1) |&gt;\n  mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2)))\n\n# A tibble: 1 × 5\n    p_1   p_2   q_1   q_2  d_kl\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.3   0.7   0.3   0.7     0\n\n\nBuilding off of that, you can make the data required for Figure 7.6 like this.\n\nt &lt;- tibble(p_1 = 0.3,\n            p_2 = 0.7,\n            q_1 = seq(from = 0.01, to = 0.99, by = 0.01)) |&gt; \n  mutate(q_2 = 1 - q_1) |&gt;\n  mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2)))\n\nhead(t)\n\n# A tibble: 6 × 5\n    p_1   p_2   q_1   q_2  d_kl\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.3   0.7  0.01  0.99 0.778\n2   0.3   0.7  0.02  0.98 0.577\n3   0.3   0.7  0.03  0.97 0.462\n4   0.3   0.7  0.04  0.96 0.383\n5   0.3   0.7  0.05  0.95 0.324\n6   0.3   0.7  0.06  0.94 0.276\n\n\nNow we have the data, plotting Figure 7.6 is a just geom_line() with stylistic flourishes.\n\nt |&gt; \n  ggplot(aes(x = q_1, y = d_kl)) +\n  geom_vline(xintercept = 0.3, color = carto_pal(7, \"BurgYl\")[5], linetype = 2) +\n  geom_line(color = carto_pal(7, \"BurgYl\")[7], linewidth = 1.5) +\n  annotate(geom = \"text\", x = 0.4, y = 1.5, label = \"q = p\",\n           color = carto_pal(7, \"BurgYl\")[5], family = \"Courier\", size = 3.5) +\n  labs(x = \"q[1]\",\n       y = \"Divergence of q from p\")\n\n\n\n\n\n\n\n\n\nWhat divergence can do for us now is help us contrast different approximations to \\(p\\). As an approximating function \\(q\\) becomes more accurate, \\(D_\\text{KL} (p, q)\\) will shrink. So if we have a pair of candidate distributions, then the candidate that minimizes the divergence will be closest to the target. Since predictive models specify probabilities of events (observations), we can use divergence to compare the accuracy of models. (p. 208)\n\n\n7.2.3.1 Rethinking: Divergence depends upon direction\nHere we see \\(H(p, q) \\neq H(q, p)\\). That is, direction matters.\n\ntibble(direction = c(\"Earth to Mars\", \"Mars to Earth\"),\n       p_1 = c(0.01, 0.7),\n       q_1 = c(0.7, 0.01)) |&gt; \n  mutate(p_2 = 1 - p_1,\n         q_2 = 1 - q_1) |&gt;\n  mutate(d_kl = (p_1 * log(p_1 / q_1)) + (p_2 * log(p_2 / q_2)))\n\n# A tibble: 2 × 6\n  direction       p_1   q_1   p_2   q_2  d_kl\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Earth to Mars  0.01  0.7   0.99  0.3   1.14\n2 Mars to Earth  0.7   0.01  0.3   0.99  2.62\n\n\nThe \\(D_\\text{KL}\\) was double when applying Martian estimates to Terran estimates.\n\nAn important practical consequence of this asymmetry, in a model fitting context, is that if we use a distribution with high entropy to approximate an unknown true distribution of events, we will reduce the distance to the truth and therefore the error. This fact will help us build generalized linear models, later on in Chapter 10. (p. 209)\n\n\n\n\n7.2.4 Estimating divergence\n\nThe point of all the preceding material about information theory and divergence is to establish both:\n\nHow to measure the distance of a model from our target. Information theory gives us the distance measure we need, the KL divergence.\nHow to estimate the divergence. Having identified the right measure of distance, we now need a way to estimate it in real statistical modeling tasks. (p. 209)\n\n\nNow we’ll start working on item #2.\nWithin the context of science, say we’ve labeled the true model for our topic of interest as \\(p\\). We don’t actually know what \\(p\\) is–we wouldn’t need the scientific method if we did. But say what we do have are two candidate models \\(q\\) and \\(r\\). We would at least like to know which is closer to \\(p\\). It turns out we don’t even need to know the absolute value of \\(p\\) to achieve this. Just the relative values of \\(q\\) and \\(r\\) will suffice. We express model \\(q\\)’s average log-probability as \\(\\text E \\log (q_i)\\). Extrapolating, the difference \\(\\text E \\log (q_i) - \\text E \\log (r_i)\\) gives us a sense about the divergence of both \\(q\\) and \\(r\\) from the target \\(p\\). That is, “we can compare the average log-probability from each model to get an estimate of the relative distance of each model from the target” (p. 210). Deviance and related statistics can help us towards this end. We define deviance as\n\\[D(q) = -2 \\sum_i \\log (q_i),\\]\nwhere \\(i\\) indexes each case and \\(q_i\\) is the likelihood for each case. Here’s the deviance from the OLS version of model m7.1.\n\nlm(data = d, brain_std ~ mass_std) |&gt; \n  logLik() * -2\n\n'log Lik.' -5.985049 (df=3)\n\n\nIn our \\(D(q)\\) formula, did you notice how we ended up multiplying \\(\\sum_i \\log (p_i)\\) by \\(-2\\)? Frequentists and Bayesians alike make use of information theory, KL divergence, and deviance. It turns out that the differences between two \\(D(q)\\) values follows a \\(\\chi^2\\) distribution (Wilks, 1938), which frequentists like to reference for the purpose of null-hypothesis significance testing. Many Bayesians, however, are not into all that significance-testing stuff and they aren’t as inclined to multiply \\(\\sum_i \\log (p_i)\\) by \\(-2\\) for the simple purpose of scaling the associated difference distribution to follow the \\(\\chi^2\\). If we leave that part out of the equation, we end up with\n\\[S(q) = \\sum_i \\log (q_i),\\]\nwhich we can think of as a log-probability score which is “the gold standard way to compare the predictive accuracy of different models. It is an estimate of \\(\\text E \\log (q_i)\\), just without the final step of dividing by the number of observations” (p. 210). When Bayesians compute \\(S(q)\\), they do so over the entire posterior distribution. “Doing this calculation correctly requires a little subtlety. The rethinking package has a function called lppd–log-pointwise-predictive-density–to do this calculation for quap models” (p. 210, emphasis in the original for the second time, but not the first). However, I’m now aware of a similar function within brms. If you’re willing to roll up your sleeves, a little, you can do it by hand. Here’s an example with b7.1.\n\nlog_lik(b7.1) |&gt; \n  data.frame() |&gt; \n  set_names(pull(d, species)) |&gt; \n  pivot_longer(everything(),\n               names_to = \"species\",\n               values_to = \"logprob\") |&gt; \n  mutate(prob = exp(logprob)) |&gt; \n  group_by(species) |&gt; \n  summarise(log_probability_score = mean(prob) |&gt; log())\n\n# A tibble: 7 × 2\n  species     log_probability_score\n  &lt;chr&gt;                       &lt;dbl&gt;\n1 afarensis                   0.355\n2 africanus                   0.379\n3 boisei                      0.371\n4 ergaster                    0.202\n5 habilis                     0.302\n6 rudolfensis                 0.247\n7 sapiens                    -0.591\n\n\n“If you sum these values, you’ll have the total log-probability score for the model and data” (p. 210). Here we sum those \\(\\log (q_i)\\) values up to compute \\(S(q)\\).\n\nlog_lik(b7.1) |&gt; \n  data.frame() |&gt; \n  set_names(pull(d, species)) |&gt; \n  pivot_longer(everything(),\n               names_to = \"species\",\n               values_to = \"logprob\") |&gt; \n  mutate(prob = exp(logprob)) |&gt; \n  group_by(species) |&gt; \n  summarise(log_probability_score = mean(prob) |&gt; log()) |&gt; \n  summarise(total_log_probability_score = sum(log_probability_score))\n\n# A tibble: 1 × 1\n  total_log_probability_score\n                        &lt;dbl&gt;\n1                        1.26\n\n\n\n7.2.4.1 Overthinking: Computing the lppd\nThe Bayesian version of the log-probability score, what we’ve been calling the lppd, has to account for the data and the posterior distribution. It follows the form\n\\[\\text{lppd}(y, \\Theta) = \\sum_i \\log \\frac{1}{S} \\sum_s p (y_i \\mid \\Theta_s),\\]\n\nwhere \\(S\\) is the number of samples and \\(\\Theta_s\\) is the \\(s\\)-th set of sampled parameter values in the posterior distribution. While in principle this is easy–you just need to compute the probability (density) of each observation \\(i\\) for each sample \\(s\\), take the average, and then the logarithm–in practice it is not so easy. The reason is that doing arithmetic in a computer often requires some tricks to retain precision. (p. 210)\n\nOur approach to McElreath’s R code 7.14 code will look very different. Going step by step, first we use the log_lik() function.\n\nlog_prob &lt;- log_lik(b7.1) \n\nlog_prob |&gt;\n  glimpse()\n\n num [1:4000, 1:7] 0.0602 -0.1208 -0.284 -0.667 -1.0836 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\nThe log_lik() function returned a matrix. Each occasion in the original data, \\(y_i\\), got a column and each HMC chain iteration gets a row. Given we used the brms default of 4,000 HMC iterations, which corresponds to \\(S = 4{,}000\\) in the formula. Note the each of these \\(7 \\times 4{,}000\\) values is a log-probability, not a probability itself. Thus, if we want to start summing these \\(s\\) iterations within cases, we’ll need to exponentiate them into probabilities.\n\nprob &lt;- log_prob |&gt; \n  # Make it a data frame\n  data.frame() |&gt; \n  # Add case names, for convenience\n  set_names(pull(d, species)) |&gt; \n  # Add an `s` iteration index, for convenience\n  mutate(s = row_number()) |&gt; \n  # Make it long\n  pivot_longer(-s,\n               names_to = \"species\",\n               values_to = \"logprob\") |&gt; \n  # Compute the probability scores\n  mutate(prob = exp(logprob))\n\nprob\n\n# A tibble: 28,000 × 4\n       s species     logprob  prob\n   &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n 1     1 afarensis    0.0602 1.06 \n 2     1 africanus   -0.0458 0.955\n 3     1 habilis     -0.491  0.612\n 4     1 boisei       0.152  1.16 \n 5     1 rudolfensis  0.197  1.22 \n 6     1 ergaster     0.130  1.14 \n 7     1 sapiens     -0.629  0.533\n 8     2 afarensis   -0.121  0.886\n 9     2 africanus   -0.208  0.812\n10     2 habilis     -0.575  0.563\n# ℹ 27,990 more rows\n\n\nNow for each case, we take the average of each of the probability sores, and then take the log of that.\n\nprob &lt;- prob |&gt; \n  group_by(species) |&gt; \n  summarise(log_probability_score = mean(prob) |&gt; log())\n\nprob\n\n# A tibble: 7 × 2\n  species     log_probability_score\n  &lt;chr&gt;                       &lt;dbl&gt;\n1 afarensis                   0.355\n2 africanus                   0.379\n3 boisei                      0.371\n4 ergaster                    0.202\n5 habilis                     0.302\n6 rudolfensis                 0.247\n7 sapiens                    -0.591\n\n\nFor our last step, we sum those values up.\n\nprob |&gt; \n  summarise(total_log_probability_score = sum(log_probability_score))\n\n# A tibble: 1 × 1\n  total_log_probability_score\n                        &lt;dbl&gt;\n1                        1.26\n\n\nThat, my friends, is the log-pointwise-predictive-density, \\(\\text{lppd}(y, \\Theta)\\).\n\n\n\n7.2.5 Scoring the right data\nSince we don’t have a lppd() function for brms, we’ll have to turn our workflow from the last two sections into a custom function. We’ll call it my_lppd().\n\nmy_lppd &lt;- function(brms_fit) {\n  log_lik(brms_fit) |&gt; \n    data.frame() |&gt; \n    pivot_longer(everything(),\n                 values_to = \"logprob\") |&gt; \n    mutate(prob = exp(logprob)) |&gt; \n    group_by(name) |&gt; \n    summarise(log_probability_score = mean(prob) |&gt; log()) |&gt; \n    summarise(total_log_probability_score = sum(log_probability_score))\n}\n\nHere’s a tidyverse-style approach for computing the lppd for each of our six brms models.\n\ntibble(name = str_c(\"b7.\", 1:6)) |&gt; \n  mutate(brms_fit = purrr::map(.x = name, .f = get)) |&gt; \n  mutate(lppd = purrr::map(.x = brms_fit, .f = my_lppd)) |&gt; \n  unnest(lppd)\n\n# A tibble: 6 × 3\n  name  brms_fit  total_log_probability_score\n  &lt;chr&gt; &lt;list&gt;                          &lt;dbl&gt;\n1 b7.1  &lt;brmsfit&gt;                      1.26  \n2 b7.2  &lt;brmsfit&gt;                      0.632 \n3 b7.3  &lt;brmsfit&gt;                      0.362 \n4 b7.4  &lt;brmsfit&gt;                      0.0140\n5 b7.5  &lt;brmsfit&gt;                      2.25  \n6 b7.6  &lt;brmsfit&gt;                     25.9   \n\n\n\nWhen we usually have data and use it to fit a statistical model, the data comprise a training sample. Parameters are estimated from it, and then we can imagine using those estimates to predict outcomes in a new sample, called the test sample. R is going to do all of this for you. But here’s the full procedure, in outline:\n\nSuppose there’s a training sample of size \\(N\\).\nCompute the posterior distribution of a model for the training sample, and compute the score on the training sample. Call this score \\(D_\\text{train}\\).\nSuppose another sample of size \\(N\\) from the same process. This is the test sample.\nCompute the score on the test sample, using the posterior trained on the training sample. Call this new score \\(D_\\text{test}\\).\n\nThe above is a thought experiment. It allows us to explore the distinction between accuracy measured in and out of sample, using a simple prediction scenario. (p. 211, emphasis in the original)\n\nWe’ll see how to carry out such a thought experiment in the next section.\n\n7.2.5.1 Overthinking: Simulated training and testing\nMcElreath plotted the results of such a thought experiment in implemented in R with the aid of his sim_train_test() function. If you’re interested in how the function pulls this off, execute the code below.\n\nsim_train_test\n\nFor the sake of brevity, I am going to show the results of a simulation based on 1,000 simulations rather than McElreath’s 10,000.\n\nt0 &lt;- Sys.time()\n\n# I've reduced this number by one order of magnitude to reduce computation time\nn_sim   &lt;- 1e3\nn_cores &lt;- 8\nkseq    &lt;- 1:5\n\n# Define the simulation function\nmy_sim &lt;- function(k) {\n  print(k);\n  r &lt;- mcreplicate(n_sim, sim_train_test(N = n, k = k), mc.cores = n_cores);\n  c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ]))\n}\n\n# Here's our dev object based on `N &lt;- 20`\nn      &lt;- 20\ndev_20 &lt;- sapply(kseq, my_sim)\n\n# Here's our dev object based on `N &lt;- 100`\nn       &lt;- 100\ndev_100 &lt;- sapply(kseq, my_sim)\n\nt1 &lt;- Sys.time()\nt1 - t0\n\nIf you didn’t quite catch it, the simulation yields dev_20 and dev_100. We’ll want to convert them to tibbles, bind them together, and wrangle extensively before we’re ready to plot.\n\ndev_tibble &lt;- rbind(dev_20, dev_100) |&gt; \n  data.frame() |&gt; \n  mutate(statistic = rep(c(\"mean\", \"sd\"), each = 2) |&gt; rep(times = 2),\n         sample    = rep(c(\"in\", \"out\"), times = 2) |&gt; rep(times = 2),\n         n         = rep(c(\"n = 20\", \"n = 100\"), each = 4)) |&gt; \n  pivot_longer(-(statistic:n)) |&gt; \n  pivot_wider(names_from = statistic, values_from = value) |&gt;\n  mutate(n     = factor(n, levels = c(\"n = 20\", \"n = 100\")),\n         n_par = str_extract(name, \"\\\\d+\") |&gt; as.double()) |&gt; \n  mutate(n_par = ifelse(sample == \"in\", n_par - 0.075, n_par + 0.075))\n\nhead(dev_tibble)\n\n# A tibble: 6 × 6\n  sample n      name   mean    sd n_par\n  &lt;chr&gt;  &lt;fct&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 in     n = 20 X1     55.8  5.85 0.925\n2 in     n = 20 X2     54.6  5.40 1.92 \n3 in     n = 20 X3     51.8  4.29 2.92 \n4 in     n = 20 X4     51.2  4.04 3.92 \n5 in     n = 20 X5     51.2  3.63 4.92 \n6 out    n = 20 X1     57.8  6.25 1.08 \n\n\nNow we’re ready to make Figure 7.6.\n\n# For the annotation\ntext &lt;- dev_tibble |&gt; \n  filter(n_par &gt; 1.5, \n         n_par &lt; 2.5) |&gt; \n  mutate(n_par = ifelse(sample == \"in\", n_par - 0.2, n_par + 0.29))\n  \n# Plot!\ndev_tibble |&gt; \n  ggplot(aes(x = n_par, y = mean,\n             ymin = mean - sd, ymax = mean + sd,\n             group = sample, color = sample, fill  = sample)) +\n  geom_pointrange(shape = 21) +\n  geom_text(data = text,\n            aes(label = sample)) +\n  scale_fill_manual(values  = carto_pal(7, \"BurgYl\")[c(5, 7)]) +\n  scale_color_manual(values = carto_pal(7, \"BurgYl\")[c(7, 5)]) +\n  labs(x = \"number of parameters\",\n       y = \"deviance\",\n       title = \"Figure 7.6. Deviance in and out of sample.\") +\n  facet_wrap(~ n, scale = \"free_y\") +\n  theme(legend.position = \"none\",\n        strip.background = element_rect(fill = alpha(carto_pal(7, \"BurgYl\")[1], 1/4), color = \"transparent\"))\n\n\n\n\n\n\n\n\nEven with a substantially smaller \\(N\\), our simulation results matched up well with those in the text.\n\nDeviance is an assessment of predictive accuracy, not of truth. The true model, in terms of which predictors are included, is not guaranteed to produce the best predictions. Likewise a false model, in terms of which predictors are included, is not guaranteed to produce poor predictions.\nThe point of this thought experiment is to demonstrate how deviance behaves, in theory. While deviance on training data always improves with additional predictor variables, deviance on future data may or may not, depending upon both the true data-generating process and how much data is available to precisely estimate the parameters. These facts form the basis for understanding both regularizing priors and information criteria. (p. 213)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "07.html#golem-taming-regularization",
    "href": "07.html#golem-taming-regularization",
    "title": "7  Ulysses’ Compass",
    "section": "7.3 Golem taming: regularization",
    "text": "7.3 Golem taming: regularization\n\nThe root of overfitting is a model’s tendency to get overexcited by the training sample. When the priors are flat or nearly flat, the machine interprets this to mean that every parameter value is equally plausible. As a result, the model returns a posterior that encodes as much of the training sample–as represented by the likelihood function–as possible.\nOne way to prevent a model from getting too excited by the training sample is to use a skeptical prior. By “skeptical,” I mean a prior that slows the rate of learning from the sample. The most common skeptical prior is a regularizing prior. Such a prior, when tuned properly, reduces overfitting while still allowing the model to learn the regular features of a sample. (p. 214, emphasis in the original)\n\nIn case you were curious, here’s how you might make a version Figure 7.7 with ggplot2.\n\ntibble(x = seq(from = - 3.5, to = 3.5, by = 0.01)) |&gt;\n  mutate(a = dnorm(x, mean = 0, sd = 0.2),\n         b = dnorm(x, mean = 0, sd = 0.5),\n         c = dnorm(x, mean = 0, sd = 1.0)) |&gt; \n  pivot_longer(-x) |&gt; \n  \n  ggplot(aes(x = x, y = value,\n             fill = name, color = name, linetype = name)) +\n  geom_area(alpha = 1/2, linewidth = 1/2, position = \"identity\") +\n  scale_x_continuous(\"parameter value\", breaks = -3:3) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  scale_fill_manual(values = carto_pal(7, \"BurgYl\")[7:5]) +\n  scale_color_manual(values = carto_pal(7, \"BurgYl\")[7:5]) +\n  scale_linetype_manual(values = 1:3) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIn our version of the plot, darker purple = more regularizing.\nTo prepare for Figure 7.8, we need to simulate. This time we’ll wrap the basic simulation code we used before into a function we’ll call make_sim(). Our make_sim() function has two parameters, n and b_sigma, both of which come from McElreath’s simulation code. So you’ll note that instead of hard coding the values for n and b_sigma within the simulation, we’re leaving them adjustable (i.e., sim_train_test(N = n, k = k, b_sigma = b_sigma)). Also notice that instead of saving the simulation results as objects, like before, we’re just converting them to a data frame with the data.frame() function at the bottom. Our goal is to use make_sim() within a purrr::map2() statement. The result will be a nested data frame into which we’ve saved the results of 6 simulations based off of two sample sizes (i.e., n = c(20, 100)) and three values of \\(\\sigma\\) for our Gaussian \\(\\beta\\) prior (i.e., b_sigma = c(1, 0.5, 0.2)).\n\nlibrary(rethinking)\n\n# I've reduced this number by one order of magnitude to reduce computation time\nn_sim   &lt;- 1e3\nn_cores &lt;- 8\n\nmake_sim &lt;- function(n, b_sigma) {\n  sapply(kseq, function(k) {\n    print(k);\n    r &lt;- mcreplicate(n_sim, sim_train_test(N = n, k = k, b_sigma = b_sigma),  # This is an augmented line of code\n                     mc.cores = n_cores);\n    c(mean(r[1, ]), mean(r[2, ]), sd(r[1, ]), sd(r[2, ])) \n    }) |&gt; \n    # This is a new line of code\n    data.frame()\n}\n\ns &lt;- crossing(n       = c(20, 100),\n              b_sigma = c(1, 0.5, 0.2)) |&gt; \n  mutate(sim = map2(n, b_sigma, make_sim)) |&gt; \n  unnest(sim)\n\nWe’ll follow the same principles for wrangling these data as we did those from the previous simulation, dev_tibble. After wrangling, we’ll feed the data directly into the code for our version of Figure 7.8.\n\n# Wrangle the simulation data\ns |&gt; \n  mutate(statistic = rep(c(\"mean\", \"sd\"), each = 2) |&gt; rep(times = 3 * 2),\n         sample    = rep(c(\"in\", \"out\"), times = 2) |&gt; rep(times = 3 * 2)) |&gt; \n  pivot_longer(-c(n:b_sigma, statistic:sample)) |&gt; \n  pivot_wider(names_from = statistic, values_from = value) |&gt;\n  mutate(n     = str_c(\"n = \", n) |&gt; factor(levels = c(\"n = 20\", \"n = 100\")),\n         n_par = str_extract(name, \"\\\\d+\") |&gt; as.double()) |&gt; \n  \n  # Plot\n  ggplot(aes(x = n_par, y = mean,\n             group = interaction(sample, b_sigma))) +\n  geom_line(aes(color = sample, linewidth = b_sigma |&gt; as.character())) +\n  # This function contains the data from the previous simulation\n  geom_point(data = dev_tibble, \n             aes(group = sample, fill = sample),\n             color = \"black\", shape = 21, size = 2.5, stroke = 0.1) +\n  scale_fill_manual(values = carto_pal(7, \"BurgYl\")[c(7, 5)]) +\n  scale_color_manual(values = carto_pal(7, \"BurgYl\")[c(7, 5)]) +\n  scale_linewidth_manual(values = c(1, 0.5, 0.2)) +\n  labs(x = \"number of parameters\",\n       y = \"deviance\") +\n  facet_wrap(~ n, scale = \"free_y\") +\n  theme(legend.position = \"none\",\n        strip.background = element_rect(fill = alpha(carto_pal(7, \"BurgYl\")[1], 1/4), \n                                        color = \"transparent\"))\n\n\n\n\n\n\n\n\nOur results don’t perfectly align with those in the text. I suspect his is because we used 1e3 iterations, rather than the 1e4 of the text. If you’d like to wait all night long for the simulation to yield more stable results, be my guest.\n\nRegularizing priors are great, because they reduce overfitting. But if they are too skeptical, they prevent the model from learning from the data. When you encounter multilevel models in Chapter 13, you’ll see that their central device is to learn the strength of the prior from the data itself. So you can think of multilevel models as adaptive regularization, where the model itself tries to learn how skeptical it should be. (p. 216)\n\nI found this connection difficult to grasp for a long time. Practice now and hopefully it’ll sink in for you faster than it did me.\n\n7.3.0.1 Rethinking: Ridge regression\nWithin the brms framework, you can do something like this with the horseshoe prior via the horseshoe() function. You can learn all about it from the horseshoe section of the brms reference manual (Bürkner, 2022a). Here’s an extract from the section:\n\nThe horseshoe prior is a special shrinkage prior initially proposed by Carvalho et al. (2009). It is symmetric around zero with fat tails and an infinitely large spike at zero. This makes it ideal for sparse models that have many regression coefficients, although only a minority of them is nonzero. The horseshoe prior can be applied on all population-level effects at once (excluding the intercept) by using set_prior(\"horseshoe(1)\"). ()p. 105\n\nTo dive even deeper into the horseshoe prior, check out Michael Betancourt’s (2018) tutorial, Bayes sparse regression. Gelman, Hill, and Vehtari cover the horseshoe prior with rstanarm in Section 12.7 of their (2020) text, Regression and other stories. I also have an example of the horseshoe prior (fit18.5) in Section 18.3 of my (2026a) ebook translation of Kruschke’s (2015) text.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "07.html#predicting-predictive-accuracy",
    "href": "07.html#predicting-predictive-accuracy",
    "title": "7  Ulysses’ Compass",
    "section": "7.4 Predicting predictive accuracy",
    "text": "7.4 Predicting predictive accuracy\n\nAll of the preceding suggests one way to navigate overfitting and underfitting: Evaluate our models out-of-sample. But we do not have the out-of-sample, by definition, so how can we evaluate our models on it? There are two families of strategies: cross-validation and information criteria. These strategies try to guess how well models will perform, on average, in predicting new data. (p. 217, emphasis in the original)\n\n\n7.4.1 Cross-validation\n\nA popular strategy for estimating predictive accuracy is to actually test the model’s predictive accuracy on another sample. This is known as cross-validation, leaving out a small chunk of observations from our sample and evaluating the model on the observations that were left out. Of course we don’t want to leave out data. So what is usually done is to divide the sample in a number of chunks, called “folds.” The model is asked to predict each fold, after training on all the others. We then average over the score for each fold to get an estimate of out-of-sample accuracy. The minimum number of folds is 2. At the other extreme, you could make each point observation a fold and fit as many models as you have individual observations. (p. 217, emphasis in the original)\n\nFolds are typically equivalent in size and we often denote the total number of folds by \\(k\\), which means that the number of cases will get smaller as \\(k\\) increases. In the extreme \\(k = N\\). Leave-one-out cross-validation (LOO-CV) is the name for this popular type of cross-validation which uses the largest number of folds possible by including a single case in each fold (de Rooij & Weeda, 2020; see Zhang & Yang, 2015). This will be our approach.\nA practical difficulty with LOO-CV is it’s costly in terms of the time and memory required to refit the model \\(k = N\\) times. Happily, we have an approximation to pure LOO-CV. Vehtari, Gelman, and Gabry (2017) proposed Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO-CV) as an efficient way to approximate true LOO-CV.\n\n\n7.4.2 Information criteria\n\nThe second approach is the use of information criteria to compute an expected score out of sample. Information criteria construct a theoretical estimate of the relative out-of-sample KL divergence. (p. 219, emphasis in the original)\n\nThe frequentist Akaike information criterion (AIC, Akaike, 1998) is the oldest and most restrictive. Among Bayesians, the deviance information criterion (DIC, Spiegelhalter et al., 2002) has been widely used for some time, now. For a great talk on the DIC, check out the authoritative David Spiegelhalter’s Retrospective read paper: Bayesian measure of model complexity and fit. However, the DIC is limited in that it presumes the posterior is multivariate Gaussian, which is not always the case.\nIn this book, our focus will be on the widely applicable information criterion (WAIC, Watanabe, 2010), which does not impose assumptions on the shape of the posterior distribution. The WAIC both provides an estimate of out-of-sample deviance and converges with LOO-CV as \\(N \\rightarrow \\infty\\). The WAIC follows the formula\n\\[\\text{WAIC}(y, \\Theta) = -2 \\big (\\text{lppd} - \\underbrace{\\sum_i \\operatorname{var}_\\theta \\log p(y_i \\mid \\theta)}_\\text{penalty term} \\big),\\]\nwhere \\(y\\) is the data, \\(\\Theta\\) is the posterior distribution, and \\(\\text{lppd}\\) is the log-posterior-predictive-density from before. The penalty term is also referred to at the effective number of parameters, \\(p_\\text{WAIC}\\). There are a few ways to compute the WAIC with brms, including with the waic() function.\n\n7.4.2.1 Overthinking: WAIC calculations\nHere is how to fit the pre-WAIC model with brms.\n\ndata(cars)\n\nb7.m &lt;- brm(\n  data = cars, \n  family = gaussian,\n  dist ~ 1 + speed,\n  prior = c(prior(normal(0, 100), class = Intercept),\n            prior(normal(0, 10), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 7,\n  file = \"fits/b07.0m\")\n\nBehold the posterior summary.\n\nprint(b7.m)\n\n Family: gaussian \n  Links: mu = identity \nFormula: dist ~ 1 + speed \n   Data: cars (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   -17.52      6.09   -29.43    -5.15 1.00     3647     2790\nspeed         3.92      0.37     3.17     4.65 1.00     3613     2878\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    13.81      1.22    11.67    16.44 1.00     4056     2960\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow use the log_lik() function to return the log-likelihood for each observation \\(i\\) at each posterior draw \\(s\\), where \\(S = 4{,}000\\).\n\nn_cases &lt;- nrow(cars)\n\nll &lt;- log_lik(b7.m) |&gt;\n  data.frame() |&gt;\n  set_names(c(str_c(0, 1:9), 10:n_cases))\n\ndim(ll)\n\nWe have a \\(4{,}000 \\times 50\\) (i.e., \\(S \\times N\\)) data frame with posterior draws in rows and cases in columns. Computing the \\(\\text{lppd}\\), the “Bayesian deviance”, takes a bit of leg work. Recall the formula for \\(\\text{lppd}\\),\n\\[\\text{lppd}(y, \\Theta) = \\sum_i \\log \\frac{1}{S} \\sum_s p (y_i \\mid \\Theta_s),\\]\nwhere \\(p (y_i \\mid \\Theta_s)\\) is the likelihood of case \\(i\\) on posterior draw \\(s\\). Since log_lik() returns the pointwise log-likelihood, our first step is to exponentiate those values. For each case \\(i\\) (i.e., \\(\\sum_i\\)), we then take the average likelihood value [i.e., \\(\\frac{1}{S} \\sum_s p (y_i \\mid \\Theta_s)\\)] and transform the result by taking its log [i.e., \\(\\log \\left (\\frac{1}{S} \\sum_s p (y_i \\mid \\Theta_s) \\right )\\)]. Here we’ll save the pointwise solution as log_mu_l.\n\nlog_mu_l &lt;- ll |&gt; \n  pivot_longer(everything(),\n               names_to = \"i\",\n               values_to = \"loglikelihood\") |&gt; \n  mutate(likelihood = exp(loglikelihood)) |&gt; \n  group_by(i) |&gt; \n  summarise(log_mean_likelihood = mean(likelihood) |&gt; log())\n\n(\n  lppd &lt;- log_mu_l |&gt; \n    summarise(lppd = sum(log_mean_likelihood)) |&gt; \n    pull(lppd) \n)\n\n[1] -206.6458\n\n\nIt’s a little easier to compute the effective number of parameters, \\(p_\\text{WAIC}\\). First, let’s use a shorthand notation and define \\(V(y_i)\\) as the variance in log-likelihood for the \\(i^\\text{th}\\) case across all \\(S\\) samples. We define \\(p_\\text{WAIC}\\) as their sum\n\\[p_\\text{WAIC} = \\sum_{i=1}^N V (y_i).\\]\nWe’ll save the pointwise results [i.e., \\(V (y_i)\\)] as v_i and their sum [i.e., \\(\\sum_{i=1}^N V (y_i)\\)] as pwaic.\n\nv_i &lt;- ll |&gt; \n  pivot_longer(everything(),\n               names_to = \"i\",\n               values_to = \"loglikelihood\") |&gt; \n  group_by(i) |&gt; \n  summarise(var_loglikelihood = var(loglikelihood))\n\npwaic &lt;- v_i |&gt;\n  summarise(pwaic = sum(var_loglikelihood)) |&gt; \n  pull()\n\npwaic\n\n[1] 4.042903\n\n\nNow we can finally plug our hand-made lppd and pwaic values into the formula \\(-2 (\\text{lppd} - p_\\text{WAIC})\\) to compute the WAIC. Compare it to the value returned by the brms waic() function.\n\n-2 * (lppd - pwaic)\n\n[1] 421.3773\n\nwaic(b7.m)\n\n\nComputed from 4000 by 50 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -210.7  8.2\np_waic         4.0  1.5\nwaic         421.4 16.4\n\n2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\nBefore we move on, did you notice the elpd_waic row in the tibble returned by thewaic() function? That value is the lppd minus the pwaic, but without multiplying the result by -2. E.g.,\n\n(lppd - pwaic)\n\n[1] -210.6887\n\n\nFinally, here’s how we compute the WAIC standard error.\n\ntibble(lppd   = pull(log_mu_l, log_mean_likelihood),\n       p_waic = pull(v_i, var_loglikelihood)) |&gt; \n  mutate(waic_vec = -2 * (lppd - p_waic)) |&gt; \n  summarise(waic_se = sqrt(n_cases * var(waic_vec)))\n\n# A tibble: 1 × 1\n  waic_se\n    &lt;dbl&gt;\n1    16.4\n\n\nIf you’d like the pointwise values from brms::waic(), just index.\n\nwaic(b7.m)$pointwise |&gt; \n  head()\n\n     elpd_waic     p_waic     waic\n[1,] -3.649019 0.02268800 7.298038\n[2,] -4.024923 0.09432405 8.049846\n[3,] -3.682755 0.02162366 7.365510\n[4,] -3.997352 0.05956809 7.994703\n[5,] -3.587876 0.01069652 7.175752\n[6,] -3.740043 0.02135511 7.480087\n\n\n\n\n\n7.4.3 Comparing CV, PSIS, and WAIC\nHere we update our make_sim() to accommodate the simulation for Figure 7.9.\n\nmake_sim &lt;- function(n, k, b_sigma) {\n  r &lt;- mcreplicate(n = n_sim, \n                   expr = sim_train_test(\n                     N       = n,\n                     k       = k,\n                     b_sigma = b_sigma,\n                     WAIC    = T,\n                     LOOCV   = T, \n                     LOOIC   = T),\n                   mc.cores = n_cores)\n  \n  tibble(\n    deviance_os = mean(unlist(r[2, ])),\n    deviance_w  = mean(unlist(r[3, ])),\n    deviance_p  = mean(unlist(r[11, ])),\n    deviance_c  = mean(unlist(r[19, ])),\n    error_w     = mean(unlist(r[7, ])),\n    error_p     = mean(unlist(r[15, ])),\n    error_c     = mean(unlist(r[20, ]))\n  )\n}\n\nComputing all three WAIC, PSIS-LOO-CV, and actual LOO-CV for many models across multiple combinations of k and b_sigma takes a long time–many hours. If you plan on running this code to replicate the figure on your own, take it for a spin first with n_sim set to something small like 10 to get a sense of the speed of your machine and the nature of the output. Alternatively, consider running the simulation for one combination of k and b_sigma at a time.\n\nn_sim &lt;- 1e3\nn_cores &lt;- 8\n\ns &lt;- crossing(n       = c(20, 100),\n              k       = 1:5,\n              b_sigma = c(0.5, 100)) |&gt;\n  mutate(sim = pmap(list(n, k, b_sigma), make_sim)) |&gt; \n  unnest(sim)\n\nNow we have our results saved as s, we’re ready to make our version of Figure 7.9. I’m going to deviate from McElreath’s version a bit and express the two average deviance plots on the left column into two columns. To my eyes, the reduced clutter makes it easier to track what I’m looking at.\n\ns |&gt; \n  pivot_longer(deviance_w:deviance_c) |&gt; \n  mutate(criteria = ifelse(name == \"deviance_w\", \"WAIC\",\n                           ifelse(name == \"deviance_p\", \"PSIS\", \"CV\"))) |&gt; \n  mutate(n       = factor(str_c(\"N = \", n), \n                          levels = str_c(\"N = \", c(20, 100))),\n         b_sigma = factor(str_c(\"sigma = \", b_sigma),\n                          levels = (str_c(\"sigma = \", c(0.5, 100))))) |&gt; \n  \n  ggplot(aes(x = k)) +\n  geom_point(aes(y = deviance_os, shape = b_sigma),\n             show.legend = F) +\n  geom_line(aes(y = value, color = criteria)) +\n  scale_shape_manual(values = c(19, 1)) +\n  scale_color_manual(values = carto_pal(7, \"BurgYl\")[c(3, 5, 7)]) +\n  labs(x = \"number of parameters (k)\",\n       y = \"average deviance\") +\n  facet_grid(n ~ b_sigma, scales = \"free_y\") +\n  theme(strip.background = element_rect(fill = alpha(carto_pal(7, \"BurgYl\")[1], 1/4), color = \"transparent\"))\n\n\n\n\n\n\n\n\nAlthough our specific values vary, the overall patterns in our simulations are well aligned with those in the text. The CV, PSIS, and WAIC all did a reasonable job approximating out-of-sample deviance. Now we’ll follow the same sensibilities to make our version of the right column of McElreath’s Figure 7.9.\n\ns |&gt; \n  pivot_longer(error_w:error_c) |&gt; \n  mutate(criteria = ifelse(name == \"error_w\", \"WAIC\",\n                           ifelse(name == \"error_p\", \"PSIS\", \"CV\"))) |&gt; \n  mutate(n       = factor(str_c(\"N = \", n),\n                          levels = str_c(\"N = \", c(20, 100))),\n         b_sigma = factor(str_c(\"sigma = \", b_sigma),\n                          levels = (str_c(\"sigma = \", c(0.5, 100))))) |&gt; \n  \n  ggplot(aes(x = k)) +\n  geom_line(aes(y = value, color = criteria)) +\n  scale_shape_manual(values = c(19, 1)) +\n  scale_color_manual(values = carto_pal(7, \"BurgYl\")[c(3, 5, 7)]) +\n  labs(x = \"number of parameters (k)\",\n       y = \"average error (test deviance)\") +\n  facet_grid(n ~ b_sigma, scales = \"free_y\") +\n  theme(strip.background = element_rect(fill = alpha(carto_pal(7, \"BurgYl\")[1], 1/4), color = \"transparent\"))\n\n\n\n\n\n\n\n\nAs in the text, the mean error was very similar across the three criteria. When \\(N = 100\\), they were nearly identical. When \\(N = 20\\), the WAIC was slightly better than the PSIS, which was slightly better than the CV. As we will see, the PSIS-LOO-CV has the advantage of a built-in diagnostic.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "07.html#model-comparison",
    "href": "07.html#model-comparison",
    "title": "7  Ulysses’ Compass",
    "section": "7.5 Model comparison",
    "text": "7.5 Model comparison\nIn the sections to follow, we’ll practice the model comparison approach, as opposed to the widely-used model selection approach.\n\n7.5.1 Model mis-selection\n\nWe must keep in mind the lessons of the previous chapters: Inferring cause and making predictions are different tasks. Cross-validation and WAIC aim to find models that make good predictions. They don’t solve any causal inference problem. If you select a model based only on expected predictive accuracy, you could easily be confounded. The reason is that backdoor paths do give us valid information about statistical associations in the data. So they can improve prediction, as long as we don’t intervene in the system and the future is like the past. But recall that our working definition of knowing a cause is that we can predict the consequences of an intervention. So a good PSIS or WAIC score does not in general indicate a good causal model. (p. 226)\n\nIf you have been following along and fitting the model on your own and saving them as external .rds files the way I have, you can use the readRDS() to retrieve them.\n\nb6.6 &lt;- readRDS(\"fits/b06.06.rds\")\nb6.7 &lt;- readRDS(\"fits/b06.07.rds\")\nb6.8 &lt;- readRDS(\"fits/b06.08.rds\")\n\nWith our brms paradigm, we also use the waic() function. Both the rethinking and brms packages get their functionality for the waic() and related functions from the loo package (Vehtari et al., 2017; Vehtari et al., 2022; Yao et al., 2018). Since the brms::brm() function fits the models with HMC, we don’t need to set a seed before calling waic() the way McElreath did with his rethinking::quap() model. We’re already drawing from the posterior.\n\nwaic(b6.7)\n\n\nComputed from 4000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -180.7  6.7\np_waic         3.4  0.5\nwaic         361.3 13.5\n\n\nThe WAIC estimate and its standard error are on the bottom row. The \\(p_\\text{WAIC}\\)–what McElreath’s output called the penalty–and its SE are stacked atop that. And look there on the top row. Remember how we pointed out, above, that we get the WAIC by multiplying (lppd - pwaic) by -2? Well, if you just do the subtraction without multiplying the result by -2, you get the elpd_waic. File that away. It’ll become important in a bit. In McElreath’s output, that was called the lppd.\nFollowing the version 2.8.0 update, part of the suggested workflow for using information criteria with brms (i.e., execute ?loo.brmsfit) is to add the estimates to the brm() fit object itself. You do that with the add_criterion() function. Here’s how we’d do so with b6.7.\n\nb6.7 &lt;- add_criterion(b6.7, criterion = \"waic\") \n\nWith that in place, here’s how you’d extract the WAIC information from the fit object.\n\nb6.7$criteria$waic\n\n\nComputed from 4000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -180.7  6.7\np_waic         3.4  0.5\nwaic         361.3 13.5\n\n\nWhy would I go through all that trouble?, you might ask. Well, two reasons. First, now your WAIC information is saved with all the rest of your fit output, which can be convenient. But second, it sets you up to use the loo_compare() function to compare models by their information criteria. To get a sense of that workflow, here we use add_criterion() for the next three models. Then we’ll use loo_compare().\n\n# Compute and save the WAIC information for the next three models\nb6.6 &lt;- add_criterion(b6.6, criterion = \"waic\")\nb6.8 &lt;- add_criterion(b6.8, criterion = \"waic\")\n\n# Compare the WAIC estimates\nw &lt;- loo_compare(b6.6, b6.7, b6.8, criterion = \"waic\")\n\nprint(w)\n\n     elpd_diff se_diff\nb6.7   0.0       0.0  \nb6.8 -20.6       4.9  \nb6.6 -22.3       5.8  \n\nprint(w, simplify = F)\n\n     elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb6.7    0.0       0.0  -180.7       6.7          3.4    0.5     361.3   13.5 \nb6.8  -20.6       4.9  -201.3       5.4          2.4    0.3     402.5   10.7 \nb6.6  -22.3       5.8  -202.9       5.7          1.5    0.2     405.9   11.4 \n\n\nYou don’t have to save those results as an object like we just did with w. But that’ll serve some pedagogical purposes in just a bit. With respect to the output, notice the elpd_diff column and the adjacent se_diff column. Those are our WAIC differences in the elpd metric. The models have been rank ordered from the highest (i.e., b6.7) to the highest (i.e., b6.6). The scores listed are the differences of b6.7 minus the comparison model. Since b6.7 is the comparison model in the top row, the values are naturally 0 (i.e., \\(x - x = 0\\)). But now here’s another critical thing to understand: Since the brms version 2.8.0 update, WAIC and LOO differences are no longer reported in the \\(-2 \\times x\\) metric. Remember how multiplying (lppd - pwaic) by -2 is a historic artifact associated with the frequentist \\(\\chi^2\\) test? We’ll, the makers of the loo package aren’t fans and they no longer support the conversion.\nSo here’s the deal. The substantive interpretations of the differences presented in an elpd_diff metric will be the same as if presented in a WAIC metric. But if we want to compare our elpd_diff results to those in the text, we will have to multiply them by -2. And also, if we want the associated standard error in the same metric, we’ll need to multiply the se_diff column by 2. You wouldn’t multiply by -2 because that would return a negative standard error, which would be silly. Here’s a quick way to do those conversions.\n\ncbind(waic_diff = w[, 1] * -2,\n      se        = w[, 2] * 2)\n\n     waic_diff        se\nb6.7   0.00000  0.000000\nb6.8  41.19024  9.834776\nb6.6  44.53342 11.588276\n\n\nNow those match up reasonably well with the values in McElreath’s dWAIC and dSE columns.\nOne more thing. On page 227, and on many other pages to follow in the text, McElreath used the rethinking::compare() function to return a rich table of information about the WAIC information for several models. If we’re tricky, we can do something similar with loo_compare. To learn how, let’s peer further into the structure of our w object.\n\nstr(w)\n\n 'compare.loo' num [1:3, 1:8] 0 -20.6 -22.27 0 4.92 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:3] \"b6.7\" \"b6.8\" \"b6.6\"\n  ..$ : chr [1:8] \"elpd_diff\" \"se_diff\" \"elpd_waic\" \"se_elpd_waic\" ...\n\n\nWhen we used print(w), a few code blocks earlier, it only returned two columns. It appears we actually have eight. We can see the full output with the simplify = F argument.\n\nprint(w, simplify = F)\n\n     elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb6.7    0.0       0.0  -180.7       6.7          3.4    0.5     361.3   13.5 \nb6.8  -20.6       4.9  -201.3       5.4          2.4    0.3     402.5   10.7 \nb6.6  -22.3       5.8  -202.9       5.7          1.5    0.2     405.9   11.4 \n\n\nThe results are quite analogous to those from rethinking::compare(). Again, the difference estimates are in the metric of the \\(\\text{elpd}\\). But the interpretation is the same and we can convert them to the traditional information criteria metric with simple multiplication. As we’ll see later, this basic workflow also applies to the PSIS-LOO.\nOkay, we’ve deviated a bit from the text. Let’s reign things back in and note that right after McElreath’s R code 7.26, he wrote: “PSIS will give you almost identical values. You can add func=PSIS to the compare call to check” (p. 227). Our brms::loo_compare() function has a similar argument, but it’s called criterion. We set it to criterion = \"waic\" to compare the models by the WAIC. What McElreath is calling func=PSIS, we’d call criterion = \"loo\". Either way, we’re asking the software the compare the models using leave-one-out cross-validation with Pareto-smoothed importance sampling.\n\nb6.6 &lt;- add_criterion(b6.6, criterion = \"loo\")\nb6.7 &lt;- add_criterion(b6.7, criterion = \"loo\")\nb6.8 &lt;- add_criterion(b6.8, criterion = \"loo\")\n\n# Compare the WAIC estimates\nloo_compare(b6.6, b6.7, b6.8, criterion = \"loo\") |&gt; \n  print(simplify = F)\n\n     elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nb6.7    0.0       0.0  -180.7      6.7         3.4    0.5    361.3   13.5  \nb6.8  -20.6       4.9  -201.3      5.4         2.5    0.3    402.5   10.7  \nb6.6  -22.3       5.8  -202.9      5.7         1.5    0.2    405.9   11.4  \n\n\nYep, the LOO values are very similar to those from the WAIC. Anyway, at the bottom of page 227, McElreath showed how to compute the standard error of the WAIC difference for models m6.7 and m6.8. Here’s the procedure for us.\n\nn &lt;- length(b6.7$criteria$waic$pointwise[, \"waic\"])\n\ntibble(waic_b6.7 = b6.7$criteria$waic$pointwise[, \"waic\"],\n       waic_b6.8 = b6.8$criteria$waic$pointwise[, \"waic\"]) |&gt; \n  mutate(diff = waic_b6.7 - waic_b6.8) |&gt; \n  summarise(diff_se = sqrt(n * var(diff)))\n\n# A tibble: 1 × 1\n  diff_se\n    &lt;dbl&gt;\n1    9.83\n\n\nSince we used different estimation methods (HMC versus the quadratic approximation) via different software (brms::brm() versus rethinking::quap()), we shouldn’t be surprised our values are a little different from those in the text. But we’re in the ballpark, for sure. For us, this value was in the second row of the second column in our w object. But remember we have to multiply that value by 2 to convert it from the \\(\\text{elpd}\\) metric to that of the WAIC.\n\nw[2, 2] * 2\n\n[1] 9.834776\n\n\nPresuming the difference is Gaussian distributed, here’s our 99% interval.\n\n(w[2, 1] * -2) + c(-1, 1) * (w[2, 2] * 2) * 2.6\n\n[1] 15.61982 66.76066\n\n\nWith our brms paradigm, we won’t get a comparison plot by inserting loo_compare(b6.6, b6.7, b6.8, criterion = \"waic\") within plot(). But with a little [] subsetting and light wrangling, we can convert the contents of our w object to a format suitable for plotting the WAIC estimates with ggplot2.\n\nw[, 7:8] |&gt; \n  data.frame() |&gt; \n  rownames_to_column(\"model_name\") |&gt; \n  mutate(model_name = fct_reorder(model_name, waic, .desc = T)) |&gt; \n  \n  ggplot(aes(x = waic, y = model_name, \n             xmin = waic - se_waic, \n             xmax = waic + se_waic)) +\n  geom_pointrange(color = carto_pal(7, \"BurgYl\")[7], \n                  fill = carto_pal(7, \"BurgYl\")[5], shape = 21) +\n  labs(x = NULL, \n       y = NULL,\n       title = \"My custom WAIC plot\") +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nWe don’t get the deviance points with this method, but that’s okay. Our primary focus is on the WAIC and its standard errors. Here’s how to hand-compute the standard error for the difference between b6.6 and b6.8.\n\ntibble(waic_b6.6 = waic(b6.6)$pointwise[, \"waic\"],\n       waic_b6.8 = waic(b6.8)$pointwise[, \"waic\"]) |&gt; \n  mutate(diff = waic_b6.6 - waic_b6.8) |&gt; \n  summarise(diff_se = sqrt(n * var(diff)))\n\n# A tibble: 1 × 1\n  diff_se\n    &lt;dbl&gt;\n1    4.72\n\n\nUnlike rethinking::compare(), the loo_compare() function will not allow us to so easily pull this value by indexing with @dSE.\n\nloo_compare(b6.6, b6.7, b6.8, criterion = \"waic\") |&gt; \n  str()\n\n 'compare.loo' num [1:3, 1:8] 0 -20.6 -22.27 0 4.92 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:3] \"b6.7\" \"b6.8\" \"b6.6\"\n  ..$ : chr [1:8] \"elpd_diff\" \"se_diff\" \"elpd_waic\" \"se_elpd_waic\" ...\n\n\nHowever, if you really want that value, make a simple WAIC comparison between b6.6 and b6.8.\n\nloo_compare(b6.6, b6.8, criterion = \"waic\")\n\n     elpd_diff se_diff\nb6.8  0.0       0.0   \nb6.6 -1.7       2.4   \n\n\nThe value we’re looking for is in the second row of the se_diff column. But remember this is in the \\(\\text{elpd}\\) metric. Here’s the conversion.\n\nloo_compare(b6.6, b6.8, criterion = \"waic\")[2, 2] * 2\n\n[1] 4.715244\n\n\nUnlike rethinking::compare(), our brms::loo_compare() (even when we print(simplify = T)) does not contain a weight column. Don’t worry; we can still compute them. We’ll just have to do so with a different function. Before we do, here’s the equation they’re based on:\n\\[w_i = \\frac{\\exp(-0.5 \\Delta_i)}{\\sum_j \\exp(-0.5 \\Delta_j)},\\]\nwhere \\(w_i\\) is the weight for the \\(i^\\text{th}\\) model and \\(\\Delta_i\\) is the difference between that model and the WAIC for the best one in the comparison set. If you want to get those WAIC weights, you can use the brms::model_weights() function like so.\n\nmodel_weights(b6.6, b6.7, b6.8, weights = \"waic\") |&gt; \n  round(digits = 2)\n\nb6.6 b6.7 b6.8 \n   0    1    0 \n\n\nIn his endnote #130, McElreath discussed how these WAIC weights can be used for model averaging. From the endnote, we read:\n\nThe first edition had a section on model averaging, but the topic has been dropped in this edition to save space. The approach is really focused on prediction, not inference, and so it doesn’t fit the flow of the second edition. But it is an important approach. (p. 565)\n\nA variety of approaches to model averaging are available with brms and I covered them in my (2026b) ebook translation of the first edition of McElreath’s text.\n\n7.5.1.1 Rethinking: WAIC metaphors\n\nThink of models as race horses. In any particular race, the best horse may not win. But it’s more likely to win than is the worst horse. And when the winning horse finishes in half the time of the second-place horse, you can be pretty sure the winning horse is also the best. But if instead it’s a photo-finish, with a near tie between first and second place, then it is much harder to be confident about which is the best horse. (p. 230)\n\n\n\n\n7.5.2 Outliers and other illusions\nTime to bring back the WaffleDivorce data.\n\ndata(WaffleDivorce, package = \"rethinking\")\n\nd &lt;- WaffleDivorce |&gt; \n  mutate(d = rethinking::standardize(Divorce),\n         m = rethinking::standardize(Marriage),\n         a = rethinking::standardize(MedianAgeMarriage))\n\nrm(WaffleDivorce)\n\nRefit the divorce models from Section 5.1.\n\nb5.1 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + a,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  sample_prior = T,\n  file = \"fits/b05.01\")\n\nb5.2 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + m,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.02\")\n\nb5.3 &lt;- brm(\n  data = d, \n  family = gaussian,\n  d ~ 1 + m + a,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.03\")\n\nCompute and save the LOO estimates for each.\n\nb5.1 &lt;- add_criterion(b5.1, criterion = \"loo\")\nb5.2 &lt;- add_criterion(b5.2, criterion = \"loo\")\nb5.3 &lt;- add_criterion(b5.3, criterion = \"loo\")\n\nNow compare the models by the PSIS-LOO-CV.\n\nloo_compare(b5.1, b5.2, b5.3, criterion = \"loo\") |&gt; \n  print(simplify = F)\n\n     elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic\nb5.1   0.0       0.0   -62.8      6.4         3.6   1.7    125.6  12.7   \nb5.3  -1.1       0.4   -63.9      6.4         4.8   1.9    127.8  12.9   \nb5.2  -6.8       4.6   -69.6      5.0         2.9   0.9    139.3   9.9   \n\n\nLike in the text, our b5.1 has the best LOO estimate, but only by a little bit when compared to b5.3. Unlike McElreath reported in the text, we did not get a warning message from loo_compare(). Let’s investigate more carefully with the loo() function.\n\nloo(b5.3)\n\n\nComputed from 4000 by 50 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo    -63.9  6.4\np_loo         4.8  1.9\nlooic       127.8 12.9\n------\nMCSE of elpd_loo is 0.1.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.7, 1.4]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nNone of the observations were in the (bad) or (very bad) ranges. One of the values, however, was a little on the high side. By opening the loo package directly, we gain access to the pareto_k_ids() function, which will help us identify which observation crossed out of the (good) range into the (ok).\n\nlibrary(loo)\n\nloo(b5.3) |&gt; \n  pareto_k_ids(threshold = 0.5)\n\n[1] 13\n\n\nThe number 13 refers to the corresponding row in the data used to fit the model. We can access that row directly with the dplyr::slice() function.\n\nd |&gt; \n  slice(13) |&gt; \n  select(Location:Loc)\n\n  Location Loc\n1    Idaho  ID\n\n\nHere we subset the 13th cell in the loo::pareto_k_values() output to see what that value was.\n\npareto_k_values(loo(b5.3))[13]\n\n[1] 0.6034469\n\n\nAlternatively, we could have extracted that value from our b5.3 fit object like so.\n\nb5.3$criteria$loo$diagnostics$pareto_k[13]\n\n[1] 0.6034469\n\n\n0.6 is a little high, but not high enough to cause loo() to return a warning message. Once a Pareto \\(k\\) value crosses the 0.7 threshold, though, the loo package will bark. Before we make our version of Figure 7.10, we’ll want to compute the WAIC for b5.3. That will give us access to the \\(p_\\text{WAIC}\\).\n\nb5.3 &lt;- add_criterion(b5.3, criterion = \"waic\", file = \"fits/b05.03\")\n\nWe’re ready to make our version of Figure 7.10.\n\nl &lt;- tibble(pareto_k = b5.3$criteria$loo$diagnostics$pareto_k,\n            p_waic   = b5.3$criteria$waic$pointwise[, \"p_waic\"],\n            Loc      = pull(d, Loc)) \n\nl |&gt; \n  ggplot(aes(x = pareto_k, y = p_waic, color = Loc == \"ID\")) +\n  geom_vline(xintercept = 0.5, linetype = 2, color = \"black\", alpha = 1/2) +\n  geom_point(aes(shape = Loc == \"ID\")) +\n  geom_text(data = l |&gt; filter(p_waic &gt; 0.5),\n            aes(x = pareto_k - 0.03, label = Loc),\n            hjust = 1) +\n  scale_color_manual(values = carto_pal(7, \"BurgYl\")[c(5, 7)]) +\n  scale_shape_manual(values = c(1, 19)) +\n  labs(subtitle = \"Gaussian model (b5.3)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFor both the Pareto \\(k\\) and the \\(p_\\text{WAIC}\\), our values are not as extreme as those McElreath reported in the text. I’m not sure if this is a consequence of us using HMC or due to recent algorithmic changes for the Stan/loo teams. But at least the overall pattern is the same. Idaho is the most extreme/influential case.\nIn the text (p. 232), McElreath reported the effective number of parameters for b5.3 was nearly 6. We can look at this with the waic() function.\n\nwaic(b5.3)\n\n\nComputed from 4000 by 50 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic    -63.8  6.4\np_waic         4.7  1.9\nwaic         127.6 12.8\n\n2 (4.0%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\nOur \\(p_\\text{WAIC}\\) was about 4.7, which is still a little high due to Idaho but not quite as high as in the text. There is some concern that Idaho is putting us at risk of overfitting due to the large influence it has on the posterior.\n\nWhat can be done about this? There is a tradition of dropping outliers. People sometimes drop outliers even before a model is fit, based only on standard deviations from the mean outcome value. You should never do that–a point can only be unexpected and highly influential in light of a model. After you fit a model, the picture changes. If there are only a few outliers, and you are sure to report results both with and without them, dropping outliers might be okay. But if there are several outliers and we really need to model them, what then?\nA basic problem here is that the Gaussian error model is easily surprised. (p. 232)\n\nThis surprise has to do with the thinness of its tails relative to those of the Student’s \\(t\\)-distribution. We can get a sense of this with Figure 7.11.\n\ntibble(x = seq(from = -6, to = 6, by = 0.01)) |&gt; \n  mutate(Gaussian    = dnorm(x),\n         `Student-t` = dstudent(x)) |&gt; \n  pivot_longer(-x,\n               names_to = \"likelihood\",\n               values_to = \"density\") |&gt; \n  mutate(`minus log density` = -log(density)) |&gt; \n  pivot_longer(contains(\"density\")) |&gt; \n  \n  ggplot(aes(x = x, y = value, group = likelihood, color = likelihood)) +\n  geom_line() +\n  scale_color_manual(values = c(carto_pal(7, \"BurgYl\")[6], \"black\")) +\n  ylim(0, NA) +\n  labs(x = \"value\", y = NULL) +\n  facet_wrap(~ name, scales = \"free_y\") +\n  theme(strip.background = element_blank())\n\n\n\n\n\n\n\n\nI’m a little baffled as to why our curves don’t quite match up with those in the text. The Gaussian curves were made using the dnorm() function with the default settings, which are \\(\\operatorname{Normal}(0, 1)\\). The Student-\\(t\\) curves were made with McElreath’s rethinking::dstudent()function with the default settings, which are \\(\\operatorname{Student-t}(2, 0, 1)\\)–you get the same results is you use dt(x, df = 2). Discrepancies aside, the main point in the text still stands. The \\(t\\) distribution, especially as \\(\\nu \\rightarrow 1\\), has thicker tails than the Gaussian. As a consequence, it is more robust to extreme values.\nThe brms package will allow us to fit a Student \\(t\\) model. Just set family = student. We are at liberty to estimate \\(\\nu\\) along with the other parameters in the model or to set it to a constant. We’ll follow McElreath and fix nu = 2. Note that brms syntax requires we do so within a bf() statement.\n\nb5.3t &lt;- brm(\n  data = d, \n  family = student,\n  bf(d ~ 1 + m + a, nu = 2),\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 5,\n  file = \"fits/b05.03t\")\n\nCheck the summary.\n\nprint(b5.3t)\n\n Family: student \n  Links: mu = identity \nFormula: d ~ 1 + m + a \n         nu = 2\n   Data: d (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.10    -0.17     0.21 1.00     3490     3090\nm             0.05      0.20    -0.33     0.47 1.00     3164     3040\na            -0.70      0.15    -1.00    -0.39 1.00     3076     2596\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.58      0.09     0.43     0.78 1.00     4205     3106\nnu        2.00      0.00     2.00     2.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we use the add_criterion() function compute both the LOO-CV and the WAIC and add them to the model fit object.\n\nb5.3t &lt;- add_criterion(b5.3t, criterion = c(\"loo\", \"waic\"))\n\nNow we might remake the plot from Figure 7.10, this time based on the \\(t\\) model.\n\nl &lt;- tibble(pareto_k = b5.3t$criteria$loo$diagnostics$pareto_k,\n            p_waic   = b5.3t$criteria$waic$pointwise[, \"p_waic\"],\n            Loc      = pull(d, Loc)) \n\nl |&gt;\n  ggplot(aes(x = pareto_k, y = p_waic, color = Loc == \"ID\")) +\n  geom_point(aes(shape = Loc == \"ID\")) +\n  geom_text(data = l |&gt; filter(Loc %in% c(\"ID\", \"ME\")),\n            aes(x = pareto_k - 0.005, label = Loc),\n            hjust = 1) +\n  scale_color_manual(values = carto_pal(7, \"BurgYl\")[c(5, 7)]) +\n  scale_shape_manual(values = c(1, 19)) +\n  labs(subtitle = \"Student-t model (b5.3t)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe high points in both the Pareto \\(k\\) and \\(p_\\text{WAIC}\\) are much smaller and both ID and ME are now closer to the center of the bivariate distribution. We can formally compare the models with the WAIC and the LOO.\n\nloo_compare(b5.3, b5.3t, criterion = \"waic\") |&gt; print(simplify = F)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  se_waic\nb5.3    0.0       0.0   -63.8       6.4          4.7    1.9     127.6  12.8  \nb5.3t  -2.7       3.0   -66.5       5.8          6.2    1.0     133.0  11.5  \n\nloo_compare(b5.3, b5.3t, criterion = \"loo\") |&gt; print(simplify = F)\n\n      elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic\nb5.3    0.0       0.0   -63.9      6.4         4.8   1.9    127.8  12.9   \nb5.3t  -2.6       3.0   -66.5      5.8         6.2   1.0    132.9  11.5   \n\n\nFor both criteria, the standard error for the difference was about the same size as the difference itself. This suggests the models were close and hard to distinguish with respect to their fit to the data. This will not always be the case.\nJust for kicks and giggles, let’s compare the parameter summaries for the two models in a coefficient plot.\n\nbind_rows(as_draws_df(b5.3),\n          as_draws_df(b5.3t)) |&gt; \n  mutate(fit = rep(c(\"Gaussian (b5.3)\", \"Student-t (b5.3t)\"), each = n() / 2)) |&gt; \n  pivot_longer(b_Intercept:sigma) |&gt; \n  mutate(name = factor(name,\n                       levels = c(\"b_Intercept\", \"b_a\", \"b_m\", \"sigma\"),\n                       labels = c(\"alpha\", \"beta[a]\", \"beta[m]\", \"sigma\"))) |&gt; \n  \n  ggplot(aes(x = value, y = fit, color = fit)) +\n  stat_pointinterval(.width = 0.95, size = 1) +\n  scale_color_manual(values = c(carto_pal(7, \"BurgYl\")[6], \"black\")) +\n  labs(x = \"posterior\", y = NULL) +\n  facet_wrap(~ name, ncol = 1, labeller = label_parsed) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        legend.position = \"none\",\n        strip.background = element_rect(fill = alpha(carto_pal(7, \"BurgYl\")[1], 1/4), color = \"transparent\"),\n        strip.text = element_text(size = 12))\n\n\n\n\n\n\n\n\nOverall, the coefficients are very similar between the two models. The most notable difference is in \\(\\sigma\\). This is, in part, because \\(\\sigma\\) has a slightly different meaning for Student-\\(t\\) models than it does for the Gaussian. For both, we can refer to \\(\\sigma\\) as the scale parameter, with is a measure of spread or variability around the mean. However, the scale is the same thing as the standard deviation only for the Gaussian. Kruschke walked this out nicely:\n\nIt is important to understand that the scale parameter \\(\\sigma\\) in the \\(t\\) distribution is not the standard deviation of the distribution. (Recall that the standard deviation is the square root of the variance, which is the expected value of the squared deviation from the mean, as defined back in Equation 4.8, p. 86.) The standard deviation is actually larger than \\(\\sigma\\) because of the heavy tails. In fact, when \\(\\nu\\) drops below 2 (but is still \\(\\geq 1\\)), the standard deviation of the mathematical \\(t\\) distribution goes to infinity. (2015, p. 159)\n\nThe variance for \\(\\operatorname{Student-t}(\\nu, 0, 1)\\) is\n\\[\\frac{\\nu}{\\nu-2} \\text{ for }  \\nu &gt;2, \\;\\;\\; \\infty \\text{ for } 1 &lt; \\nu \\leq 2,\\]\nand the standard deviation is the square of that. To give a sense of how that formula works, here are the standard deviation values when \\(\\nu\\) ranges from 2.1 to 20.\n\n# For the annotation\ntext &lt;- tibble(\n  nu    = c(1.35, 20),\n  sd    = c(sqrt(2.1 / (2.1 - 2)), 0.875),\n  angle = c(90, 0),\n  hjust = 1,\n  label = \"asymptote\")\n\n# Wrangle\ntibble(nu = seq(from = 2.1, to = 20, by = 0.01)) |&gt; \n  mutate(var = nu / (nu - 2)) |&gt; \n  mutate(sd = sqrt(var)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = nu, y = sd)) +\n  geom_hline(yintercept = 1, color = carto_pal(7, \"BurgYl\")[2], linetype = 2) +\n  geom_vline(xintercept = 2, color = carto_pal(7, \"BurgYl\")[2], linetype = 2) +\n  geom_text(data = text,\n            aes(label = label, angle = angle, hjust = hjust),\n            color = carto_pal(7, \"BurgYl\")[3]) +\n  geom_line(color = carto_pal(7, \"BurgYl\")[7]) +\n  scale_x_continuous(expression(nu), breaks = c(2, 1:4 * 5)) +\n  labs(y = expression(standard~deviation*\", \"*~sqrt(nu/(nu-2))),\n       subtitle = expression(Student-t(nu*', '*0*', '*1)))\n\n\n\n\n\n\n\n\nAs \\(\\nu \\rightarrow \\infty\\), the standard deviation approaches the scale, \\(\\sigma\\). We’ll have more practice with robust Student’s \\(t\\) regression later on. In the mean time, you might check out chapters 16, 17, 18, 19, and 20 from my (2026a) ebook translation of Kruschke’s text. I’ve also blogged on using the robust Student’s \\(t\\) for regression and correlations. Gelman et al. (2020) covered robust Student’s \\(t\\) regression in Section 15.6, too.\n\n7.5.2.1 Rethinking: The Curse of Tippecanoe\n\nOne concern with model comparison is, if we try enough combinations and transformations of predictors, we might eventually find a model that fits any sample very well. But this fit will be badly overfit, unlikely to generalize….\nFiddling with and constructing many predictor variables is a great way to find coincidences, but not necessarily a great way to evaluate hypotheses. However, fitting many possible models isn’t always a dangerous idea, provided some judgment is exercised in weeding down the list of variables at the start. There are two scenarios in which this strategy appears defensible. First, sometimes all one wants to do is explore a set of data, because there are no clear hypotheses to evaluate. This is rightly labeled pejoratively as data dredging, when one does not admit to it. But when used together with model averaging, and freely admitted, it can be a way to stimulate future investigation. Second, sometimes we need to convince an audience that we have tried all of the combinations of predictors, because none of the variables seem to help much in prediction. (p. 234, emphasis in the original)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "07.html#summary-bonus-r2-talk",
    "href": "07.html#summary-bonus-r2-talk",
    "title": "7  Ulysses’ Compass",
    "section": "7.6 Summary Bonus: \\(R^2\\) talk",
    "text": "7.6 Summary Bonus: \\(R^2\\) talk\nAt the beginning of the chapter (pp. 193–201), McElreath introduced \\(R^2\\) as a popular way to assess the variance explained in a model. He pooh-poohed it because of its tendency to overfit. It’s also limited in that it doesn’t generalize well outside of the single-level Gaussian framework, which will be a big deal for us starting in Chapter 10. However, if you should find yourself in a situation where \\(R^2\\) suits your purposes, the brms bayes_R2() function might be of use. Simply feeding a model brm() fit object into bayes_R2() will return the posterior mean, \\(\\textit{SD}\\), and 95% intervals. For example:\n\nbayes_R2(b5.3) |&gt; round(digits = 3)\n\n   Estimate Est.Error  Q2.5 Q97.5\nR2    0.331     0.088 0.142  0.48\n\n\nWith just a little data processing, you can get a tibble table of each of models’ \\(R^2\\) ‘Estimate’.\n\nrbind(bayes_R2(b5.1), \n      bayes_R2(b5.2), \n      bayes_R2(b5.3)) |&gt;\n  data.frame() |&gt;\n  mutate(model                   = c(\"b5.1\", \"b5.2\", \"b5.3\"),\n         r_square_posterior_mean = round(Estimate, digits = 3)) |&gt;\n  select(model, r_square_posterior_mean)\n\n     model r_square_posterior_mean\nR2    b5.1                   0.327\nR2.1  b5.2                   0.131\nR2.2  b5.3                   0.331\n\n\nIf you want the full distribution of the \\(R^2\\), you’ll need to add a summary = F argument. Note how this returns a numeric vector.\n\nr2_b5.1 &lt;- bayes_R2(b5.1, summary = F)\n\nr2_b5.1 |&gt;\n  glimpse()\n\n num [1:4000, 1] 0.254 0.223 0.501 0.429 0.185 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr \"R2\"\n\n\nIf you want to use these in ggplot2, you’ll need to put them in tibbles or data frames. Here we do so for two of our model fits.\n\nr2 &lt;- cbind(bayes_R2(b5.1, summary = F),\n            bayes_R2(b5.2, summary = F)) |&gt; \n  data.frame() |&gt; \n  set_names(str_c(\"b5.\", 1:2)) \n  \nr2 |&gt; \n  ggplot() +\n  geom_density(aes(x = b5.1),\n               alpha = 3/4, linewidth = 0, fill = carto_pal(7, \"BurgYl\")[4]) +\n  geom_density(aes(x = b5.2),\n               alpha = 3/4, linewidth = 0, fill = carto_pal(7, \"BurgYl\")[6]) +\n  annotate(geom = \"text\", \n           x = c(0.1, 0.34), y = 3.5, \n           label = c(\"b5.2\", \"b5.1\"), \n           color = alpha(\"white\", 3/4), family = \"Courier\") +\n  scale_x_continuous(NULL, limits = c(0, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(expression(italic(R)^2~distributions))\n\n\n\n\n\n\n\n\nIf you do your work in a field where folks use \\(R^2\\) change, you might do that with a simple difference score. Here’s the \\(\\Delta R^2\\) plot.\n\nr2 |&gt;\n  mutate(diff = b5.2 - b5.1) |&gt; \n  \n  ggplot(aes(x = diff, y = 0)) +\n  stat_halfeye(point_interval = median_qi, .width = 0.95,\n               fill = carto_pal(7, \"BurgYl\")[5], \n               color = carto_pal(7, \"BurgYl\")[7]) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = expression(Delta*italic(R)^2),\n       subtitle = expression(Turns~out~b5.2~had~a~lower~italic(R)^2~than~b5.1))\n\n\n\n\n\n\n\n\nThe brms package did not get these \\(R^2\\) values by traditional method used in, say, ordinary least squares estimation. To learn more about how the Bayesian \\(R^2\\) sausage is made, check out the paper by Gelman et al. (2019), R-squared for Bayesian regression models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "07.html#session-info",
    "href": "07.html#session-info",
    "title": "7  Ulysses’ Compass",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] loo_2.9.0.9000       rethinking_2.42      posterior_1.6.1.9000 cmdstanr_0.9.0       patchwork_1.3.2      tidybayes_3.0.7     \n [7] brms_2.23.0          Rcpp_1.1.0           ggrepel_0.9.6        rcartocolor_2.1.2    lubridate_1.9.4      forcats_1.0.1       \n[13] stringr_1.6.0        dplyr_1.1.4          purrr_1.2.1          readr_2.1.5          tidyr_1.3.2          tibble_3.3.1        \n[19] ggplot2_4.0.1        tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.8            tidyselect_1.2.1        viridisLite_0.4.2       farver_2.1.2            S7_0.2.1               \n [6] fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39           estimability_1.5.1     \n[11] timechange_0.3.0        lifecycle_1.0.5         StanHeaders_2.36.0.9000 processx_3.8.6          survival_3.8-3         \n[16] magrittr_2.0.4          compiler_4.5.1          rlang_1.1.7             tools_4.5.1             utf8_1.2.6             \n[21] yaml_2.3.12             knitr_1.51              labeling_0.4.3          bridgesampling_1.2-1    htmlwidgets_1.6.4      \n[26] curl_7.0.0              pkgbuild_1.4.8          plyr_1.8.9              RColorBrewer_1.1-3      BH_1.90.0-1            \n[31] abind_1.4-8             multcomp_1.4-29         withr_3.0.2             grid_4.5.1              stats4_4.5.1           \n[36] xtable_1.8-4            inline_0.3.21           emmeans_1.11.2-8        scales_1.4.0            MASS_7.3-65            \n[41] cli_3.6.5               mvtnorm_1.3-3           rmarkdown_2.30          generics_0.1.4          RcppParallel_5.1.11-1  \n[46] rstudioapi_0.17.1       reshape2_1.4.5          tzdb_0.5.0              rstan_2.36.0.9000       splines_4.5.1          \n[51] bayesplot_1.15.0.9000   matrixStats_1.5.0       vctrs_0.7.0             V8_8.0.1                Matrix_1.7-3           \n[56] sandwich_3.1-1          jsonlite_2.0.0          callr_3.7.6             arrayhelpers_1.1-0      hms_1.1.4              \n[61] ggdist_3.3.3            glue_1.8.0              ps_1.9.1                codetools_0.2-20        distributional_0.5.0   \n[66] shape_1.4.6.1           stringi_1.8.7           gtable_0.3.6            QuickJSR_1.8.1          pillar_1.11.1          \n[71] htmltools_0.5.9         Brobdingnag_1.2-9       RcppEigen_0.3.4.0.2     R6_2.6.1                evaluate_1.0.5         \n[76] lattice_0.22-7          backports_1.5.0         rstantools_2.5.0.9000   coda_0.19-4.1           gridExtra_2.3          \n[81] nlme_3.1-168            checkmate_2.3.3         xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "07.html#comments",
    "href": "07.html#comments",
    "title": "7  Ulysses’ Compass",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nAkaike, H. (1998). Information theory and an extension of the maximum likelihood principle. In Selected papers of Hirotugu Akaike (pp. 199–213). Springer. https://www.springer.com/gp/book/9780387983554\n\n\nBetancourt, M. (2018). Bayes sparse regression. https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html\n\n\nBürkner, P.-C. (2022a). brms reference manual, Version 2.18.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2022b). Estimating distributional models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html\n\n\nBürkner, P.-C. (2022c). Define custom response distributions with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html\n\n\nCarvalho, C. M., Polson, N. G., & Scott, J. G. (2009). Handling sparsity via the horseshoe. Artificial Intelligence and Statistics, 73–80. http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf\n\n\nCover, T. M., & Thomas, J. A. (2006). Elements of information theory (2nd Edition). John Wiley & Sons. https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959\n\n\nde Rooij, M., & Weeda, W. (2020). Cross-validation: A method every psychologist should know. Advances in Methods and Practices in Psychological Science, 3(2), 248–263. https://doi.org/10.1177/2515245919898466\n\n\nGelman, A., Goodrich, B., Gabry, J., & Vehtari, A. (2019). R-squared for Bayesian regression models. The American Statistician, 73(3), 307–309. https://doi.org/10.1080/00031305.2018.1549100\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction. Springer Science & Business Media. https://doi.org/10.1007/978-0-387-84858-7\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKullback, S., & Leibler, R. A. (1951). On information and sufficiency. Annals of Mathematical Statistics, 22(1), 79–86. https://doi.org/10.1214/aoms/1177729694\n\n\nKurz, A. S. (2026a). Doing Bayesian data analysis in brms and the tidyverse (Version 1.3.0). https://solomon.quarto.pub/dbda2/\n\n\nKurz, A. S. (2026b). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.4.0). https://solomon.quarto.pub/sr/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nNowosad, J. (2019). rcartocolor: ’CARTOColors’ palettes. https://CRAN.R-project.org/package=rcartocolor\n\n\nShannon, C. E. (1948). A mathematical theory of communication. Bell System Technical Journal, 27(3), 379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\n\n\nSpiegelhalter, D. J., Best, N. G., Carlin, B. P., & Linde, A. V. D. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 64(4), 583–639. https://doi.org/10.1111/1467-9868.00353\n\n\nVehtari, A., Gabry, J., Magnusson, M., Yao, Y., & Gelman, A. (2022). loo: Efficient leave-one-out cross-validation and WAIC for bayesian models. https://CRAN.R-project.org/package=loo/\n\n\nVehtari, A., Gelman, A., & Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4\n\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA statement on p-values: Context, process, and purpose. The American Statistician, 70(2), 129–133. https://doi.org/10.1080/00031305.2016.1154108\n\n\nWatanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory. Journal of Machine Learning Research, 11(116), 3571–3594. http://jmlr.org/papers/v11/watanabe10a.html\n\n\nWilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses. The Annals of Mathematical Statistics, 9(1), 60–62. https://doi.org/10.1214/aoms/1177732360\n\n\nYao, Y., Vehtari, A., Simpson, D., & Gelman, A. (2018). Using stacking to average Bayesian predictive distributions (with discussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091\n\n\nYarkoni, T., & Westfall, J. (2017). Choosing prediction over explanation in psychology: Lessons from machine learning. Perspectives on Psychological Science : A Journal of the Association for Psychological Science, 12(6), 1100–1122. https://doi.org/10.1177/1745691617693393\n\n\nZhang, Y., & Yang, Y. (2015). Cross-validation for selecting a model selection procedure. Journal of Econometrics, 187(1), 95–112. https://doi.org/10.1016/j.jeconom.2015.02.006",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ulysses' Compass</span>"
    ]
  },
  {
    "objectID": "08.html",
    "href": "08.html",
    "title": "8  Conditional Manatees",
    "section": "",
    "text": "8.1 Building an interaction\n“Africa is special” (p. 239). Let’s load the rugged data (Nunn & Puga, 2012) to see one of the reasons why.\ndata(rugged, package = \"rethinking\")\nd &lt;- rugged\nrm(rugged)\n\n# May as well load this, too\nlibrary(brms)\nFor this chapter, we’ll take our plot theme from the ggthemes package (Arnold, 2021).\nlibrary(tidyverse)\nlibrary(ggthemes)\n\ntheme_set(\n  theme_pander() +\n    theme(text = element_text(family = \"Times\"),\n          panel.background = element_rect(color = \"black\")) \n)\nWe’ll use the pander color scheme to help us make our first DAG.\nlibrary(ggdag)\n\ndag_coords &lt;- tibble(\n  name = c(\"R\", \"G\", \"C\", \"U\"),\n  x    = c(1, 2, 3, 2),\n  y    = c(2, 2, 2, 1))\n\ndagify(R ~ U,\n       G ~ R + U + C,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"U\"),\n                 alpha = 1/2, size = 6, show.legend = F) +\n  geom_point(x = 2, y = 1, \n             color = palette_pander(n = 2)[2], shape = 1, size = 6, stroke = 3/4) +\n  geom_dag_text(color = \"black\", family = \"Times\") +\n  geom_dag_edges() +\n  scale_colour_pander() +\n  theme_dag()\nIt’s generally not a good idea to split up your data and run separate analyses when examining an interaction. McElreath listed four reasons why:",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conditional Manatees</span>"
    ]
  },
  {
    "objectID": "08.html#building-an-interaction",
    "href": "08.html#building-an-interaction",
    "title": "8  Conditional Manatees",
    "section": "",
    "text": "Let’s ignore \\(U\\) for now… Focus instead on the implication that \\(R\\) and \\(C\\) both influence \\(G\\). This could mean that they are independent influences or rather that they interact (one moderates the influence of the other). The DAG does not display an interaction. That’s because DAGs do not specify how variables combine to influence other variables. The DAG above implies only that there is some function that uses \\(R\\) and \\(C\\) to generate \\(G\\). In typical notation, \\(G = f(R, C)\\). (p. 240)\n\n\n\n“There are usually some parameters, such as \\(\\sigma\\), that the model says do not depend in any way upon continent. By splitting the data table, you are hurting the accuracy of the estimates for these parameters” (p. 241).\n“In order to acquire probability statements about the variable you used to split the data, cont_africa, in this case, you need to include it in the model” (p. 241).\n“We many want to use information criteria or another method to compare models” (p. 241).\n“Once you begin using multilevel models (Chapter 13), you’ll see that there are advantages to borrowing information across categories like ‘Africa’ and ‘not Africa’” (p. 241).\n\n\n8.1.0.1 Overthinking: Not so simple causation\nHere’s the DAG for a fuller model for the data.\n\ndag_coords &lt;- tibble(\n  name = c(\"G\", \"R\", \"H\", \"C\", \"U\"),\n  x    = c(1, 1.5, 2.5, 3.5, 1),\n  y    = c(3, 2, 2, 2, 1))\n\ndagify(G ~ R + U + H,\n       R ~ U,\n       H ~ R + U + C,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"U\"),\n                 alpha = 1/2, size = 6, show.legend = F) +\n  geom_point(x = 1, y = 1, \n             color = palette_pander(n = 2)[2], shape = 1, size = 6, stroke = 3/4) +\n  geom_dag_text(color = \"black\", family = \"Times\") +\n  geom_dag_edges() +\n  scale_colour_pander() +\n  theme_dag()\n\n\n\n\n\n\n\n\n“The data contain a large number of potential confounds that you might consider. Natural systems like this are terrifyingly complex” (p. 241). In the words of the great Dan Simpson, “Pictures and fear–this is what we do [in statistics]; we draw pictures and have fear” (see here).\n\n\n8.1.1 Making a rugged model\nWe’ll continue to use tidyverse-style syntax to wrangle the data.\n\n# Make the log version of criterion\nd &lt;- d |&gt;\n  mutate(log_gdp = log(rgdppc_2000))\n\n# Extract countries with GDP data\ndd &lt;- d |&gt;\n  filter(complete.cases(rgdppc_2000)) |&gt; \n  # Re-scale variables\n  mutate(log_gdp_std = log_gdp / mean(log_gdp), \n         rugged_std  = rugged / max(rugged))\n\nBefore we fit our first Bayesian models, let’s back track a bit and make our version of Figure 8.2. In the title, McElreath indicated it was a depiction of two linear regressions separated by whether the nations were African. A fairly simple way to make those plots is to simultaneously fit and plot the two regression models using OLS via the geom_smooth() function using the method = \"lm\" argument. After dividing the data with cont_africa, make each plot separately and then combine them with patchwork syntax.\n\nlibrary(ggrepel)\nlibrary(patchwork)\n\n# African nations\np1 &lt;- dd |&gt; \n  filter(cont_africa == 1) |&gt; \n  ggplot(aes(x = rugged_std, y = log_gdp_std)) +\n  geom_smooth(method = \"lm\", formula = y ~ x,\n              fill = palette_pander(n = 2)[1],\n              color = palette_pander(n = 2)[1]) +\n  geom_point(color = palette_pander(n = 2)[1]) +\n  geom_text_repel(data = dd |&gt; \n                    filter(cont_africa == 1) |&gt; \n                    filter(country %in% c(\"Lesotho\", \"Seychelles\")),  \n                  aes(label = country), \n                  size = 3, family = \"Times\", seed = 8) +\n  labs(x = \"ruggedness (standardized)\",\n       y = \"log GDP (as proportion of mean)\",\n       subtitle = \"African nations\")\n\n# Non-African nations\np2 &lt;- dd |&gt; \n  filter(cont_africa == 0) |&gt; \n  ggplot(aes(x = rugged_std, y = log_gdp_std)) +\n  geom_smooth(method = \"lm\", formula = y ~ x,\n              fill = palette_pander(n = 2)[2],\n              color = palette_pander(n = 2)[2]) +\n  geom_point(color = palette_pander(n = 2)[2]) +\n  geom_text_repel(data = dd |&gt; \n                    filter(cont_africa == 0) |&gt; \n                    filter(country %in% c(\"Switzerland\", \"Tajikistan\")),  \n                  aes(label = country), \n                  size = 3, family = \"Times\", seed = 8) +\n  xlim(0, 1) +\n  labs(x = \"ruggedness (standardized)\",\n       y = \"log GDP (as proportion of mean)\",\n       subtitle = \"Non-African nations\")\n\n# Combine\np1 + p2 + plot_annotation(title = \"Figure 8.2. Separate linear regressions inside and outside of Africa\")\n\n\n\n\n\n\n\n\nOur first Bayesian model will follow the form\n\\[\\begin{align*}\n\\text{log\\_gdp\\_std}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  & = \\alpha + \\beta \\left (\\text{rugged\\_std}_i - \\overline{\\text{rugged\\_std}} \\right ) \\\\\n\\alpha & \\sim \\operatorname{Normal}(1, 1) \\\\\n\\beta  & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nHere we compute \\(\\overline{\\text{rugged\\_std}}\\).\n\nmean(dd$rugged_std)\n\n[1] 0.2149601\n\n\nA naïve translation of McElreath’s rethinking code into a brms::brm() formula argument might be log_gdp_std ~ 1 + (rugged_std - 0.215 ). However, this kind of syntax will not work outside of the non-linear syntax. Our approach will be to make a mean-centered version of rugged_std.\n\ndd &lt;- dd |&gt;\n  mutate(rugged_std_c  = rugged_std - mean(rugged_std))\n\nNow fit the model.\n\nb8.1 &lt;- brm(\n  data = dd, \n  family = gaussian,\n  log_gdp_std ~ 1 + rugged_std_c,\n  prior = c(prior(normal(1, 1), class = Intercept),\n            prior(normal(0, 1), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 8,\n  sample_prior = T,\n  file = \"fits/b08.01\")\n\nDid you notice the sample_prior = T argument? Because of that, we can now use the prior_draws() function to help us plot the prior predictive distribution for m8.1 and make our version of the left panel of Figure 8.3.\n\nprior &lt;- prior_draws(b8.1)\n\nset.seed(8)\n\np1 &lt;- prior |&gt; \n  slice_sample(n = 50) |&gt; \n  rownames_to_column() |&gt; \n  expand_grid(rugged_std_c = c(-2, 2)) |&gt; \n  mutate(log_gdp_std = Intercept + b * rugged_std_c,\n         rugged_std  = rugged_std_c + mean(dd$rugged_std)) |&gt; \n  \n  ggplot(aes(x = rugged_std, y = log_gdp_std, group = rowname)) +\n  geom_hline(yintercept = range(dd$log_gdp_std), linetype = 2) +\n  geom_line(alpha = 0.4, color = palette_pander(n = 2)[2]) +\n  geom_abline(intercept = 1.3, slope = -0.6,\n              color = palette_pander(n = 2)[1], linewidth = 2) +\n  labs(x = \"ruggedness\",\n       y = \"log GDP (prop of mean)\",\n       subtitle = \"Intercept ~ dnorm(1, 1)\\nb ~ dnorm(0, 1)\") +\n  coord_cartesian(xlim = c(0, 1),\n                  ylim = c(0.5, 1.5))\n\np1\n\n\n\n\n\n\n\n\nToward the bottom of page 243, McElreath wrote: “The slope of such a line must be about \\(1.3 − 0.7 = 0.6\\), the difference between the maximum and minimum observed proportional log GDP.” The math appears backwards, there. Rather, the slope of our solid blue line is \\(0.7 - 1.3 = -0.6\\). But anyway, “under the \\(\\beta \\sim \\operatorname{Normal}(0, 1)\\) prior, more than half of all slopes will have [an] absolute value greater than \\(0.6\\)” (p. 244).\n\nprior |&gt;\n  summarise(a = sum(abs(b) &gt; abs(-0.6)) / nrow(prior))\n\n       a\n1 0.5465\n\n\nOur updated model is\n\\[\\begin{align*}\n\\text{log\\_gdp\\_std}_i & \\sim \\operatorname{Normal} (\\mu_i, \\sigma) \\\\\n\\mu_i  & = \\alpha + \\beta \\left (\\text{rugged\\_std}_i - \\overline{\\text{rugged\\_std}} \\right ) \\\\\n\\alpha & \\sim \\operatorname{Normal}(1, 0.1) \\\\\n\\beta  & \\sim \\operatorname{Normal}(0, 0.3) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nFit the model.\n\nb8.1b &lt;- brm(\n  data = dd, \n  family = gaussian,\n  log_gdp_std ~ 1 + rugged_std_c,\n  prior = c(prior(normal(1, 0.1), class = Intercept),\n            prior(normal(0, 0.3), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 8,\n  sample_prior = T,\n  file = \"fits/b08.01b\")\n\nNow we’ll use prior_draws(b8.1b) to make the left panel of Figure 8.3 and present both panels together with a little patchwork syntax.\n\nset.seed(8)\n\np2 &lt;- prior_draws(b8.1b) |&gt; \n  slice_sample(n = 50) |&gt; \n  rownames_to_column() |&gt; \n  expand_grid(rugged_std_c = c(-2, 2)) |&gt; \n  mutate(log_gdp_std = Intercept + b * rugged_std_c,\n         rugged_std  = rugged_std_c + mean(dd$rugged_std)) |&gt; \n  \n  ggplot(aes(x = rugged_std, y = log_gdp_std, group = rowname)) +\n  geom_hline(yintercept = range(dd$log_gdp_std), linetype = 2) +\n  geom_line(alpha = 0.4, color = palette_pander(n = 2)[2]) +\n  scale_y_continuous(\"\", breaks = NULL) +\n  labs(x = \"ruggedness\",\n       subtitle = \"Intercept ~ dnorm(1, 0.1)\\nb ~ dnorm(0, 0.3)\") +\n  coord_cartesian(xlim = c(0, 1),\n                  ylim = c(0.5, 1.5))\n\np1 + p2 + \n  plot_annotation(title = \"Simulating in search of reasonable priors for the terrain ruggedness example.\",\n                  theme = theme(plot.title = element_text(size = 12)))\n\n\n\n\n\n\n\n\nNow check the summary for b8.1b.\n\nprint(b8.1b)\n\n Family: gaussian \n  Links: mu = identity \nFormula: log_gdp_std ~ 1 + rugged_std_c \n   Data: dd (Number of observations: 170) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        1.00      0.01     0.98     1.02 1.00     4897     2971\nrugged_std_c     0.00      0.06    -0.11     0.12 1.00     3724     2517\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.14      0.01     0.12     0.15 1.00     4275     2799\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n8.1.2 Adding an indicator variable isn’t enough\nWhen you’d like to allow a model intercept and slope to differ by levels of a dichotomous variable, a typical approach is to use a 0/1 coded dummy variable. In this section and throughout much of the text, McElreath opted to highlight the index variable approach, instead. We’ll follow along. But if you’d like to practice using brms to fit interaction models with dummy variables, see Section 7.1 of my (2026b) translation of McElreath’s (2015) first edition or Chapters 7 and beyond in my (2026a) translation of Andrew Hayes’s (2017) text on mediation and moderation.\nMake the index variable.\n\ndd &lt;- dd |&gt; \n  mutate(cid = if_else(cont_africa == 1, \"1\", \"2\"))\n\nIn case you were curious, here’s a plot showing how the cid index works.\n\ndd |&gt; \n  mutate(cid = str_c(\"cid: \", cid)) |&gt; \n  arrange(cid, country) |&gt; \n  group_by(cid) |&gt; \n  mutate(rank = 1:n()) |&gt; \n  \n  ggplot(aes(x = cid, y = rank, label = country)) +\n  geom_text(size = 2, hjust = 0, family = \"Times\") +\n  scale_y_reverse() +\n  facet_wrap(~ cid, scales = \"free_x\") +\n  theme_void()\n\n\n\n\n\n\n\n\nIf you recall from the latter sections of Chapter 5, the conventional brms syntax can accommodate an index variable by simply suppressing the default intercept via the 0 + .... syntax. That will be our approach, here.\n\nb8.2 &lt;- brm(\n  data = dd, \n  family = gaussian,\n  log_gdp_std ~ 0 + cid + rugged_std_c,\n  prior = c(prior(normal(1, 0.1), class = b, coef = cid1),\n            prior(normal(1, 0.1), class = b, coef = cid2),\n            prior(normal(0, 0.3), class = b, coef = rugged_std_c),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 8,\n  file = \"fits/b08.02\")\n\nUse add_criterion() and loo_compare() to compare b8.1b and b8.2 with the WAIC.\n\nb8.1b &lt;- add_criterion(b8.1b, criterion = \"waic\")\nb8.2  &lt;- add_criterion(b8.2,  criterion = \"waic\")\n\nloo_compare(b8.1b, b8.2, criterion = \"waic\") |&gt; print(simplify = F)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb8.2     0.0       0.0   126.3       7.4          4.0    0.8    -252.5   14.8 \nb8.1b  -31.9       7.3    94.4       6.5          2.6    0.3    -188.8   13.0 \n\n\nHere are the WAIC weights.\n\nmodel_weights(b8.1b, b8.2, weights = \"waic\") |&gt; round(digits = 3)\n\nb8.1b  b8.2 \n    0     1 \n\n\nHere is the summary for the model with all the weight, b8.2.\n\nprint(b8.2)\n\n Family: gaussian \n  Links: mu = identity \nFormula: log_gdp_std ~ 0 + cid + rugged_std_c \n   Data: dd (Number of observations: 170) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ncid1             0.88      0.02     0.85     0.91 1.00     4382     3026\ncid2             1.05      0.01     1.03     1.07 1.00     4403     3007\nrugged_std_c    -0.05      0.05    -0.13     0.04 1.00     3487     2953\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.11      0.01     0.10     0.13 1.00     4274     2892\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow extract the posterior draws, make a difference score for the two intercepts, and use tidybayes::qi() to compute the percentile-based 89% intervals for the difference.\n\npost &lt;- as_draws_df(b8.2) |&gt; \n  mutate(diff = b_cid1 - b_cid2)\n\nlibrary(tidybayes)\n\nqi(post$diff, .width = 0.89)\n\n           [,1]       [,2]\n[1,] -0.1991936 -0.1377515\n\n\nNow it’s time to use fitted() to prepare to plot the implications of the model in Figure 8.4.\n\nnd &lt;- crossing(cid        = 1:2,\n               rugged_std = seq(from = -0.2, to = 1.2, length.out = 30)) |&gt; \n  mutate(rugged_std_c = rugged_std - mean(dd$rugged_std))\n\nf &lt;- fitted(b8.2, \n            newdata = nd,\n            probs = c(0.015, 0.985)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt;\n  mutate(cont_africa = ifelse(cid == 1, \"Africa\", \"not Africa\"))\n\n# What did we do?\nhead(f)\n\n   Estimate  Est.Error      Q1.5     Q98.5 cid   rugged_std rugged_std_c cont_africa\n1 0.8996118 0.02344083 0.8507620 0.9512432   1 -0.200000000   -0.4149601      Africa\n2 0.8973828 0.02191910 0.8514090 0.9455349   1 -0.151724138   -0.3666842      Africa\n3 0.8951538 0.02052108 0.8518612 0.9400632   1 -0.103448276   -0.3184083      Africa\n4 0.8929247 0.01927371 0.8518968 0.9353160   1 -0.055172414   -0.2701325      Africa\n5 0.8906957 0.01820799 0.8512457 0.9305453   1 -0.006896552   -0.2218566      Africa\n6 0.8884667 0.01735740 0.8502807 0.9264986   1  0.041379310   -0.1735808      Africa\n\n\nBehold our Figure 8.4.\n\ndd |&gt;\n  mutate(cont_africa = ifelse(cont_africa == 1, \"Africa\", \"not Africa\")) |&gt;\n  \n  ggplot(aes(x = rugged_std, fill = cont_africa, color = cont_africa)) +\n  geom_smooth(data = f,\n              aes(y = Estimate, ymin = Q1.5, ymax = Q98.5),\n              stat = \"identity\",\n              alpha = 1/4, linewidth = 1/2) +\n  geom_point(aes(y = log_gdp_std),\n             size = 2/3) +\n  scale_fill_pander() +\n  scale_colour_pander() +\n  labs(x = \"ruggedness (standardized)\",\n       y = \"log GDP (as proportion of mean)\",\n       subtitle = \"b8.2\") +\n  coord_cartesian(xlim = c(0, 1)) +\n  theme(legend.background = element_blank(),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.67, 0.93),\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\nThough adding our index variable cid to b8.2 allowed us to give the African nations a different intercept than the non-African nations, it did nothing for the slope. We need a better method.\n\n8.1.2.1 Rethinking: Why 97%?\nDid you notice the probs = c(0.015, 0.985) argument in our fitted() code, above? This is one of those rare moments when we went along with McElreath and used intervals other than the conventional 95%.\n\nIn the code block just above, and therefore also in Figure 8.4, I used 97% intervals of the expected mean. This is a rather non-standard percentile interval. So why use 97%? In this book, [McElreath used] non-standard percents to constantly remind the reader that conventions like 95% and 5% are arbitrary. Furthermore, boundaries are meaningless. There is continuous change in probability as we move away from the expected value. So one side of the boundary is almost equally probable as the other side. (p. 247)\n\nBuilding off of McElreath’s “boundaries are meaningless” point, here we use a combination of summary = F within fitted() and a little tidybayes::stat_lineribbon() magic to re-imagine Figure 8.4. This time we use a sequence of overlapping semitransparent credible intervals to give the posterior a 3D-like appearance.\n\nfitted(b8.2, \n       newdata = nd,\n       summary = F) |&gt;\n  data.frame() |&gt;\n  pivot_longer(everything()) |&gt; \n  bind_cols(expand_grid(draws = 1:4000, nd)) |&gt;\n  mutate(cont_africa = ifelse(cid == 1, \"Africa\", \"not Africa\")) |&gt; \n  \n  ggplot(aes(x = rugged_std, y = value, fill = cont_africa, color = cont_africa)) +\n  stat_lineribbon(.width = seq(from = 0.03, to = 0.99, by = 0.03),\n                  alpha = 0.1, linewidth = 0) +\n  geom_point(data = dd |&gt; \n               mutate(cont_africa = ifelse(cont_africa == 1, \"Africa\", \"not Africa\")),\n             aes(y = log_gdp_std),\n             size = 2/3) +\n  scale_fill_pander() +\n  scale_colour_pander() +\n  labs(x = \"ruggedness (standardized)\",\n       y = \"log GDP (as proportion of mean)\",\n       subtitle = \"b8.2\") +\n  coord_cartesian(xlim = c(0, 1)) +\n  theme(legend.background = element_blank(),\n        legend.direction = \"horizontal\",\n        legend.position = c(0.67, 0.93),\n        legend.title = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n8.1.3 Adding an interaction does work\nThe 0 + ... syntax works fine when we just want to use an index variable to fit a model with multiple intercepts, this approach will not work for fitting brms models that apply the index variable to slopes. Happily, we have alternatives. If we’d like to use the cid index to make intercepts and slopes as in McElreath’s m8.3, we can use the brms non-linear syntax (Bürkner, 2022b). Here it is for b8.3.\n\nb8.3 &lt;- brm(\n  data = dd, \n  family = gaussian,\n  bf(log_gdp_std ~ 0 + a + b * rugged_std_c, \n     a ~ 0 + cid, \n     b ~ 0 + cid,\n     nl = TRUE),\n  prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),\n            prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),\n            prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),\n            prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 8,\n  file = \"fits/b08.03\")\n\nCheck the summary of the marginal distributions.\n\nprint(b8.3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: log_gdp_std ~ 0 + a + b * rugged_std_c \n         a ~ 0 + cid\n         b ~ 0 + cid\n   Data: dd (Number of observations: 170) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cid1     0.89      0.02     0.86     0.92 1.00     5144     3169\na_cid2     1.05      0.01     1.03     1.07 1.00     5082     3190\nb_cid1     0.13      0.08    -0.02     0.28 1.00     4563     3181\nb_cid2    -0.14      0.06    -0.25    -0.03 1.00     5140     3216\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.11      0.01     0.10     0.12 1.00     5023     2997\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSuccess! Our results look just like McElreath’s. Now make haste with add_criterion() so we can compare the models by the PSIS-LOO-CV.\n\nb8.1b &lt;- add_criterion(b8.1b, criterion = \"loo\")\nb8.2  &lt;- add_criterion(b8.2,  criterion = \"loo\")\nb8.3  &lt;- add_criterion(b8.3,  criterion = c(\"loo\", \"waic\"))\n\nloo_compare(b8.1b, b8.2, b8.3, criterion = \"loo\") |&gt; print(simplify = F)\n\n      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nb8.3     0.0       0.0   129.5      7.3         5.0    0.9   -259.1   14.7  \nb8.2    -3.3       3.2   126.3      7.4         4.0    0.8   -252.5   14.8  \nb8.1b  -35.1       7.5    94.4      6.5         2.6    0.3   -188.8   13.0  \n\n\nHere are the LOO weights.\n\nmodel_weights(b8.1b, b8.2, b8.3, weights = \"loo\") |&gt; round(digits = 2)\n\nb8.1b  b8.2  b8.3 \n 0.00  0.04  0.96 \n\n\nWe can get a Pareto \\(k\\) diagnostic plot for b8.3 by feeding the results of the loo() function into plot().\n\nloo(b8.3) |&gt; \n  plot()\n\n\n\n\n\n\n\n\nAs in the text (p. 428), our results suggest one the cases had Pareto \\(k\\) value just above the 0.5 threshold. We can confirm by rank ordering them and taking a look at the top values.\n\ntibble(k   = b8.3$criteria$loo$diagnostics$pareto_k,\n       row = 1:170) |&gt; \n  arrange(desc(k))\n\n# A tibble: 170 × 2\n       k   row\n   &lt;dbl&gt; &lt;int&gt;\n 1 0.615   145\n 2 0.558    93\n 3 0.363    27\n 4 0.339   133\n 5 0.274     8\n 6 0.265    35\n 7 0.252   144\n 8 0.223    63\n 9 0.219   167\n10 0.218   117\n# ℹ 160 more rows\n\n\nSo the largest one is just above 0.5, which isn’t all that bad.\n\n8.1.3.1 Bonus: Give me Student-\\(t\\)\nMcElreath remarked: “This is possibly a good context for robust regression, like the Student-t regression we did in Chapter 7” (p. 249). Let’s practice fitting the alternative model using the Student-\\(t\\) likelihood for which \\(\\nu = 2\\).\n\nb8.3t &lt;- brm(\n  data = dd,\n  family = student,\n  bf(log_gdp_std ~ 0 + a + b * rugged_std_c,\n     a ~ 0 + cid, \n     b ~ 0 + cid,\n     nu = 2,\n     nl = TRUE),\n  prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),\n            prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),\n            prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),\n            prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 8,\n  file = \"fits/b08.03t\")\n\nUse the LOO to compare this with the Gaussian model.\n\nb8.3t &lt;- add_criterion(b8.3t, criterion = c(\"loo\", \"waic\"))\n\nloo_compare(b8.3, b8.3t, criterion = \"loo\") |&gt; print(simplify = F)\n\n      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nb8.3     0.0       0.0   129.5      7.3         5.0    0.9   -259.1   14.7  \nb8.3t  -19.0       2.7   110.5      8.8         6.3    0.7   -221.0   17.5  \n\n\nThe PSIS-LOO-CV comparison suggests the robust Student-\\(t\\) model might be overfit. Just for kicks, we might make our own diagnostic plot to compare the two likelihoods by the Pareto \\(k\\) values. To get a nice fine-grain sense of the distributions, we’ll employ the handy tidybayes::stat_dots() function which will display each value as an individual dot.\n\ntibble(Normal      = b8.3$criteria$loo$diagnostics$pareto_k,\n       `Student-t` = b8.3t$criteria$loo$diagnostics$pareto_k) |&gt; \n  pivot_longer(everything(),\n               values_to = \"pareto_k\") |&gt; \n  \n  ggplot(aes(x = pareto_k, y = name)) +\n  geom_vline(xintercept = 0.5, color = palette_pander(n = 5)[5], linetype = 2) +\n  stat_dots(slab_fill = palette_pander(n = 4)[4], \n            slab_color = palette_pander(n = 4)[4]) + \n  annotate(geom = \"text\",\n           x = 0.48, y = 1.5, label = \"threshold\", \n           angle = 90, color = palette_pander(n = 5)[5], family = \"Times\") +\n  ylab(NULL) +\n  coord_cartesian(ylim = c(1.5, 2.4))\n\n\n\n\n\n\n\n\nTo close this exercise out, compare the \\(\\alpha\\) and \\(\\beta\\) parameters of the two models using fixef().\n\nfixef(b8.3) |&gt; round(digits = 2)\n\n       Estimate Est.Error  Q2.5 Q97.5\na_cid1     0.89      0.02  0.86  0.92\na_cid2     1.05      0.01  1.03  1.07\nb_cid1     0.13      0.08 -0.02  0.28\nb_cid2    -0.14      0.06 -0.25 -0.03\n\nfixef(b8.3t) |&gt; round(digits = 2)\n\n       Estimate Est.Error  Q2.5 Q97.5\na_cid1     0.87      0.02  0.83  0.90\na_cid2     1.05      0.01  1.02  1.07\nb_cid1     0.13      0.09 -0.01  0.32\nb_cid2    -0.21      0.07 -0.33 -0.08\n\n\n\n\n\n8.1.4 Plotting the interaction\nThe code for Figure 8.5 is a minor extension of the code we used for Figure 8.4. Other than which fit we use, the code we use for fitted() is the same for both plots. Two of the largest changes are the addition of labels with ggrepel::geom_text_repel() and using facet_wrap() to split the plot into two panels.\n\ncountries &lt;- c(\"Equatorial Guinea\", \"South Africa\", \"Seychelles\", \"Swaziland\", \"Lesotho\", \"Rwanda\", \"Burundi\", \"Luxembourg\", \"Greece\", \"Switzerland\", \"Lebanon\", \"Yemen\", \"Tajikistan\", \"Nepal\")\n\nf &lt;- fitted(b8.3, \n            # We already defined `nd`, above\n            newdata = nd,\n            probs = c(0.015, 0.985)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt;\n  mutate(cont_africa = ifelse(cid == 1, \"African nations\", \"Non-African nations\"))\n\ndd |&gt;\n  mutate(cont_africa = ifelse(cont_africa == 1, \"African nations\", \"Non-African nations\")) |&gt;\n  \n  ggplot(aes(x = rugged_std, y = log_gdp_std, fill = cont_africa, color = cont_africa)) +\n  geom_smooth(data = f,\n              aes(y = Estimate, ymin = Q1.5, ymax = Q98.5),\n              stat = \"identity\",\n              alpha = 1/4, linewidth = 1/2) +\n  geom_text_repel(data = dd |&gt;\n                    mutate(cont_africa = ifelse(cont_africa == 1, \"African nations\", \"Non-African nations\")) |&gt;\n                    filter(country %in% countries),  \n                  aes(label = country), \n                  size = 3, seed = 8, \n                  segment.color = \"grey25\", min.segment.length = 0) +\n  geom_point(aes(y = log_gdp_std),\n             size = 2/3) +\n  scale_fill_pander() +\n  scale_colour_pander() +\n  labs(x = \"ruggedness (standardized)\",\n       y = \"log GDP (as proportion of mean)\") +\n  coord_cartesian(xlim = c(0, 1)) +\n  facet_wrap(~ cont_africa) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n“Finally, the slope reverses direction inside and outside of Africa. And because we achieved this inside a single model, we could statistically evaluate the value of this reversal” (p. 250).\n\n8.1.4.1 Rethinking: All Greek to me\n\nWe use these Greek symbols \\(\\alpha\\) and \\(\\beta\\) because it is conventional. They don’t have special meanings. If you prefer some other Greek symbol like \\(\\omega\\)–why should \\(\\alpha\\) get all the attention?–feel free to use that instead. It is conventional to use Greek letters for unobserved variables (parameters) and Roman letters for observed variables (data). That convention does have some value, because it helps others read your models. But breaking the convention is not an error, and sometimes it is better to use a familiar Roman symbol than an unfamiliar Greek one like \\(\\xi\\) or \\(\\zeta\\). If your readers cannot say the symbol’s name, it could make understanding the model harder. (p. 249)\n\nThis topic is near and dear my heart. In certain areas of psychology, people presume symbols like \\(\\beta\\) and \\(b\\) have universal meanings. This presumption is a mistake and will not serve one well beyond a narrow section of the scientific literature. My recommendation is whatever notation you fancy in a given publication, clearly define your terms, especially if there could be any confusion over whether your results are standardized or not.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conditional Manatees</span>"
    ]
  },
  {
    "objectID": "08.html#symmetry-of-interactions",
    "href": "08.html#symmetry-of-interactions",
    "title": "8  Conditional Manatees",
    "section": "8.2 Symmetry of interactions",
    "text": "8.2 Symmetry of interactions\nIf you’re unfamiliar with Buridan’s ass, here’s a brief clip to catch up up to speed. With that ass still on your mind, recall the model for \\(\\mu_i\\) from the last example,\n\\[\\mu_i = \\alpha_{\\text{cid}[i]} + \\beta_{\\text{cid}[i]} \\left (\\text{rugged\\_std}_i - \\overline{\\text{rugged\\_std}} \\right ).\\]\nWith this model, it is equally true that that slope is conditional on the intercept as it is that the intercept is conditional on the slope. Another way to express the model is\n\\[\\begin{align*}\n\\mu_i & = \\underbrace{(2 - \\text{cid}_{i}) \\left (\\alpha_1 + \\beta_1 \\left [\\text{rugged\\_std}_i - \\overline{\\text{rugged\\_std}} \\right ] \\right )}_{\\text{cid}[i] = 1} \\\\\n      & \\;\\;\\; + \\underbrace{(\\text{cid}_{i} - 1) \\left (\\alpha_2 + \\beta_2 \\left [\\text{rugged\\_std}_i - \\overline{\\text{rugged\\_std}} \\right ] \\right )}_{\\text{cid}[i] = 2},\n\\end{align*}\\]\nwhere the first term vanishes when \\(\\text{cid}_i = 2\\) and the second term vanishes when \\(\\text{cid}_i = 1\\). In contrast to the plots above, we can re-express this equation as saying “The association of being in Africa with log GDP depends upon terrain ruggedness” (p. 251, emphasis in the original). Here we follow McElreath’s Figure 8.6 and plot the difference between a nation in Africa and outside Africa, conditional on ruggedness.\n\nfitted(b8.3, \n       newdata = nd,\n       summary = F) |&gt;\n  data.frame() |&gt;\n  pivot_longer(everything()) |&gt; \n  bind_cols(expand_grid(draws = 1:4000, nd)) |&gt; \n  select(-name) |&gt; \n  pivot_wider(names_from = cid, values_from = value) |&gt; \n  mutate(delta = `1` - `2`) |&gt; \n  \n  ggplot(aes(x = rugged_std, y = delta)) +\n  stat_lineribbon(.width = 0.95, fill = palette_pander(n = 8)[8], alpha = 3/4) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  annotate(geom = \"text\",\n           x = 0.2, y = 0,\n           label = \"Africa higher GDP\\nAfrica lower GDP\",\n           family = \"Times\") +\n  labs(x = \"ruggedness (standardized)\",\n       y = \"expected difference log GDP\") +\n  coord_cartesian(xlim = c(0, 1),\n                  ylim = c(-0.3, 0.2))\n\n\n\n\n\n\n\n\n\nThis perspective on the GDP and terrain ruggedness is completely consistent with the previous perspective. It’s simultaneously true in these data (and with this model) that (1) the influence of ruggedness depends upon continent and (2) the influence of continent depends upon ruggedness.\n\n\nSimple interactions are symmetric, just like the choice facing Buridan’s ass. Within the model, there’s no basis to prefer one interpretation over the other, because in fact they are the same interpretation. But when we reason causally about models, our minds tend to prefer one interpretation over the other, because it’s usually easier to imagine manipulating one of the predictor variables instead of the other. (pp. 251–252)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conditional Manatees</span>"
    ]
  },
  {
    "objectID": "08.html#continuous-interactions",
    "href": "08.html#continuous-interactions",
    "title": "8  Conditional Manatees",
    "section": "8.3 Continuous interactions",
    "text": "8.3 Continuous interactions\n\nIt’s one thing to make a slope conditional upon a category. In such a context, the model reduces to estimating a different slope for each category. But it’s quite a lot harder to understand that a slope varies in a continuous fashion with a continuous variable. Interpretation is much harder in this case, even though the mathematics of the model are essentially the same. (p. 252, emphasis in the original)\n\n\n8.3.1 A winter flower\nLook at the tulips data, which were adapted from Grafen & Hails (2002).\n\ndata(tulips, package = \"rethinking\")\nd &lt;- tulips\nrm(tulips)\n\nglimpse(d)\n\nRows: 27\nColumns: 4\n$ bed    &lt;fct&gt; a, a, a, a, a, a, a, a, a, b, b, b, b, b, b, b, b, b, c, c, c, c, c, c, c, c, c\n$ water  &lt;int&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3\n$ shade  &lt;int&gt; 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3\n$ blooms &lt;dbl&gt; 0.00, 0.00, 111.04, 183.47, 59.16, 76.75, 224.97, 83.77, 134.95, 80.10, 85.95, 19.87, 213.13, 124.99, 65.48, 361.…\n\n\n\n\n8.3.2 The models\nWrangle a little.\n\nd &lt;- d |&gt; \n  mutate(blooms_std = blooms / max(blooms),\n         water_cent = water - mean(water),\n         shade_cent = shade - mean(shade))\n\nWith the variables in hand, the basic model is \\(B = f(W, S)\\), where \\(B\\) = blooms, \\(W\\) = water, \\(S\\) = shade, and \\(f(\\cdot)\\) indicates a function. We can also express this as \\(W \\rightarrow B \\leftarrow S\\). Neither expression clarifies whether the effects of \\(W\\) and \\(S\\) are additive or conditional on each other in some way. We might express an unconditional (additive) model as\n\\[\\begin{align*}\n\\text{blooms\\_std}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\text{water\\_cent}_i + \\beta_2 \\text{shade\\_cent}_i \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0.5, 1) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1),\n\\end{align*}\\]\nwhere \\(\\text{water\\_cent}_i = \\left (\\text{water}_i - \\overline{\\text{water}}  \\right )\\) and \\(\\text{shade\\_cent}_i = \\left (\\text{shade}_i - \\overline{\\text{shade}}  \\right )\\). Even though “the intercept \\(\\alpha\\) must be greater than zero and less than one,… this prior assigns most of the probability outside that range” (p. 254).\n\nset.seed(8)\n\ntibble(a = rnorm(1e4, mean = 0.5, sd = 1)) |&gt; \n  summarise(proportion_outside_of_the_range = sum(a &lt; 0 | a &gt; 1) / n())\n\n# A tibble: 1 × 1\n  proportion_outside_of_the_range\n                            &lt;dbl&gt;\n1                           0.621\n\n\nTightening up the prior to \\(\\operatorname{Normal}(0, 0.25)\\) helps.\n\nset.seed(8)\n\ntibble(a = rnorm(1e4, mean = 0.5, sd = 0.25)) |&gt; \n  summarise(proportion_outside_of_the_range = sum(a &lt; 0 | a &gt; 1) / n())\n\n# A tibble: 1 × 1\n  proportion_outside_of_the_range\n                            &lt;dbl&gt;\n1                          0.0501\n\n\nHere are the ranges for our two predictors.\n\nrange(d$water_cent)\n\n[1] -1  1\n\nrange(d$shade_cent)\n\n[1] -1  1\n\n\nPutting the same \\(\\operatorname{Normal}(0, 0.25)\\) prior on each would indicate a .95 probability each coefficient would be within -0.5 to 0.5. Since the total range for both is \\(1 - (-1) = 2\\), that would imply either could account for the full range of blooms_std because \\(0.5 \\cdot 2 = 1\\), which is the full range of blooms_std. Our first model, then, will be\n\\[\\begin{align*}\n\\text{blooms\\_std}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\text{water\\_cent}_i + \\beta_2 \\text{shade\\_cent}_i \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0.5, 0.25) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.25) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.25) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nFit the model.\n\nb8.4 &lt;- brm(\n  data = d,\n  family = gaussian,\n  blooms_std ~ 1 + water_cent + shade_cent,\n  prior = c(prior(normal(0.5, 0.25), class = Intercept),\n            prior(normal(0, 0.25), class = b, coef = water_cent),\n            prior(normal(0, 0.25), class = b, coef = shade_cent),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 8,\n  file = \"fits/b08.04\")\n\nCheck the model summary.\n\nprint(b8.4)\n\n Family: gaussian \n  Links: mu = identity \nFormula: blooms_std ~ 1 + water_cent + shade_cent \n   Data: d (Number of observations: 27) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      0.36      0.03     0.29     0.42 1.00     3950     2686\nwater_cent     0.20      0.04     0.13     0.29 1.00     4166     2584\nshade_cent    -0.11      0.04    -0.20    -0.03 1.00     3862     2958\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.18      0.03     0.13     0.24 1.00     3330     3167\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nUsing the \\(\\gamma\\) notation, we can express an interaction between water_cent and shade_cent by\n\\[\\begin{align*}\n\\mu_i   & = \\alpha + \\color{#009E73}{\\gamma_{1, i}} \\text{water\\_cent}_i + \\beta_2 \\text{shade\\_cent}_i \\\\\n\\color{#009E73}{\\gamma_{1, i}} & \\color{#009E73}= \\color{#009E73}{\\beta_1 + \\beta_3 \\text{shade\\_cent}_i},\n\\end{align*}\\]\nwhere both \\(\\mu_i\\) and \\(\\gamma_{1, i}\\) get a linear model. We could do the converse by switching the positions of water_cent and shade_cent. If we substitute the equation for \\(\\gamma_{1, i}\\) into the equation for \\(\\mu_i\\), we get\n\\[\\begin{align*}\n\\mu_i   & = \\alpha + \\color{#009E73}{\\underbrace{(\\beta_1 + \\beta_3 \\text{shade\\_cent}_i)}_{\\gamma_{1, i}}} \\text{water\\_cent}_i + \\beta_2 \\text{shade\\_cent}_i \\\\\n        & = \\alpha + \\color{#009E73}{\\beta_1}  \\text{water\\_cent}_i + (\\color{#009E73}{\\beta_3 \\text{shade\\_cent}_i} \\cdot \\text{water\\_cent}_i) + \\beta_2 \\text{shade\\_cent}_i \\\\\n        & = \\alpha + \\color{#009E73}{\\beta_1} \\text{water\\_cent}_i + \\beta_2 \\text{shade\\_cent}_i  + \\color{#009E73}{\\beta_3} (\\color{#009E73}{\\text{shade\\_cent}_i} \\cdot \\text{water\\_cent}_i),\n\\end{align*}\\]\nwhere \\(\\beta_3\\) is the interaction term which makes water_cent and shade_cent conditional on each other. If we use the same priors as before, we might write the full equation for our interaction model as\n\\[\\begin{align*}\n\\text{blooms\\_std}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\color{#009E73}{\\beta_1} \\text{water\\_cent}_i + \\beta_2 \\text{shade\\_cent}_i + \\color{#009E73}{\\beta_3 \\text{shade\\_cent}_i} \\cdot \\text{water\\_cent}_i\\\\\n\\alpha  & \\sim \\operatorname{Normal}(0.5, 0.25) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.25) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.25) \\\\\n\\beta_3 & \\sim \\operatorname{Normal}(0, 0.25) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nFit the model.\n\nb8.5 &lt;- brm(\n  data = d,\n  family = gaussian,\n  blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent,\n  prior = c(prior(normal(0.5, 0.25), class = Intercept),\n            prior(normal(0, 0.25), class = b, coef = water_cent),\n            prior(normal(0, 0.25), class = b, coef = shade_cent),\n            prior(normal(0, 0.25), class = b, coef = \"water_cent:shade_cent\"),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 8,\n  file = \"fits/b08.05\")\n\nCheck the summary.\n\nprint(b8.5)\n\n Family: gaussian \n  Links: mu = identity \nFormula: blooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent \n   Data: d (Number of observations: 27) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept                 0.36      0.03     0.31     0.41 1.00     5489     2574\nwater_cent                0.21      0.03     0.14     0.27 1.00     4706     3018\nshade_cent               -0.11      0.03    -0.18    -0.04 1.00     4387     2549\nwater_cent:shade_cent    -0.14      0.04    -0.22    -0.06 1.00     4564     3043\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.14      0.02     0.11     0.19 1.00     3012     3154\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe row for the interaction term, water_cent:shade_cent, indicates the marginal posterior is negative.\n\n\n8.3.3 Plotting posterior predictions\nNow we’re ready for the top row of Figure 8.8. Here’s our variation on McElreath’s triptych loop code, adjusted for brms and ggplot2.\n\n# Loop over values of `water_c` and plot predictions\nfor(s in -1:1) {\n  \n  # Define the subset of the original data\n  dt &lt;- d[d$shade_cent == s, ]\n  # Defining our new data\n  nd &lt;- tibble(shade_cent = s, water_cent = c(-1, 1))\n  # Use our sampling skills, like before\n  f &lt;- fitted(b8.4, \n              newdata = nd,\n              summary = F) |&gt;\n    data.frame() |&gt;\n    set_names(\"-1\", \"1\") |&gt; \n    slice_sample(n = 20) |&gt; \n    mutate(row = 1:n()) |&gt; \n    pivot_longer(-row,\n                 names_to = \"water_cent\",\n                 values_to = \"blooms_std\") |&gt; \n    mutate(water_cent = as.double(water_cent))\n  \n  # Specify our custom plot\n  fig &lt;- ggplot(data = dt,\n                aes(x = water_cent, y = blooms_std)) +\n    geom_line(data = f,\n              aes(group = row),\n              color = palette_pander(n = 6)[6], alpha = 1/5, linewidth = 1/2) +\n    geom_point(color = palette_pander(n = 6)[6]) +\n    scale_x_continuous(\"Water (centered)\", breaks = c(-1, 0, 1)) +\n    labs(y = \"Blooms (standardized)\",\n         title = paste(\"Shade (centered) =\", s)) +\n    coord_cartesian(xlim = c(-1, 1), \n                    ylim = c(0, 1))\n  \n  # Plot that joint\n  plot(fig)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe don’t necessarily need a loop. We can achieve all of McElreath’s Figure 8.8 with fitted(), some data wrangling, and a little help from ggplot2::facet_grid().\n\n# Augment the data\npoints &lt;- d |&gt;\n  expand_grid(fit = c(\"b8.4\", \"b8.5\")) |&gt;\n  mutate(x_grid = str_c(\"shade_cent = \", shade_cent),\n         y_grid = fit)\n\n# Redefine `nd`\nnd &lt;- crossing(shade_cent = -1:1, \n               water_cent = c(-1, 1))\n\n# Use `fitted()`\nset.seed(8)\n\nrbind(fitted(b8.4, newdata = nd, summary = F, ndraws = 20),\n      fitted(b8.5, newdata = nd, summary = F, ndraws = 20)) |&gt;\n  # Wrangle\n  data.frame() |&gt;\n  set_names(mutate(nd, name = str_c(shade_cent, water_cent, sep = \"_\")) |&gt; pull()) |&gt;\n  mutate(row = 1:n(),\n         fit = rep(c(\"b8.4\", \"b8.5\"), each = n() / 2)) |&gt;\n  pivot_longer(-c(row:fit), values_to = \"blooms_std\") |&gt;\n  separate(name, into = c(\"shade_cent\", \"water_cent\"), sep = \"_\") |&gt;\n  mutate(shade_cent = shade_cent |&gt; as.double(),\n         water_cent = water_cent |&gt; as.double()) |&gt;\n  # These will come in handy for `ggplot2::facet_grid()`\n  mutate(x_grid = str_c(\"shade_cent = \", shade_cent),\n         y_grid = fit) |&gt;\n  \n  # Plot!\n  ggplot(aes(x = water_cent, y = blooms_std)) +\n  geom_line(aes(group = row),\n            color = palette_pander(n = 6)[6], alpha = 1/5, linewidth = 1/2) +\n  geom_point(data = points,\n             color = palette_pander(n = 6)[6]) +\n  scale_x_continuous(\"Water (centered)\", breaks = c(-1, 0, 1)) +\n  scale_y_continuous(\"Blooms (standardized)\", breaks = c(0, 0.5, 1)) +\n  ggtitle(\"Posterior predicted blooms\") +\n  coord_cartesian(xlim = c(-1, 1),\n                  ylim = c(0, 1)) +\n  facet_grid(y_grid ~ x_grid) +\n  theme(strip.background = element_rect(fill = alpha(palette_pander(n = 2)[2], 1/3)))\n\n\n\n\n\n\n\n\n\n\n8.3.4 Plotting prior predictions\nIn some of the earlier models in this book, we used the sample_prior = T argument within brm() to simultaneously sample from the posterior and prior distributions. As far as the priors go, we could then retrieve their draws from the prior_samples() function and plot as desired. And to be clear, we could use this method to remake Figure 8.8 with our brms fits.\nHowever, a limitation of the sample_prior = T method is it will not work if you’re trying to use a fitted()-oriented work flow. Happily, we have an alternative. Within brm(), set sample_prior = \"only\". The resulting fit object will be based solely on the priors. Here we’ll use this method within update() to refit the last two models.\n\nb8.4p &lt;- update(\n  b8.4,\n  sample_prior = \"only\",\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 8,\n  file = \"fits/b08.04p\")\n\nb8.5p &lt;- update(\n  b8.5,\n  sample_prior = \"only\",\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 8,\n  file = \"fits/b08.05p\")\n\nNow we can insert b8.4p and b8.5p into the fitted() function and plot the prior predictions we desire.\n\nset.seed(8)\n\nrbind(fitted(b8.4p, newdata = nd, summary = F, ndraws = 20),\n      fitted(b8.5p, newdata = nd, summary = F, ndraws = 20)) |&gt;\n  # Wrangle\n  data.frame() |&gt;\n  set_names(mutate(nd, name = str_c(shade_cent, water_cent, sep = \"_\")) |&gt; pull()) |&gt;\n  mutate(row = rep(1:20, times = 2),\n         fit = rep(c(\"b8.4\", \"b8.5\"), each = n() / 2)) |&gt;\n  pivot_longer(-c(row:fit), values_to = \"blooms_std\") |&gt;\n  separate(name, into = c(\"shade_cent\", \"water_cent\"), sep = \"_\") |&gt;\n  mutate(shade_cent = shade_cent |&gt; as.double(),\n         water_cent = water_cent |&gt; as.double()) |&gt;\n  # These will come in handy for `ggplot2::facet_grid()`\n  mutate(x_grid = str_c(\"shade_cent = \", shade_cent),\n         y_grid = fit) |&gt;\n  \n  # Plot!\n  ggplot(aes(x = water_cent, y = blooms_std, group = row)) +\n  geom_hline(yintercept = 0:1, linetype = 2) +\n  geom_line(aes(alpha = row == 1, size = row == 1),\n            color = palette_pander(n = 6)[6]) +\n  scale_x_continuous(\"Water (centered)\", breaks = c(-1, 0, 1)) +\n  scale_y_continuous(\"Blooms (standardized)\", breaks = c(0, 0.5, 1)) +\n  scale_alpha_manual(values = c(1/3, 1)) +\n  scale_size_manual(values = c(1/2, 1)) +\n  ggtitle(\"Prior predicted blooms\") +\n  coord_cartesian(xlim = c(-1, 1),\n                  ylim = c(-0.5, 1.5)) +\n  facet_grid(y_grid ~ x_grid) +\n  theme(legend.position = \"none\",\n        strip.background = element_rect(fill = alpha(palette_pander(n = 2)[2], 1/3)))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nIt was the aes() statement within geom_line() and the scale_size_manual() and scale_alpha_manual() lines that followed that allowed us to bold the one line in each panel. Relatedly, it was the set.seed() line at the top of the code block that made the results reproducible.\n\nWhat can we say about these priors, overall? They are harmless, but only weakly realistic. Most of the lines stay within the valid outcome space. But silly trends are not rare. We could do better. We could also do a lot worse, such as flat priors which would consider plausible that even a tiny increase in shade would kill all the tulips. If you displayed these priors to your colleagues, a reasonable summary might be, “These priors contain no bias towards positive or negative effects, and at the same time they very weakly bound the effects to realistic ranges.” (p. 260)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conditional Manatees</span>"
    ]
  },
  {
    "objectID": "08.html#summary-bonus-conditional_effects",
    "href": "08.html#summary-bonus-conditional_effects",
    "title": "8  Conditional Manatees",
    "section": "8.4 Summary Bonus: conditional_effects()",
    "text": "8.4 Summary Bonus: conditional_effects()\nThe brms package includes the conditional_effects() function as a convenient way to look at simple effects and two-way interactions. Recall the simple univariable model, b8.1b.\n\nb8.1b$formula\n\nlog_gdp_std ~ 1 + rugged_std_c \n\n\nWe can look at the regression line and its percentile-based intervals like so.\n\nconditional_effects(b8.1b)\n\n\n\n\n\n\n\n\nIf we feed the conditional_effects() output into the plot() function with a points = T argument, we can add the original data to the figure.\n\nconditional_effects(b8.1b) |&gt; \n  plot(points = T)\n\n\n\n\n\n\n\n\nWe can further customize the plot. For example, we can replace the intervals with a spaghetti plot. While we’re at it, we can use point_args to adjust the geom_jitter() parameters and line_args() to adjust the line marking off the posterior median.\n\nconditional_effects(b8.1b,\n                    spaghetti = T, \n                    ndraws = 200) |&gt; \n  plot(points = T,\n       point_args = c(alpha = 1/2, size = 1),\n       line_args = c(colour = \"black\"))\n\n\n\n\n\n\n\n\nWith multiple predictors, things get more complicated. Consider our multivariable, non-interaction model, b8.2.\n\nb8.2$formula\n\nlog_gdp_std ~ 0 + cid + rugged_std_c \n\nconditional_effects(b8.2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe got one plot for each predictor, controlling the other predictor at zero. Note how the plot for cid treated it as a categorical variable. This is because the variable was saved as a character in the original data set.\n\nb8.2$data |&gt; \n  glimpse()\n\nRows: 170\nColumns: 3\n$ log_gdp_std  &lt;dbl&gt; 0.8797119, 0.9647547, 1.1662705, 1.1044854, 0.9149038, 1.0816501, 1.1909183, 1.2063508, 0.9219115, 0.755229…\n$ cid          &lt;chr&gt; \"1\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"1\", \"2\", \"1\", \"1\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2\", \"2…\n$ rugged_std_c &lt;dbl&gt; -0.07661760, 0.33760362, -0.09096781, -0.09000038, 0.21844851, -0.21399264, -0.19190299, 0.35147011, 0.0546…\n\n\nThe results would have been similar had we saved cid as a factor. Things get more complicated with the non-linear interaction model, b8.3.\n\nb8.3$formula\n\nlog_gdp_std ~ 0 + a + b * rugged_std_c \na ~ 0 + cid\nb ~ 0 + cid\n\nconditional_effects(b8.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe conditional_effects() function correctly picked up on how we used the a parameter to make two intercepts according to the two levels of cid. However, it did not pick up our intent to use the b parameter as a stand-in for the two levels of the slope for rugged_std_c. Instead, it only showed the slope for rugged_std_c == 1. In GitHub issue #925, Bürkner clarified this is because “brms will only display interactions by default if the interactions are explicitly provided within linear formulas. brms has no way of knowing what variables in non-linear models actually interact.” However, the effects argument provides a workaround.\n\nconditional_effects(b8.3, effects = \"rugged_std_c:cid\")\n\n\n\n\n\n\n\n\nThe conditional_effects() function defaults to expressing interactions such that the first variable in the term–in this case, rugged_std_c–is on the \\(x\\)-axis and the second variable in the term–cid, treated as an integer–is treated as a factor using different values for the fill and color of the trajectories. See what happens when we change the ordering.\n\nconditional_effects(b8.3, effects = \"cid:rugged_std_c\") |&gt; \n  plot(cat_args = list(size = 2))\n\n\n\n\n\n\n\n\nNow our binary index variable cid is on the \\(x\\)-axis and the error bars for the effects are now depicted by three levels of the continuous variable rugged_std_c. By default, those are the mean \\(\\pm\\) one standard deviation. We might confirm those values like this.\n\nb8.3$data |&gt; \n  summarize(mean          = mean(rugged_std_c),\n            `mean + 1 sd` = mean(rugged_std_c) + sd(rugged_std_c),\n            `mean - 1 sd` = mean(rugged_std_c) - sd(rugged_std_c)) |&gt; \n  mutate_all(round, digits = 2)\n\n  mean mean + 1 sd mean - 1 sd\n1    0        0.19       -0.19\n\n\nNow we might use b8.5, our interaction model with two continuous variables, to get a sense of how this behavior works in that context.\n\nb8.5$formula\n\nblooms_std ~ 1 + water_cent + shade_cent + water_cent:shade_cent \n\nconditional_effects(b8.5, effects = \"water_cent:shade_cent\")\n\n\n\n\n\n\n\n\nOnce again, the three levels of the second variable in the interaction term, shade_cent, are the mean \\(\\pm\\) one standard deviation. If you’d like to set these to different values, such as -1, 0, and 1, define those within a list and feed that list into the int_conditions argument within conditional_effects(). We’ll do that in the next plot in a bit.\nThough the paradigm of using the conditional_effects() and plot() functions allows users to augment the results with a variety of options, users do not have the full flexibility of ggplot2 with this approach. If you’re picky and want to augment the plot further with other ggplot2 settings, you need to:\n\nsave the settings from conditional_effects() as an object,\nfeed that object into plot(),\nset the plot argument to FALSE within the plot() function,\nindex using bracket, and finally\ncustomize away with other ggplot2 functions.\n\nHere’s an example of what that might look like.\n\np1 &lt;- conditional_effects(b8.5, \n                          effects = \"water_cent:shade_cent\",\n                          int_conditions = list(shade_cent = -1:1))\n\nplot(p1,\n     points = T,\n     plot = F)[[1]] +\n  scale_x_continuous(breaks = -1:1) +\n  scale_fill_pander() +\n  scale_colour_pander() +\n  facet_wrap(~ shade_cent, labeller = label_both) +\n  theme(legend.position = \"none\",\n        panel.grid.minor = element_blank())\n\n\n\n\n\n\n\n\nI tend to prefer using other plotting methods when visualizing models, so we won’t be seeing much more from the conditional_effects() function in this ebook. But if you like method, you can find more ideas by checking out the conditional_effects section of the brms reference manual (Bürkner, 2022a) or searching for “conditional_effects” under the “brms” tag on the Stan Forums.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conditional Manatees</span>"
    ]
  },
  {
    "objectID": "08.html#session-info",
    "href": "08.html#session-info",
    "title": "8  Conditional Manatees",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.7 patchwork_1.3.2 ggrepel_0.9.6   ggdag_0.2.13    ggthemes_5.1.0  lubridate_1.9.4 forcats_1.0.1  \n [8] stringr_1.6.0   dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1  \n[15] tidyverse_2.0.0 brms_2.23.0     Rcpp_1.1.0     \n\nloaded via a namespace (and not attached):\n [1] gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7             magrittr_2.0.4         \n [6] multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          mgcv_1.9-3              loo_2.9.0.9000         \n[11] vctrs_0.7.0             reshape2_1.4.5          quadprog_1.5-8          pkgconfig_2.0.3         arrayhelpers_1.1-0     \n[16] fastmap_1.2.0           backports_1.5.0         labeling_0.4.3          ggraph_2.2.2            pander_0.6.6           \n[21] rmarkdown_2.30          tzdb_0.5.0              xfun_0.55               cachem_1.1.0            jsonlite_2.0.0         \n[26] tweenr_2.0.3            parallel_4.5.1          R6_2.6.1                stringi_1.8.7           RColorBrewer_1.1-3     \n[31] StanHeaders_2.36.0.9000 boot_1.3-31             estimability_1.5.1      rstan_2.36.0.9000       knitr_1.51             \n[36] zoo_1.8-14              bayesplot_1.15.0.9000   Matrix_1.7-3            splines_4.5.1           igraph_2.2.0           \n[41] timechange_0.3.0        tidyselect_1.2.1        rstudioapi_0.17.1       abind_1.4-8             yaml_2.3.12            \n[46] viridis_0.6.5           codetools_0.2-20        curl_7.0.0              dagitty_0.3-4           pkgbuild_1.4.8         \n[51] lattice_0.22-7          plyr_1.8.9              withr_3.0.2             bridgesampling_1.2-1    S7_0.2.1               \n[56] posterior_1.6.1.9000    coda_0.19-4.1           evaluate_1.0.5          survival_3.8-3          RcppParallel_5.1.11-1  \n[61] polyclip_1.10-7         ggdist_3.3.3            pillar_1.11.1           tensorA_0.36.2.1        checkmate_2.3.3        \n[66] stats4_4.5.1            distributional_0.5.0    generics_0.1.4          hms_1.1.4               rstantools_2.5.0.9000  \n[71] scales_1.4.0            xtable_1.8-4            glue_1.8.0              emmeans_1.11.2-8        tools_4.5.1            \n[76] mvtnorm_1.3-3           graphlayouts_1.2.2      tidygraph_1.3.1         grid_4.5.1              QuickJSR_1.8.1         \n[81] nlme_3.1-168            ggforce_0.5.0           cli_3.6.5               svUnit_1.0.8            viridisLite_0.4.2      \n[86] Brobdingnag_1.2-9       V8_8.0.1                gtable_0.3.6            digest_0.6.39           TH.data_1.1-4          \n[91] htmlwidgets_1.6.4       farver_2.1.2            memoise_2.0.1           htmltools_0.5.9         lifecycle_1.0.5        \n[96] MASS_7.3-65",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conditional Manatees</span>"
    ]
  },
  {
    "objectID": "08.html#comments",
    "href": "08.html#comments",
    "title": "8  Conditional Manatees",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nArnold, J. B. (2021). ggthemes: Extra themes, scales and geoms for ’ggplot2’. https://CRAN.R-project.org/package=ggthemes\n\n\nBürkner, P.-C. (2022a). brms reference manual, Version 2.18.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2022b). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html\n\n\nGrafen, A., & Hails, R. (2002). Modern statistics for the life sciences. Oxford University Press. https://global.oup.com/academic/product/modern-statistics-for-the-life-sciences-9780199252312?\n\n\nHayes, A. F. (2017). Introduction to mediation, moderation, and conditional process analysis: A regression-based approach. Guilford publications. https://www.guilford.com/books/Introduction-to-Mediation-Moderation-and-Conditional-Process-Analysis/Andrew-Hayes/9781462534654\n\n\nKurz, A. S. (2026a). Recoding Introduction to Mediation, Moderation, and Conditional Process Analysis (version 1.4.0). https://solomon.quarto.pub/immcpa2/\n\n\nKurz, A. S. (2026b). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.4.0). https://solomon.quarto.pub/sr/\n\n\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nNunn, N., & Puga, D. (2012). Ruggedness: The blessing of bad geography in Africa. Review of Economics and Statistics, 94(1), 20–36. https://doi.org/10.1162/REST_a_00161",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Conditional Manatees</span>"
    ]
  },
  {
    "objectID": "09.html",
    "href": "09.html",
    "title": "9  Markov Chain Monte Carlo",
    "section": "",
    "text": "9.0.0.1 Rethinking: Stan was a man\nThis chapter introduces one commonplace example of Fortuna and Minerva’s cooperation: the estimation of posterior probability distributions using a stochastic process known as Markov chain Monte Carlo (MCMC)” (McElreath, 2020, p. 263, emphasis in the original). Though we’ve been using MCMC via the brms package for chapters, now, this chapter should clarify some of the questions you might have about the details.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Markov Chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "09.html#good-king-markov-and-his-island-kingdom",
    "href": "09.html#good-king-markov-and-his-island-kingdom",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.1 Good King Markov and his island kingdom",
    "text": "9.1 Good King Markov and his island kingdom\nHere we simulate King Markov’s journey. In this version of the code, we’ve added set.seed(), which helps make the exact results reproducible.\n\nset.seed(9)\n\nnum_weeks &lt;- 1e5\npositions &lt;- rep(0, num_weeks) \ncurrent   &lt;- 10\n\nfor (i in 1:num_weeks) {\n  \n  # Record current position \n  positions[i] &lt;- current\n  # Flip coin to generate proposal\n  proposal &lt;- current + sample(c(-1, 1), size = 1)\n  # Now make sure he loops around the archipelago \n  if (proposal &lt; 1) proposal &lt;- 10\n  if (proposal &gt; 10) proposal &lt;- 1\n  # Move?\n  prob_move &lt;- proposal / current\n  current &lt;- ifelse(runif(1) &lt; prob_move, proposal, current)\n}  \n\nIn this chapter, we’ll take our plotting theme from the ggpomological package (Aden-Buie, 2022).\n\nlibrary(ggpomological)\n\nTo get the full benefits from ggpomological, you may need to download some fonts. Throughout this chapter, I make extensive use of Marck Script, which you find at https://fonts.google.com/specimen/Marck+Script. Once you’ve installed the font on your computer, you may also have to execute extrafont::font_import(). Here we get a sense of the colors we’ll be using with our dots and lines and so on.\n\nscales::show_col(ggpomological:::pomological_palette)\n\n\n\n\n\n\n\n\nThis will make it easier to access those colors.\n\npomological_palette &lt;- ggpomological:::pomological_palette\n\nNow make Figure 9.2.a.\n\nlibrary(tidyverse)\n\ntibble(week   = 1:1e5,\n       island = positions) |&gt;\n  ggplot(aes(x = week, y = island)) +\n  geom_point(color = pomological_palette[1], shape = 1) +\n  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 20)) +\n  scale_y_continuous(breaks = seq(from = 0, to = 10, by = 2)) +\n  coord_cartesian(xlim = c(0, 100)) +\n  labs(title = \"Behold the Metropolis algorithm in action!\",\n       subtitle = \"The dots show the king's path over the first 100 weeks.\") +\n  theme_pomological_fancy(base_family = \"Marck Script\")\n\n\n\n\n\n\n\n\nFigure 9.2.b.\n\ntibble(week   = 1:1e5,\n       island = positions) |&gt;\n  mutate(island = factor(island)) |&gt;\n  \n  ggplot(aes(x = island)) +\n  geom_bar(fill = pomological_palette[2]) +\n  scale_y_continuous(\"number of weeks\", expand = expansion(mult = c(0, 0.05))) +\n  labs(title = \"Old Metropolis shines in the long run.\",\n       subtitle = \"Sure enough, the time the king spent on each island\\nwas proportional to its population size.\") +\n  theme_pomological_fancy(base_family = \"Marck Script\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Markov Chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "09.html#metropolis-algorithms",
    "href": "09.html#metropolis-algorithms",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.2 Metropolis algorithms",
    "text": "9.2 Metropolis algorithms\n“The Metropolis algorithm is the grandparent of several different strategies for getting samples from unknown posterior distributions” (p. 267). If you’re interested, Robert & Casella (2011) wrote a good historical overview of MCMC.\n\n9.2.1 Gibbs sampling\nThe Gibbs sampler (Casella & George, 1992; Geman & Geman, 1984) uses conjugate pairs (i.e., pairs of priors and likelihoods that have analytic solutions for the posterior of an individual parameter) to efficiently sample from the posterior. Gibbs was the workhorse algorithm during the rise of Bayesian computation in the 1990s and it was highlighted in Bayesian software like BUGS (Spiegelhalter et al., 2003) and JAGS (Plummer, 2003). We will not be using the Gibbs sampler in this project. It’s available for use in R. For an extensive applied introduction, check out Kruschke’s (2015) text.\n\n\n9.2.2 High-dimensional problems\nThe Gibbs sampler is limited in that (a) you might not want to use conjugate priors and (b) it can be quite inefficient with complex hierarchical models, which we’ll be fitting soon.\nEarlier versions of this ebook did not focus on McElreath’s example of the pathology high autocorrelations can create when using the Metropolis algorithm, which is depicted in Figure 9.3. However, James Henegan kindly reached out with a tidyverse workflow for reproducing this example. Here is a slightly amended version of that workflow.\nThe first step is to simulate a bivariate distribution “with a strong negative correlation of -0.9” (p. 268). Henagen simulated the from a distribution where the two variables \\(\\text a_1\\) and \\(\\text a_2\\) followed the bivariate normal distribution\n\\[\\begin{align*}\n\\begin{bmatrix} \\text a_1 \\\\ \\text a_2 \\end{bmatrix} & \\sim \\operatorname{MVNormal} \\left (\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma \\right) \\\\\n\\mathbf \\Sigma & = \\mathbf{SRS} \\\\\n\\mathbf S & = \\begin{bmatrix} 0.22 & 0 \\\\ 0 & 0.22 \\end{bmatrix} \\\\\n\\mathbf R & = \\begin{bmatrix} 1 & -.9 \\\\ -.9 & 1 \\end{bmatrix},\n\\end{align*}\\]\nwhere the variance/covariance matrix is decomposed into a \\(2 \\times 2\\) matrix of standard deviations and a \\(2 \\times 2\\) correlation matrix. In this example, both variables (\\(\\text a_1\\) and \\(\\text a_2\\)) have standard deviations of 0.22. We’ll have more practice with data of this kind in Chapter 14. For now, just go with it. Here’s how to simulate from this distribution.\n\n# Mean vector\nmu &lt;- c(0, 0)\n\n# Variance/covariance matrix\nsd_a1 &lt;- 0.22\nsd_a2 &lt;- 0.22\nrho   &lt;- -0.9\n\nSigma &lt;- matrix(data = c(sd_a1^2,\n                         rho * sd_a1 * sd_a2,\n                         rho * sd_a1 * sd_a2,\n                         sd_a2^2),\n                nrow = 2)\n\n# Sample from the distribution with the `mvtnorm::rmvnorm()` function\nset.seed(9)\n\nmy_samples &lt;- mvtnorm::rmvnorm(n = 1000, mean = mu, sigma = Sigma)\n\nCheck the sample correlation.\n\ndata.frame(my_samples) |&gt; \n  set_names(str_c(\"a\", 1:2)) |&gt; \n  summarise(rho = cor(a1, a2))\n\n         rho\n1 -0.9106559\n\n\nWe won’t actually be using the values from this simulation. Instead, we can evaluate the density function for this distribution using the mvtnorm::dmvnorm() function. But even before that, we’ll want to create a grid of values for the contour lines in Figure 9.3. Here we do so with a custom function called x_y_grid().\n\n# Define the function\nx_y_grid &lt;- function(x_start = -1.6,\n                     x_stop = 1.6,\n                     x_length = 100,\n                     y_start = -1.6,\n                     y_stop = 1.6,\n                     y_length = 100) {\n  x_domain &lt;- seq(from = x_start, to = x_stop, length.out = x_length)\n  y_domain &lt;- seq(from = y_start, to = y_stop, length.out = y_length)\n  \n  tidyr::expand_grid(a1 = x_domain, a2 = y_domain)\n}\n\n# Simulate\ncontour_plot_dat &lt;- x_y_grid()\n\n# What have we done?\nstr(contour_plot_dat)\n\ntibble [10,000 × 2] (S3: tbl_df/tbl/data.frame)\n $ a1: num [1:10000] -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 -1.6 ...\n $ a2: num [1:10000] -1.6 -1.57 -1.54 -1.5 -1.47 ...\n\n\nNow compute the density values for each combination of a1 and a2.\n\ncontour_plot_dat &lt;- contour_plot_dat |&gt; \n  mutate(d = mvtnorm::dmvnorm(as.matrix(contour_plot_dat), mean = mu, sigma = Sigma))\n\nhead(contour_plot_dat)\n\n# A tibble: 6 × 3\n     a1    a2         d\n  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1  -1.6 -1.6  1.47e-229\n2  -1.6 -1.57 6.08e-225\n3  -1.6 -1.54 2.24e-220\n4  -1.6 -1.50 7.38e-216\n5  -1.6 -1.47 2.17e-211\n6  -1.6 -1.44 5.68e-207\n\n\nTo get a sense of what we’ve done, here are those data as a 2D density plot.\n\ncontour_plot_dat |&gt; \n  ggplot(aes(x = a1, y = a2, fill = d)) +\n  geom_raster(interpolate = T) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_fill_gradientn(colors = pomological_palette[c(8, 2, 4)],\n                       limits = c(0, NA)) +\n  theme_pomological_fancy(base_family = \"Marck Script\")\n\n\n\n\n\n\n\n\nBut we don’t want a density plot. We want contour lines!\n\ncontour_plot &lt;- contour_plot_dat |&gt; \n  ggplot() + \n  geom_contour(aes(x = a1, y = a2, z = d),\n               breaks = 9^(-(10 * 1:25)), \n               color = pomological_palette[4], linewidth = 1/8) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_pomological_fancy(base_family = \"Marck Script\")\n\ncontour_plot\n\n\n\n\n\n\n\n\nNote how we saved that plot as contour_plot, which will serve as the base for the two panels in our Figure 9.3. Next we use the Metropolis algorithm to sample from the posterior defined, above. Henagen’s implementation is wrapped in a custom function he called metropolis(), which is designed to track\n\ncoordinates of candidate (i.e., proposal) points, and\nwhether or not the candidate points were accepted.\n\nHere we define metropolis() with slight amendments to Henagen’s original.\n\nmetropolis &lt;- function(num_proposals = 50,\n                       step_size = 0.1,\n                       starting_point = c(-1, 1)) {\n  \n  # Initialize vectors where we will keep track of relevant\n  candidate_x_history &lt;- rep(-Inf, num_proposals)\n  candidate_y_history &lt;- rep(-Inf, num_proposals)\n  did_move_history    &lt;- rep(FALSE, num_proposals)\n  \n  # Prepare to begin the algorithm...\n  current_point &lt;- starting_point\n  \n  for(i in 1:num_proposals) {\n    \n    # \"Proposals are generated by adding random Gaussian noise\n    # to each parameter\"\n    noise &lt;- rnorm(n = 2, mean = 0, sd = step_size)\n    candidate_point &lt;- current_point + noise\n    \n    # Store coordinates of the proposal point\n    candidate_x_history[i] &lt;- candidate_point[1]\n    candidate_y_history[i] &lt;- candidate_point[2]\n    \n    # Evaluate the density of our posterior at the proposal point\n    candidate_prob &lt;- mvtnorm::dmvnorm(candidate_point, mean = mu, sigma = Sigma)\n    \n    # Evaluate the density of our posterior at the current point\n    current_prob &lt;- mvtnorm::dmvnorm(current_point, mean = mu, sigma = Sigma)\n    \n    # Decide whether or not we should move to the candidate point\n    acceptance_ratio &lt;- candidate_prob / current_prob\n    should_move &lt;- ifelse(runif(n = 1) &lt; acceptance_ratio, TRUE, FALSE)\n    \n    # Keep track of the decision\n    did_move_history[i] &lt;- should_move\n    \n    # Move if necessary\n    if(should_move) {\n      current_point &lt;- candidate_point\n    }\n  }\n  \n  # Once the loop is complete, store the relevant results in a tibble\n  results &lt;- tibble::tibble(\n    candidate_x = candidate_x_history,\n    candidate_y = candidate_y_history,\n    accept = did_move_history\n  )\n  \n  # Compute the \"acceptance rate\" by dividing the total number of \"moves\"\n  # by the total number of proposals\n  \n  number_of_moves &lt;- results |&gt; dplyr::pull(accept) |&gt; sum()\n  acceptance_rate &lt;- number_of_moves/num_proposals\n  \n  list(results = results, acceptance_rate = acceptance_rate)\n}\n\nRun the algorithm for the panel on the left, which uses a step size of 0.1.\n\nset.seed(9)\n\nround_1 &lt;- metropolis(num_proposals = 50,\n                      step_size = 0.1,\n                      starting_point = c(-1,1))\n\nUse round_1 to make Figure 9.3a.\n\np1 &lt;- contour_plot + \n  geom_point(data = round_1$results,\n             aes(x = candidate_x, y = candidate_y, \n                 color = accept, shape = accept)) +\n  scale_shape_manual(values = c(21, 19)) +\n  scale_color_manual(values = pomological_palette[8:9]) +\n  labs(x = \"a1\",\n       y = \"a2\",\n       subtitle = str_c(\"step size 0.1,\\naccept rate \", round_1$acceptance_rate),) +\n  theme_pomological_fancy(base_family = \"Marck Script\")\n\np1\n\n\n\n\n\n\n\n\nNow run the algorithm with step size set to 0.25. Then make Figure 9.3b, combine the two ggplots, and return the our full version of Figure 9.3.\n\n# Simulate\nset.seed(9)\n\nround_2 &lt;- metropolis(num_proposals = 50,\n                      step_size = 0.25,\n                      starting_point = c(-1, 1))\n\n# Plot\np2 &lt;- contour_plot + \n  geom_point(data = round_2$results,\n             aes(x = candidate_x, y = candidate_y, \n                 color = accept, shape = accept)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  scale_color_manual(values = pomological_palette[8:9]) +\n  scale_shape_manual(values = c(21, 19)) +\n  labs(x = \"a1\",\n       subtitle = str_c(\"step size 0.25,\\naccept rate \", round_2$acceptance_rate)) +\n  theme_pomological_fancy(base_family = \"Marck Script\")\n\n# Combine\nlibrary(patchwork)\n(p1 + p2) + \n  plot_annotation(theme = theme_pomological_fancy(base_family = \"Marck Script\"),\n                  title = \"Metropolis chains under high correlation\") +\n  plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nOur acceptance rates differ from McElreath’s due to simulation variance. If you want to get a sense of stability in the acceptance rates, just simulate some more. For example, we might wrap metropolis() inside another function that takes a simulation seed value.\n\nmetropolis_with_seed &lt;- function(seed, step_size = 0.1) {\n  set.seed(seed)\n  met &lt;- metropolis(\n    num_proposals = 50,\n    step_size = step_size,\n    starting_point = c(-1, 1))\n  \n  met$acceptance_rate\n}\n\nKick the tires and iterate 500 times.\n\nars &lt;- tibble(seed = 1:500) |&gt; \n  mutate(acceptance_rate = map_dbl(seed, metropolis_with_seed)) \n\nNow summarize the results in a histogram.\n\nars |&gt; \n  ggplot(aes(x = acceptance_rate)) +\n  geom_histogram(binwidth = 0.025, fill = pomological_palette[5]) +\n  theme_pomological_fancy(base_family = \"Marck Script\")\n\n\n\n\n\n\n\n\nIf you iterate with step_size = 0.25, instead, the resulting histogram will look very different.\nNow we turn our focus to Figure 9.4. McElreath threw us a bone and gave us the code in his R code 9.4. Here we’ll warp his code into a function called concentration_sim() which adds a seed argument for reproducibility.\n\nconcentration_sim &lt;- function(d = 1, t = 1e3, seed = 9) {\n  set.seed(seed)\n  y &lt;- rethinking::rmvnorm(t, rep(0, d), diag(d))\n  rad_dist &lt;- function(y) sqrt(sum(y^2))\n  rd &lt;- sapply(1:t, function(i) rad_dist( y[i, ])) \n}\n\nNow run the simulation four times and plot.\n\nd &lt;- tibble(d = c(1, 10, 100, 1000)) |&gt; \n  mutate(con = map(d, concentration_sim)) |&gt; \n  unnest(con) |&gt; \n  mutate(`# dimensions` = factor(d)) \n\nd  |&gt; \n  ggplot(aes(x = con, fill = `# dimensions`)) +\n  geom_density(linewidth = 0, alpha = 3/4) +\n  scale_fill_pomological() +\n  xlab(\"Radial distance from mode\") +\n  theme_pomological_fancy(base_family = \"Marck Script\") +\n  theme(legend.position = c(0.7, 0.625))\n\n\n\n\n\n\n\n\nWith high-dimensional posteriors,\n\nsampled points are in a thin, high-dimensional shell very far from the mode. This shell can create very hard paths for a sampler to follow.\nThis is why we need MCMC algorithms that focus on the entire posterior at once, instead of one or a few dimensions at a time like Metropolis and Gibbs. Otherwise we get stuck in a narrow, highly curving region of parameter space. (p. 270)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Markov Chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "09.html#hamiltonian-monte-carlo",
    "href": "09.html#hamiltonian-monte-carlo",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.3 Hamiltonian Monte Carlo",
    "text": "9.3 Hamiltonian Monte Carlo\nHamiltonian Monte Carlo (HMC) is more computationally costly and more efficient than Gibbs at sampling from the posterior. It needs fewer samples, especially when fitting models with many parameters. To learn more about how HMC works, check out McElreath’s lecture on the topic from January 2019; his blog post, Markov chains: Why walk when you can flow?; or one of these lectures (here, here, or here) by Michael Betancourt.\n\n9.3.1 Another parable\nThis section is beyond the scope of this project.\n\n\n9.3.2 Particles in space\nThis section is beyond the scope of this project.\n\n\n9.3.3 Limitations\n\nAs always, there are some limitations. HMC requires continuous parameters. It can’t glide through a discrete parameter. In practice, this means that certain techniques, like the imputation of discrete missing data, have to be done differently with HMC. HMC can certainly sample from such models, often much more efficiently than a Gibbs sampler could. But you have to change how you code them. (p. 278)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Markov Chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "09.html#easy-hmc-ulam-brm",
    "href": "09.html#easy-hmc-ulam-brm",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.4 Easy HMC: ulam brm()",
    "text": "9.4 Easy HMC: ulam brm()\nMuch like McElreath’s rethinking package, brms provides a convenient interface to HMC via Stan. Other packages providing Stan interfaces include rstanarm (Brilleman et al., 2018; Gabry & Goodrich, 2022) and blavaan (Merkle et al., 2021, 2022; Merkle & Rosseel, 2018). I’m not aware of any up-to-date comparisons across the packages. If you’re ever inclined to make one, let the rest of us know!\nHere we load the rugged data and brms.\n\nlibrary(brms)\ndata(rugged, package = \"rethinking\")\nd &lt;- rugged\nrm(rugged)\n\nIt takes just a sec to do a little data manipulation.\n\nd &lt;- d |&gt;\n  mutate(log_gdp = log(rgdppc_2000))\n\ndd &lt;- d |&gt;\n  drop_na(rgdppc_2000) |&gt; \n  mutate(log_gdp_std = log_gdp / mean(log_gdp),\n         rugged_std  = rugged / max(rugged),\n         cid         = ifelse(cont_africa == 1, \"1\", \"2\")) |&gt; \n  mutate(rugged_std_c = rugged_std - mean(rugged_std))\n\nIn the context of this chapter, it doesn’t make sense to translate McElreath’s m8.3 quap() code to brm() code. Below, we’ll just go directly to the brm() variant of his m9.1.\n\n9.4.1 Preparation\nWhen working with brms, you don’t need to do the data processing McElreath did on page 280. If you wanted to, however, here’s how you might do it within the tidyverse.\n\ndat_slim &lt;- dd |&gt;\n  mutate(cid = as.integer(cid)) |&gt; \n  select(log_gdp_std, rugged_std, cid, rugged_std_c) |&gt; \n  list()\n\nstr(dat_slim)\n\n\n\n9.4.2 Sampling from the posterior\nFinally, we get to work that sweet HMC via brms::brm().\n\nb9.1 &lt;- brm(\n  data = dd, \n  family = gaussian,\n  bf(log_gdp_std ~ 0 + a + b * (rugged_std - 0.215), \n     a ~ 0 + cid, \n     b ~ 0 + cid,\n     nl = TRUE),\n  prior = c(prior(normal(1, 0.1), class = b, coef = cid1, nlpar = a),\n            prior(normal(1, 0.1), class = b, coef = cid2, nlpar = a),\n            prior(normal(0, 0.3), class = b, coef = cid1, nlpar = b),\n            prior(normal(0, 0.3), class = b, coef = cid2, nlpar = b),\n            prior(exponential(1), class = sigma)),\n  chains = 1, cores = 1,\n  seed = 9,\n  file = \"fits/b09.01\")\n\nThis was another instance of the brms non-linear syntax, We’ve already introduced this in Section 4.4.2.1, Section 5.3.2, and Section 6.2.1. For even more details, you can always peruse Bürkner’s (2022) vignette, Estimating non-linear models with brms.\nHere is a summary of the posterior.\n\nprint(b9.1)\n\n Family: gaussian \n  Links: mu = identity \nFormula: log_gdp_std ~ 0 + a + b * (rugged_std - 0.215) \n         a ~ 0 + cid\n         b ~ 0 + cid\n   Data: dd (Number of observations: 170) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cid1     0.89      0.02     0.86     0.92 1.00     1456      506\na_cid2     1.05      0.01     1.03     1.07 1.00     1609      794\nb_cid1     0.13      0.08    -0.02     0.29 1.00     1092      592\nb_cid2    -0.14      0.06    -0.25    -0.02 1.00     1365      820\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.11      0.01     0.10     0.12 1.00     1106      569\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nUnlike McElreath’s precis() output, our output has an Rhat instead of Rhat4. McElreath’s documentation indicated his Rhat4 values are based on the \\(\\widehat R\\) from Gelman et al. (2013). To my knowledge, brms uses the same formula. McElreath also remarked he expected to update to an Rhat5 in the near future. I believe he was referencing Vehtari et al. (2019). I am under the impression this will be implemented at the level of the underlying Stan code, which means brms will get the update, too. To learn more, check out the New R-hat and ESS thread on the Stan Forums.\nAlso of note, McElreath’s rethinking::precis() returns highest posterior density intervals (HPDIs) when summarizing ulam() models. Not so with brms. If you want HPDIs, you might use the convenience functions from the tidybayes package. Here’s an example.\n\nlibrary(tidybayes)\n\npost &lt;- as_draws_df(b9.1)\n\npost |&gt; \n  pivot_longer(b_a_cid1:sigma) |&gt; \n  group_by(name) |&gt; \n  mean_hdi(value, .width = 0.89)  # Note our rare use of 89% intervals\n\n# A tibble: 5 × 7\n  name      value   .lower  .upper .width .point .interval\n  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 b_a_cid1  0.887  0.858    0.910    0.89 mean   hdi      \n2 b_a_cid2  1.05   1.04     1.07     0.89 mean   hdi      \n3 b_b_cid1  0.130  0.00680  0.240    0.89 mean   hdi      \n4 b_b_cid2 -0.142 -0.231   -0.0527   0.89 mean   hdi      \n5 sigma     0.111  0.101    0.121    0.89 mean   hdi      \n\n\nThere’s one more important difference in our brms summary output compared to McElreath’s rethinking::precis() output. In the text we learn precis() returns n_eff values for each parameter. Earlier versions of brms used to have a direct analogue named Eff.Sample. Both were estimates of the effective number of samples (a.k.a. the effective sample size) for each parameter. As with typical sample size, the more the merrier. Starting with version 2.10.0, brms now returns two columns: Bulk_ESS and Tail_ESS. These originate from the same Vehtari et al. (2019) paper that introduced the upcoming change to the \\(\\widehat R\\). From the paper, we read:\n\nIf you plan to report quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3 we show that convergence of Markov chains is not uniform across the parameter space and propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as the “bulk-ESS”), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (p. 5, emphasis in the original)\n\nFor more technical details, see the paper. In short, Bulk_ESS in the output from brms 2.10.0+ is what was previously referred to as Eff.Sample in earlier versions. It’s also what corresponds to what McElreath calls n_eff. This indexed the number of effective samples in ‘the center of the’ posterior distribution (i.e., the posterior mean or median). But since we also care about uncertainty in our parameters, we care about stability in the 95% intervals and such. The new Tail_ESS in brms output allows us to gauge the effective sample size for those intervals.\n\n\n9.4.3 Sampling again, in parallel\n\nYou can easily parallelize those chains… They can all run at the same time, instead of in sequence. So as long as your computer has four cores (it probably does), it won’t take longer to run four chains than one chain. To run four independent Markov chains for the model above, and to distribute them across separate cores in your computer, just increase the number of chains and add a cores argument. (p. 281)\n\nIf you don’t know how many cores you have on your computer, you can always check with the parallel::detectCores() function. My current laptop has 12.\n\nparallel::detectCores()\n\n[1] 12\n\n\nHere we sample from four HMC chains in parallel by adding cores = 4.\n\nb9.1b &lt;- update(\n  b9.1, \n  chains = 4, cores = 4,\n  seed = 9,\n  file = \"fits/b09.01b\")\n\nThis model sampled so fast that it really didn’t matter if we sampled in parallel or not. It will for others.\n\nprint(b9.1b)\n\n Family: gaussian \n  Links: mu = identity \nFormula: log_gdp_std ~ 0 + a + b * (rugged_std - 0.215) \n         a ~ 0 + cid\n         b ~ 0 + cid\n   Data: dd (Number of observations: 170) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cid1     0.89      0.02     0.85     0.92 1.00     5156     2758\na_cid2     1.05      0.01     1.03     1.07 1.00     5249     2359\nb_cid1     0.13      0.08    -0.02     0.28 1.00     5097     2952\nb_cid2    -0.14      0.06    -0.25    -0.03 1.00     5448     3167\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.11      0.01     0.10     0.12 1.00     4368     2998\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe show() function does not work for brms models the same way it does with those from rethinking. Rather, show() returns the same information we’d get from print() or summary().\n\nshow(b9.1b)\n\n Family: gaussian \n  Links: mu = identity \nFormula: log_gdp_std ~ 0 + a + b * (rugged_std - 0.215) \n         a ~ 0 + cid\n         b ~ 0 + cid\n   Data: dd (Number of observations: 170) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cid1     0.89      0.02     0.85     0.92 1.00     5156     2758\na_cid2     1.05      0.01     1.03     1.07 1.00     5249     2359\nb_cid1     0.13      0.08    -0.02     0.28 1.00     5097     2952\nb_cid2    -0.14      0.06    -0.25    -0.03 1.00     5448     3167\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.11      0.01     0.10     0.12 1.00     4368     2998\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nYou can get a focused look at the formula and prior information from a brms fit object by subsetting them directly.\n\nb9.1b$formula\n\nlog_gdp_std ~ 0 + a + b * (rugged_std - 0.215) \na ~ 0 + cid\nb ~ 0 + cid\n\nb9.1b$prior\n\n          prior class coef group resp dpar nlpar lb ub tag  source\n         (flat)     b                          a           default\n normal(1, 0.1)     b cid1                     a              user\n normal(1, 0.1)     b cid2                     a              user\n         (flat)     b                          b           default\n normal(0, 0.3)     b cid1                     b              user\n normal(0, 0.3)     b cid2                     b              user\n exponential(1) sigma                             0           user\n\n\nYou can get that information on the model priors with the prior_summary() function.\n\nprior_summary(b9.1b)\n\n          prior class coef group resp dpar nlpar lb ub tag  source\n         (flat)     b                          a           default\n normal(1, 0.1)     b cid1                     a              user\n normal(1, 0.1)     b cid2                     a              user\n         (flat)     b                          b           default\n normal(0, 0.3)     b cid1                     b              user\n normal(0, 0.3)     b cid2                     b              user\n exponential(1) sigma                             0           user\n\n\nI am not aware of a convenient way to pull the information on how long each chain ran for. As to the sample size, our output is a little different than McElreath’s. The brms default is to run 4 chains, each containing 2,000 total samples, the first 1,000 of which are warmups. Since we used those defaults, we ended up with \\((2{,}000 - 1{,}000) \\times 4 = 4{,}000\\) post-warmup HMC samples. But anyways, just like all of McElreath’s n_eff values were above 2,000, most of our Bulk_ESS values were above 4,000, which is\n\nno mistake. The adaptive sampler that Stan uses is so good, it can actually produce sequential samples that are better than uncorrelated. They are anti-correlated. This means it can explore the posterior distribution so efficiently that it can beat random. (p. 282)\n\nIn addition to sampling HMC chains in parallel, the Stan team have been working on within-chain parallelization, too. This is a new development and was just made available to brms users with the release of brms version 2.14.0. I don’t plan on covering within-chain parallelization in this ebook, but you can learn more in Weber and Bürkner’s (2022) vignette, Running brms models with within-chain parallelization, or Weber’s guest post on Gelman’s blog, Stan’s within-chain parallelization now available with brms. If you have some large rough model that’s taking hours upon hours to fit, it might be worth your while.\n\n\n9.4.4 Visualization\nAs with McElreath’s rethinking, brms allows users to put the fit object directly into the pairs() function.\n\npairs(b9.1b,\n      off_diag_args = list(size = 1/5, alpha = 1/5))\n\n\n\n\n\n\n\n\nOur output is a little different in that we don’t get a lower-triangle of Pearson’s correlation coefficients. If you’d like those values, use vcov().\n\nvcov(b9.1b, correlation = T) |&gt; round(digits = 2)\n\n       a_cid1 a_cid2 b_cid1 b_cid2\na_cid1   1.00   0.02   0.20   0.00\na_cid2   0.02   1.00   0.00  -0.08\nb_cid1   0.20   0.00   1.00   0.02\nb_cid2   0.00  -0.08   0.02   1.00\n\n\nNote, however, that this will only return the correlations among the ‘Population-Level Effects’. Within brms, \\(\\sigma\\) is classified among the ‘Family Specific Parameters’. We have more options still. As a first step, use the brms::as_draws_df() function to extract the posterior samples within a data frame.\n\npost &lt;- as_draws_df(b9.1b)\n\nglimpse(post)\n\nRows: 4,000\nColumns: 10\n$ b_a_cid1   &lt;dbl&gt; 0.8902290, 0.8859746, 0.8851561, 0.8624812, 0.8817271, 0.8715740, 0.8808950, 0.8952688, 0.8829123, 0.9007783,…\n$ b_a_cid2   &lt;dbl&gt; 1.041682, 1.037597, 1.057069, 1.029111, 1.051701, 1.046893, 1.051390, 1.055610, 1.064900, 1.041198, 1.049531,…\n$ b_b_cid1   &lt;dbl&gt; 0.193352390, 0.172975339, 0.060368098, 0.005113573, 0.242495092, 0.090044745, 0.012027168, 0.221261151, 0.068…\n$ b_b_cid2   &lt;dbl&gt; -0.212097650, -0.211968344, -0.158824208, -0.078588554, -0.225123754, -0.079836101, -0.133941767, -0.14647897…\n$ sigma      &lt;dbl&gt; 0.1180972, 0.1163596, 0.1063421, 0.1156475, 0.1071916, 0.1111975, 0.1167769, 0.1038341, 0.1111960, 0.1130315,…\n$ lprior     &lt;dbl&gt; 2.072297, 2.084396, 2.248335, 2.199314, 1.788850, 2.211099, 2.278767, 2.139294, 2.196891, 2.106575, 2.245404,…\n$ lp__       &lt;dbl&gt; 132.5012, 132.4970, 133.8521, 129.7689, 132.2144, 133.5621, 133.0054, 133.2437, 133.3786, 132.4297, 131.4722,…\n$ .chain     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ .iteration &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30…\n$ .draw      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30…\n\n\nAnother nice way to customize your pairs plot is with the GGally package. For this approach, you feed the post data into the ggpairs() function.\n\nlibrary(GGally)\n\npost |&gt;\n  select(b_a_cid1:sigma) |&gt;\n  ggpairs()\n\n\n\n\n\n\n\n\nNow we get the pairs plot on the lower triangle and the Pearson’s correlation coefficients in the upper. Since GGally::ggpairs() returns a ggplot2 object, you can customize it as you please.\n\n# Make the custom functions\nmy_diag &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_density(fill = pomological_palette[7],\n                 color = pomological_palette[6])\n}\n\nmy_upper &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_bin2d() +\n    scale_fill_gradient(low = pomological_palette[4], \n                        high = pomological_palette[1])\n}\n\n# plot!\npost |&gt;\n  select(b_a_cid1:sigma) |&gt;\n  ggpairs(lower = list(continuous = wrap(\"cor\", family = \"Marck Script\", color = \"black\")),\n          diag = list(continuous = my_diag),\n          upper = list(continuous = my_upper)) +\n  labs(subtitle = \"My custom pairs plot\") +\n  theme_pomological_fancy(base_family = \"Marck Script\")\n\n\n\n\n\n\n\n\nFor more ideas on customizing a ggpairs() plot, go here.\n\n\n9.4.5 Checking the chain\nUsing plot() for a brm() fit returns both density and trace lots for the parameters.\n\nplot(b9.1b, widths = c(1, 2))\n\n\n\n\n\n\n\n\nThe bayesplot package allows a little more control. Here, we use the bayesplot::mcmc_trace() function to show only trace plots with our custom theme. Note that mcmc_trace() does not work with brmfit objects, but it will accept input from as_draws_df().\n\nlibrary(bayesplot)\n\nas_draws_df(b9.1b) |&gt; \n  mcmc_trace(pars = vars(b_a_cid1:sigma),\n             facet_args = list(ncol = 3), \n             linewidth = 0.15) +\n  scale_color_pomological() +\n  labs(title = \"My custom trace plots\") +\n  theme_pomological_fancy(base_family = \"Marck Script\") +\n  theme(legend.position = c(0.95, 0.2))\n\n\n\n\n\n\n\n\nThe bayesplot package offers a variety of diagnostic plots. Here we make autocorrelation plots for all model parameters, one for each HMC chain.\n\nas_draws_df(b9.1b) |&gt; \n  mcmc_acf(pars = vars(b_a_cid1:sigma), lags = 5) +\n  theme_pomological_fancy(base_family = \"Marck Script\") \n\n\n\n\n\n\n\n\nThat’s just what we like to see–nice L-shaped autocorrelation plots. Those are the kinds of shapes you’d expect when you have reasonably large effective samples.\nBefore we move on, there’s an important difference between the trace plots McElreath showed in the text and the ones we just made. McElreath’s trace plots include the warmup iterations. Ours did not. To my knowledge, neither the brms::plot() nor the bayesplot::mcmc_trace() functions support including warmups in their trace plots. One quick way to get them is with the ggmcmc package (Fernández i Marín, 2016, 2021).\n\nlibrary(ggmcmc)\n\nThe ggmcmc package has a variety of convenience functions for working with MCMC chains. The ggs() function extracts the posterior draws, including warmup, and arranges them in a tidy tibble. Just make sure to set inc_warmup = TRUE.\n\nggs(b9.1b, inc_warmup = TRUE) |&gt; \n  str()\n\ntibble [48,000 × 4] (S3: tbl_df/tbl/data.frame)\n $ Iteration: int [1:48000] 1 2 3 4 5 6 7 8 9 10 ...\n $ Chain    : int [1:48000] 1 1 1 1 1 1 1 1 1 1 ...\n $ Parameter: Factor w/ 6 levels \"b_a_cid1\",\"b_a_cid2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ value    : num [1:48000] 1.04 1.04 1.04 1.04 1.04 ...\n - attr(*, \"nChains\")= int 4\n - attr(*, \"nParameters\")= int 6\n - attr(*, \"nIterations\")= int 2000\n - attr(*, \"nBurnin\")= num 0\n - attr(*, \"nThin\")= num 1\n - attr(*, \"description\")= chr \"anon_model\"\n\n\nWith this in hand, we can now include those warmup draws in our trace plots. Here’s how to do so without convenience functions like bayesplot::mcmc_trace().\n\nggs(b9.1b, inc_warmup = TRUE) |&gt;\n  mutate(chain = factor(Chain)) |&gt; \n  \n  ggplot(aes(x = Iteration, y = value)) +\n  # This marks off the warmups\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = pomological_palette[9], alpha = 1/6, linewidth = 0) +\n  geom_line(aes(color = chain),\n            linewidth = 0.15) +\n  scale_color_pomological() +\n  labs(x = NULL, y = NULL,\n       title = \"My custom trace plots with warmups via ggmcmc::ggs()\") +\n  theme_pomological_fancy(base_family = \"Marck Script\") +\n  facet_wrap(~ Parameter, scales = \"free_y\") +\n  theme(legend.position = c(0.95, 0.225))\n\n\n\n\n\n\n\n\nFollowing brms defaults, we won’t include warmup iterations in the trace plots for other models in this book. A nice thing about plots that do contain them, though, is they reveal how quickly our HMC chains transition away from their start values into the posterior. To get a better sense of this, let’s make those trace plots once more, but this time zooming in on the first 50 iterations.\n\nggs(b9.1b, inc_warmup = TRUE) |&gt;\n  mutate(chain = factor(Chain)) |&gt; \n  \n  ggplot(aes(x = Iteration, y = value, color = chain)) +\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = pomological_palette[9], alpha = 1/6, linewidth = 0) +\n  geom_line(linewidth = 0.5) +\n  scale_color_pomological() +\n  labs(x = NULL, y = NULL,\n       title = \"Another custom trace plots with warmups via ggmcmc::ggs()\") +\n  coord_cartesian(xlim = c(1, 50)) +\n  theme_pomological_fancy(base_family = \"Marck Script\") +\n  facet_wrap(~ Parameter, scales = \"free_y\") +\n  theme(legend.position = c(0.95, 0.225))\n\n\n\n\n\n\n\n\nFor each parameter, the all four chains had moved away from their starting values to converge on the marginal posteriors by the 30th iteration or so.\nBut anyway, we’ve veered a bit from the text. McElreath pointed out a second way to visualize the chains is by the distribution of the ranked samples, which he called a trank plot (short for trace rank plot). I’m not aware that brms has a built-in function for that. Happily, we can make them with mcmc_rank_overlay().\n\nas_draws_df(b9.1b) |&gt;  \n  mcmc_rank_overlay(pars = vars(b_a_cid1:sigma)) +\n  scale_color_pomological() +\n  labs(x = NULL,\n       title = \"My custom trank plots\") +\n  coord_cartesian(ylim = c(25, NA)) +\n  theme_pomological_fancy(base_family = \"Marck Script\") +\n  theme(legend.position = c(0.95, 0.2))\n\n\n\n\n\n\n\n\n\nWhat this means is to take all the samples for each individual parameter and rank them. The lowest sample gets rank 1. The largest gets the maximum rank (the number of samples across all chains). Then we draw a histogram of these ranks for each individual chain. Why do this? Because if the chains are exploring the same space efficiently, the histograms should be similar to one another and largely overlapping.\n…The horizontal is rank, from 1 to the number of samples across all chains (2000 in this example). The vertical axis is the frequency of ranks in each bin of the histogram. This trank plot is what we hope for: Histograms that overlap and stay within the same range. (pp. 284–285)\n\n\n9.4.5.1 Overthinking: Raw Stan model code\nThe stancode() function works with brms much like it does with rethinking.\n\nbrms::stancode(b9.1b)\n\n// generated with brms 2.23.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K_a;  // number of population-level effects\n  matrix[N, K_a] X_a;  // population-level design matrix\n  int&lt;lower=1&gt; K_b;  // number of population-level effects\n  matrix[N, K_b] X_b;  // population-level design matrix\n  // covariates for non-linear functions\n  vector[N] C_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  vector[K_a] b_a;  // regression coefficients\n  vector[K_b] b_b;  // regression coefficients\n  real&lt;lower=0&gt; sigma;  // dispersion parameter\n}\ntransformed parameters {\n  // prior contributions to the log posterior\n  real lprior = 0;\n  lprior += normal_lpdf(b_a[1] | 1, 0.1);\n  lprior += normal_lpdf(b_a[2] | 1, 0.1);\n  lprior += normal_lpdf(b_b[1] | 0, 0.3);\n  lprior += normal_lpdf(b_b[2] | 0, 0.3);\n  lprior += exponential_lpdf(sigma | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] nlp_a = rep_vector(0.0, N);\n    // initialize linear predictor term\n    vector[N] nlp_b = rep_vector(0.0, N);\n    // initialize non-linear predictor term\n    vector[N] mu;\n    nlp_a += X_a * b_a;\n    nlp_b += X_b * b_b;\n    for (n in 1:N) {\n      // compute non-linear predictor values\n      mu[n] = (0 + nlp_a[n] + nlp_b[n] * (C_1[n] - 0.215));\n    }\n    target += normal_lpdf(Y | mu, sigma);\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n}\n\n\nYou can also get that information by executing b9.1b$model or b9.1b$fit@stanmodel. We will explore very little raw Stan code in this ebook. However, if you would like to learn more about Stan code, I’ve written a short blog series on the Stan code underlying simple brms models, the first post of which you can find here. In addition, this book has a sister ebook Kurz (2024), in which I translated McElreath’s second edition into rstan code. You can find that ebook here.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Markov Chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "09.html#care-and-feeding-of-your-markov-chain",
    "href": "09.html#care-and-feeding-of-your-markov-chain",
    "title": "9  Markov Chain Monte Carlo",
    "section": "9.5 Care and feeding of your Markov chain",
    "text": "9.5 Care and feeding of your Markov chain\n\nMarkov chain Monte Carlo is a highly technical and usually automated procedure. You might write your own MCMC code, for the sake of learning. But it is very easy to introduce subtle biases. A package like Stan, in contrast, is continuously tested against expected output. Most people who use Stan don’t really understand what it is doing, under the hood. That’s okay. Science requires division of labor, and if every one of us had to write our own Markov chains from scratch, a lot less research would get done in the aggregate. (p. 287)\n\nIf you do want to learn more about HMC, McElreath has some nice introductory lectures on the topic (see here and here). To dive even deeper, Michael Betancourt from the Stan team has given many lectures on the topic (e.g., here and here).\n\n9.5.1 How many samples do you need?\nThe brms defaults are iter = 2000 and warmup = 1000, which are twice the number as in McElreath’s rethinking package.\n\nIf all you want are posterior means, it doesn’t take many samples at all to get very good estimates. Even a couple hundred samples will do. But if you care about the exact shape in the extreme tails of the posterior, the 99th percentile or so, then you’ll need many more. So there is no universally useful number of samples to aim for. In most typical regression applications, you can get a very good estimate of the posterior mean with as few as 200 effective samples. And if the posterior is approximately Gaussian, then all you need in addition is a good estimate of the variance, which can be had with one order of magnitude more, in most cases. For highly skewed posteriors, you’ll have to think more about which region of the distribution interests you. Stan will sometimes warn you about “tail ESS,” the effective sample size (similar to n_eff) in the tails of the posterior. In those cases, it is nervous about the quality of extreme intervals, like 95%. Sampling more usually helps. (pp. 287-288)\n\nAnd remember, with changes from brms version 2.10.0, we now have both Bulk_ESS and Tail_ESS to consult when thinking about the effective sample size. What McElreath referred to as n_eff is what we now think of as Bulk_ESS when using brms. When McElreath referred to the “tail ESS” in the end of that block quote, that’s our brms Tail_ESS number.\n\n9.5.1.1 Rethinking: Warmup is not burn-in\n\nOther MCMC algorithms and software often discuss burn-in….\nBut Stan’s sampling algorithms use a different approach. What Stan does during warmup is quite different from what it does after warmup. The warmup samples are used to adapt sampling, to find good values for the step size and the number of steps. Warmup samples are not representative of the target posterior distribution, no matter how long warmup continues. They are not burning in, but rather more like cycling the motor to heat things up and get ready for sampling. When real sampling begins, the samples will be immediately from the target distribution, assuming adaptation was successful. (p. 288)\n\n\n\n\n9.5.2 How many chains do you need?\n\nIt is very common to run more than one Markov chain, when estimating a single model. To do this with [brms], the chains argument specifies the number of independent Markov chains to sample from. And the optional cores argument lets you distribute the chains across different processors, so they can run simultaneously, rather than sequentially….\nfor typical regression models, you can live by the motto one short chain to debug, four chains for verification and inference. (pp. 288–289, emphasis in the original)\n\n\n9.5.2.1 Rethinking: Convergence diagnostics\nWe’ve already covered how brms has expanded the traditional notion of effective samples (i.e., n_eff) to Bulk_ESS and Tail_ESS. Times are changing for the \\(\\widehat R\\), too. However, it turns out the Stan team has found some deficiencies with the \\(\\widehat R\\), for which they’ve made recommendations that will be implemented in the Stan ecosystem sometime soon (see here for a related thread on the Stan Forums). In the meantime, you can read all about it in Vehtari et al. (2019) and in one of Dan Simpson’s blog posts.\nFor more on these topics, you might also check out Gabry and Modrák’s (2022) vignette, Visual MCMC diagnostics using the bayesplot package.\n\n\n\n9.5.3 Taming a wild chain\nAs with rethinking, brms can take data in the form of a list. Recall however, that in order to specify starting values, you need to specify a list of lists with an inits argument rather than with start.\n\nb9.2 &lt;- brm(\n  data = list(y = c(-1, 1)), \n  family = gaussian,\n  y ~ 1,\n  prior = c(prior(normal(0, 1000), class = Intercept),\n            prior(exponential(0.0001), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 3,\n  seed = 9,\n  file = \"fits/b09.02\")\n\nLet’s peek at the summary.\n\nprint(b9.2)\n\nWarning: There were 246 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See\nhttp://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: y ~ 1 \n   Data: list(y = c(-1, 1)) (Number of observations: 2) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    12.34    382.85  -803.54   985.42 1.03      300      306\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma   678.66   1291.29    12.02  3680.62 1.05       83       42\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nMuch like in the text, this summary is a disaster. Note the warning about divergent transitions. The brms::nuts_params() function allows use to pull a wealth of diagnostic information for the chains from a brms fit. The different kinds of diagnostics are listed in the Parameter column.\n\nnuts_params(b9.2) |&gt; \n  distinct(Parameter)\n\n      Parameter\n1 accept_stat__\n2    stepsize__\n3   treedepth__\n4  n_leapfrog__\n5   divergent__\n6      energy__\n\n\nOur interest is for when Parameter == \"divergent__\".\n\nnuts_params(b9.2) |&gt; \n  filter(Parameter == \"divergent__\") |&gt; \n  count(Value)\n\n  Value    n\n1     0 2754\n2     1  246\n\n\nThis indicates that among the 3,000 post-warmup draws, 393 were classified as divergent transitions. We can use the np argument within brms::pairs() to include this information in the pairs() plot.\n\npairs(b9.2, \n      # Currently not working with bayesplot 1.10.0+.\n      # See https://github.com/stan-dev/bayesplot/issues/298\n      # np = nuts_params(b9.2),\n      off_diag_args = list(size = 1/4))\n\n\n\n\n\n\n\n\nThat np = nuts_params(b9.2) trick will work in a similar way with bayesplot functions like mcmc_pairs() and mcmc_trace(). The red x marks show us where the divergent transitions are within the bivariate posterior. To my eye, the pattern in this plot isn’t very strong. Sometimes the pattern of divergent transitions can give you clear clues about where the problems are in the model.\nLet’s further inspect the damage by making the top two rows of Figure 9.9.\n\np1 &lt;- as_draws_df(b9.2) |&gt; \n  mcmc_trace(pars = vars(b_Intercept:sigma),\n             linewidth = 0.25)\n\np2 &lt;- as_draws_df(b9.2) |&gt; \n  mcmc_rank_overlay(pars = vars(b_Intercept:sigma))\n\n(\n  (p1 / p2) &\n    scale_color_pomological() &\n    theme_pomological_fancy(base_family = \"Marck Script\") &\n    theme(legend.position = \"none\")\n) +\n  plot_annotation(subtitle = \"These chains are not healthy\")\n\n\n\n\n\n\n\n\nOkay, that’s enough disaster. Let’s try a model that adds just a little information by way of weakly-regularizing priors:\n\\[\\begin{align*}\ny_i & \\sim \\operatorname{Normal}(\\mu, \\sigma) \\\\\n\\mu    & = \\alpha \\\\\n\\alpha & \\sim \\operatorname{Normal}(1, 10) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nWatch our new priors save the day.\n\nb9.3 &lt;- brm(\n  data = list(y = c(-1, 1)), \n  family = gaussian,\n  y ~ 1,\n  prior = c(prior(normal(1, 10), class = Intercept),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 3,\n  seed = 9,\n  file = \"fits/b09.03\")\n\n\nprint(b9.3)\n\n Family: gaussian \n  Links: mu = identity \nFormula: y ~ 1 \n   Data: list(y = c(-1, 1)) (Number of observations: 2) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.04      1.20    -2.45     2.49 1.00     1155      779\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.53      0.79     0.59     3.66 1.00      786     1255\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs in the text, no more warning signs and no more silly estimates. The trace and trank plots look better, too.\n\np1 &lt;- as_draws_df(b9.3) |&gt; \n  mcmc_trace(pars = vars(b_Intercept:sigma),\n             linewidth = 0.25)\n\np2 &lt;- as_draws_df(b9.3) |&gt; \n  mcmc_rank_overlay(pars = vars(b_Intercept:sigma)) +\n  ylim(35, NA)\n\n(\n  (p1 / p2) &\n    scale_color_pomological() &\n    theme_pomological_fancy(base_family = \"Marck Script\") &\n    theme(legend.position = \"none\")\n) +\n  plot_annotation(subtitle = \"Weakly informative priors cleared up the condition right away\")\n\n\n\n\n\n\n\n\nNow behold our version of Figure 9.10.\n\npost &lt;- as_draws_df(b9.3)\n\n# Left\np1 &lt;- post |&gt;\n  select(b_Intercept) |&gt;\n  \n  ggplot(aes(x = b_Intercept)) +\n  geom_density(trim = T) +\n  geom_line(data = tibble(x = seq(from = -15, to = 15, length.out = 50)),\n            aes(x = x, y = dnorm(x = x, mean = 0, sd = 10)),\n            color = pomological_palette[5], linetype = 2) +\n  xlab(expression(alpha))\n\n# Right\np2 &lt;- post |&gt;\n  select(sigma) |&gt;\n  \n  ggplot(aes(x = sigma)) +\n  geom_density(trim = T) +\n  geom_line(data = tibble(x = seq(from = 0, to = 10, length.out = 50)),\n            aes(x = x, y = dexp(x = x, rate = 1)),\n            color = pomological_palette[9], linetype = 2) +\n  labs(x = expression(sigma),\n       y = NULL) +\n  coord_cartesian(xlim = c(0, 10),\n                  ylim = c(0, 0.7))\n\n# Combine\n(\n  (p1 + p2) &\n    theme_pomological_fancy(base_family = \"Marck Script\")\n) + plot_annotation(subtitle = \"Prior (dashed) and posterior (solid) distributions for the\\nmodel with weakly-informative priors, b9.3\")\n\n\n\n\n\n\n\n\n\nThese weakly informative priors have helped by providing a very gentle nudge towards reasonable values of the parameters. Now values like 30 million are no longer equally plausible as small values like 1 or 2. Lots of problematic chains want subtle priors like these, designed to tune estimation by assuming a tiny bit of prior information about each parameter. And even though the priors end up getting washed out right away–two observations were enough here–they still have a big effect on inference, by allowing us to get an answer. (pp. 292–293)\n\n\n9.5.3.1 Rethinking: The folk theorem of statistical computing\n\nThe example above illustrates Andrew Gelman’s folk theorem of statistical computing: When you have computational problems, often there’s a problem with your model. Before we begin to tune the software and pour more computer power into a problem, it can be useful to go over the model specification again, and the data itself, to make sure the problem isn’t in the pre-sampling stage. (p. 293)\n\n\n\n9.5.3.2 Overthinking: Divergent transitions are your friend\n\nYou’ll see divergent transition warnings often in using [brms::brm()] and Stan. They are your friend, providing a helpful warning. These warnings arise when the numerical simulation that HMC uses is inaccurate. HMC can detect these inaccuracies. That is one of its major advantages over other sampling approaches, most of which provide few automatic ways to discover bad chains. (p. 293)\n\nThis is an issue that comes up frequently on the Stan Forums (here’s a link to the brms section). It’s a good idea to take divergent transitions seriously. Reaching out to others in the community is a great way to get guidance. But before you start a new post, make sure you look through the previous threads to keep from posting redundant questions.\n\n\n\n9.5.4 Non-identifiable parameters\nIt appears that the only way to get a brms version of McElreath’s m9.4 and m9.5 is to augment the data. In addition to the Gaussian y vector, we’ll add two constants to the data, intercept_1 = 1 and intercept_2 = 1.\n\nset.seed(9)\ny &lt;- rnorm(100, mean = 0, sd = 1)\n\n\nb9.4 &lt;- brm(\n  data = list(y  = y,\n              a1 = 1,\n              a2 = 1), \n  family = gaussian,\n  y ~ 0 + a1 + a2,\n  prior = c(prior(normal(0, 1000), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 3,\n  seed = 9,\n  file = \"fits/b09.04\")\n\nOur model results don’t perfectly mirror McElreath’s, but they’re identical in spirit.\n\nprint(b9.4)\n\nWarning: Parts of the model have not converged (some Rhats are &gt; 1.05). Be careful when analysing the results! We recommend\nrunning more iterations and/or setting stronger priors.\n\n\n Family: gaussian \n  Links: mu = identity \nFormula: y ~ 0 + a1 + a2 \n   Data: list(y = y, a1 = 1, a2 = 1) (Number of observations: 100) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na1   -27.88    400.74  -844.83   535.32 1.78        5       18\na2    27.83    400.74  -535.37   844.83 1.78        5       18\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.99      0.07     0.85     1.12 1.19       11       51\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote the frightening warning message. Those results are a mess! Let’s try again.\n\nb9.5 &lt;- brm(\n  data = list(y  = y,\n              a1 = 1,\n              a2 = 1), \n  family = gaussian,\n  y ~ 0 + a1 + a2,\n  prior = c(prior(normal(0, 10), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 3,\n  seed = 9,\n  file = \"fits/b09.05\")\n\n\nprint(b9.5)\n\n Family: gaussian \n  Links: mu = identity \nFormula: y ~ 0 + a1 + a2 \n   Data: list(y = y, a1 = 1, a2 = 1) (Number of observations: 100) \n  Draws: 3 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 3000\n\nRegression Coefficients:\n   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na1     0.19      7.08   -13.40    13.96 1.00      768     1002\na2    -0.24      7.08   -14.06    13.32 1.00      769     1055\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.97      0.07     0.85     1.11 1.00      982      960\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n“The estimates for a1 and a2 are better identified now. Well, they still aren’t individually identified. But their sum is identified” (p. 296). Now it’s time to make our version of Figure 9.11. Before we do, one of the challenges we’ll have to overcome is the bayesplot::mcmc_rank_overlay() doesn’t seem to give users an easy way to control the faceting behavior of the plotting function. If you try to simultaneously plot all three model parameters with mcmc_rank_overlay(), you’ll end up with the one row and three columns. But I want the reverse. One solution is to make a custom plotting function. Since we’re juggling both mcmc_trace() and mcmc_rank_overlay(), we’ll make the function do both at once. The trick will be to set the function and workflow up so that we only enter in one parameter at a time. Here’s the function.\n\ntrace_rank &lt;- function(data, var, subtitle = NULL, ymin = NA) {\n  p1 &lt;- data |&gt; \n    mcmc_trace(pars = var,\n               linewidth = 0.25,\n               facet_args = list(ncol = 1)) +\n    labs(y = NULL,\n         subtitle = subtitle) +\n    facet_wrap(~ parameter)\n  \n  p2 &lt;- data |&gt;\n    mcmc_rank_overlay(pars = var) +\n    coord_cartesian(ylim = c(ymin, NA)) +\n    xlab(NULL)\n  \n  p1 + p2\n}\n\nNow use our custom trace_rank() function to make the six rows one at a time and then combine them with a little patchwork at the end to make our version of Figure 9.11.\n\n# `b9.4`\npost &lt;- as_draws_df(b9.4)\n\np1 &lt;- trace_rank(data = post, var = \"b_a1\", subtitle = \"b9.4 (bad priors)\")\np2 &lt;- trace_rank(data = post, var = \"b_a2\")\np3 &lt;- trace_rank(data = post, var = \"sigma\")\n\n# `b9.5`\npost &lt;- as_draws_df(b9.5)\n\np4 &lt;- trace_rank(data = post, var = \"b_a1\", subtitle = \"b9.5 (good priors)\", ymin = 30)\np5 &lt;- trace_rank(data = post, var = \"b_a2\", ymin = 30)\np6 &lt;- trace_rank(data = post, var = \"sigma\", ymin = 30)\n\n# Combine!\n(p1 / p2 / p3 / p4 / p5 / p6) &\n  scale_color_pomological() &\n  theme_pomological_fancy(base_family = \"Marck Script\") &\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe central message in the text, default to weakly-regularizing priors, holds for brms just as it does for rethinking. For more on the topic, see the recommendations from the Stan team. If you want to dive deeper, check out Simpson’s post on Gelman’s blog, (It’s never a) total eclipse of the prior and their corresponding (2017) paper with Betancourt, The prior can often only be understood in the context of the likelihood.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Markov Chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "09.html#session-info",
    "href": "09.html#session-info",
    "title": "9  Markov Chain Monte Carlo",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggmcmc_1.5.1.2        bayesplot_1.15.0.9000 GGally_2.4.0          tidybayes_3.0.7       brms_2.23.0          \n [6] Rcpp_1.1.0            patchwork_1.3.2       lubridate_1.9.4       forcats_1.0.1         stringr_1.6.0        \n[11] dplyr_1.1.4           purrr_1.2.1           readr_2.1.5           tidyr_1.3.2           tibble_3.3.1         \n[16] tidyverse_2.0.0       ggpomological_0.1.2   ggplot2_4.0.1        \n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.8            tidyselect_1.2.1        farver_2.1.2            loo_2.9.0.9000          S7_0.2.1               \n [6] fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39           timechange_0.3.0       \n[11] estimability_1.5.1      lifecycle_1.0.5         StanHeaders_2.36.0.9000 survival_3.8-3          processx_3.8.6         \n[16] magrittr_2.0.4          posterior_1.6.1.9000    compiler_4.5.1          rlang_1.1.7             tools_4.5.1            \n[21] utf8_1.2.6              yaml_2.3.12             knitr_1.51              labeling_0.4.3          bridgesampling_1.2-1   \n[26] htmlwidgets_1.6.4       curl_7.0.0              pkgbuild_1.4.8          plyr_1.8.9              RColorBrewer_1.1-3     \n[31] cmdstanr_0.9.0          abind_1.4-8             multcomp_1.4-29         withr_3.0.2             grid_4.5.1             \n[36] stats4_4.5.1            inline_0.3.21           xtable_1.8-4            extrafontdb_1.1         emmeans_1.11.2-8       \n[41] scales_1.4.0            rethinking_2.42         MASS_7.3-65             isoband_0.3.0           cli_3.6.5              \n[46] mvtnorm_1.3-3           rmarkdown_2.30          generics_0.1.4          RcppParallel_5.1.11-1   rstudioapi_0.17.1      \n[51] reshape2_1.4.5          tzdb_0.5.0              rstan_2.36.0.9000       splines_4.5.1           parallel_4.5.1         \n[56] matrixStats_1.5.0       vctrs_0.7.0             V8_8.0.1                Matrix_1.7-3            sandwich_3.1-1         \n[61] jsonlite_2.0.0          arrayhelpers_1.1-0      hms_1.1.4               ggdist_3.3.3            glue_1.8.0             \n[66] ggstats_0.11.0          codetools_0.2-20        ps_1.9.1                distributional_0.5.0    stringi_1.8.7          \n[71] shape_1.4.6.1           gtable_0.3.6            QuickJSR_1.8.1          extrafont_0.20          pillar_1.11.1          \n[76] htmltools_0.5.9         Brobdingnag_1.2-9       R6_2.6.1                evaluate_1.0.5          lattice_0.22-7         \n[81] backports_1.5.0         rstantools_2.5.0.9000   gridExtra_2.3           coda_0.19-4.1           nlme_3.1-168           \n[86] Rttf2pt1_1.3.14         checkmate_2.3.3         xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Markov Chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "09.html#comments",
    "href": "09.html#comments",
    "title": "9  Markov Chain Monte Carlo",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nAden-Buie, G. (2022). ggpomological: Pomological plot theme for ggplot2 [Manual]. https://github.com/gadenbuie/ggpomological\n\n\nBrilleman, S., Crowther, M., Moreno-Betancur, M., Buros Novik, J., & Wolfe, R. (2018). Joint longitudinal and time-to-event models via Stan. https://github.com/stan-dev/stancon_talks/\n\n\nBürkner, P.-C. (2022). Estimating non-linear models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html\n\n\nCasella, G., & George, E. I. (1992). Explaining the Gibbs sampler. The American Statistician, 46(3), 167–174. https://doi.org/10.1080/00031305.1992.10475878\n\n\nFernández i Marín, X. (2016). ggmcmc: Analysis of MCMC samples and Bayesian inference. Journal of Statistical Software, 70(9), 1–20. https://doi.org/10.18637/jss.v070.i09\n\n\nFernández i Marín, X. (2021). ggmcmc: Tools for analyzing MCMC simulations from Bayesian inference [Manual]. https://CRAN.R-project.org/package=ggmcmc\n\n\nGabry, J., & Goodrich, B. (2022). rstanarm: Bayesian applied regression modeling via stan [Manual]. https://CRAN.R-project.org/package=rstanarm\n\n\nGabry, J., & Modrák, M. (2022). Visual MCMC diagnostics using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/visual-mcmc-diagnostics.html\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/\n\n\nGelman, A., Simpson, D., & Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies, 19(10), 555. https://doi.org/10.3390/e19100555\n\n\nGeman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-6(6), 721–741. https://doi.org/10.1109/TPAMI.1984.4767596\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKurz, A. S. (2024). Statistical Rethinking 2 with rstan and the tidyverse (version 0.0.3). https://solomon.quarto.pub/sr2rstan/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMerkle, E. C., Fitzsimmons, E., Uanhoro, J., & Goodrich, B. (2021). Efficient Bayesian structural equation modeling in Stan. Journal of Statistical Software, 100(6), 1–22. https://doi.org/10.18637/jss.v100.i06\n\n\nMerkle, E. C., & Rosseel, Y. (2018). blavaan: Bayesian structural equation models via parameter expansion. Journal of Statistical Software, 85(4), 1–30. https://doi.org/10.18637/jss.v085.i04\n\n\nMerkle, E. C., Rosseel, Y., & Goodrich, B. (2022). blavaan: Bayesian latent variable analysis. https://CRAN.R-project.org/package=blavaan\n\n\nPlummer, M. (2003). JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling. Working Papers, 8. http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Plummer.pdf\n\n\nRobert, C., & Casella, G. (2011). A short history of Markov chain Monte Carlo: Subjective recollections from incomplete data. Statistical Science, 26(1), 102–115. https://arxiv.org/pdf/0808.2902.pdf\n\n\nSpiegelhalter, D., Thomas, A., Best, N., & Lunn, D. (2003). WinBUGS user manual. https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/manual14.pdf\n\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved for assessing convergence of MCMC. https://arxiv.org/abs/1903.08008?\n\n\nWeber, S., & Bürkner, P.-C. (2022). Running brms models with within-chain parallelization. https://CRAN.R-project.org/package=brms/vignettes/brms_threading.html",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Markov Chain Monte Carlo</span>"
    ]
  },
  {
    "objectID": "10.html",
    "href": "10.html",
    "title": "10  Big Entropy and the Generalized Linear Model",
    "section": "",
    "text": "10.0.0.1 Rethinking: Bayesian updating is entropy maximization",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Big Entropy and the Generalized Linear Model</span>"
    ]
  },
  {
    "objectID": "10.html#maximum-entropy",
    "href": "10.html#maximum-entropy",
    "title": "10  Big Entropy and the Generalized Linear Model",
    "section": "10.1 Maximum entropy",
    "text": "10.1 Maximum entropy\n\nIn Chapter 7, you met the basics of information theory. In brief, we seek a measure of uncertainty that satisfies three criteria: (1) the measure should be continuous; (2) it should increase as the number of possible events increases; and (3) it should be additive. The resulting unique measure of the uncertainty of a probability distribution \\(p\\) with probabilities \\(p_i\\) for each possible event \\(i\\) turns out to be just the average log-probability:\n\\[H(p) = - \\sum_i p_i \\log p_i\\]\nThis function is known as information entropy.\nThe principle of maximum entropy applies this measure of uncertainty to the problem of choosing among probability distributions. Perhaps the simplest way to state the maximum entropy principle is:\n\nThe distribution that can happen the most ways is also the distribution with the biggest information entropy. The distribution with the biggest entropy is the most conservative distribution that obeys its constraints.\n\nThere’s nothing intuitive about this idea, so if it seems weird, you are normal. (pp. 300–301, emphasis in the original)\n\nLet’s execute the code for the pebbles-in-buckets example.\n\nlibrary(tidyverse)\n\nd &lt;- tibble(a = c(0, 0, 10, 0, 0),\n            b = c(0, 1, 8, 1, 0),\n            c = c(0, 2, 6, 2, 0),\n            d = c(1, 2, 4, 2, 1),\n            e = 2)\n\n# This is our analogue to McElreath's `lapply()` code\nd |&gt; \n  mutate(across(.cols = everything(), .fns = \\(x) x / sum(x))) |&gt; \n  # The next few lines constitute our analogue to his `sapply()` code\n  pivot_longer(everything(), names_to = \"plot\") |&gt; \n  group_by(plot) |&gt; \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value))))\n\n# A tibble: 5 × 2\n  plot      h\n  &lt;chr&gt; &lt;dbl&gt;\n1 a     0    \n2 b     0.639\n3 c     0.950\n4 d     1.47 \n5 e     1.61 \n\n\nFor more on the formula syntax we used within mutate(across()), you might check out this.\nAnyway, we’re almost ready to plot, which brings us to color. For the plots in this chapter, we’ll be taking our color palettes from the ghibli package (Henderson, 2022), which provides palettes based on scenes from anime films by the Studio Ghibli.\n\nlibrary(ghibli)\n\nThe main function is ghibli_palette() which you can use to both preview the palettes before using them and also index in order to use specific colors. For example, we’ll play with “MarnieMedium1”, first.\n\nghibli_palette(\"MarnieMedium1\")\n\n\n\n\n\n\n\nghibli_palette(\"MarnieMedium1\")[1:7]\n\n[1] \"#28231DFF\" \"#5E2D30FF\" \"#008E90FF\" \"#1C77A3FF\" \"#C5A387FF\" \"#67B8D6FF\" \"#E9D097FF\"\n\n\nNow we’re ready to plot five of the six panels of Figure 10.1.\n\n# For annotation\nd_text &lt;- tibble(\n    letter  = letters[1:5],\n    bucket  = 5.5,\n    pebbles = 10.5,\n    label   = str_c(c(1, 90, 1260, 37800, 113400), \n                    rep(c(\" way\", \" ways\"), times = c(1, 4))))\nd |&gt; \n  mutate(bucket = 1:5) |&gt; \n  pivot_longer(-bucket,\n               names_to = \"letter\",\n               values_to = \"pebbles\") |&gt; \n  \n  ggplot(aes(x = bucket, y = pebbles)) +\n  geom_col(width = 1/5, fill = ghibli_palette(\"MarnieMedium1\")[2]) +\n  geom_text(aes(y = pebbles + 1, label = pebbles)) +\n  geom_text(data = d_text,\n            aes(label = label), \n            hjust = 1) +\n  scale_y_continuous(breaks = c(0, 5, 10), limits = c(0, 12)) +\n  facet_wrap(~ letter, ncol = 2) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"MarnieMedium1\")[6]),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = ghibli_palette(\"MarnieMedium1\")[7]))\n\n\n\n\n\n\n\n\nWe might plot our version of the final panel like so.\n\nd |&gt; \n  # The next four lines are the same from above\n  mutate(across(.cols = everything(), .fns = \\(x) x / sum(x))) |&gt; \n  pivot_longer(everything()) |&gt; \n  group_by(name) |&gt; \n  summarise(h = -sum(ifelse(value == 0, 0, value * log(value)))) |&gt; \n  # Here's the R code 9.4 stuff\n  mutate(n_ways = c(1, 90, 1260, 37800, 113400)) |&gt; \n  group_by(name) |&gt; \n  mutate(log_ways = log(n_ways) / 10,\n         text_y   = ifelse(name &lt; \"c\", h + .15, h - .15)) |&gt;\n  \n  # Plot\n  ggplot(aes(x = log_ways, y = h)) +\n  geom_abline(intercept = 0, slope = 1.37, color = \"white\") +\n  geom_point(size = 2.5, color = ghibli_palette(\"MarnieMedium1\")[7]) +\n  geom_text(aes(y = text_y, label = name)) +\n  labs(x = \"log(ways) per pebble\",\n       y = \"entropy\") +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"MarnieMedium1\")[6]),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n“The distribution that can happen the greatest number of ways is the most plausible distribution. Call this distribution the maximum entropy distribution” (p. 303, emphasis in the original). Among the pebbles, the maximum entropy distribution was e (i.e., the uniform).\n\n10.1.0.1 Rethinking: What good is intuition?\n“Like many aspects of information theory, maximum entropy is not very intuitive. But note that intuition is just a guide to developing methods. When a method works, it hardly matters whether our intuition agrees” (p. 303).\n\n\n10.1.1 Gaussian\nBehold the probability density for the generalized normal distribution:\n\\[\\Pr (y \\mid \\mu, \\alpha, \\beta) = \\frac{\\beta}{2 \\alpha \\Gamma \\left (\\frac{1}{\\beta} \\right )} e ^ {- \\left (\\frac{|y - \\mu|}{\\alpha} \\right ) ^ {\\beta}},\\]\nwhere \\(\\alpha =\\) the scale, \\(\\beta =\\) the shape, \\(\\mu =\\) the location, and \\(\\Gamma =\\) the gamma function. If you read closely in the text, you’ll discover that the densities in the right panel of Figure 10.2 were all created with the constraint \\(\\sigma^2 = 1\\). But \\(\\sigma^2 \\neq \\alpha\\) and there’s no \\(\\sigma\\) in the equations in the text. However, it appears the variance for the generalized normal distribution follows the form\n\\[\\sigma^2 = \\frac{\\alpha^2 \\Gamma (3/\\beta)}{\\Gamma (1/\\beta)}.\\]\nSo if you do the algebra, you’ll see that you can compute \\(\\alpha\\) for a given \\(\\sigma^2\\) and \\(\\beta\\) with the equation\n\\[\\alpha = \\sqrt{ \\frac{\\sigma^2 \\Gamma (1/\\beta)}{\\Gamma (3/\\beta)} }.\\]\nI got the formula from Wikipedia.com. Don’t judge. We can wrap that formula in a custom function, alpha_per_beta(), use it to solve for the desired \\(\\beta\\) values, and plot. But one more thing: McElreath didn’t tell us exactly which \\(\\beta\\) values the left panel of Figure 10.2 was based on. So the plot below is my best guess.\n\nalpha_per_beta &lt;- function(beta, variance = 1) {\n  sqrt((variance * gamma(1 / beta)) / gamma(3 / beta))\n}\n\ncrossing(value = seq(from = -5, to = 5, by = 0.1),\n         # I arrived at these values by trial and error\n         beta  = c(1, 1.5, 2, 4)) |&gt; \n  mutate(mu    = 0,\n         alpha = alpha_per_beta(beta)) |&gt; \n  # Behold the formula for the generalized normal distribution in code!\n  mutate(density = (beta / (2 * alpha * gamma(1 / beta))) * \n           exp(1) ^ (-1 * (abs(value - mu) / alpha) ^ beta)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = value, y = density, group = beta)) +\n  geom_line(aes(color = beta == 2, linewidth = beta == 2)) +\n  scale_color_manual(values = c(ghibli_palette(\"MarnieMedium2\")[c(2, 4)])) +\n  scale_linewidth_manual(values = c(1/4, 1.25)) +\n  labs(subtitle = \"Guess which color denotes the Gaussian.\") +\n  coord_cartesian(xlim = c(-4, 4)) +\n  theme(legend.position = \"none\",\n        panel.background = element_rect(fill = ghibli_palette(\"MarnieMedium2\")[7]),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nOnce you have \\(\\alpha\\) and \\(\\beta\\), the entropy equation for the generalized normal distribution is\n\\[\n\\text{entropy} = \\frac{1}{\\beta} -\\log\\left[\\frac{\\beta}{2\\alpha\\Gamma(1/\\beta)}\\right].\n\\]\nHere’s how we can use that equation to make our version of right panel of Figure 10.2.\n\ntibble(beta = seq(from = 1, to = 4, length.out = 100)) |&gt; \n  mutate(alpha = alpha_per_beta(beta)) |&gt; \n  mutate(entropy = (1 / beta) - log((beta) / (2 * alpha * gamma(1 / beta))))  |&gt; \n  \n  ggplot(aes(x = beta, y = entropy)) +\n  geom_vline(xintercept = 2, color = \"white\") +\n  geom_line(linewidth = 2, color = ghibli_palette(\"MarnieMedium2\")[6]) +\n  xlab(expression(beta~(i.e.*\", \"*shape))) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"MarnieMedium2\")[7]),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nI should note the solution to this plot was initially beyond me. However, fellow enthusiast Hamed Bastan-Hagh generously shared the correct solution on GitHub.\nBut getting back on track:\n\nThe take-home lesson from all of this is that, if all we are willing to assume about a collection of measurements is that they have a finite variance, then the Gaussian distribution represents the most conservative probability distribution to assign to those measurements. But very often we are comfortable assuming something more. And in those cases, provided our assumptions are good ones, the principle of maximum entropy leads to distributions other than the Gaussian. (p. 306)\n\n\n\n10.1.2 Binomial\nThe binomial likelihood entails\n\ncounting the numbers of ways that a given observation could arise, according to our assumptions. The resulting distribution is known as the binomial distribution. If only two things can happen (blue or white marble, for example), and there’s a constant chance \\(p\\) of each across \\(n\\) trials, then the probability of observing \\(y\\) events of type 1 and \\(n - y\\) events of type 2 is:\n\\[\\Pr (y \\mid n, p) = \\frac{n!}{y! (n - y)!} p^y (1 - p)^{n - y}\\]\nIt may help to note that the fraction with the factorials is just saying how many different ordered sequences of \\(n\\) outcomes have a count of \\(y\\). (p. 307, emphasis in the original)\n\nFor me, that last sentence made more sense when I walked it out in an example. To do so, let’s wrap that fraction of factorials into a function.\n\ncount_ways &lt;- function(n, y) {\n  # `n` is the total number of trials (i.e., the number of rows in your vector)\n  # `y` is the total number of 1s (i.e., successes) in your vector\n  (factorial(n) / (factorial(y) * factorial(n - y)))\n}\n\nNow consider three sequences:\n\n0, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 0\\))\n1, 0, 0, 0 (i.e., \\(n = 4\\) and \\(y = 1\\))\n1, 1, 0, 0 (i.e., \\(n = 4\\) and \\(y = 2\\))\n\nWe can organize that information in a little tibble and then demo our count_ways() function.\n\ntibble(sequence = 1:3,\n       n        = 4,\n       y        = c(0, 1, 2)) |&gt; \n  mutate(n_ways = count_ways(n = n, y = y))\n\n# A tibble: 3 × 4\n  sequence     n     y n_ways\n     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1        1     4     0      1\n2        2     4     1      4\n3        3     4     2      6\n\n\nHere’s the pre-Figure 10.3 data McElreath presented on page 308.\n\n# Data\nd &lt;- tibble(distribution = letters[1:4],\n            ww = c(1/4, 2/6, 1/6, 1/8),\n            bw = c(1/4, 1/6, 2/6, 4/8),\n            wb = c(1/4, 1/6, 2/6, 2/8),\n            bb = c(1/4, 2/6, 1/6, 1/8))\n\n# Table\nd |&gt; \n  mutate(across(.cols = ww:bb, .fns = \\(x) MASS::fractions(x) |&gt; as.character())) |&gt; \n  flextable::flextable()\n\ndistributionwwbwwbbba1/41/41/41/4b1/31/61/61/3c1/61/31/31/6d1/81/21/41/8\n\n\nThose data take just a tiny bit of wrangling before they’re ready to plot in our version of Figure 10.3.\n\nd &lt;- d |&gt; \n  pivot_longer(-distribution,\n               names_to = \"sequence\", \n               values_to = \"probability\") |&gt; \n  mutate(sequence = factor(sequence, levels = c(\"ww\", \"bw\", \"wb\", \"bb\")))\n\nd  |&gt; \n  ggplot(aes(x = sequence, y = probability, group = 1)) +\n  geom_point(size = 2, color = ghibli_palette(\"PonyoMedium\")[4]) +\n  geom_line(color = ghibli_palette(\"PonyoMedium\")[5]) +\n  labs(x = NULL, y = NULL) +\n  coord_cartesian(ylim = 0:1) +\n  facet_wrap(~ distribution) +\n  theme(axis.ticks.x = element_blank(),\n        panel.background = element_rect(fill = ghibli_palette(\"PonyoMedium\")[2]),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = ghibli_palette(\"PonyoMedium\")[6]))\n\n\n\n\n\n\n\n\nIf we go step by step, we might count the expected value for each distribution like follows.\n\nd |&gt; \n  # `str_count()` will count the number of times \"b\" occurs \n  # within a given row of `sequence`\n  mutate(n_b = str_count(sequence, \"b\")) |&gt; \n  mutate(product = probability * n_b) |&gt; \n  group_by(distribution) |&gt; \n  summarise(expected_value = sum(product))\n\n# A tibble: 4 × 2\n  distribution expected_value\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 a                         1\n2 b                         1\n3 c                         1\n4 d                         1\n\n\nWe can use the same group_by() strategy on the way to computing the entropy values.\n\nd |&gt; \n  group_by(distribution) |&gt; \n  summarise(entropy = -sum(probability * log(probability)))\n\n# A tibble: 4 × 2\n  distribution entropy\n  &lt;chr&gt;          &lt;dbl&gt;\n1 a               1.39\n2 b               1.33\n3 c               1.33\n4 d               1.21\n\n\nLike in the text, distribution == \"a\" had the largest entropy of the four. In the next example, the \\(\\text{expected value} = 1.4\\) and \\(p = 0.7\\).\n\np &lt;- 0.7\n\na &lt;- c((1 - p)^2, \n       p * (1 - p), \n       (1 - p) * p, \n       p^2)\n\na\n\n[1] 0.09 0.21 0.21 0.49\n\n\nHere’s the entropy for our distribution a.\n\n-sum(a * log(a))\n\n[1] 1.221729\n\n\nI’m going to alter McElreath’s simulation function from R code 10.9 to take a seed argument. In addition, I altered the names of the objects within the function and changed the output to a tibble that will also include the conditions “ww”, “bw”, “wb”, and “bb”.\n\nsim_p &lt;- function(seed, g = 1.4) {\n  set.seed(seed)\n  \n  x_123 &lt;- runif(3)\n  x_4   &lt;- ((g) * sum(x_123) - x_123[2] - x_123[3]) / (2 - g)\n  z     &lt;- sum(c(x_123, x_4))\n  p     &lt;- c(x_123, x_4) / z\n  \n  tibble(h   = -sum(p * log(p)), \n         p   = p,\n         key = factor(c(\"ww\", \"bw\", \"wb\", \"bb\"), levels = c(\"ww\", \"bw\", \"wb\", \"bb\")))\n}\n\nFor a given seed and g value, our augmented sim_p() function returns a \\(4 \\times 3\\) tibble.\n\nsim_p(seed = 9.9, g = 1.4)\n\n# A tibble: 4 × 3\n      h      p key  \n  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n1  1.02 0.197  ww   \n2  1.02 0.0216 bw   \n3  1.02 0.184  wb   \n4  1.02 0.597  bb   \n\n\nSo the next step is to determine how many replications we’d like, create a tibble with seed values ranging from 1 to that number, and then feed those seed values into sim_p() via purrr::map2(), which will return a nested tibble. We’ll then unnest() and take a peek.\n\n# How many replications would you like?\nn_rep &lt;- 1e5\n\nd &lt;- tibble(seed = 1:n_rep) |&gt; \n  mutate(sim = map2(.x = seed, .y = 1.4, .f = sim_p)) |&gt; \n  unnest(sim)\n\nTake a look.\n\nhead(d)\n\n# A tibble: 6 × 4\n   seed     h      p key  \n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;\n1     1  1.21 0.108  ww   \n2     1  1.21 0.151  bw   \n3     1  1.21 0.233  wb   \n4     1  1.21 0.508  bb   \n5     2  1.21 0.0674 ww   \n6     2  1.21 0.256  bw   \n\n\nIn order to intelligently choose which four replications we want to highlight in Figure 10.4, we’ll want to rank order them by entropy, h.\n\nranked_d &lt;- d |&gt; \n  group_by(seed) |&gt; \n  arrange(desc(h)) |&gt; \n  ungroup() |&gt;\n  # Here's the rank order step\n  mutate(rank = rep(1:n_rep, each = 4))\n\nhead(ranked_d)\n\n# A tibble: 6 × 5\n   seed     h      p key    rank\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;\n1 55665  1.22 0.0903 ww        1\n2 55665  1.22 0.209  bw        1\n3 55665  1.22 0.210  wb        1\n4 55665  1.22 0.490  bb        1\n5 71132  1.22 0.0902 ww        2\n6 71132  1.22 0.210  bw        2\n\n\nAnd we’ll also want a subset of the data to correspond to McElreath’s “A” through “D” distributions.\n\nsubset_d &lt;- ranked_d |&gt;\n  # I arrived at these `rank` values by trial and error\n  filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) |&gt; \n  # I arrived at the `height` values by trial and error, too\n  mutate(height       = rep(c(8, 2.25, 0.75, 0.5), each = 4),\n         distribution = rep(letters[1:4], each = 4))\n\nhead(subset_d)\n\n# A tibble: 6 × 7\n   seed     h      p key    rank height distribution\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;       \n1 55665 1.22  0.0903 ww        1   8    a           \n2 55665 1.22  0.209  bw        1   8    a           \n3 55665 1.22  0.210  wb        1   8    a           \n4 55665 1.22  0.490  bb        1   8    a           \n5 50981 1.000 0.0459 ww    87373   2.25 b           \n6 50981 1.000 0.0459 bw    87373   2.25 b           \n\n\nWe’re finally ready to make our version of the left panel of Figure 10.4.\n\np1 &lt;- d |&gt; \n  ggplot(aes(x = h)) +\n  geom_density(linewidth = 0, fill = ghibli_palette(\"LaputaMedium\")[3],\n               adjust = 1/4) +\n  # Note the data statements for the next two geoms\n  geom_linerange(data = subset_d |&gt; group_by(seed) |&gt; slice(1),\n                 aes(ymin = 0, ymax = height),\n                 color = ghibli_palette(\"LaputaMedium\")[5]) +\n  geom_text(data = subset_d |&gt; group_by(seed) |&gt; slice(1),\n            aes(y = height + 0.5, label = distribution)) +\n  scale_x_continuous(\"Entropy\", breaks = seq(from = 0.7, to = 1.2, by = 0.1)) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"LaputaMedium\")[7]),\n        panel.grid = element_blank())\n\nDid you notice how our adjust = 1/4 with geom_density() served a similar function to the adj=0.1 in McElreath’s dens() code? Anyways, here we make the right panel and combine the two with patchwork.\n\np2 &lt;- ranked_d |&gt;\n  filter(rank %in% c(1, 87373, n_rep - 1500, n_rep - 10)) |&gt; \n  mutate(distribution = rep(letters[1:4], each = 4)) |&gt; \n  \n  ggplot(aes(x = key, y = p, group = 1)) +\n  geom_line(color = ghibli_palette(\"LaputaMedium\")[5]) +\n  geom_point(color = ghibli_palette(\"LaputaMedium\")[4], size = 2, ) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 0.75)) +\n  xlab(NULL) +\n  facet_wrap(~ distribution) +\n  theme(axis.ticks.x = element_blank(),\n        panel.background = element_rect(fill = ghibli_palette(\"LaputaMedium\")[7]),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = ghibli_palette(\"LaputaMedium\")[6]))\n\n# Combine and display\nlibrary(patchwork)\np1 | p2\n\n\n\n\n\n\n\n\nBecause we simulated, our values won’t match up identically with those in the text. We got pretty close, eh?\nSince we saved our sim_p() output in a nested tibble, which we then unnested(), there’s no need to separate the entropy values from the distributional values the way McElreath did in his R code 10.11. If we wanted to determine our highest entropy value–and the corresponding seed and p values, while we’re at it–, we might execute something like this.\n\nranked_d |&gt; \n  group_by(key) |&gt; \n  arrange(desc(h)) |&gt; \n  slice(1)\n\n# A tibble: 4 × 5\n# Groups:   key [4]\n   seed     h      p key    rank\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;int&gt;\n1 55665  1.22 0.0903 ww        1\n2 55665  1.22 0.209  bw        1\n3 55665  1.22 0.210  wb        1\n4 55665  1.22 0.490  bb        1\n\n\nThat maximum h value matched up nicely with the one in the text. If you look at the p column, you’ll see our values approximated McElreath’s distribution values, too. In both cases, they’re real close to the a values we computed, above.\n\na\n\n[1] 0.09 0.21 0.21 0.49\n\n\n“All four of these distributions really do have expected value 1.4. But among the infinite distributions that satisfy this constraint, it is only the most even distribution, the exact one nominated by the binomial distribution, that has greatest entropy” (p. 310).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Big Entropy and the Generalized Linear Model</span>"
    ]
  },
  {
    "objectID": "10.html#generalized-linear-models",
    "href": "10.html#generalized-linear-models",
    "title": "10  Big Entropy and the Generalized Linear Model",
    "section": "10.2 Generalized linear models",
    "text": "10.2 Generalized linear models\n\nFor an outcome variable that is continuous and far from any theoretical maximum or minimum, [a simple] Gaussian model has maximum entropy. But when the outcome variable is either discrete or bounded, a Gaussian likelihood is not the most powerful choice. (p. 312)\n\nI winged the values for our Figure 10.5.\n\ntibble(x = seq(from = -1, to = 3, by = 0.01)) |&gt;\n  mutate(probability = 0.35 + x * 0.5) |&gt; \n  \n  ggplot(aes(x = x, y = probability)) +\n  geom_rect(xmin = -1, xmax = 3,\n            ymin = 0,  ymax = 1,\n            fill = ghibli_palette(\"MononokeMedium\")[5]) +\n  geom_hline(yintercept = 0:1, linetype = 2, color = ghibli_palette(\"MononokeMedium\")[7]) +\n  geom_line(aes(linetype = probability &gt; 1, color = probability &gt; 1),\n            linewidth = 1) +\n  geom_segment(x = 1.3, xend = 3,\n               y = 1, yend = 1,\n               linewidth = 2/3, color = ghibli_palette(\"MononokeMedium\")[3]) +\n  annotate(geom = \"text\",\n           x = 1.28, y = 1.04, hjust = 1,\n           label = \"This is why we need link functions\",\n           color = ghibli_palette(\"MononokeMedium\")[4], size = 2.6) + \n  scale_y_continuous(breaks = c(0, 0.5, 1)) +\n  scale_color_manual(values = c(ghibli_palette(\"MononokeMedium\")[3:4])) +\n  coord_cartesian(xlim = c(0, 2),\n                  ylim = c(0, 1.2)) +\n  theme(legend.position = \"none\",\n        panel.background = element_rect(fill = ghibli_palette(\"MononokeMedium\")[1]),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\n\nLuckily, it’s easy to do better. By using all of our prior knowledge about the outcome variable, usually in the form of constraints on the possible values it can take, we can appeal to maximum entropy for the choice of distribution. Then all we have to do is generalize the linear regression strategy–replace a parameter describing the shape of the likelihood with a linear model–to probability distributions other than the Gaussian. (p. 313)\n\nAs we will see, doing better will often involve using link functions.\n\n10.2.0.1 Rethinking: The scourge of Histomancy\n\nOne strategy for choosing an outcome distribution is to plot the histogram of the outcome variable and, by gazing into its soul, decide what sort of distribution function to use. Call this strategy Histomancy, the ancient art of divining likelihood functions from empirical histograms. This sorcery is used, for example, when testing for normality before deciding whether or not to use a non-parametric procedure. Histomancy is a false god. (p. 314, emphasis in the original)\n\nStop worshiping at alter of this false god. Use domain knowledge and principles maximum entropy to pick your likelihoods.\n\n\n10.2.1 Meet the family\n\nThe most common distributions used in statistical modeling are members of a family known as the exponential family. Every member of this family is a maximum entropy distribution, for some set of constraints. And conveniently, just about every other statistical modeling tradition employs the exact same distributions, even though they arrive at them via justifications other than maximum entropy. (p. 314, emphasis in the original)\n\nHere are the Gamma and Exponential panels for Figure 10.6.\n\nlength_out &lt;- 100\n\ntibble(x = seq(from = 0, to = 5, length.out = length_out)) |&gt; \n  mutate(Gamma       = dgamma(x, shape = 2, rate = 2),\n         Exponential = dexp(x)) |&gt; \n  pivot_longer(-x, values_to = \"density\") |&gt; \n  mutate(label = ifelse(name == \"Gamma\", \"y %~% Gamma(lambda, kappa)\", \"y %~% Exponential(lambda)\")) |&gt; \n  \n  ggplot(aes(x = x, y = density)) +\n  geom_area(fill = ghibli_palette(\"SpiritedMedium\")[3]) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 4)) +\n  facet_wrap(~ label, scales = \"free_y\", labeller = label_parsed) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"SpiritedMedium\")[5]),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = ghibli_palette(\"SpiritedMedium\")[7]))\n\n\n\n\n\n\n\n\nThe Gaussian:\n\ntibble(x = seq(from = -5, to = 5, length.out = length_out)) |&gt; \n  mutate(density = dnorm(x),\n         strip   = \"y %~% Normal(mu, sigma)\") |&gt; \n  \n  ggplot(aes(x = x, y = density)) +\n  geom_area(fill = ghibli_palette(\"SpiritedMedium\")[3]) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(-4, 4)) +\n  facet_wrap(~ strip, labeller = label_parsed) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"SpiritedMedium\")[5]),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = ghibli_palette(\"SpiritedMedium\")[7]))\n\n\n\n\n\n\n\n\nHere is the Poisson.\n\ntibble(x = 0:20) |&gt; \n  mutate(density = dpois(x, lambda = 2.5),\n         strip   = \"y %~% Poisson(lambda)\") |&gt; \n  \n  ggplot(aes(x = x, y = density)) +\n  geom_col(fill = ghibli_palette(\"SpiritedMedium\")[2], width = 1/2) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 10)) +\n  facet_wrap(~ strip, labeller = label_parsed) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"SpiritedMedium\")[5]),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = ghibli_palette(\"SpiritedMedium\")[7]))\n\n\n\n\n\n\n\n\nFinally, the Binomial:\n\ntibble(x = 0:10) |&gt; \n  mutate(density = dbinom(x, size = 10, prob = 0.85),\n         strip   = \"y %~% Binomial(n, p)\") |&gt; \n  \n  ggplot(aes(x = x, y = density)) +\n  geom_col(fill = ghibli_palette(\"SpiritedMedium\")[2], width = 1/2) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 10)) +\n  facet_wrap(~ strip, labeller = label_parsed) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"SpiritedMedium\")[5]),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = ghibli_palette(\"SpiritedMedium\")[7]))\n\n\n\n\n\n\n\n\n\n10.2.1.1 Rethinking: A likelihood is a prior\n\nIn traditional statistics, likelihood functions are “objective” and prior distributions “subjective.” In Bayesian statistics, likelihoods are deeply related to prior probability distributions: They are priors for the data, conditional on the parameters. And just like with other priors, there is no correct likelihood. But there are better and worse likelihoods, depending upon the context. (p. 316)\n\nFor a little more in this, check out McElreath’s great lecture, Bayesian statistics without frequentist language. This subsection also reminds me of the title of one of Gelman’s blog posts, “It is perhaps merely an accident of history that skeptics and subjectivists alike strain on the gnat of the prior distribution while swallowing the camel that is the likelihood”. The title, which itself is a quote, comes from one of his papers, which he linked to in the blog, along with several related papers. It’s taken some time for the weight of that quote to sink in with me, and indeed it’s still sinking. Perhaps you’ll benefit from it, too.\n\n\n\n10.2.2 Linking linear models to distributions\n\nTo build a regression model from any of the exponential family distributions is just a matter of attaching one or more linear models to one or more of the parameters that describe the distribution’s shape. But as hinted at earlier, usually we require a link function to prevent mathematical accidents like negative distances or probability masses that exceed 1. (p. 316, emphasis in the original)\n\nThese models generally follow the form\n\\[\\begin{align*}\ny_i         & \\sim \\color{#4D6D93}{\\operatorname{Some distribution}} (\\theta_i, \\phi) \\\\\n\\color{#4D6D93}{f(\\theta_i)} & = \\alpha + \\beta (x_i - \\bar x),\n\\end{align*}\\]\nwhere \\(\\theta_i\\) is a parameter of central interest (e.g., the probability of 1 in a Binomial distribution) and \\(\\phi\\) is a placeholder for any other parameters necessary for the likelihood but not typically of primary substantive interest (e.g., \\(\\sigma\\) in conventional Gaussian models). The \\(f(\\cdot)\\) portion is the link function.\nSpeaking about links,\n\nthe logit link maps a parameter that is defined as a probability mass, and therefore constrained to lie between zero and one, onto a linear model that can take on any real value. This link is extremely common when working with binomial GLMs. In the context of a model definition, it looks like this:\n\\[\\begin{align*}\ny_i & \\sim \\color{#4D6D93}{\\operatorname{Binomial}}(n, p_i) \\\\\n\\color{#4D6D93}{\\operatorname{logit}}(p_i) & = \\alpha + \\beta x_i\n\\end{align*}\\]\nAnd the logit function itself is defined as the log-odds:\n\\[\\operatorname{logit}(p_i) = \\log \\frac{p_i}{1 - p_i}\\]\nThe “odds” of an event are just the probability it happens divided by the probability it does not happen. So really all that is being stated here is:\n\\[\\log \\frac{p_i}{1 - p_i} = \\alpha + \\beta x_i\\]\n\nIf we do the final algebraic manipulation on page 317, we can solve for \\(p_i\\) in terms of the linear model\n\\[p_i = \\frac{\\exp (\\alpha + \\beta x_i)}{1 + \\exp (\\alpha + \\beta x_i)}.\\]\nAs we’ll see later, we will make great use of this formula via the plogis() function when making sense of logistic regression models. Now we have that last formula in hand, we can make the data necessary for Figure 10.7.\n\n# First, we'll make data for the horizontal lines\nalpha &lt;- 0\nbeta  &lt;- 4\n\nlines &lt;- tibble(x = seq(from = -1, to = 1, by = 0.25)) |&gt; \n  mutate(`log-odds`  = alpha + x * beta,\n         probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta)))\n\n# Now we're ready to make the primary data\nbeta  &lt;- 2\nd &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) |&gt; \n  mutate(`log-odds`  = alpha + x * beta,\n         probability = exp(alpha + x * beta) / (1 + exp(alpha + x * beta))) \n\n# Now we make the individual plots\np1 &lt;- d |&gt; \n  ggplot(aes(x = x, y = `log-odds`)) +\n  geom_hline(data = lines,\n             aes(yintercept = `log-odds`),\n             color = ghibli_palette(\"YesterdayMedium\")[6]) +\n  geom_line(color = ghibli_palette(\"YesterdayMedium\")[3], linewidth = 1.5) +\n  coord_cartesian(xlim = c(-1, 1)) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"YesterdayMedium\")[5]),\n        panel.grid = element_blank())\n\np2 &lt;- d |&gt; \n  ggplot(aes(x = x, y = probability)) +\n  geom_hline(data = lines,\n             aes(yintercept = probability),\n             color = ghibli_palette(\"YesterdayMedium\")[6]) +\n  geom_line(color = ghibli_palette(\"YesterdayMedium\")[3], linewidth = 1.5) +\n  coord_cartesian(xlim = c(-1, 1)) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"YesterdayMedium\")[7]),\n        panel.grid = element_blank())\n\n# Finally, we're ready to mash the plots together and behold their nerdy glory\n(p1 | p2) +\n  plot_annotation(subtitle = \"The logit link transforms a linear model (left) into a probability (right).\")\n\n\n\n\n\n\n\n\n\nThe key lesson for now is just that no regression coefficient, such as \\(\\beta\\), from a GLM ever produces a constant change on the outcome scale. Recall that we defined interaction (Chapter 8) as a situation in which the effect of a predictor depends upon the value of another predictor. Well now every predictor essentially interacts with itself, because the impact of a change in a predictor depends upon the value of the predictor before the change….\nThe second very common link function is the log link. This link function maps a parameter that is defined over only positive real values onto a linear model. For example, suppose we want to model the standard deviation \\(\\sigma\\) of a Gaussian distribution so it is a function of a predictor variable \\(x\\). The parameter \\(\\sigma\\) must be positive, because a standard deviation cannot be negative nor can it be zero. The model might look like:\n\\[\\begin{align*}\ny_i & \\sim \\operatorname{Normal}(\\mu, \\color{#0E84B4}{\\sigma_i}) \\\\\n\\color{#0E84B4}{\\log (\\sigma_i)} & = \\alpha + \\beta x_i\n\\end{align*}\\]\nIn this model, the mean \\(\\mu\\) is constant, but the standard deviation scales with the value \\(x_i\\). (p. 318, emphasis in the original)\n\nThis kind of model is trivial in the brms framework, which you can learn more about in Bürkner’s (2022) vignette, Estimating distributional models with brms. Before moving on with the text, let’s detour and see how we might fit such a model. First, we’ll simulate some continuous data y for which the \\(\\textit{SD}\\) is affected by a dummy variable x.\n\nset.seed(10)\n\nd &lt;- tibble(x = rep(0:1, each = 100)) |&gt; \n  mutate(y = rnorm(n = n(), mean = 100, sd = 10 + x * 10))\n\nd\n\n# A tibble: 200 × 2\n       x     y\n   &lt;int&gt; &lt;dbl&gt;\n 1     0 100. \n 2     0  98.2\n 3     0  86.3\n 4     0  94.0\n 5     0 103. \n 6     0 104. \n 7     0  87.9\n 8     0  96.4\n 9     0  83.7\n10     0  97.4\n# ℹ 190 more rows\n\n\nThese data are based on IQ data. In psychology, general intelligence is often operationalized by and measured with standardized intelligence tests. The results from these tests are often summarized with an a single intelligence quotient (IQ) score, often called the full-scale IQ score. For many years now, the convention within among IQ test developers is to scale full-scale IQ scores so they have a population mean of 100 and a standard deviation of 15. One of the old and continuing controversies in the literature is whether men and women differ not in their means–they don’t–but in their standard deviations (e.g., Johnson et al., 2008). To give a sense of how one might explore such a controversy, we simulated data where the y variables have a mean of 100 and standard deviations of either 10 or 20, depending on one’s status on x. We can view what data like these look like with aid from tidybayes::stat_halfeye().\n\nlibrary(tidybayes)\n\nd |&gt; \n  mutate(x = x |&gt; as.character()) |&gt; \n  \n  ggplot(aes(x = y, y = x, fill = x)) +\n  stat_halfeye(point_interval = mean_qi, .width = 0.68,\n               color = ghibli_palette(\"KikiMedium\")[2]) +\n  scale_fill_manual(values = c(ghibli_palette(\"KikiMedium\")[c(4, 6)])) +\n  coord_cartesian(ylim = c(1.5, 2)) +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_rect(fill = ghibli_palette(\"KikiMedium\")[7]),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nEven though the means of y are the same for both levels of the x dummy, the variance for x == 1 is substantially larger than that for x == 0. Let’s open brms.\n\nlibrary(brms)\n\nFor such a model, we have two formulas: one for \\(\\mu\\) and one for \\(\\sigma\\). We wrap both within the bf() function.\n\nb10.1 &lt;- brm(\n  data = d,\n  family = gaussian,\n  bf(y ~ 1, sigma ~ 1 + x),\n  prior = c(prior(normal(100, 5), class = Intercept),\n            prior(normal(2.70805, 0.5), class = Intercept, dpar = sigma),\n            prior(normal(0, 0.5), class = b, dpar = sigma)),\n  seed = 10,\n  file = \"fits/b10.01\")\n\nDo note our use of the dpar arguments in the prior() functions Here’s the summary.\n\nprint(b10.1)\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: y ~ 1 \n         sigma ~ 1 + x\n   Data: d (Number of observations: 200) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept          98.55      0.84    96.83   100.17 1.00     4288     2582\nsigma_Intercept     2.26      0.07     2.13     2.40 1.00     3856     2991\nsigma_x             0.69      0.10     0.50     0.88 1.00     4140     2963\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we get an intercept for both \\(\\mu\\) and \\(\\sigma\\), with the intercept for sigma labeled as sigma_Intercept. And note the \\(\\beta\\) coefficient for \\(\\sigma\\) was named sigma_x. Also notice the scale the sigma_i coefficients are on. These are not in the original metric, but rather based on a logarithmic transformation of \\(\\sigma\\). You can confirm that by the second line of the print() output: Links: mu = identity; sigma = log. So if you want to get a sense of the effects of x on the \\(\\sigma\\) for y, you have to exponentiate the formula. Here we’ll do so with the output from as_draws_df().\n\npost &lt;- as_draws_df(b10.1)\nhead(post)\n\n# A draws_df: 6 iterations, 1 chains, and 7 variables\n  b_Intercept b_sigma_Intercept b_sigma_x Intercept Intercept_sigma lprior lp__\n1          99               2.2      0.69        99             2.6   -4.0 -808\n2          99               2.2      0.74        99             2.6   -4.1 -808\n3         100               2.3      0.71       100             2.6   -4.0 -809\n4          97               2.3      0.60        97             2.6   -3.9 -810\n5          98               2.4      0.50        98             2.6   -3.6 -810\n6          99               2.2      0.89        99             2.6   -4.6 -810\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nWith the samples in hand, we’ll use the model formula to compute the model-implied standard deviations of y based on the x dummy and then examine them in a plot.\n\npost |&gt; \n  mutate(`x == 0` = exp(b_sigma_Intercept + b_sigma_x * 0),\n         `x == 1` = exp(b_sigma_Intercept + b_sigma_x * 1)) |&gt; \n  pivot_longer(contains(\"==\")) |&gt;\n  \n  ggplot(aes(x = value, y = name, fill = name)) +\n  stat_halfeye(point_interval = median_qi, .width = 0.95,\n               color = ghibli_palette(\"KikiMedium\")[2]) +\n  scale_fill_manual(values = c(ghibli_palette(\"KikiMedium\")[c(4, 6)])) +\n  labs(x = expression(sigma[x]), \n       y = NULL,\n       subtitle = \"Model-implied standard deviations by group\") +\n  coord_cartesian(ylim = c(1.5, 2)) +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\",\n        panel.background = element_rect(fill = ghibli_palette(\"KikiMedium\")[7]),\n        panel.grid = element_blank())\n\n\n\n\n\n\n\n\nIf we looked back at the data, those \\(\\textit{SD}\\) estimates are right about what we’d expect.\n\nd |&gt; \n  group_by(x) |&gt; \n  summarise(sd = sd(y) |&gt; round(digits = 1)) \n\n# A tibble: 2 × 2\n      x    sd\n  &lt;int&gt; &lt;dbl&gt;\n1     0   9.4\n2     1  19.4\n\n\nFor more on models like this, check out Christakis’s blog post, 2014: What scientific idea is ready for retirement?, or his paper with Subramanian and Kim, The “average” treatment effect: A construct ripe for retirement. A commentary on Deaton and Cartwright, (Subramanian et al., 2018). Kruschke covered modeling \\(\\sigma\\) a bit in his (2015) Doing Bayesian data analysis, second edition: A tutorial with R, JAGS, and Stan, my (2026) translation for which lives here. Finally, this is foreshadowing a bit because it requires the multilevel model (see Chapter 13 and Chapter 14), but you might also check out the (2020) paper by Williams, Martin, Liu, and Rast, Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity or Williams’s blog post, A defining feature of cognitive interference tasks: Heterogeneous within-person variance.\nBut getting back to the text,\n\nWhat the log link effectively assumes is that the parameter’s value is the exponentiation of the linear model. Solving \\(\\log (\\sigma_i) = \\alpha + \\beta x_i\\) for \\(\\sigma_i\\) yields the inverse link:\n\\[\\sigma_i = \\exp (\\alpha + \\beta x_i)\\]\nThe impact of this assumption can be seen in [our version of] Figure 10.8. (pp. 318–319)\n\n\n# First, we'll make data that'll be make the horizontal lines\nalpha &lt;- 0\nbeta  &lt;- 2\n\nlines &lt;- tibble(`log-measurement`      = -3:3,\n                `original measurement` = exp(-3:3))\n\n# Now we're ready to make the primary data\nd &lt;- tibble(x = seq(from = -1.5, to = 1.5, length.out = 50)) |&gt; \n  mutate(`log-measurement`      = alpha + x * beta,\n         `original measurement` = exp(alpha + x * beta))\n\n# Now we make the individual plots\np1 &lt;- d |&gt; \n  ggplot(aes(x = x, y = `log-measurement`)) +\n  geom_hline(data = lines,\n             aes(yintercept = `log-measurement`),\n             color = ghibli_palette(\"YesterdayMedium\")[6]) +\n  geom_line(color = ghibli_palette(\"YesterdayMedium\")[3], linewidth = 1.5) +\n  coord_cartesian(xlim = c(-1, 1)) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"YesterdayMedium\")[5]),\n        panel.grid = element_blank())\n\np2 &lt;- d |&gt; \n  ggplot(aes(x = x, y = `original measurement`)) +\n  geom_hline(data = lines,\n             aes(yintercept = `original measurement`),\n             color = ghibli_palette(\"YesterdayMedium\")[6]) +\n  geom_line(color = ghibli_palette(\"YesterdayMedium\")[3], linewidth = 1.5) +\n  scale_y_continuous(position = \"right\", limits = c(0, 10)) +\n  coord_cartesian(xlim = c(-1, 1)) +\n  theme(panel.background = element_rect(fill = ghibli_palette(\"YesterdayMedium\")[7]),\n        panel.grid = element_blank())\n\n# Combine the ggplots\np1 | p2\n\n\n\n\n\n\n\n\n\nUsing a log link for a linear model (left) implies an exponential scaling of the outcome with the predictor variable (right). Another way to think of this relationship is to remember that logarithms are magnitudes. An increase of one unit on the log scale means an increase of an order of magnitude on the untransformed scale. And this fact is reflected in the widening intervals between the horizontal lines in the right-hand plot of Figure 10.8. (p. 319, emphasis in the original)\n\n\n10.2.2.1 Rethinking: When in doubt, play with assumptions\n\nLink functions are assumptions. And like all assumptions, they are useful in different contexts. The conventional logit and log links are widely useful, but they can sometimes distort inference. If you ever have doubts, and want to reassure yourself that your conclusions are not sensitive to choice of link function, then you can use sensitivity analysis. A sensitivity analysis explores how changes in assumptions influence inference. (p. 319, emphasis in the original)\n\nAs an example, a common alternative to the logit link is the probit. Both are available with brms.\n\n\n\n10.2.3 Omitted variable bias again\n\nBack in Chapter 5 and Chapter 6, you saw some examples of omitted variable bias, where leaving a causally important variable out of a model leads to biased inference. The same thing can of course happen in GLMs. But it can be worse in GLMs, because even a variable that isn’t technically a confounder can bias inference, once we have a link function. The reason is that the ceiling and floor effects described above can distort estimates by suppressing the causal influence of a variable. (p. 320)\n\n\n\n10.2.4 Absolute and relative differences\nWithin the context of GLMs with non-identity link functions,\n\nparameter estimates do not by themselves tell you the importance of a predictor on the outcome. The reason is that each parameter represents a relative difference on the scale of the linear model, ignoring other parameters, while we are really interested in absolute differences in outcomes that must incorporate all parameters. (p. 320, emphasis in the original)\n\nThis will make more sense after we start playing around with logistic regression, count regression, and so on. For now, just file it away.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Big Entropy and the Generalized Linear Model</span>"
    ]
  },
  {
    "objectID": "10.html#session-info",
    "href": "10.html#session-info",
    "title": "10  Big Entropy and the Generalized Linear Model",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] brms_2.23.0     Rcpp_1.1.0      tidybayes_3.0.7 patchwork_1.3.2 ghibli_0.3.4    lubridate_1.9.4 forcats_1.0.1  \n [8] stringr_1.6.0   dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2     tibble_3.3.1    ggplot2_4.0.1  \n[15] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7             magrittr_2.0.4         \n [6] multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          loo_2.9.0.9000          systemfonts_1.3.1      \n[11] vctrs_0.7.0             reshape2_1.4.5          pkgconfig_2.0.3         arrayhelpers_1.1-0      fastmap_1.2.0          \n[16] backports_1.5.0         labeling_0.4.3          utf8_1.2.6              rmarkdown_2.30          tzdb_0.5.0             \n[21] ragg_1.5.0              xfun_0.55               jsonlite_2.0.0          uuid_1.2-1              parallel_4.5.1         \n[26] R6_2.6.1                stringi_1.8.7           RColorBrewer_1.1-3      StanHeaders_2.36.0.9000 estimability_1.5.1     \n[31] rstan_2.36.0.9000       knitr_1.51              zoo_1.8-14              bayesplot_1.15.0.9000   Matrix_1.7-3           \n[36] splines_4.5.1           timechange_0.3.0        tidyselect_1.2.1        rstudioapi_0.17.1       abind_1.4-8            \n[41] yaml_2.3.12             codetools_0.2-20        curl_7.0.0              pkgbuild_1.4.8          lattice_0.22-7         \n[46] plyr_1.8.9              withr_3.0.2             bridgesampling_1.2-1    S7_0.2.1                flextable_0.9.10       \n[51] askpass_1.2.1           posterior_1.6.1.9000    coda_0.19-4.1           evaluate_1.0.5          survival_3.8-3         \n[56] RcppParallel_5.1.11-1   zip_2.3.3               ggdist_3.3.3            xml2_1.4.0              pillar_1.11.1          \n[61] tensorA_0.36.2.1        checkmate_2.3.3         stats4_4.5.1            distributional_0.5.0    generics_0.1.4         \n[66] hms_1.1.4               rstantools_2.5.0.9000   scales_1.4.0            xtable_1.8-4            glue_1.8.0             \n[71] gdtools_0.4.4           emmeans_1.11.2-8        tools_4.5.1             data.table_1.17.8       mvtnorm_1.3-3          \n[76] grid_4.5.1              QuickJSR_1.8.1          nlme_3.1-168            cli_3.6.5               textshaping_1.0.4      \n[81] officer_0.7.2           fontBitstreamVera_0.1.1 svUnit_1.0.8            Brobdingnag_1.2-9       V8_8.0.1               \n[86] gtable_0.3.6            digest_0.6.39           fontquiver_0.2.1        prismatic_1.1.2         TH.data_1.1-4          \n[91] htmlwidgets_1.6.4       farver_2.1.2            htmltools_0.5.9         lifecycle_1.0.5         fontLiberation_0.1.0   \n[96] openssl_2.3.4           MASS_7.3-65            \n\n\n\n\n\n\nBürkner, P.-C. (2022). Estimating distributional models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html\n\n\nHenderson, E. (2022). ghibli: Studio ghibli colour palettes [Manual]. https://CRAN.R-project.org/package=ghibli\n\n\nJohnson, W., Carothers, A., & Deary, I. J. (2008). Sex differences in variability in general intelligence: A new look at the old question. Perspectives on Psychological Science, 3(6), 518–531. https://doi.org/10.1111/j.1745-6924.2008.00096.x\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKurz, A. S. (2026). Doing Bayesian data analysis in brms and the tidyverse (Version 1.3.0). https://solomon.quarto.pub/dbda2/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nSubramanian, S. V., Kim, R., & Christakis, N. A. (2018). The “average” treatment effect: A construct ripe for retirement. A commentary on Deaton and Cartwright. Social Science & Medicine, 210, 77–82. https://doi.org/10.1016/j.socscimed.2018.04.027\n\n\nWilliams, D. R., Martin, S. R., Liu, S., & Rast, P. (2020). Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity. European Journal of Psychological Assessment, 36(6), 981–997. https://doi.org/10.1027/1015-5759/a000624",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Big Entropy and the Generalized Linear Model</span>"
    ]
  },
  {
    "objectID": "11.html",
    "href": "11.html",
    "title": "11  God Spiked the Integers",
    "section": "",
    "text": "11.1 Binomial regression\nIn this chapter, we focus on the two most common types of count models: the binomial and the Poisson.\nThe basic binomial model follows the form\n\\[y \\sim \\operatorname{Binomial}(n, p),\\]\nwhere \\(y\\) is some count variable, \\(n\\) is the number of trials, and \\(p\\) it the probability a given trial was a 1, which is sometimes termed a success. When \\(n = 1\\), then \\(y\\) is a vector of 0’s and 1’s. Presuming the logit link,1 which we just covered in Section 10.2.2, models of this type are commonly termed logistic regression. When \\(n &gt; 1\\), and still presuming the logit link, we might call our model an aggregated logistic regression model, or more generally an aggregated binomial regression model.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>God Spiked the Integers</span>"
    ]
  },
  {
    "objectID": "11.html#sec-Binomial-regression",
    "href": "11.html#sec-Binomial-regression",
    "title": "11  God Spiked the Integers",
    "section": "",
    "text": "1 Though McElreath didn’t cover it, here, it’s also fine to fit binomial models using the probit link. Gelman et al. (2020) covered probit regression in Section 15.4. With brms, it’s simply a matter of setting family = binomial(link = \"probit\") within brm().\n11.1.1 Logistic regression: Prosocial chimpanzees\nLoad the Silk et al. (2005) chimpanzees data.\n\ndata(chimpanzees, package = \"rethinking\")\nd &lt;- chimpanzees\nrm(chimpanzees)\n\nThe data include two experimental conditions, prosoc_left and condition, each of which has two levels. This results in four combinations.\n\nlibrary(tidyverse)\nlibrary(flextable)\n\nd |&gt; \n  distinct(prosoc_left, condition) |&gt; \n  mutate(description = str_c(\"Two food items on \", c(\"right and no partner\",\n                                                     \"left and no partner\",\n                                                     \"right and partner present\",\n                                                     \"left and partner present\"))) |&gt; \n  flextable() |&gt; \n  width(width = c(1, 1, 4))\n\nprosoc_leftconditiondescription00Two food items on right and no partner10Two food items on left and no partner01Two food items on right and partner present11Two food items on left and partner present\n\n\nIt would be conventional to include these two variables and their interaction using dummy variables. We’re going to follow McElreath and use an index variable approach, instead. If you’d like to see what this would look like using the dummy variable approach, check out my (2026c) translation of the corresponding section from McElreath’s first (2015) edition. For now, make the index, which we’ll be saving as a factor.\n\nd &lt;- d |&gt; \n  mutate(treatment = factor(1 + prosoc_left + 2 * condition)) |&gt; \n  # This will come in handy, later\n  mutate(labels = factor(treatment,\n                         levels = 1:4,\n                         labels = c(\"r/n\", \"l/n\", \"r/p\", \"l/p\")))\n\nWe can use the dplyr::count() function to get a sense of the distribution of the conditions in the data.\n\nd |&gt; \n  count(condition, treatment, prosoc_left)\n\n  condition treatment prosoc_left   n\n1         0         1           0 126\n2         0         2           1 126\n3         1         3           0 126\n4         1         4           1 126\n\n\nFire up brms.\n\nlibrary(brms)\n\nWe start with the simple intercept-only logistic regression model, which follows the statistical formula\n\\[\\begin{align*}\n\\text{pulled\\_left}_i & \\sim \\operatorname{Binomial}(1, p_i) \\\\\n\\operatorname{logit}(p_i) & = \\alpha \\\\\n\\alpha & \\sim \\operatorname{Normal}(0, w),\n\\end{align*}\\]\nwhere \\(w\\) is the hyperparameter for \\(\\sigma\\) the value for which we have yet to choose. To start things off, we’ll set \\(w = 10\\), fit a model with where we set sample_prior = TRUE, and get a sense of the prior on a plot.\nIn the brm() formula syntax, including a | bar on the left side of a formula indicates we have extra supplementary information about our criterion. In this case, that information is that each pulled_left value corresponds to a single trial (i.e., trials(1)), which itself corresponds to the \\(n = 1\\) portion of the statistical formula, above.\n\nb11.1 &lt;- brm(\n  data = d, \n  family = binomial,\n  pulled_left | trials(1) ~ 1,\n  prior(normal(0, 10), class = Intercept),\n  seed = 11,\n  sample_prior = T,\n  file = \"fits/b11.01\")\n\nBefore we go any further, let’s discuss the plot theme. For this chapter, we’ll take our color scheme from the \"Moonrise2\" palette from the wesanderson package (Ram & Wickham, 2018).\n\nlibrary(wesanderson)\nwes_palette(\"Moonrise2\")\n\n\n\n\n\n\n\nwes_palette(\"Moonrise2\")[1:4]\n\n[1] \"#798E87\" \"#C27D38\" \"#CCC591\" \"#29211F\"\n\n\nWe’ll also take a few formatting cues from Edward Tufte (2001), courtesy of the ggthemes package. The theme_tufte() function will change the default font and remove some chart junk. The theme_set() function, below, will make these adjustments the default for all subsequent ggplot2 plots. To undo this, just execute theme_set(theme_default()).\n\nlibrary(ggthemes)\n\ntheme_set(\n  theme_default() + \n    theme_tufte() +\n    theme(plot.background = element_rect(fill = wes_palette(\"Moonrise2\")[3],\n                                         color = wes_palette(\"Moonrise2\")[3]))\n)\n\nNow we’re ready to plot. We’ll extract the prior draws with prior_draws(), convert them from the log-odds metric to the probability metric with the plogis() function, and adjust the bandwidth of the density plot with the adjust argument within geom_density().\n\nprior_draws(b11.1) |&gt; \n  mutate(p = plogis(Intercept)) |&gt; \n  \n  ggplot(aes(x = p)) +\n  geom_density(adjust = 0.1,\n               fill = wes_palette(\"Moonrise2\")[4], \n               linewidth = 0) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"prior prob pull left\")\n\n\n\n\n\n\n\n\nAt this point in the analysis, we were only able to make part of the left panel of McElreath’s Figure 11.3. We’ll add to it in a bit. Now update the model so that \\(w = 1.5\\).\n\nb11.1b &lt;- brm(\n  data = d, \n  family = binomial,\n  pulled_left | trials(1) ~ 1,\n  prior(normal(0, 1.5), class = Intercept),\n  seed = 11,\n  sample_prior = T,\n  file = \"fits/b11.01b\")\n\nNow we can make the full version of the left panel of Figure 11.3.\n\n# Wrangle\nbind_rows(prior_draws(b11.1), prior_draws(b11.1b)) |&gt; \n  mutate(p = plogis(Intercept),\n         w = factor(rep(c(10, 1.5), each = n() / 2),\n                    levels = c(10, 1.5))) |&gt; \n  \n  # Plot\n  ggplot(aes(x = p, fill = w)) +\n  geom_density(adjust = 0.1, alpha = 3/4, linewidth = 0) +\n  scale_fill_manual(expression(italic(w)), values = wes_palette(\"Moonrise2\")[c(4, 1)]) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"prior prob pull left\",\n       title = expression(alpha%~%Normal(0*\", \"*italic(w))))\n\n\n\n\n\n\n\n\nIf we’d like to fit a model that includes an overall intercept and uses McElreath’d index variable approach for the predictor variable treatment, we’ll have to switch to the brms non-linear syntax. Here it is for the models using \\(w = 10\\) and then \\(w = 0.5\\).\n\n# w = 10\nb11.2 &lt;- brm(\n  data = d, \n  family = binomial,\n  bf(pulled_left | trials(1) ~ a + b,\n     a ~ 1, \n     b ~ 0 + treatment,\n     nl = TRUE),\n  prior = c(prior(normal(0, 1.5), nlpar = a),\n            prior(normal(0, 10), nlpar = b, coef = treatment1),\n            prior(normal(0, 10), nlpar = b, coef = treatment2),\n            prior(normal(0, 10), nlpar = b, coef = treatment3),\n            prior(normal(0, 10), nlpar = b, coef = treatment4)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  sample_prior = T,\n  file = \"fits/b11.02\")\n\n# w = 0.5\nb11.3 &lt;- brm(\n  data = d, \n  family = binomial,\n  bf(pulled_left | trials(1) ~ a + b,\n     a ~ 1, \n     b ~ 0 + treatment,\n     nl = TRUE),\n  prior = c(prior(normal(0, 1.5), nlpar = a),\n            prior(normal(0, 0.5), nlpar = b, coef = treatment1),\n            prior(normal(0, 0.5), nlpar = b, coef = treatment2),\n            prior(normal(0, 0.5), nlpar = b, coef = treatment3),\n            prior(normal(0, 0.5), nlpar = b, coef = treatment4)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  sample_prior = T,\n  file = \"fits/b11.03\")\n\nIf all you want to do is fit the models, you wouldn’t have to add a separate prior() statement for each level of treatment. You could have just included a single line, prior(normal(0, 0.5), nlpar = b), that did not include a coef argument. The problem with this approach is we’d only get one column for treatment when using the prior_draws() function to retrieve the prior samples. To get separate columns for the prior samples of each of the levels of treatment, you need to take the verbose approach, above.\nAnyway, here’s how to make a version of the right panel of Figure 11.3.\n\n# Wrangle\nprior &lt;- bind_rows(prior_draws(b11.2), prior_draws(b11.3)) |&gt; \n  mutate(w  = factor(rep(c(10, 0.5), each = n() / 2),\n                     levels = c(10, 0.5)),\n         p1 = plogis(b_a + b_b_treatment1),\n         p2 = plogis(b_a + b_b_treatment2)) |&gt; \n  mutate(diff = abs(p1 - p2)) \n\n# Plot\nprior |&gt; \n  ggplot(aes(x = diff, fill = w)) +\n  geom_density(adjust = 0.1, alpha = 3/4, linewidth = 0) +\n  scale_fill_manual(expression(italic(w)), values = wes_palette(\"Moonrise2\")[c(4, 2)]) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"prior diff between treatments\",\n       title = expression(alpha%~%Normal(0*\", \"*italic(w))))\n\n\n\n\n\n\n\n\nHere are the averages of the two prior-predictive difference distributions.\n\nprior |&gt; \n  group_by(w) |&gt; \n  summarise(mean = mean(diff))\n\n# A tibble: 2 × 2\n  w       mean\n  &lt;fct&gt;  &lt;dbl&gt;\n1 10    0.484 \n2 0.5   0.0972\n\n\nBefore we move on to fit the full model, it might be useful to linger here and examine the nature of the model we just fit. Here’s the parameter summary for b11.3.\n\nprint(b11.3)\n\n Family: binomial \n  Links: mu = logit \nFormula: pulled_left | trials(1) ~ a + b \n         a ~ 1\n         b ~ 0 + treatment\n   Data: d (Number of observations: 504) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_Intercept      0.31      0.26    -0.20     0.81 1.00      965     1160\nb_treatment1    -0.11      0.29    -0.64     0.47 1.00     1157     1385\nb_treatment2     0.31      0.28    -0.24     0.89 1.00     1207     1447\nb_treatment3    -0.36      0.28    -0.89     0.24 1.00     1157     1635\nb_treatment4     0.22      0.29    -0.33     0.80 1.00     1182     1308\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow focus on the likelihood portion of the model formula,\n\\[\\begin{align*}\n\\text{pulled\\_left}_i & \\sim \\operatorname{Binomial}(1, p_i) \\\\\n\\operatorname{logit}(p_i) & = \\alpha + \\beta_\\text{treatment} .\n\\end{align*}\\]\nWhen you have one overall intercept \\(\\alpha\\) and then use the non-linear approach for the treatment index, you end up with as many \\(\\beta\\) parameters as there levels for treatment. This means the formula for treatment == 1 is \\(\\alpha + \\beta_{\\text{treatment}[1]}\\), the formula for treatment == 2 is \\(\\alpha + \\beta_{\\text{treatment}[2]}\\), and so on. This also effectively makes \\(\\alpha\\) the grand mean. Here’s the empirical grand mean.\n\nd |&gt; \n  summarise(grand_mean = mean(pulled_left))\n\n  grand_mean\n1  0.5793651\n\n\nNow here’s the summary of \\(\\alpha\\) after transforming it back into the probability metric with the plogis() function.\n\nlibrary(tidybayes)\n\nas_draws_df(b11.3) |&gt; \n  transmute(alpha = plogis(b_a_Intercept)) |&gt; \n  mean_qi()\n\n# A tibble: 1 × 6\n  alpha .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.576  0.449  0.693   0.95 mean   qi       \n\n\nHere are the empirical probabilities for each of the four levels of treatment.\n\nd |&gt; \n  group_by(treatment) |&gt; \n  summarise(mean = mean(pulled_left))\n\n# A tibble: 4 × 2\n  treatment  mean\n  &lt;fct&gt;     &lt;dbl&gt;\n1 1         0.548\n2 2         0.659\n3 3         0.476\n4 4         0.635\n\n\nHere are the corresponding posteriors.\n\nas_draws_df(b11.3) |&gt; \n  pivot_longer(b_b_treatment1:b_b_treatment4) |&gt; \n  mutate(treatment = str_remove(name, \"b_b_treatment\"),\n         mean      = plogis(b_a_Intercept + value)) |&gt;\n  group_by(treatment) |&gt; \n  mean_qi(mean)\n\n# A tibble: 4 × 7\n  treatment  mean .lower .upper .width .point .interval\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 1         0.550  0.468  0.630   0.95 mean   qi       \n2 2         0.649  0.569  0.727   0.95 mean   qi       \n3 3         0.488  0.407  0.570   0.95 mean   qi       \n4 4         0.628  0.546  0.708   0.95 mean   qi       \n\n\nOkay, let’s get back on track with the text. Now we’re ready to fit the full model, which follows the form\n\\[\\begin{align*}\n\\text{pulled\\_left}_i      & \\sim \\operatorname{Binomial}(1, p_i) \\\\\n\\operatorname{logit}(p_i) & = \\alpha_{\\color{#54635e}{\\text{actor}}[i]} + \\beta_{\\color{#a4692f}{\\text{treatment}}[i]} \\\\\n\\alpha_{\\color{#54635e}j} & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\beta_{\\color{#a4692f}k}  & \\sim \\operatorname{Normal}(0, 0.5).\n\\end{align*}\\]\nBefore fitting the model, we should save actor as a factor.\n\nd &lt;- d |&gt; \n  mutate(actor = factor(actor))\n\nNow fit the model.\n\nb11.4 &lt;- brm(\n  data = d, \n  family = binomial,\n  bf(pulled_left | trials(1) ~ a + b,\n     a ~ 0 + actor, \n     b ~ 0 + treatment,\n     nl = TRUE),\n  prior = c(prior(normal(0, 1.5), nlpar = a),\n            prior(normal(0, 0.5), nlpar = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = \"fits/b11.04\")\n\nInspect the parameter summary.\n\nprint(b11.4)\n\n Family: binomial \n  Links: mu = logit \nFormula: pulled_left | trials(1) ~ a + b \n         a ~ 0 + actor\n         b ~ 0 + treatment\n   Data: d (Number of observations: 504) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_actor1        -0.46      0.33    -1.09     0.19 1.00     1226     2092\na_actor2         3.90      0.76     2.56     5.53 1.00     3766     2965\na_actor3        -0.75      0.34    -1.42    -0.09 1.01     1219     1745\na_actor4        -0.76      0.34    -1.42    -0.09 1.01     1182     1829\na_actor5        -0.45      0.34    -1.12     0.23 1.00     1353     2300\na_actor6         0.47      0.34    -0.18     1.16 1.00     1257     2076\na_actor7         1.96      0.42     1.16     2.82 1.00     1826     2459\nb_treatment1    -0.03      0.29    -0.59     0.52 1.01     1074     1996\nb_treatment2     0.49      0.29    -0.08     1.03 1.01     1122     2008\nb_treatment3    -0.38      0.29    -0.95     0.18 1.01     1141     2085\nb_treatment4     0.37      0.29    -0.19     0.93 1.00     1081     2126\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere’s how we might make our version of McElreath’s coefficient plot of the \\(\\alpha\\) parameters.\n\nlibrary(tidybayes)\n\npost &lt;- as_draws_df(b11.4)\n\npost |&gt; \n  pivot_longer(contains(\"actor\")) |&gt;\n  mutate(probability = plogis(value),\n         actor       = str_remove(name, \"b_a_actor\") |&gt; \n           factor(levels = 7:1)) |&gt; \n  \n  ggplot(aes(x = probability, y = actor)) +\n  geom_vline(xintercept = 0.5, color = wes_palette(\"Moonrise2\")[1], linetype = 3) +\n  stat_pointinterval(.width = 0.95,\n                     color = wes_palette(\"Moonrise2\")[4], size = 1/2) +\n  scale_x_continuous(expression(alpha[actor]), limits = 0:1) +\n  ylab(NULL) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nHere’s the corresponding coefficient plot of the \\(\\beta\\) parameters.\n\ntx &lt;- c(\"R/N\", \"L/N\", \"R/P\", \"L/P\")\n\npost |&gt; \n  select(contains(\"treatment\")) |&gt; \n  set_names(\"R/N\", \"L/N\", \"R/P\", \"L/P\") |&gt; \n  pivot_longer(everything()) |&gt;\n  mutate(probability = plogis(value),\n         treatment   = factor(name, levels = tx)) |&gt; \n  mutate(treatment = fct_rev(treatment)) |&gt; \n  \n  ggplot(aes(x = value, y = treatment)) +\n  geom_vline(xintercept = 0, color = wes_palette(\"Moonrise2\")[2], linetype = 3) +\n  stat_pointinterval(.width = 0.95,\n                     color = wes_palette(\"Moonrise2\")[4], size = 1/2) +\n  labs(x = expression(beta[treatment]),\n       y = NULL) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nNow make the coefficient plot for the primary contrasts of interest.\n\npost |&gt; \n  mutate(db13 = b_b_treatment1 - b_b_treatment3,\n         db24 = b_b_treatment2 - b_b_treatment4) |&gt; \n  pivot_longer(db13:db24) |&gt;\n  mutate(diffs = factor(name, levels = c(\"db24\", \"db13\"))) |&gt; \n  \n  ggplot(aes(x = value, y = diffs)) +\n  geom_vline(xintercept = 0, color = wes_palette(\"Moonrise2\")[2], linetype = 3) +\n  stat_pointinterval(.width = 0.95,\n                     color = wes_palette(\"Moonrise2\")[4], size = 1/2) +\n  labs(x = \"difference\",\n       y = NULL) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n“These are the contrasts between the no-partner/partner treatments” (p. 331). Next, we prepare for the posterior predictive check. McElreath showed how to compute empirical proportions by the levels of actor and treatment with the by() function. Our approach will be with a combination of group_by() and summarise(). Here’s what that looks like for actor == 1.\n\nd |&gt;\n  group_by(actor, treatment) |&gt;\n  summarise(proportion = mean(pulled_left)) |&gt; \n  filter(actor == 1)\n\n# A tibble: 4 × 3\n# Groups:   actor [1]\n  actor treatment proportion\n  &lt;fct&gt; &lt;fct&gt;          &lt;dbl&gt;\n1 1     1              0.333\n2 1     2              0.5  \n3 1     3              0.278\n4 1     4              0.556\n\n\nNow we’ll follow that through to make the top panel of Figure 11.4. Instead of showing the plot, we’ll save it for the next code block.\n\np1 &lt;- d |&gt;\n  group_by(actor, treatment) |&gt;\n  summarise(proportion = mean(pulled_left)) |&gt; \n  left_join(d |&gt; distinct(actor, treatment, labels, condition, prosoc_left),\n            by = c(\"actor\", \"treatment\")) |&gt; \n  mutate(condition = factor(condition)) |&gt; \n  \n  ggplot(aes(x = labels, y = proportion)) +\n  geom_hline(yintercept = 0.5, color = wes_palette(\"Moonrise2\")[3]) +\n  geom_line(aes(group = prosoc_left),\n            color = wes_palette(\"Moonrise2\")[4], linewidth = 1/4) +\n  geom_point(aes(color = condition),\n             size = 2.5, show.legend = F) + \n  labs(subtitle = \"observed proportions\")\n\nNext we use brms() fitted to get the posterior predictive distributions for each unique combination of actor and treatment, wrangle, and plot. First, we save the plot as p2 and then we use patchwork syntax to combine the two subplots.\n\nnd &lt;- d |&gt; \n  distinct(actor, treatment, labels, condition, prosoc_left)\n\np2 &lt;- fitted(b11.4, newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  mutate(condition = factor(condition)) |&gt; \n  \n  ggplot(aes(x = labels, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = 0.5, color = wes_palette(\"Moonrise2\")[3]) +\n  geom_line(aes(group = prosoc_left),\n            color = wes_palette(\"Moonrise2\")[4], linewidth = 1/4) +\n  geom_pointrange(aes(color = condition),\n                  size = 1/2, show.legend = F) + \n  labs(subtitle = \"posterior predictions\")\n\n# Combine the two ggplots, adjust, and display\nlibrary(patchwork)\n\n(p1 / p2) &\n  scale_y_continuous(\"proportion left lever\", \n                     breaks = c(0, 0.5, 1), limits = c(0, 1)) &\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[c(2:1)]) &\n  xlab(NULL) &\n  facet_wrap(~ actor, nrow = 1, labeller = label_both) &\n  theme(axis.ticks.x = element_blank(),\n        panel.background = element_rect(fill = alpha(\"white\", 1/10), linewidth = 0))\n\n\n\n\n\n\n\n\nLet’s make two more index variables.\n\nd &lt;- d |&gt; \n  mutate(side = factor(prosoc_left + 1),  # Right 1, left 2\n         cond = factor(condition + 1))    # No partner 1, partner 2\n\nNow fit the model without the interaction between prosoc_left and condition.\n\nb11.5 &lt;- brm(\n  data = d, \n  family = binomial,\n  bf(pulled_left | trials(1) ~ a + bs + bc,\n     a ~ 0 + actor, \n     bs ~ 0 + side, \n     bc ~ 0 + cond,\n     nl = TRUE),\n  prior = c(prior(normal(0, 1.5), nlpar = a),\n            prior(normal(0, 0.5), nlpar = bs),\n            prior(normal(0, 0.5), nlpar = bc)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = \"fits/b11.05\")\n\nCompare b11.4 and b11.5 by the PSIS-LOO and the WAIC.\n\nb11.4 &lt;- add_criterion(b11.4, criterion = c(\"loo\", \"waic\"))\nb11.5 &lt;- add_criterion(b11.5, criterion = c(\"loo\", \"waic\"))\n\nloo_compare(b11.4, b11.5, criterion = \"loo\") |&gt; print(simplify = F)\n\n      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nb11.5    0.0       0.0  -265.5      9.6         7.8    0.4    531.0   19.2  \nb11.4   -0.5       0.7  -266.0      9.5         8.3    0.4    531.9   19.0  \n\nloo_compare(b11.4, b11.5, criterion = \"waic\") |&gt; print(simplify = F)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb11.5    0.0       0.0  -265.5       9.6          7.8    0.4     531.0   19.2 \nb11.4   -0.5       0.7  -266.0       9.5          8.3    0.4     531.9   19.0 \n\n\nHere are the weights.\n\nmodel_weights(b11.4, b11.5, weights = \"loo\") |&gt; round(digits = 2)\n\nb11.4 b11.5 \n 0.38  0.62 \n\nmodel_weights(b11.4, b11.5, weights = \"waic\") |&gt; round(digits = 2)\n\nb11.4 b11.5 \n 0.38  0.62 \n\n\nHere’s a quick check of the parameter summary for the non-interaction model, b11.5.\n\nprint(b11.5)\n\n Family: binomial \n  Links: mu = logit \nFormula: pulled_left | trials(1) ~ a + bs + bc \n         a ~ 0 + actor\n         bs ~ 0 + side\n         bc ~ 0 + cond\n   Data: d (Number of observations: 504) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_actor1    -0.63      0.44    -1.48     0.26 1.00     1226     1893\na_actor2     3.75      0.80     2.33     5.38 1.00     2188     2288\na_actor3    -0.92      0.45    -1.80    -0.03 1.00     1186     1758\na_actor4    -0.92      0.45    -1.78    -0.03 1.00     1248     1972\na_actor5    -0.62      0.44    -1.47     0.27 1.00     1246     1843\na_actor6     0.31      0.45    -0.55     1.22 1.00     1289     2006\na_actor7     1.80      0.51     0.82     2.83 1.00     1465     2057\nbs_side1    -0.20      0.34    -0.87     0.46 1.00     1353     1759\nbs_side2     0.49      0.34    -0.17     1.15 1.00     1322     1542\nbc_cond1     0.27      0.33    -0.37     0.89 1.00     1531     2383\nbc_cond2     0.02      0.33    -0.64     0.68 1.00     1546     2215\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBecause it’s good practice, here’s the b11.5 version of the bottom panel of Figure 11.4.\n\nnd &lt;- d |&gt; \n  distinct(actor, treatment, labels, cond, side)\n\nfitted(b11.5, newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = labels, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = 0.5, color = wes_palette(\"Moonrise2\")[3]) +\n  geom_line(aes(group = side),\n            color = wes_palette(\"Moonrise2\")[4], linewidth = 1/4) +\n  geom_pointrange(aes(color = cond),\n                  size = 1/2, show.legend = F) + \n  scale_color_manual(values = wes_palette(\"Moonrise2\")[c(2:1)]) +\n  scale_y_continuous(\"proportion left lever\", \n                     breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n  labs(x = NULL,\n       subtitle = \"posterior predictions for b11.5\") +\n  facet_wrap(~ actor, nrow = 1, labeller = label_both) +\n  theme(axis.ticks.x = element_blank(),\n        panel.background = element_rect(fill = alpha(\"white\", 1/10), linewidth = 0))\n\n\n\n\n\n\n\n\n\n11.1.1.1 Overthinking: Adding log-probability calculations to a Stan model\nFor retrieving log-probability summaries, our approach with brms is a little different than the one you might take with McElreath’s rethinking. Rather than adding a log_lik=TRUE argument within rethinking::ulam(), we just use the log_lik() function after fitting a brms model. You may recall we already practiced this way back in Section 7.2.4.1. Here’s a quick example of what that looks like for b11.5.\n\nlog_lik(b11.5) |&gt; str()\n\n num [1:4000, 1:504] -0.415 -0.518 -0.363 -0.634 -0.243 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\n\n\n11.1.1.2 Bonus: A truly flat prior\nWe just used the normal(0, 1.5) prior for the intercept \\(\\alpha\\) as a very weakly regularizing, almost flat prior. I think this prior is a great default for intercepts in logistic regression models. However, if you want a truly flat prior for the intercept in these cases, you can use prior(logistic(0, 1)). As Matthijs Hollanders shared on Bluesky (here), “logistic(0, 1) is literally uniform on [0, 1].” He, of course, is right.\nLet’s practice by fitting an alternative version to b11.4 with that prior.\n\nb11.4_flat &lt;- brm(\n  data = d, \n  family = binomial,\n  bf(pulled_left | trials(1) ~ a + b,\n     a ~ 0 + actor, \n     b ~ 0 + treatment,\n     nl = TRUE),\n  prior = c(prior(logistic(0, 1), nlpar = a),\n            prior(normal(0, 0.5), nlpar = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = \"fits/b11.4_flat\")\n\nConfirm we applied the prior, as intended.\n\nb11.4_flat$prior\n\n          prior class       coef group resp dpar nlpar lb ub tag       source\n logistic(0, 1)     b                                a                   user\n logistic(0, 1)     b     actor1                     a           (vectorized)\n logistic(0, 1)     b     actor2                     a           (vectorized)\n logistic(0, 1)     b     actor3                     a           (vectorized)\n logistic(0, 1)     b     actor4                     a           (vectorized)\n logistic(0, 1)     b     actor5                     a           (vectorized)\n logistic(0, 1)     b     actor6                     a           (vectorized)\n logistic(0, 1)     b     actor7                     a           (vectorized)\n normal(0, 0.5)     b                                b                   user\n normal(0, 0.5)     b treatment1                     b           (vectorized)\n normal(0, 0.5)     b treatment2                     b           (vectorized)\n normal(0, 0.5)     b treatment3                     b           (vectorized)\n normal(0, 0.5)     b treatment4                     b           (vectorized)\n\n\nSuccess. We might compare the log-odds \\(\\alpha_{\\text{actor}[i]}\\) posteriors in a coefficient plot.\n\n# This is just for fancy annotation\ntext &lt;- tibble(value = c(3.7, 3.0),\n               actor  = \"2\",\n               fit   = c(\"b11.4_flat\", \"b11.4\"))\n\nbind_rows(as_draws_df(b11.4),\n          as_draws_df(b11.4_flat)) |&gt; \n  mutate(fit = rep(c(\"b11.4\", \"b11.4_flat\"), each = n() / 2)) |&gt; \n  pivot_longer(b_a_actor1:b_a_actor7) |&gt;\n  mutate(actor = str_remove(name, \"b_a_actor\") |&gt; \n           factor(levels = 7:1)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = value, y = actor, color = fit)) +\n  stat_pointinterval(.width = 0.95,\n                     position = position_dodge(width = 0.6), size = 2/3) +\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[2:1]) +\n  geom_text(data = text,\n            aes(label = fit),\n            family = \"Times\", position = position_dodge(width = 2.5)) +\n  labs(x = expression(alpha[actor]~\"(log-odds scale)\"),\n       y = NULL) +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nWell look at that. The posteriors were very similar overall. But for good old chimp #2, who always pulled one way, McElreath’s weakly-regularizing normal(0, 1.5) did indeed reign that posterior in a bit more than the very flat logistic(0, 1). Set your priors with care, friends.\n\n\n\n11.1.2 Relative shark and absolute deer\nBased on the full model, b11.4, here’s how you might compute the posterior mean and 95% intervals for the proportional odds of switching from treatment == 2 to treatment == 4.\n\nas_draws_df(b11.4) |&gt; \n  mutate(proportional_odds = exp(b_b_treatment4 - b_b_treatment2)) |&gt; \n  mean_qi(proportional_odds)\n\n# A tibble: 1 × 6\n  proportional_odds .lower .upper .width .point .interval\n              &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1             0.927  0.524   1.53   0.95 mean   qi       \n\n\n\nOn average, the switch multiplies the odds of pulling the left lever by 0.92, an 8% reduction in odds. This is what is meant by proportional odds. The new odds are calculated by taking the old odds and multiplying them by the proportional odds, which is 0.92 in this example. (p. 336)\n\nA limitation of relative measures measures like proportional odds is they ignore what you might think of as the reference or the baseline.\n\nConsider for example a rare disease which occurs in 1 per 10-million people. Suppose also that reading this textbook increased the odds of the disease 5-fold. That would mean approximately 4 more cases of the disease per 10-million people. So only 5-in-10-million chance now. The book is safe. (p. 336)\n\nHere that is in code.\n\ntibble(disease_rate  = 1/1e7,\n       fold_increase = 5) |&gt; \n  mutate(new_disease_rate = disease_rate * fold_increase)\n\n# A tibble: 1 × 3\n  disease_rate fold_increase new_disease_rate\n         &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;\n1    0.0000001             5        0.0000005\n\n\nThe hard part, though, is that “neither absolute nor relative risk is sufficient for all purposes” (p. 337). Each provides its own unique perspective on the data. Again, welcome to applied statistics. 🤷‍♂\n\n\n11.1.3 Aggregated binomial: Chimpanzees again, condensed\nWith the tidyverse, we can use group_by() and summarise() to achieve what McElreath did with aggregate().\n\nd_aggregated &lt;- d |&gt;\n  group_by(treatment, actor, side, cond) |&gt;\n  summarise(left_pulls = sum(pulled_left)) |&gt; \n  ungroup()\n\nd_aggregated |&gt;\n  head(n = 8)\n\n# A tibble: 8 × 5\n  treatment actor side  cond  left_pulls\n  &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt; &lt;fct&gt;      &lt;int&gt;\n1 1         1     1     1              6\n2 1         2     1     1             18\n3 1         3     1     1              5\n4 1         4     1     1              6\n5 1         5     1     1              6\n6 1         6     1     1             14\n7 1         7     1     1             14\n8 2         1     2     1              9\n\n\nTo fit an aggregated binomial model with brms, we augment the &lt;criterion&gt; | trials() syntax where the value that goes in trials() is either a fixed number, as in this case, or variable in the data indexing \\(n\\). Either way, at least some of those trials will have an \\(n &gt; 1\\). Here we’ll use the hard-code method, just like McElreath did in the text.\n\nb11.6 &lt;- brm(\n  data = d_aggregated, \n  family = binomial,\n  bf(left_pulls | trials(18) ~ a + b,\n     a ~ 0 + actor, \n     b ~ 0 + treatment,\n     nl = TRUE),\n  prior = c(prior(normal(0, 1.5), nlpar = a),\n            prior(normal(0, 0.5), nlpar = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = \"fits/b11.06\")\n\nCheck the posterior summary.\n\nprint(b11.6)\n\n Family: binomial \n  Links: mu = logit \nFormula: left_pulls | trials(18) ~ a + b \n         a ~ 0 + actor\n         b ~ 0 + treatment\n   Data: d_aggregated (Number of observations: 28) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_actor1        -0.46      0.32    -1.08     0.18 1.00     1632     2071\na_actor2         3.90      0.76     2.57     5.55 1.00     4057     2582\na_actor3        -0.76      0.33    -1.45    -0.12 1.00     1667     2136\na_actor4        -0.76      0.33    -1.42    -0.11 1.00     1721     2269\na_actor5        -0.46      0.32    -1.10     0.17 1.00     1664     2221\na_actor6         0.47      0.33    -0.17     1.10 1.00     1661     2158\na_actor7         1.94      0.41     1.18     2.79 1.00     2406     2736\nb_treatment1    -0.03      0.28    -0.56     0.53 1.00     1338     2022\nb_treatment2     0.49      0.28    -0.06     1.05 1.00     1428     2213\nb_treatment3    -0.37      0.29    -0.94     0.18 1.00     1474     2307\nb_treatment4     0.38      0.28    -0.16     0.92 1.00     1441     2046\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIt might be easiest to compare b11.4 and b11.6 with a coefficient plot.\n\n# This is just for fancy annotation\ntext &lt;- tibble(value = c(1.4, 2.6),\n               name  = \"b_a_actor7\",\n               fit   = c(\"b11.6\", \"b11.4\"))\n\n# Rope in the posterior draws and wrangle\nbind_rows(as_draws_df(b11.4),\n          as_draws_df(b11.6)) |&gt; \n  mutate(fit = rep(c(\"b11.4\", \"b11.6\"), each = n() / 2)) |&gt; \n  pivot_longer(b_a_actor1:b_b_treatment4) |&gt; \n  \n  # Plot\n  ggplot(aes(x = value, y = name, color = fit)) +\n  stat_pointinterval(.width = 0.95,\n                     position = position_dodge(width = 0.5), size = 2/3) +\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[2:1]) +\n  geom_text(data = text,\n            aes(label = fit),\n            family = \"Times\", position = position_dodge(width = 2.25)) +\n  labs(x = \"posterior (log-odds scale)\",\n       y = NULL) +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nDid you catch our position = position_dodge() tricks? Try executing the plot without those parts of the code to get a sense of what they did. Now compute and save the PSIS-LOO estimates for the two models so we might compare them.\n\nb11.4 &lt;- add_criterion(b11.4, criterion = \"loo\")\nb11.6 &lt;- add_criterion(b11.6, criterion = \"loo\")\n\nHere’s how we might attempt the comparison.\n\nloo_compare(b11.4, b11.6, criterion = \"loo\") |&gt; print(simplify = F)\n\nUnlike with McElreath’s compare() code in the text, loo_compare() wouldn’t even give us the results. All we get is the warning message informing us that because these two models are not based on the same data, comparing them with the LOO is invalid and brms refuses to let us do it. We can, however, look at their LOO summaries separately.\n\nloo(b11.4)\n\n\nComputed from 4000 by 504 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -266.0  9.5\np_loo         8.3  0.4\nlooic       531.9 19.0\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [1.3, 2.4]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\nloo(b11.6)\n\n\nComputed from 4000 by 28 log-likelihood matrix.\n\n         Estimate  SE\nelpd_loo    -57.1 4.2\np_loo         8.5 1.5\nlooic       114.3 8.4\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.9]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     27    96.4%   491     \n   (0.7, 1]   (bad)       1     3.6%   &lt;NA&gt;    \n   (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nTo understand what’s going on, consider how you might describe six 1’s out of nine trials in the aggregated form,\n\\[\\Pr(6 \\mid 9, p) = \\frac{6!}{6!(9 - 6)!} p^6 (1 - p)^{9 - 6}.\\]\nIf we still stick with the same data, but this time re-express those as nine dichotomous data points, we now describe their joint probability as\n\\[\\Pr(1, 1, 1, 1, 1, 1, 0, 0, 0 \\mid p) = p^6 (1 - p)^{9 - 6}.\\]\nLet’s work this out in code.\n\n# Deviance of aggregated 6-in-9 \n-2 * dbinom(6, size = 9, prob = 0.2, log = TRUE)\n\n[1] 11.79048\n\n# Deviance of dis-aggregated \n-2 * sum(dbinom(c(1, 1, 1, 1, 1, 1, 0, 0, 0), size = 1, prob = 0.2, log = TRUE))\n\n[1] 20.65212\n\n\n\nBut this difference is entirely meaningless. It is just a side effect of how we organized the data. The posterior distribution for the probability of success on each trial will end up the same, either way. (p. 339)\n\nThis is what our coefficient plot showed us, above. The posterior distribution was the same within simulation variance for b11.4 and b11.6. Just like McElreath reported in the text, we also got a warning about high Pareto \\(k\\) values from the aggregated binomial model, b11.6. To access the message and its associated table directly, we can feed the results of loo() into the loo::pareto_k_table function.\n\nloo(b11.6) |&gt; \n  loo::pareto_k_table()\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     27    96.4%   491     \n   (0.7, 1]   (bad)       1     3.6%   &lt;NA&gt;    \n   (1, Inf)   (very bad)  0     0.0%   &lt;NA&gt;    \n\n\n\nBefore looking at the Pareto \\(k\\) values, you might have noticed already that we didn’t get a similar warning before in the disaggregated logistic models of the same data. Why not? Because when we aggregated the data by actor-treatment, we forced PSIS (and WAIC) to imagine cross-validation that leaves out all 18 observations in each actor-treatment combination. So instead of leave-one-out cross-validation, it is more like leave-eighteen-out. This makes some observations more influential, because they are really now 18 observations.\nWhat’s the bottom line? If you want to calculate WAIC or PSIS, you should use a logistic regression data format, not an aggregated format. Otherwise you are implicitly assuming that only large chunks of the data are separable. (p. 340)\n\n\n\n11.1.4 Aggregated binomial: Graduate school admissions\nLoad the infamous UCBadmit data (see Bickel et al., 1975).\n\ndata(UCBadmit, package = \"rethinking\")\nd &lt;- UCBadmit\nrm(UCBadmit)\n\nd\n\n   dept applicant.gender admit reject applications\n1     A             male   512    313          825\n2     A           female    89     19          108\n3     B             male   353    207          560\n4     B           female    17      8           25\n5     C             male   120    205          325\n6     C           female   202    391          593\n7     D             male   138    279          417\n8     D           female   131    244          375\n9     E             male    53    138          191\n10    E           female    94    299          393\n11    F             male    22    351          373\n12    F           female    24    317          341\n\n\nNow compute our new index variable, gid. We’ll also slip in a case variable that saves the row numbers as a factor. That’ll come in handy later when we plot.\n\nd &lt;- d |&gt;  \n  mutate(gid  = factor(applicant.gender, levels = c(\"male\", \"female\")),\n         case = factor(1:n()))\n\nNote the difference in how we defined out gid. Whereas McElreath used numeral indices, we retained the text within an ordered factor. brms can handle either approach just fine. The advantage of the factor approach is it will be easier to understand the output. You’ll see in just a bit.\nThe univariable logistic model with male as the sole predictor of admit follows the form\n\\[\\begin{align*}\n\\text{admit}_i    & \\sim \\operatorname{Binomial}(n_i, p_i) \\\\\n\\text{logit}(p_i) & = \\alpha_{\\text{gid}[i]} \\\\\n\\alpha_j          & \\sim \\operatorname{Normal}(0, 1.5),\n\\end{align*}\\]\nwhere \\(n_i = \\text{applications}_i\\), the rows are indexed by \\(i\\), and the two levels of \\(\\text{gid}\\) are indexed by \\(j\\). Since we’re only using our index variable gid to model two intercepts with no further complications, we don’t need to use the verbose non-linear syntax to fit this model with brms.\n\nb11.7 &lt;- brm(\n  data = d, \n  family = binomial,\n  admit | trials(applications) ~ 0 + gid,\n  prior(normal(0, 1.5), class = b),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  file = \"fits/b11.07\")\n\n\nprint(b11.7)\n\n Family: binomial \n  Links: mu = logit \nFormula: admit | trials(applications) ~ 0 + gid \n   Data: d (Number of observations: 12) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ngidmale      -0.22      0.04    -0.30    -0.14 1.00     3800     2702\ngidfemale    -0.83      0.05    -0.93    -0.73 1.00     4126     2987\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur results are very similar to those in the text. But notice how our two rows have more informative row names than a[1] and a[2]. This is why you might consider using the ordered factor approach rather than using numeral indices.\nAnyway, here we’ll compute the difference score in two metrics and summarize them with a little help from mean_qi().\n\nas_draws_df(b11.7) |&gt; \n  mutate(diff_a = b_gidmale - b_gidfemale,\n         diff_p = plogis(b_gidmale) - plogis(b_gidfemale)) |&gt; \n  pivot_longer(contains(\"diff\")) |&gt; \n  group_by(name) |&gt; \n  mean_qi(value, .width = 0.89)\n\n# A tibble: 2 × 7\n  name   value .lower .upper .width .point .interval\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 diff_a 0.608  0.506  0.710   0.89 mean   qi       \n2 diff_p 0.141  0.118  0.164   0.89 mean   qi       \n\n\nbrms doesn’t have a convenience function that works quite like rethinking::postcheck(). But we have options, the most handy of which in this case is probably predict().\n\np &lt;- predict(b11.7) |&gt; \n  data.frame() |&gt; \n  bind_cols(d)\n\ntext &lt;- d |&gt;\n  group_by(dept) |&gt;\n  summarise(case  = mean(as.numeric(case)),\n            admit = mean(admit / applications) + 0.05)\n\np |&gt; \n  ggplot(aes(x = case, y = admit / applications)) +\n  geom_pointrange(aes(y    = Estimate / applications,\n                      ymin = Q2.5     / applications ,\n                      ymax = Q97.5    / applications),\n                  alpha = 1/3,\n                  color = wes_palette(\"Moonrise2\")[1],\n                  shape = 1) +\n  geom_point(color = wes_palette(\"Moonrise2\")[2]) +\n  geom_line(aes(group = dept),\n            color = wes_palette(\"Moonrise2\")[2]) +\n  geom_text(data = text,\n            aes(y = admit, label = dept),\n            color = wes_palette(\"Moonrise2\")[2],\n            family = \"serif\") +\n  scale_y_continuous(\"Proportion admitted\", limits = 0:1) +\n  ggtitle(\"Posterior validation check\") +\n  theme(axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\nSometimes a fit this bad is the result of a coding mistake. In this case, it is not. The model did correctly answer the question we asked of it: What are the average probabilities of admission for women and men, across all departments? The problem in this case is that men and women did not apply to the same departments, and departments vary in their rates of admission. This makes the answer misleading….\nInstead of asking “What are the average probabilities of admission for women and men across all departments?” we want to ask “What is the average difference in probability of admission between women and men within departments?” (pp. 342–343, emphasis in the original).\n\nThe model better suited to answer that question follows the form\n\\[\\begin{align*}\n\\text{admit}_i    & \\sim \\operatorname{Binomial} (n_i, p_i) \\\\\n\\text{logit}(p_i) & = \\alpha_{\\text{gid}[i]} + \\delta_{\\text{dept}[i]} \\\\\n\\alpha_j          & \\sim \\operatorname{Normal} (0, 1.5) \\\\\n\\delta_k          & \\sim \\operatorname{Normal} (0, 1.5),\n\\end{align*}\\]\nwhere departments are indexed by \\(k\\). To fit a model including two index variables like this in brms, we’ll need to switch back to the non-linear syntax. Though if you’d like to see an analogous approach using conventional brms syntax, check out model b10.9 in Section 10.1.3 of my translation of McElreath’s first edition.\n\nb11.8 &lt;- brm(\n  data = d, \n  family = binomial,\n  bf(admit | trials(applications) ~ a + d,\n     a ~ 0 + gid, \n     d ~ 0 + dept,\n     nl = TRUE),\n  prior = c(prior(normal(0, 1.5), nlpar = a),\n            prior(normal(0, 1.5), nlpar = d)),\n  iter = 4000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  file = \"fits/b11.08\") \n\n\nprint(b11.8)\n\n Family: binomial \n  Links: mu = logit \nFormula: admit | trials(applications) ~ a + d \n         a ~ 0 + gid\n         d ~ 0 + dept\n   Data: d (Number of observations: 12) \n  Draws: 4 chains, each with iter = 4000; warmup = 1000; thin = 1;\n         total post-warmup draws = 12000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_gidmale      -0.56      0.54    -1.65     0.49 1.01      987     1039\na_gidfemale    -0.46      0.54    -1.56     0.58 1.01      995     1010\nd_deptA         1.14      0.54     0.09     2.24 1.01      998     1077\nd_deptB         1.10      0.54     0.04     2.20 1.01     1004     1075\nd_deptC        -0.12      0.54    -1.17     0.98 1.01      995     1119\nd_deptD        -0.15      0.54    -1.19     0.94 1.01      997     1060\nd_deptE        -0.60      0.54    -1.66     0.51 1.01     1025     1142\nd_deptF        -2.15      0.55    -3.24    -1.06 1.01     1031     1199\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLike with the earlier model, here we compute the difference score for \\(\\alpha\\) in two metrics.\n\nas_draws_df(b11.8) |&gt; \n  mutate(diff_a = b_a_gidmale - b_a_gidfemale,\n         diff_p = plogis(b_a_gidmale) - plogis(b_a_gidfemale)) |&gt; \n  pivot_longer(contains(\"diff\")) |&gt; \n  group_by(name) |&gt; \n  mean_qi(value, .width = 0.89)\n\n# A tibble: 2 × 7\n  name     value  .lower  .upper .width .point .interval\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 diff_a -0.0973 -0.228  0.0309    0.89 mean   qi       \n2 diff_p -0.0216 -0.0521 0.00678   0.89 mean   qi       \n\n\n\nWhy did adding departments to the model change the inference about gender so much? The earlier figure gives you a hint–the rates of admission vary a lot across departments. Furthermore, women and men applied to different departments. Let’s do a quick tabulation to show that: (p. 344)\n\nHere’s our tidyverse-style tabulation of the proportions of applicants in each department by gid.\n\nd |&gt; \n  group_by(dept) |&gt; \n  mutate(proportion = applications / sum(applications)) |&gt; \n  select(dept, gid, proportion) |&gt; \n  pivot_wider(names_from = dept,\n              values_from = proportion) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 2 × 7\n  gid        A     B     C     D     E     F\n  &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 male    0.88  0.96  0.35  0.53  0.33  0.52\n2 female  0.12  0.04  0.65  0.47  0.67  0.48\n\n\nTo make it even easier to see, we’ll depict it in a tile plot.\n\nd |&gt; \n  group_by(dept) |&gt; \n  mutate(proportion = applications / sum(applications)) |&gt; \n  mutate(label = round(proportion, digits = 2),\n         gid   = fct_rev(gid)) |&gt; \n  \n  ggplot(aes(x = dept, y = gid, fill = proportion, label = label)) +\n  geom_tile() +\n  geom_text(aes(color = proportion &gt; 0.25),\n            family = \"serif\") +\n  scale_x_discrete(NULL, position = \"top\") +\n  scale_fill_gradient(low = wes_palette(\"Moonrise2\")[4],\n                      high = wes_palette(\"Moonrise2\")[1],\n                      limits = c(0, 1)) +\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[c(1, 4)]) +\n  ylab(NULL) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nAs it turns out, “The departments with a larger proportion of women applicants are also those with lower overall admissions rates” (p. 344). If we presume gender influences both choice of department and admission rates, we might depict that in a simple DAG where \\(G\\) is applicant gender, \\(D\\) is department, and \\(A\\) is acceptance into grad school.\n\nlibrary(ggdag)\n\ndag_coords &lt;- tibble(\n  name = c(\"G\", \"D\", \"A\"),\n  x    = c(1, 2, 3),\n  y    = c(1, 2, 1))\n\ndagify(D ~ G,\n       A ~ D + G,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(color = wes_palette(\"Moonrise2\")[4], family = \"serif\") +\n  geom_dag_edges(edge_color = wes_palette(\"Moonrise2\")[4]) + \n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL)\n\n\n\n\n\n\n\n\nAlthough our b11.8 model did not contain a parameter corresponding to the \\(G \\rightarrow D\\) pathway, it did condition on both \\(G\\) and \\(D\\). If we make another Figure like 11.5, we’ll see conditioning on both substantially improved the posterior predictive distribution.\n\npredict(b11.8) |&gt; \n  data.frame() |&gt; \n  bind_cols(d) |&gt; \n  \n  ggplot(aes(x = case, y = admit / applications)) +\n  geom_pointrange(aes(y = Estimate / applications,\n                      ymin = Q2.5  / applications ,\n                      ymax = Q97.5 / applications),\n                  alpha = 1/3,\n                  color = wes_palette(\"Moonrise2\")[1],\n                  shape = 1) +\n  geom_point(color = wes_palette(\"Moonrise2\")[2]) +\n  geom_line(aes(group = dept),\n            color = wes_palette(\"Moonrise2\")[2]) +\n  geom_text(data = text,\n            aes(y = admit, label = dept),\n            color = wes_palette(\"Moonrise2\")[2],\n            family = \"serif\") +\n  scale_y_continuous(\"Proportion admitted\", limits = 0:1) +\n  labs(title = \"Posterior validation check\",\n       subtitle = \"Though imperfect, this model is a big improvement\") +\n  theme(axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\nHere’s the DAG that proposes an unobserved confound, \\(U\\), that might better explain the \\(D \\rightarrow A\\) pathway.\n\ndag_coords &lt;- tibble(\n  name = c(\"G\", \"D\", \"A\", \"U\"),\n  x    = c(1, 2, 3, 3),\n  y    = c(1, 2, 1, 2))\n\ndagify(D ~ G + U,\n       A ~ D + G + U,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_point(x = 3, y = 2, \n             color = wes_palette(\"Moonrise2\")[2], size = 5) +\n  geom_dag_text(color = wes_palette(\"Moonrise2\")[4], family = \"serif\") +\n  geom_dag_edges(edge_color = wes_palette(\"Moonrise2\")[4]) + \n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL)\n\n\n\n\n\n\n\n\nMcElreath recommended we look at the pairs() plot to get a sense of how highly correlated the parameters in our b11.8 model are. Why not get a little extra about it and use custom settings the upper triangle, the diagonal, and the lower triangle with a GGally::ggpairs() plot? First we save our custom settings.\n\nmy_upper &lt;- function(data, mapping, ...) {\n  \n  # Get the x and y data to use the other code\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n  \n  r  &lt;- unname(cor.test(x, y)$estimate)\n  rt &lt;- format(r, digits = 2)[1]\n  tt &lt;- as.character(rt)\n  \n  # Plot the cor value\n  ggally_text(\n    label = tt, \n    mapping = aes(),\n    alpha = 4/5,\n    color = wes_palette(\"Moonrise2\")[4], \n    family = \"Times\",\n    size = 4) +\n    theme_void()\n}\n\nmy_diag &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_density(fill = wes_palette(\"Moonrise2\")[2], linewidth = 0) +\n    theme_void()\n}\n\nmy_lower &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_point(alpha = 1/10,\n               color = wes_palette(\"Moonrise2\")[1], \n               size = 1/10) +\n    theme_void()\n}\n\nTo learn more about the nature of the code for the my_upper() function, check out Issue #139 in the GGally GitHub repository. Here is the plot.\n\nlibrary(GGally)\n\nas_draws_df(b11.8) |&gt; \n  select(starts_with(\"b_\")) |&gt; \n  set_names(c(\"alpha[male]\", \"alpha[female]\", str_c(\"delta[\", LETTERS[1:6], \"]\"))) |&gt;\n  ggpairs(upper = list(continuous = my_upper),\n          diag  = list(continuous = my_diag),\n          lower = list(continuous = my_lower),\n          labeller = \"label_parsed\") +\n  labs(title = \"Model: b11.8\",\n       subtitle = \"The parameters are strongly correlated.\") +\n  theme(strip.text = element_text(size = 11))\n\n\n\n\n\n\n\n\n\nWhy might we want to over-parameterize the model? Because it makes it easier to assign priors. If we made one of the genders baseline and measured the other as a deviation from it, we would stumble into the issue of assuming that the acceptance rate for one of the genders is pre-data more uncertain than the other. This isn’t to say that over-parameterizing a model is always a good idea. But it isn’t a violation of any statistical principle. You can always convert the posterior, post sampling, to any alternative parameterization. The only limitation is whether the algorithm we use to approximate the posterior can handle the high correlations. In this case, it can. (p. 345)\n\n\n11.1.4.1 Rethinking: Simpson’s paradox is not a paradox\n\nThis empirical example is a famous one in statistical teaching. It is often used to illustrate a phenomenon known as Simpson’s paradox. Like most paradoxes, there is no violation of logic, just of intuition. And since different people have different intuition, Simpson’s paradox means different things to different people. The poor intuition being violated in this case is that a positive association in the entire population should also hold within each department. (p. 345, emphasis in the original)\n\nIn my field of clinical psychology, Simpson’s paradox is an important, if under-appreciated, phenomenon. If you’re in the social sciences as well, I highly recommend spending more time thinking about it. To get you started, I blogged about it here and Kievit et al. (2013) wrote a great tutorial paper called Simpson’s paradox in psychological science: a practical guide.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>God Spiked the Integers</span>"
    ]
  },
  {
    "objectID": "11.html#poisson-regression",
    "href": "11.html#poisson-regression",
    "title": "11  God Spiked the Integers",
    "section": "11.2 Poisson regression",
    "text": "11.2 Poisson regression\n\nWhen a binomial distribution has a very small probability of an event \\(p\\) and a very large number of trials \\(N\\), then it takes on a special shape. The expected value of a binomial distribution is just \\(Np\\), and its variance is \\(Np(1 - p)\\). But when \\(N\\) is very large and \\(p\\) is very small, then these are approximately the same. (p. 346)\n\nData of this kind are often called count data. Here we simulate some.\n\nset.seed(11)\n\ntibble(y = rbinom(1e5, 1000, 1/1000)) |&gt; \n  summarise(y_mean     = mean(y),\n            y_variance = var(y))\n\n# A tibble: 1 × 2\n  y_mean y_variance\n   &lt;dbl&gt;      &lt;dbl&gt;\n1   1.00       1.01\n\n\nYes, those statistics are virtually the same. When dealing with pure Poisson data, \\(\\mu = \\sigma^2\\). When you have a number of trials for which \\(n\\) is unknown or much larger than seen in the data, the Poisson likelihood is a useful tool. We define it as\n\\[y_i \\sim \\text{Poisson}(\\lambda),\\]\nwhere \\(\\lambda\\) expresses both mean and variance because within this model, the variance scales right along with the mean. Since \\(\\lambda\\) is constrained to be positive, we typically use the log link. Thus the basic Poisson regression model is\n\\[\\begin{align*}\ny_i             & \\sim \\operatorname{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i) & = \\alpha + \\beta (x_i - \\bar x),\n\\end{align*}\\]\nwhere all model parameters receive priors following the forms we’ve been practicing.\n\n11.2.1 Example: Oceanic tool complexity\nLoad the Kline data (see Kline & Boyd, 2010).\n\ndata(Kline, package = \"rethinking\")\nd &lt;- Kline\nrm(Kline)\n\nd\n\n      culture population contact total_tools mean_TU\n1    Malekula       1100     low          13     3.2\n2     Tikopia       1500     low          22     4.7\n3  Santa Cruz       3600     low          24     4.0\n4         Yap       4791    high          43     5.0\n5    Lau Fiji       7400    high          33     5.0\n6   Trobriand       8000    high          19     4.0\n7       Chuuk       9200    high          40     3.8\n8       Manus      13000     low          28     6.6\n9       Tonga      17500    high          55     5.4\n10     Hawaii     275000     low          71     6.6\n\n\nHere are our new columns.\n\nd &lt;- d |&gt;\n  mutate(log_pop_std = (log(population) - mean(log(population))) / sd(log(population)),\n         cid         = contact)\n\nOur statistical model will follow the form\n\\[\\begin{align*}\n\\text{total\\_tools}_i & \\sim \\operatorname{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i)      & = \\alpha_{\\text{cid}[i]} + \\beta_{\\text{cid}[i]} \\text{log\\_pop\\_std}_i \\\\\n\\alpha_j             & \\sim \\; ? \\\\\n\\beta_j              & \\sim \\; ?,\n\\end{align*}\\]\nwhere the priors for \\(\\alpha_j\\) and \\(\\beta_j\\) have yet be defined. If we continue our convention of using a Normal prior on the \\(\\alpha\\) parameters, we should recognize those will be log-Normal distributed on the outcome scale. Why? Because we’re modeling \\(\\lambda\\) with the log link. Here’s our version of Figure 11.7, depicting the two log-Normal priors considered in the text.\n\nd_plot &lt;- tibble(\n  x = c(3, 22),\n  y = c(0.055, 0.04),\n  meanlog = c(0, 3),\n  sdlog   = c(10, 0.5)) |&gt; \n  expand_grid(number = seq(from = 0, to = 100, length.out = 200)) |&gt; \n  mutate(density = dlnorm(number, meanlog, sdlog),\n         group   = str_c(\"alpha%~%Normal(\", meanlog, \", \", sdlog, \")\"))\n\nd_plot |&gt; \n  ggplot(aes(fill = group, color = group)) +\n  geom_area(aes(x = number, y = density),\n            alpha = 3/4, linewidth = 0, position = \"identity\") +\n  geom_text(data = d_plot |&gt; group_by(group) |&gt; slice(1),\n            aes(x = x, y = y, label = group),\n            family = \"Times\",  hjust = 0, parse = T) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[1:2]) +\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[1:2]) +\n  xlab(\"mean number of tools\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIn this context, \\(\\alpha \\sim \\operatorname{Normal}(0, 10)\\) has a very long tail on the outcome scale. The mean of the log-Normal distribution, recall, is \\(\\exp (\\mu + \\sigma^2/2)\\). Here that is in code.\n\nexp(0 + 10^2 / 2)\n\n[1] 5.184706e+21\n\n\nThat is very large. Here’s the same thing in a simulation.\n\nset.seed(11)\n\nrnorm(1e4, 0, 10) |&gt; \n  exp() |&gt; \n  mean()\n\n[1] 1.61276e+12\n\n\nNow compute the mean for the other prior under consideration, \\(\\alpha \\sim \\operatorname{Normal}(3, 0.5)\\).\n\nexp(3 + 0.5^2 / 2)\n\n[1] 22.7599\n\n\nThis is much smaller and more reasonable. In case you were curious, here are the same priors, this time on the scale of \\(\\lambda\\).\n\nd_plot &lt;- tibble(\n  x = c(10, 4),\n  y = c(0.05, 0.5),\n  mean = c(0, 3),\n  sd   = c(10, 0.5)) |&gt; \n  expand_grid(number = seq(from = -25, to = 25, length.out = 500)) |&gt; \n  mutate(density = dnorm(number, mean, sd),\n         group   = str_c(\"alpha%~%Normal(\", mean, \", \", sd, \")\"))\n\nd_plot |&gt; \n  ggplot(aes(fill = group, color = group)) +\n  geom_area(aes(x = number, y = density),\n            alpha = 3/4, linewidth = 0, position = \"identity\") +\n  geom_text(data = d_plot |&gt; group_by(group) |&gt; slice(1),\n            aes(x = x, y = y, label = group),\n            family = \"Times\",  hjust = 0, parse = T) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[1:2]) +\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[1:2]) +\n  xlab(expression(lambda~scale)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow let’s prepare to make the top row of Figure 11.8. In this portion of the figure, we consider the implications of two competing priors for \\(\\beta\\) while holding the prior for \\(\\alpha\\) at \\(\\operatorname{Normal}(3, 0.5)\\). The two \\(\\beta\\) priors under consideration are \\(\\operatorname{Normal}(0, 10)\\) and \\(\\operatorname{Normal}(0, 0.2)\\).\n\nset.seed(11)\n\n# How many lines would you like?\nn &lt;- 100\n\n# Simulate and wrangle\ntibble(i = 1:n,\n       a = rnorm(n, mean = 3, sd = 0.5)) |&gt; \n  mutate(`beta%~%Normal(0*', '*10)`  = rnorm(n, mean = 0 , sd = 10),\n         `beta%~%Normal(0*', '*0.2)` = rnorm(n, mean = 0 , sd = 0.2)) |&gt; \n  pivot_longer(contains(\"beta\"),\n               values_to = \"b\",\n               names_to = \"prior\") |&gt; \n  expand_grid(x = seq(from = -2, to = 2, length.out = 100)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4],\n            linewidth = 1/4) +\n  labs(x = \"log population (std)\",\n       y = \"total tools\") +\n  coord_cartesian(ylim = c(0, 100)) +\n  facet_wrap(~ prior, labeller = label_parsed)\n\n\n\n\n\n\n\n\nIt turns out that many of the lines considered plausible under \\(\\operatorname{Normal}(0, 10)\\) are disturbingly extreme. Here is what \\(\\alpha \\sim \\operatorname{Normal}(3, 0.5)\\) and \\(\\beta \\sim \\operatorname{Normal}(0, 0.2)\\) would mean when the \\(x\\)-axis is on the log population scale and the population scale.\n\nset.seed(11)\n\nprior &lt;- tibble(\n  i = 1:n,\n  a = rnorm(n, mean = 3, sd = 0.5),\n  b = rnorm(n, mean = 0, sd = 0.2)) |&gt; \n  expand_grid(x = seq(from = log(100), to = log(200000), length.out = 100))\n\n# Left\np1 &lt;- prior |&gt; \n  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +\n  geom_line(alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4],\n            linewidth = 1/4) +\n  labs(x = \"log population\",\n       y = \"total tools\",\n       subtitle = expression(beta%~%Normal(0*', '*0.2))) +\n  coord_cartesian(xlim = c(log(100), log(200000)),\n                  ylim = c(0, 500))\n# Right\np2 &lt;- prior |&gt; \n  ggplot(aes(x = exp(x), y = exp(a + b * x), group = i)) +\n  geom_line(alpha = 2/3,\n            color = wes_palette(\"Moonrise2\")[4],\n            linewidth = 1/4) +\n  labs(x = \"population\",\n       y = \"total tools\",\n       subtitle = expression(beta%~%Normal(0*', '*0.2))) +\n  coord_cartesian(xlim = c(100, 200000),\n                  ylim = c(0, 500))\n\n# Combine\np1 | p2\n\n\n\n\n\n\n\n\nOkay, after settling on our two priors, the updated model formula is\n\\[\\begin{align*}\ny_i             & \\sim \\operatorname{Poisson}(\\lambda_i) \\\\\n\\log(\\lambda_i) & = \\alpha + \\beta (x_i - \\bar x) \\\\\n\\alpha          & \\sim \\operatorname{Normal}(3, 0.5) \\\\\n\\beta           & \\sim \\operatorname{Normal}(0, 0.2).\n\\end{align*}\\]\nWe’re finally ready to fit the model. The only new thing in our model code is family = poisson. In this case, brms defaults to the log() link. We’ll fit both an intercept-only Poisson model and an interaction model.\n\n# Intercept only\nb11.9 &lt;- brm(\n  data = d, \n  family = poisson,\n  total_tools ~ 1,\n  prior(normal(3, 0.5), class = Intercept),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = \"fits/b11.09\")\n\n# Interaction model\nb11.10 &lt;- brm(\n  data = d, \n  family = poisson,\n  bf(total_tools ~ a + b * log_pop_std,\n     a + b ~ 0 + cid,\n     nl = TRUE),\n  prior = c(prior(normal(3, 0.5), nlpar = a),\n            prior(normal(0, 0.2), nlpar = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = \"fits/b11.10\")\n\nCheck the model summaries.\n\nprint(b11.9)\n\n Family: poisson \n  Links: mu = log \nFormula: total_tools ~ 1 \n   Data: d (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     3.54      0.05     3.44     3.64 1.00     1716     1970\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(b11.10)\n\n Family: poisson \n  Links: mu = log \nFormula: total_tools ~ a + b * log_pop_std \n         a ~ 0 + cid\n         b ~ 0 + cid\n   Data: d (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cidhigh     3.61      0.07     3.46     3.75 1.00     4073     2877\na_cidlow      3.32      0.09     3.15     3.48 1.00     3281     2661\nb_cidhigh     0.19      0.16    -0.12     0.50 1.00     4249     3105\nb_cidlow      0.38      0.05     0.27     0.48 1.00     3285     2977\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow compute the LOO estimates and compare the models by the LOO.\n\nb11.9  &lt;- add_criterion(b11.9,  criterion = \"loo\")\nb11.10 &lt;- add_criterion(b11.10, criterion = \"loo\")\n\nloo_compare(b11.9, b11.10, criterion = \"loo\") |&gt; print(simplify = F)\n\n       elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic\nb11.10   0.0       0.0   -42.9      6.7         7.2   2.7     85.7  13.3   \nb11.9  -27.7      16.6   -70.6     16.9         7.9   3.5    141.2  33.7   \n\n\nHere’s the LOO weight.\n\nmodel_weights(b11.9, b11.10, weights = \"loo\") |&gt; round(digits = 2)\n\n b11.9 b11.10 \n     0      1 \n\n\nMcElreath reported getting a warning from his rethinking::compare(). Our warning came from the add_criterion() function. We can inspect the Pareto \\(k\\) values with loo::pareto_k_table().\n\nloo(b11.10) |&gt; loo::pareto_k_table()\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     8     80.0%   201     \n   (0.7, 1]   (bad)      2     20.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad) 0      0.0%   &lt;NA&gt;    \n\n\nLet’s take a closer look.\n\ntibble(culture = d$culture,\n       k       = b11.10$criteria$loo$diagnostics$pareto_k) |&gt; \n  arrange(desc(k)) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 10 × 2\n   culture        k\n   &lt;fct&gt;      &lt;dbl&gt;\n 1 Hawaii      0.93\n 2 Tonga       0.8 \n 3 Trobriand   0.56\n 4 Malekula    0.45\n 5 Yap         0.34\n 6 Tikopia     0.34\n 7 Manus       0.29\n 8 Lau Fiji    0.27\n 9 Santa Cruz  0.24\n10 Chuuk       0.21\n\n\nIt turns out Hawaii is very influential. Figure 11.9 will clarify why. Here we make the left panel.\n\ncultures &lt;- c(\"Hawaii\", \"Tonga\", \"Trobriand\", \"Yap\")\n\nlibrary(ggrepel)\n\nnd &lt;- distinct(d, cid) |&gt; \n  expand_grid(log_pop_std = seq(from = -4.5, to = 2.5, length.out = 100))\n\nf &lt;- fitted(b11.10,\n            newdata = nd,\n            probs = c(0.055, 0.945)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd)\n\np1 &lt;- f |&gt;\n  ggplot(aes(x = log_pop_std, group = cid, color = cid)) +\n  geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid),\n              stat = \"identity\",\n              alpha = 1/4, linewidth = 1/2) +\n  geom_point(data = bind_cols(d, b11.10$criteria$loo$diagnostics),\n             aes(y = total_tools, size = pareto_k),\n             alpha = 4/5) +\n  geom_text_repel(data = bind_cols(d, b11.10$criteria$loo$diagnostics) |&gt; \n                    filter(culture %in% cultures) |&gt; \n                    mutate(label = str_c(culture, \" (\", round(pareto_k, digits = 2), \")\")),\n                  aes(y = total_tools, label = label), \n                  color = \"black\", family = \"Times\", size = 3, seed = 11) +\n  labs(x = \"log population (std)\",\n       y = \"total tools\") +\n  coord_cartesian(xlim = range(b11.10$data$log_pop_std),\n                  ylim = c(0, 80))\n\nNow make the right panel of Figure 11.9.\n\np2 &lt;- f |&gt;\n  mutate(population = exp((log_pop_std * sd(log(d$population))) + mean(log(d$population)))) |&gt; \n\n  ggplot(aes(x = population, group = cid, color = cid)) +\n  geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid),\n              stat = \"identity\",\n              alpha = 1/4, linewidth = 1/2) +\n  geom_point(data = bind_cols(d, b11.10$criteria$loo$diagnostics),\n             aes(y = total_tools, size = pareto_k),\n             alpha = 4/5) +\n  scale_x_continuous(\"population\", breaks = c(0, 50000, 150000, 250000)) +\n  ylab(\"total tools\") +\n  coord_cartesian(xlim = range(d$population),\n                  ylim = c(0, 80))\n\nCombine the two subplots with patchwork and adjust the settings a little.\n\n(p1 | p2) &\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[1:2]) &\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[1:2]) &\n  scale_size(range = c(2, 5)) &\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nHawaii is influential in that it has a very large population relative to the other islands.\n\n11.2.1.1 Overthinking: Modeling tool innovation\nMcElreath’s theoretical, or scientific, model for total_tools is\n\\[\\widehat{\\text{total\\_tools}} = \\frac{\\alpha_{\\text{cid}[i]} \\: \\text{population}^{\\beta_{\\text{cid}[i]}}}{\\gamma}.\\]\nWe can use the Poisson likelihood to express this in a Bayesian model as\n\\[\\begin{align*}\n\\text{total\\_tools} & \\sim \\operatorname{Poisson}(\\lambda_i) \\\\\n\\lambda_i & = \\left[ \\exp (\\alpha_{\\text{cid}[i]}) \\text{population}_i^{\\beta_{\\text{cid}[i]}} \\right] / \\gamma \\\\\n\\alpha_j  & \\sim \\operatorname{Normal}(1, 1) \\\\\n\\beta_j   & \\sim \\operatorname{Exponential}(1) \\\\\n\\gamma    & \\sim \\operatorname{Exponential}(1),\n\\end{align*}\\]\nwhere we exponentiate \\(\\alpha_{\\text{cid}[i]}\\) to restrict the posterior to zero and above. Here’s how we might fit that model with brms.\n\nb11.11 &lt;- brm(\n  data = d, \n  family = poisson(link = \"identity\"),\n  bf(total_tools ~ exp(a) * population^b / g,\n     a + b ~ 0 + cid,\n     g ~ 1,\n     nl = TRUE),\n  prior = c(prior(normal(1, 1), nlpar = a),\n            prior(exponential(1), nlpar = b, lb = 0),\n            prior(exponential(1), nlpar = g, lb = 0)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = \"fits/b11.11\")\n\nDid you notice the family = poisson(link = \"identity\") part of the code? Yes, it’s possible to use the Poisson likelihood without the log link. However, if you’re going to buck tradition and use some other link, make sure you know what you’re doing.\nCheck the model summary.\n\nprint(b11.11)\n\n Family: poisson \n  Links: mu = identity \nFormula: total_tools ~ exp(a) * population^b/g \n         a ~ 0 + cid\n         b ~ 0 + cid\n         g ~ 1\n   Data: d (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_cidhigh       0.96      0.88    -0.78     2.61 1.00     1542     1443\na_cidlow        0.89      0.68    -0.53     2.19 1.00     1569     1305\nb_cidhigh       0.29      0.11     0.05     0.49 1.00     1124      647\nb_cidlow        0.26      0.03     0.19     0.33 1.00     2093     1511\ng_Intercept     1.13      0.73     0.22     3.04 1.00     1465     1286\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompute and check the PSIS-LOO estimates along with their diagnostic Pareto \\(k\\) values.\n\nb11.11 &lt;- add_criterion(b11.11, criterion = \"loo\", moment_match = T)\nloo(b11.11)\n\n\nComputed from 4000 by 10 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo    -41.3  6.1\np_loo         6.2  2.1\nlooic        82.7 12.1\n------\nMCSE of elpd_loo is NA.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.0]).\n\nPareto k diagnostic values:\n                         Count Pct.    Min. ESS\n(-Inf, 0.7]   (good)     8     80.0%   169     \n   (0.7, 1]   (bad)      2     20.0%   &lt;NA&gt;    \n   (1, Inf)   (very bad) 0      0.0%   &lt;NA&gt;    \nSee help('pareto-k-diagnostic') for details.\n\n\nThe first time through, we still had Pareto high \\(k\\) values. Recall that due to the very small sample size, this isn’t entirely surprising. Newer versions of brms might prompt you to set moment_match = TRUE, which is what I did, here. You might perform the operation both ways to get a sense of the difference.\nOkay, it’s time to make Figure 11.10.\n\n# For the annotation\ntext &lt;- distinct(d, cid) |&gt; \n  mutate(population  = c(210000, 72500),\n         total_tools = c(59, 68),\n         label       = str_c(cid, \" contact\"))\n\n# Redefine the new data\nnd &lt;- distinct(d, cid) |&gt; \n  expand_grid(population = seq(from = 0, to = 300000, length.out = 100))\n\n# Compute the poster predictions for lambda\nfitted(b11.11,\n       newdata = nd,\n       probs = c(0.055, 0.945)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt;\n  \n  # Plot!\n  ggplot(aes(x = population, group = cid, color = cid)) +\n  geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid),\n              stat = \"identity\",\n              alpha = 1/4, linewidth = 1/2) +\n  geom_point(data = bind_cols(d, b11.11$criteria$loo$diagnostics),\n             aes(y = total_tools, size = pareto_k),\n             alpha = 4/5) +\n  geom_text(data = text,\n            aes(y = total_tools, label = label),\n            family = \"serif\") +\n  scale_x_continuous(\"population\", breaks = c(0, 50000, 150000, 250000)) +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[1:2]) +\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[1:2]) +\n  scale_size(range = c(2, 5)) +\n  ylab(\"total tools\") +\n  coord_cartesian(xlim = range(d$population),\n                  ylim = range(d$total_tools)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIn case you were curious, here are the results if we compare b11.10 and b11.11 by the PSIS-LOO.\n\nloo_compare(b11.10, b11.11, criterion = \"loo\") |&gt; print(simplify = F)\n\n       elpd_diff se_diff elpd_loo se_elpd_loo p_loo se_p_loo looic se_looic\nb11.11   0.0       0.0   -41.3      6.1         6.2   2.1     82.7  12.1   \nb11.10  -1.5       2.6   -42.9      6.7         7.2   2.7     85.7  13.3   \n\nmodel_weights(b11.10, b11.11, weights = \"loo\") |&gt; round(digits = 3)\n\nb11.10 b11.11 \n  0.18   0.82 \n\n\nFinally, here’s a comparison of the two models by the Pareto \\(k\\) values.\n\ntibble(b11.10 = b11.10$criteria$loo$diagnostics$pareto_k,\n       b11.11 = b11.11$criteria$loo$diagnostics$pareto_k) |&gt; \n  pivot_longer(everything()) |&gt; \n  \n  ggplot(aes(x = value, y = name)) +\n  geom_vline(xintercept = c(0.5, 0.7, 1), linetype = 3, color = wes_palette(\"Moonrise2\")[2]) +\n  stat_dots(slab_fill = wes_palette(\"Moonrise2\")[1], \n            slab_color = wes_palette(\"Moonrise2\")[1]) + \n  scale_x_continuous(expression(Pareto~italic(k)), breaks = c(0.5, 0.7, 1)) +\n  ylab(NULL) +\n  coord_cartesian(ylim = c(1.5, 2.4))\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 Negative binomial (gamma-Poisson) models\n\nTypically there is a lot of unexplained variation in Poisson models. Presumably this additional variation arises from unobserved influences that vary from case to case, generating variation in the true \\(\\lambda\\)’s. Ignoring this variation, or rate heterogeneity, can cause confounds just like it can for binomial models. So a very common extension of Poisson GLMs is to swap the Poisson distribution for something called the negative binomial distribution. This is really a Poisson distribution in disguise, and it is also sometimes called the gamma-Poisson distribution for this reason. It is a Poisson in disguise, because it is a mixture of different Poisson distributions. This is the Poisson analogue of the Student-t model, which is a mixture of different normal distributions. We’ll work with mixtures in the next chapter. (p. 357, emphasis in the original)\n\n\n\n11.2.3 Example: Exposure and the offset\n\nFor the last Poisson example, we’ll look at a case where the exposure varies across observations. When the length of observation, area of sampling, or intensity of sampling varies, the counts we observe also naturally vary. Since a Poisson distribution assumes that the rate of events is constant in time (or space), it’s easy to handle this. All we need to do, as explained above, is to add the logarithm of the exposure to the linear model. The term we add is typically called an offset. (p. 357, emphasis in the original)\n\nHere we simulate our data.\n\nset.seed(11)\n\nnum_days &lt;- 30\ny        &lt;- rpois(num_days, lambda = 1.5)\n\nnum_weeks &lt;- 4\ny_new     &lt;- rpois(num_weeks, lambda = 0.5 * 7)\n\nNow tidy the data and add log_days.\n\nd &lt;- tibble(y = c(y, y_new), \n            days = rep(c(1, 7), times = c(num_days, num_weeks)),  # this is the exposure\n            monastery = rep(0:1, times = c(num_days, num_weeks))) |&gt;\n  mutate(log_days = log(days))\n\nd\n\n# A tibble: 34 × 4\n       y  days monastery log_days\n   &lt;int&gt; &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;\n 1     1     1         0        0\n 2     0     1         0        0\n 3     1     1         0        0\n 4     0     1         0        0\n 5     0     1         0        0\n 6     4     1         0        0\n 7     0     1         0        0\n 8     1     1         0        0\n 9     3     1         0        0\n10     0     1         0        0\n# ℹ 24 more rows\n\n\nWithin the context of the Poisson likelihood, we can decompose \\(\\lambda\\) into two parts, \\(\\mu\\) (mean) and \\(\\tau\\) (exposure), like this:\n\\[\ny_i \\sim \\operatorname{Poisson}(\\lambda_i) \\\\\n\\log \\lambda_i = \\log \\frac{\\mu_i}{\\tau_i} = \\log \\mu_i - \\log \\tau_i.\n\\]\nTherefore, you can rewrite the equation if the exposure (\\(\\tau\\)) varies in your data and you still want to model the mean (\\(\\mu\\)). Using the model we’re about to fit as an example, here’s what that might look like:\n\\[\\begin{align*}\ny_i & \\sim \\operatorname{Poisson}(\\mu_i) \\\\\n\\log \\mu_i & = \\color{#a4692f}{\\log \\tau_i} + \\alpha + \\beta \\text{monastery}_i \\\\\n\\alpha     & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\beta      & \\sim \\operatorname{Normal}(0, 1),\n\\end{align*}\\]\nwhere the offset \\(\\log \\tau_i\\) does not get a prior. In this context, its value is added directly to the right side of the formula. With the brms package, you use the offset() function in the formula syntax. You just insert a pre-processed variable like log_days or the log of a variable, such as log(days). Fit the model.\n\nb11.12 &lt;- brm(\n  data = d, \n  family = poisson,\n  y ~ 1 + offset(log_days) + monastery,\n  prior = c(prior(normal(0, 1), class = Intercept),\n            prior(normal(0, 1), class = b)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  file = \"fits/b11.12\")\n\nAs we look at the model summary, keep in mind that the parameters are on the per-one-unit-of-time scale. Since we simulated the data based on summary information from two units of time–one day and seven days–, this means the parameters are in the scale of \\(\\log (\\lambda)\\) per one day.\n\nprint(b11.12)\n\n Family: poisson \n  Links: mu = log \nFormula: y ~ 1 + offset(log_days) + monastery \n   Data: d (Number of observations: 34) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.01      0.18    -0.39     0.34 1.00     2300     1994\nmonastery    -0.88      0.33    -1.56    -0.27 1.00     2327     2561\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe model summary helps clarify that when you use offset(), brm() fixes the value. Thus there is no parameter estimate for the offset(). It’s a fixed part of the model not unlike the \\(\\nu\\) parameter of the Student-\\(t\\) distribution gets fixed to infinity when you use the Gaussian likelihood.\nTo get the posterior distributions for average daily outputs for the old and new monasteries, respectively, we’ll use use the formulas\n\\[\\begin{align*}\n\\lambda_\\text{old} & = \\exp (\\alpha) \\;\\;\\; \\text{and} \\\\\n\\lambda_\\text{new} & = \\exp (\\alpha + \\beta_\\text{monastery}).\n\\end{align*}\\]\nFollowing those transformations, we’ll summarize the \\(\\lambda\\) distributions with medians and 89% HDIs with help from the tidybayes::mean_hdi() function.\n\nposterior_samples(b11.12) |&gt;\n  mutate(lambda_old = exp(b_Intercept),\n         lambda_new = exp(b_Intercept + b_monastery)) |&gt;\n  pivot_longer(contains(\"lambda\")) |&gt; \n  mutate(name = factor(name, levels = c(\"lambda_old\", \"lambda_new\"))) |&gt;\n  group_by(name) |&gt;\n  mean_hdi(value, .width = 0.89) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 2 × 7\n  name       value .lower .upper .width .point .interval\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 lambda_old  1.01   0.72   1.29   0.89 mean   hdi      \n2 lambda_new  0.42   0.25   0.6    0.89 mean   hdi      \n\n\nBecause we don’t know what seed McElreath used to simulate his data, our simulated data differed a little from his and, as a consequence, our results differ a little, too.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>God Spiked the Integers</span>"
    ]
  },
  {
    "objectID": "11.html#multinomial-and-categorical-models",
    "href": "11.html#multinomial-and-categorical-models",
    "title": "11  God Spiked the Integers",
    "section": "11.3 Multinomial and categorical models",
    "text": "11.3 Multinomial and categorical models\n\nWhen more than two types of unordered events are possible, and the probability of each type of event is constant across trials, then the maximum entropy distribution is the multinomial distribution. [We] already met the multinomial, implicitly, in Chapter 10 when we tossed pebbles into buckets as an introduction to maximum entropy. The binomial is really a special case of this distribution. And so its distribution formula resembles the binomial, just extrapolated out to three or more types of events. If there are \\(K\\) types of events with probabilities \\(p_1, \\dots, p_K\\), then the probability of observing \\(y_1, \\dots, y_K\\) events of each type out of n total trials is:\n\\[\\Pr (y_1, \\dots, y_K \\mid n, p_1, \\dots, p_K) = \\frac{n!}{\\prod_i y_i!} \\prod_{i = 1}^K p_i^{y_i}\\]\nThe fraction with \\(n!\\) on top just expresses the number of different orderings that give the same counts \\(y_1, \\dots, y_K\\). It’s the famous multiplicity from the previous chapter….\nThe conventional and natural link in this context is the multinomial logit, also known as the softmax function. This link function takes a vector of scores, one for each of \\(K\\) event types, and computes the probability of a particular type of event \\(k\\) as\n\\[\\Pr (k \\mid s_1, s_2, \\dots, s_K) = \\frac{\\exp (s_k)}{\\sum_{i = 1}^K \\exp (s_i)}\\] (p. 359, emphasis in the original)\n\nMcElreath then went on to explain how multinomial logistic regression models are among the more difficult of the GLMs to master. He wasn’t kidding. To get a grasp on these, we’ll cover them in a little more detail than he did in the text. Before we begin, I’d like to give a big shout out to Adam Bear, whose initial comment on a GitHub issue turned into a friendly and productive email collaboration on what, exactly, is going on with this section. Hopefully we got it.\n\n11.3.1 Predictors matched to outcomes\nTo begin, let’s simulate the data just like McElreath did in the R code 11.55 block.\n\nlibrary(rethinking)\n\n# Simulate career choices among 500 individuals\nn      &lt;- 500           # Number of individuals\nincome &lt;- c(1, 2, 5)    # Expected income of each career\nscore  &lt;- 0.5 * income  # Scores for each career, based on income\n\n# Next line converts scores to probabilities\np &lt;- softmax(score[1], score[2], score[3])\n\n# Now simulate choice\n# Outcome career holds event type values, not counts\ncareer &lt;- rep(NA, n)  # Empty vector of choices for each individual\n\n# Sample chosen career for each individual\nset.seed(34302)\nfor(i in 1:n) career[i] &lt;- sample(1:3, size = 1, prob = p)\n\nBefore moving on, it might be useful to examine what we just did. With the three lines below the “# simulate career choices among 500 individuals” comment, we defined the formulas for three scores. Those were\n\\[\\begin{align*}\ns_1 & = 0.5 \\times \\text{income}_1 \\\\\ns_2 & = 0.5 \\times \\text{income}_2 \\\\\ns_3 & = 0.5 \\times \\text{income}_3,\n\\end{align*}\\]\nwhere \\(\\text{income}_1 = 1\\), \\(\\text{income}_2 = 2\\), and \\(\\text{income}_3 = 5\\). What’s a little odd about this setup and conceptually important to get is that although \\(\\text{income}_i\\) varies across the three levels of \\(s\\), the \\(\\text{income}_i\\) value is constant within each level of \\(s\\). E.g., \\(\\text{income}_1\\) is not a variable within the context of \\(s_1\\). Therefore, we could also write the above as\n\\[\\begin{align*}\ns_1 & = 0.5 \\cdot 1 = 0.5 \\\\\ns_2 & = 0.5 \\cdot 2 = 1.0 \\\\\ns_3 & = 0.5 \\cdot 5 = 2.5.\n\\end{align*}\\]\nLet’s confirm.\n\nprint(score)\n\n[1] 0.5 1.0 2.5\n\n\nWe then converted those score values to probabilities with the softmax() function. This will become important when we set up the model code. For now, here’s what the data look like.\n\n# Put them in a tibble\nd &lt;- tibble(career = career) |&gt; \n  mutate(career_income = ifelse(career == 3, 5, career))\n\n# Plot \nd |&gt;\n  ggplot(aes(x = career)) +\n  geom_bar(linewidth = 0, fill = wes_palette(\"Moonrise2\")[2])\n\n\n\n\n\n\n\n\nOur career variable is composed of three categories, 1:3, with each category more likely than the one before. Here’s a breakdown of the counts, percentages, and probabilities of each category.\n\nd |&gt; \n  count(career) |&gt; \n  mutate(percent     = (100 * n / sum(n)),\n         probability =        n / sum(n))\n\n# A tibble: 3 × 4\n  career     n percent probability\n   &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1      1    48     9.6       0.096\n2      2    79    15.8       0.158\n3      3   373    74.6       0.746\n\n\nTo further build an appreciation for how we simulated data with these proportions and how the process links in with the formulas, above, we’ll retrace the first few simulation steps within a tidyverse-centric workflow. Recall how in those first few steps we defined values for income, score, and p. Here they are again in a tibble.\n\ntibble(income = c(1, 2, 5)) |&gt; \n  mutate(score = 0.5 * income) |&gt; \n  mutate(p = exp(score) / sum(exp(score)))\n\n# A tibble: 3 × 3\n  income score      p\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1      1   0.5 0.0996\n2      2   1   0.164 \n3      5   2.5 0.736 \n\n\nNotice how the values in the p column match up well with the probability values from the output from the block just above. Our simulation successfully produces data corresponding to the data-generating values. Woot! Also note how the code we just used to compute those p values, p = exp(score) / sum(exp(score)), corresponds nicely with the formula from above,\n\\[\\Pr (k \\mid s_1, s_2, \\dots, s_K) = \\frac{\\exp (s_k)}{\\sum_{i = 1}^K \\exp (s_i)}.\\]\nWhat still might seem mysterious is what those \\(s\\) values in the equation are. In the simulation and in the prose, McElreath called them scores. Another way to think about them is as weights. The thing to get is that their exact values aren’t important so much as their difference one from another. You’ll note that score for income == 2 was 0.5 larger than that of income == 1. The same was true for income == 3 and income == 2. So if we add an arbitrary constant to each of those score values, like 11, we’ll get the same p values.\n\ntibble(income        = c(1, 2, 5), \n       some_constant = 11) |&gt; \n  mutate(score = (0.5 * income) + some_constant) |&gt; \n  mutate(p = exp(score) / sum(exp(score)))\n\n# A tibble: 3 × 4\n  income some_constant score      p\n   &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1      1            11  11.5 0.0996\n2      2            11  12   0.164 \n3      5            11  13.5 0.736 \n\n\nNow keeping that in mind, recall how McElreath said that though we have \\(K\\) categories, \\(K = 3\\) in this case, we only estimate \\(K - 1\\) linear models. “In a multinomial (or categorical) GLM, you need \\(K - 1\\) linear models for \\(K\\) types of events. One of the outcome values is chosen as a ‘pivot’ and the others are modeled relative to it.” (p. 360). You could also think of the pivot category as the reference category.\nBefore we practice fitting multinomial models with brms, it’ll be helpful if we first follow along with the text and fit the model directly in Stan. We will be working directly with Stan very infrequently in this ebook. If you’re interested in learning more about modeling directly with Stan, you might check out the Stan user’s guide (Stan Development Team, 2022c), the Stan reference manual (Stan Development Team, 2022b), and the Stan functions reference (Stan Development Team, 2022a). Fit the model with Stan.\n\n# Define the model\ncode_m11.13 &lt;- \"\ndata{\n  int N; // Number of individuals\n  int K; // Number of possible careers \n  // int career[N]; // This is the old syntax, which is now discouraged\n  array[N] int career; // Outcome; This is the new recommended syntax\n  vector[K] career_income;\n}\nparameters{\n  vector[K - 1] a; // Intercepts\n  real&lt;lower=0&gt; b; // Association of income with choice\n}\nmodel{\n  vector[K] p;\n  vector[K] s;\n  a ~ normal(0, 1);\n  b ~ normal(0, 0.5);\n  s[1] = a[1] + b * career_income[1]; \n  s[2] = a[2] + b * career_income[2]; \n  s[3] = 0; // Pivot\n  p = softmax(s);\n  career ~ categorical(p);\n} \n\"\n\n# Wrangle the data\ndat_list &lt;- list(\n  N = n, \n  K = 3, \n  career = career, \n  career_income = income)\n\n# Fit the model\nm11.13 &lt;- stan(\n  data = dat_list,\n  model_code = code_m11.13,\n  chains = 4)\n\nCheck the summary.\n\nprecis(m11.13, depth = 2) |&gt; round(digits = 2)\n\n      mean   sd  5.5% 94.5% rhat ess_bulk\na[1] -2.13 0.18 -2.41 -1.86    1   570.16\na[2] -1.78 0.23 -2.21 -1.47    1   519.63\nb     0.13 0.11  0.01  0.33    1   402.22\n\n\nOne of the primary reasons we went through this exercise is to show that McElreath’s R code 11.56 and 11.57 do not return the results he reported on page 361. The plot thickens when we attempt the counterfactual simulation on page 362, as reported in R code 11.58.\n\npost &lt;- extract.samples(m11.13)\n\n# Set up logit scores\ns1      &lt;- with(post, a[, 1] + b * income[1])\ns2_orig &lt;- with(post, a[, 2] + b * income[2])\ns2_new  &lt;- with(post, a[, 2] + b * income[2] * 2)\n\n# Compute probabilities for original and counterfactual \np_orig &lt;- sapply(1:length(post$b), function(i)\n  softmax(c(s1[i], s2_orig[i], 0)))\n\np_new &lt;- sapply(1:length(post$b), function(i)\n  softmax(c(s1[i], s2_new[i], 0)))\n\n# Summarize\np_diff &lt;- p_new[2, ] - p_orig[2, ] \nprecis(p_diff)\n\n             mean         sd        5.5%     94.5%      histogram\np_diff 0.03905125 0.03731175 0.003182132 0.1097809 ▇▅▂▂▁▁▁▁▁▁▁▁▁▁\n\n\nEven though we used the same code, our counterfactual simulation doesn’t match up with the results McElreath reported in the text, either. Keep this all in mind as we switch to brms. But before we move on to brms, check this out.\n\ndata.frame(s1 = score[3] + s1, \n           s2 = score[3] + s2_orig, \n           s3 = score[3] + 0) |&gt; \n  pivot_longer(everything()) |&gt; \n  group_by(name) |&gt; \n  mean_qi(value) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 3 × 7\n  name  value .lower .upper .width .point .interval\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 s1     0.5    0.21   0.79   0.95 mean   qi       \n2 s2     0.97   0.74   1.19   0.95 mean   qi       \n3 s3     2.5    2.5    2.5    0.95 mean   qi       \n\n\nIn his Stan code (R code 11.56), you’ll see McElreath chose the third category to be his pivot and that he used zero as a constant value. As it turns out, it is common practice to set the score value for the reference category to zero. It’s also a common practice to use the first event type as the reference category. Importantly, in his (2022c) vignette, Parameterization of response distributions in brms, Bürkner clarified the brms default is to use the first response category as the reference and set it to a zero as well. However, we can control this behavior with the refcat argument. In the examples to follow, we’ll follow McElreath and use the third event type as the reference category by setting refcat = 3.\nIn addition to the discrepancies with the code and results in the text, one of the things I don’t care for in this section is how fast McElreath covered the material. Our approach will be to slow down a little and start off by fitting a intercepts-only model before adding the covariate. Before we fit the model, we might take a quick look at the prior structure with brms::get_prior().\n\nget_prior(data = d, \n          family = categorical(link = logit, refcat = 3),\n          career ~ 1)\n\n                prior     class coef group resp dpar nlpar lb ub tag  source\n student_t(3, 0, 2.5) Intercept                  mu1                 default\n student_t(3, 0, 2.5) Intercept                  mu2                 default\n\n\nWe have two “intercepts”, which are differentiated in the dpar column. We’ll talk more about what these are in just a bit; don’t worry. I show this here because as of brms 2.12.0, “specifying global priors for regression coefficients in categorical models is deprecated.” The upshot is even if we want to use the same prior for both, we need to use the dpar argument for each. With that in mind, here’s our multinomial model in brms. Do note the specification family = categorical(link = logit, refcat = 3). The categorical part is what instructs brms to use the multinomial likelihood and the refcat = 3 part will allow us to use the third event type as the pivot.\n\nb11.13io &lt;- brm(\n  data = d, \n  family = categorical(link = logit, refcat = 3),\n  career ~ 1,\n  prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),\n            prior(normal(0, 1), class = Intercept, dpar = mu2)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  file = \"fits/b11.13io\")\n\nThe summary can be difficult to interpret.\n\nprint(b11.13io)\n\n Family: categorical \n  Links: mu1 = logit; mu2 = logit \nFormula: career ~ 1 \n   Data: d (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmu1_Intercept    -2.01      0.16    -2.33    -1.71 1.00     3390     2673\nmu2_Intercept    -1.53      0.12    -1.77    -1.29 1.00     2964     2810\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nbrms::brm() referred to the \\(K\\) categories as mu1, mu2, and mu3. Since career == 3 is the reference category, the score for which was set to zero, there is no parameter for mu3_Intercept. That’s a zero. Now notice how mu1_Intercept is about -2 and mu2_Intercept is about -1.5. If we double back to the income and score values we played with at the beginning of this section, you’ll notice that the score for the reference category was 2.5. Here’s what happens if we rescale the three scores such that the score value for the reference category is 0.\n\ntibble(income = c(1, 2, 5)) |&gt; \n  mutate(score = 0.5 * income) |&gt; \n  mutate(rescaled_score = score - 2.5)\n\n# A tibble: 3 × 3\n  income score rescaled_score\n   &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n1      1   0.5           -2  \n2      2   1             -1.5\n3      5   2.5            0  \n\n\nNow notice how the rescaled_score values for the first two rows correspond nicely to mu1_Intercept and mu2_Intercept from our model. What I hope this clarifies is that our statistical model returned the scores. But recall these are not quite probabilities. Why? Because the weights are all relative to one another. The easiest way to get what we want, the probabilities for the three categories, is with brms::fitted(). Since this model has no predictors, only intercepts, we won’t specify any newdata. In such a case, fitted() will return fitted values for each case in the data. Going slow, let’s take a look at the structure of the output.\n\nf &lt;- fitted(b11.13io)\n\nstr(f)\n\n num [1:500, 1:4, 1:3] 0.0998 0.0998 0.0998 0.0998 0.0998 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ : NULL\n  ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"\n  ..$ : chr [1:3] \"P(Y = 1)\" \"P(Y = 2)\" \"P(Y = 3)\"\n\n\nJust as expected, we have 500 rows–one for each case in the original data. We have four summary columns, the typical Estimate, Est.Error, Q2.5, and Q97.5. We also have third dimension composed of three levels, P(Y = 1), P(Y = 2), and P(Y = 3). Those index which of the three career categories each probability summary is for. Since the results are identical for each row, we’ll simplify the output by only keeping the first row.\n\nf[1, , ] |&gt; \n  round(digits = 2)\n\n          P(Y = 1) P(Y = 2) P(Y = 3)\nEstimate      0.10     0.16     0.74\nEst.Error     0.01     0.02     0.02\nQ2.5          0.07     0.13     0.70\nQ97.5         0.13     0.20     0.78\n\n\nIf we take the transpose of that, it will put the results in the format to which we’ve become accustomed.\n\nf[1, , ] |&gt; \n  round(digits = 2) |&gt; \n  t()\n\n         Estimate Est.Error Q2.5 Q97.5\nP(Y = 1)     0.10      0.01 0.07  0.13\nP(Y = 2)     0.16      0.02 0.13  0.20\nP(Y = 3)     0.74      0.02 0.70  0.78\n\n\nNow compare those summaries with the empirically-derived percent and probability values we computed earlier.\n\ntibble(income = c(1, 2, 5)) |&gt; \n  mutate(score = 0.5 * income) |&gt; \n  mutate(p = exp(score) / sum(exp(score)))\n\n# A tibble: 3 × 3\n  income score      p\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1      1   0.5 0.0996\n2      2   1   0.164 \n3      5   2.5 0.736 \n\n\nNow here’s how to make use of the formula from the last mutate() line, \\(\\frac{\\exp (s_k)}{\\sum_{i = 1}^K \\exp (s_i)}\\), to compute the marginal probabilities from b11.13io by hand.\n\nas_draws_df(b11.13io) |&gt; \n  mutate(b_mu3_Intercept = 0) |&gt; \n  mutate(p1 = exp(b_mu1_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)),\n         p2 = exp(b_mu2_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)),\n         p3 = exp(b_mu3_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept))) |&gt; \n  pivot_longer(p1:p3) |&gt; \n  group_by(name) |&gt; \n  mean_qi(value) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 3 × 7\n  name  value .lower .upper .width .point .interval\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 p1     0.1    0.07   0.13   0.95 mean   qi       \n2 p2     0.16   0.13   0.2    0.95 mean   qi       \n3 p3     0.74   0.7    0.78   0.95 mean   qi       \n\n\nHurray; we did it! Not only did we fit a simple multinomial model with brms, we actually made sense of the parameters by connecting them to the original data-generating values. We’re almost ready to contend with the model McElreath fit with stan(). But before we do, it’ll be helpful to show alternative ways to fit these models. We used conventional style syntax when we fit b11.13io. There are at least two alternative ways to fit the model:\n\n# Verbose syntax\nb11.13io_verbose &lt;- brm(\n  data = d, \n  family = categorical(link = logit, refcat = 3),\n  bf(career ~ 1,\n     mu1 ~ 1,\n     mu2 ~ 1),\n  prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),\n            prior(normal(0, 1), class = Intercept, dpar = mu2)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  file = \"fits/b11.13io_verbose\")\n\n# Nonlinear syntax\nb11.13io_nonlinear &lt;- brm(\n  data = d, \n  family = categorical(link = logit, refcat = 3),\n  bf(career ~ 1,\n     nlf(mu1 ~ a1),\n     nlf(mu2 ~ a2),\n     a1 + a2 ~ 1),\n  prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n            prior(normal(0, 1), class = b, nlpar = a2)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  file = \"fits/b11.13io_nonlinear\")\n\nFor the sake of space, I’m not going to show the results for those two models. If you fit them yourself, you’ll see the results for b11.13io and b11.13io_verbose are exactly the same and b11.13io_nonlinear differs from them only within simulation variation. I point this out because it’s the nonlinear approach that will allow us to fit a model like McElreath’s m11.13. My hope is the syntax we used in the b11.13io_verbose model will help clarify what’s going on with the non-linear syntax. When we fit multinomial models with brms, the terse conventional formula syntax might not make clear how there are actually \\(K - 1\\) formulas. The more verbose syntax of our b11.13io_verbose model shows how we can specify those models directly. In our case, that was with those mu1 ~ 1, mu2 ~ 1 lines. Had we used the brms default and used the first level of career as the pivot, those lines would have instead been mu2 ~ 1, mu3 ~ 1. So anyway, when we switch to the non-linear syntax, we explicitly model mu1 and mu2 and, as is typical of the non-linear syntax, we name our parameters. You can see another comparison of these three ways of fitting a multinomial model at the Nonlinear syntax with a multinomial model? thread on the Stan Forums.\nNow it’s time to focus on the brms version of McElreath’s m11.13. To my eye, McElreath’s model has two odd features. First, though he has two intercepts, he only has one \\(\\beta\\) parameter. Second, if you look at McElreath’s parameters block, you’ll see that he restricted his \\(\\beta\\) parameter to be zero and above (real&lt;lower=0&gt; b;).\nWith the brms non-linear syntax, we can fit the model with one \\(\\beta\\) parameter or allow the one \\(\\beta\\) parameter to differ for mu1 and mu2. As to setting a lower bound to the b parameter[s], we can do that with the lb argument within the prior() function. If we fit our version of m11.13 by systemically varying these two features, we’ll end up with the four versions listed in the table below.\n\ncrossing(b  = factor(c(\"b1 & b2\", \"b\"), levels = c(\"b1 & b2\", \"b\")),\n         lb = factor(c(\"NA\", 0), levels = c(\"NA\", 0))) |&gt; \n  mutate(fit = str_c(\"b11.13\", letters[1:n()])) |&gt; \n  select(fit, everything()) |&gt; \n  \n  flextable() |&gt; \n  width(width = 1.25)\n\nfitblbb11.13ab1 & b2NAb11.13bb1 & b20b11.13cbNAb11.13db0\n\n\nFit b11.13a through b11.13d, the four variants on the model.\n\nb11.13a &lt;- brm(\n  data = d, \n  family = categorical(link = logit, refcat = 3),\n  bf(career ~ 1,\n     nlf(mu1 ~ a1 + b1 * 1),\n     nlf(mu2 ~ a2 + b2 * 2),\n     a1 + a2 + b1 + b2 ~ 1),\n  prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n            prior(normal(0, 1), class = b, nlpar = a2),\n            prior(normal(0, 0.5), class = b, nlpar = b1),\n            prior(normal(0, 0.5), class = b, nlpar = b2)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  file = \"fits/b11.13a\")\n\nb11.13b &lt;- brm(\n  data = d, \n  family = categorical(link = logit, refcat = 3),\n  bf(career ~ 1,\n     nlf(mu1 ~ a1 + b1 * 1),\n     nlf(mu2 ~ a2 + b2 * 2),\n     a1 + a2 + b1 + b2 ~ 1),\n  prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n            prior(normal(0, 1), class = b, nlpar = a2),\n            prior(normal(0, 0.5), class = b, nlpar = b1, lb = 0),\n            prior(normal(0, 0.5), class = b, nlpar = b2, lb = 0)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/b11.13b\")\n\nb11.13c &lt;- brm(\n  data = d, \n  family = categorical(link = logit, refcat = 3),\n  bf(career ~ 1,\n     nlf(mu1 ~ a1 + b * 1),\n     nlf(mu2 ~ a2 + b * 2),\n     a1 + a2 + b ~ 1),\n  prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n            prior(normal(0, 1), class = b, nlpar = a2),\n            prior(normal(0, 0.5), class = b, nlpar = b)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  file = \"fits/b11.13c\")\n\nb11.13d &lt;- brm(\n  data = d, \n  family = categorical(link = logit, refcat = 3),\n  bf(career ~ 1,\n     nlf(mu1 ~ a1 + b * 1),\n     nlf(mu2 ~ a2 + b * 2),\n     a1 + a2 + b ~ 1),\n  prior = c(prior(normal(0, 1), class = b, nlpar = a1),\n            prior(normal(0, 1), class = b, nlpar = a2),\n            prior(normal(0, 0.5), class = b, nlpar = b, lb = 0)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  control = list(adapt_delta = 0.99),\n  file = \"fits/b11.13d\")\n\nI’m not going to exhaustively show the print() output for each. If you check, you’ll see they all fit reasonably well. Here we’ll look at their parameter summaries in bulk with a coefficient plot.\n\ntibble(fit = str_c(\"b11.13\", letters[1:4])) |&gt; \n  mutate(fixef = purrr::map(.x = fit, .f = function(x) {\n    get(x) |&gt; \n      fixef() |&gt;\n      data.frame() |&gt; \n      rownames_to_column(\"parameter\")\n  })) |&gt; \n  unnest(fixef) |&gt; \n  mutate(parameter = str_remove(parameter, \"_Intercept\"),\n         fit       = factor(fit, levels = str_c(\"b11.13\", letters[4:1]))) |&gt; \n  \n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = fit)) +\n  geom_vline(xintercept = 0, color = wes_palette(\"Moonrise2\")[3]) +\n  geom_pointrange(color = wes_palette(\"Moonrise2\")[4], size = 1/5) +\n  ylab(NULL) +\n  facet_wrap(~ parameter, nrow = 1) +\n  theme(axis.ticks.y = element_blank(),\n        panel.background = element_rect(fill = alpha(\"white\", 1/8), linewidth = 0))\n\n\n\n\n\n\n\n\nThe results differed across models. None of them match up with the results McElreath reported in the text. However, the parameters from b11.13d are very close to those from our m11.13.\n\nprecis(m11.13, depth = 2)\n\n           mean        sd        5.5%      94.5%     rhat ess_bulk\na[1] -2.1296821 0.1767568 -2.41132445 -1.8584381 1.003320 570.1559\na[2] -1.7759214 0.2348872 -2.20617740 -1.4736330 1.004843 519.6281\nb     0.1252703 0.1062524  0.01201407  0.3301056 1.002790 402.2191\n\nfixef(b11.13d) |&gt; round(digits = 2)\n\n             Estimate Est.Error  Q2.5 Q97.5\na1_Intercept    -2.15      0.19 -2.59 -1.81\na2_Intercept    -1.81      0.28 -2.52 -1.41\nb_Intercept      0.15      0.13  0.01  0.49\n\n\nIt might be instructive to compare b11.13a through b11.13d with the PSIS-LOO.\n\nb11.13a &lt;- add_criterion(b11.13a, criterion = \"loo\")\nb11.13b &lt;- add_criterion(b11.13b, criterion = \"loo\")\nb11.13c &lt;- add_criterion(b11.13c, criterion = \"loo\")\nb11.13d &lt;- add_criterion(b11.13d, criterion = \"loo\")\n\nloo_compare(b11.13a, b11.13b, b11.13c, b11.13d, criterion = \"loo\") |&gt; \n  print(simplify = F)\n\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nb11.13d    0.0       0.0  -369.5     16.9         1.9    0.1    739.1   33.8  \nb11.13b    0.0       0.1  -369.5     16.8         1.9    0.1    739.1   33.7  \nb11.13a   -0.1       0.2  -369.6     17.0         2.0    0.1    739.2   34.1  \nb11.13c   -0.1       0.2  -369.7     17.1         2.1    0.1    739.3   34.2  \n\nmodel_weights(b11.13a, b11.13b, b11.13c, b11.13d, weights = \"loo\") |&gt; \n  round(digits = 2)\n\nb11.13a b11.13b b11.13c b11.13d \n   0.25    0.26    0.23    0.26 \n\n\nTwo things pop out, here. First, all models are essentially equivalent in terms of LOO estimates and LOO weights. Second, the effective number of parameters (\\(p_\\text{LOO}\\)) is about 2 for each model. At first glance, this might be surprising given that b11.13a and b11.13b both have 4 parameters and b11.13c and b11.13d both have three parameters. But recall that none of these models contain predictor variables from the data. All those \\(\\beta\\) parameters, whether they’re held equal or allowed to vary across \\(s_1\\) and \\(s_2\\), are just constants. In the absence of actual income values that vary within the data, those \\(\\beta\\) parameters are kinda like extra intercepts. For context, go back and review our multicollinear legs from Section 6.1.1 or our double intercepts from Section 9.5.4.\nNow see what happens when we compare these four models with our intercepts-only model, b11.13io.\n\nb11.13io &lt;- add_criterion(b11.13io, criterion = \"loo\")\n\nloo_compare(b11.13io, b11.13a, b11.13b, b11.13c, b11.13d, criterion = \"loo\") |&gt; \n  print(simplify = F)\n\n         elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nb11.13d     0.0       0.0  -369.5     16.9         1.9    0.1    739.1   33.8  \nb11.13b     0.0       0.1  -369.5     16.8         1.9    0.1    739.1   33.7  \nb11.13a    -0.1       0.2  -369.6     17.0         2.0    0.1    739.2   34.1  \nb11.13io   -0.1       0.0  -369.6     16.9         2.0    0.1    739.3   33.8  \nb11.13c    -0.1       0.2  -369.7     17.1         2.1    0.1    739.3   34.2  \n\nmodel_weights(b11.13io, b11.13a, b11.13b, b11.13c, b11.13d, weights = \"loo\") |&gt; \n  round(digits = 2)\n\nb11.13io  b11.13a  b11.13b  b11.13c  b11.13d \n    0.19     0.20     0.21     0.19     0.21 \n\n\nThey’re all the same. Each model effectively has 2 parameters. Though it doesn’t do much by way of cross-validation, McElreath’s extra \\(\\beta\\) parameter will let us perform a counterfactual simulation. Here is a brms/tidyverse workflow to make a counterfactual simulation for two levels of income based on our b11.13d, the brms model most closely corresponding to our rethinking-based m11.13.\n\nas_draws_df(b11.13d) |&gt; \n  transmute(s1      = b_a1_Intercept + b_b_Intercept * income[1],\n            s2_orig = b_a2_Intercept + b_b_Intercept * income[2],\n            s2_new  = b_a2_Intercept + b_b_Intercept * income[2] * 2) |&gt; \n  mutate(p_orig = purrr::map2_dbl(.x = s1, .y = s2_orig, .f = \\(x, y) softmax(x, y, 0)[2]),\n         p_new  = purrr::map2_dbl(.x = s1, .y = s2_new, .f = \\(x, y) softmax(x, y, 0)[2])) |&gt; \n  mutate(p_diff = p_new - p_orig) |&gt; \n  mean_qi(p_diff) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 1 × 6\n  p_diff .lower .upper .width .point .interval\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1   0.05      0   0.17   0.95 mean   qi       \n\n\nNow let’s build.\n\n\n11.3.2 Predictors matched to observations\n\nNow consider an example in which each observed outcome has unique predictor values. Suppose you are still modeling career choice. But now you want to estimate the association between each person’s family income and which career they choose. So the predictor variable must have the same value in each linear model, for each row in the data. But now there is a unique parameter multiplying it in each linear model. This provides an estimate of the impact of family income on choice, for each type of career. (p. 362)\n\n\nn &lt;- 500\nset.seed(11)\n\n# Simulate family incomes for each individual\nfamily_income &lt;- runif(n)\n\n# Assign a unique coefficient for each type of event\nb &lt;- c(-2, 0, 2)\ncareer &lt;- rep(NA, n)  # Empty vector of choices for each individual\nfor (i in 1:n) {\n    score     &lt;- 0.5 * (1:3) + b * family_income[i]\n    p         &lt;- softmax(score[1], score[2], score[3])\n    career[i] &lt;- sample(1:3, size = 1, prob = p)\n}\n\nIn effect, we now have three data-generating equations:\n\\[\\begin{align*}\ns_1 & = 0.5 + -2 \\cdot \\text{family\\_income}_i \\\\\ns_2 & = 1.0 +  0 \\cdot \\text{family\\_income}_i \\\\\ns_3 & = 1.5 +  2 \\cdot \\text{family\\_income}_i,\n\\end{align*}\\]\nwhere, because family_income is an actual variable that can take on unique values for each row in the data, we can call the first term in each equation the \\(\\alpha\\) parameter and the second term in each equation the \\(\\beta\\) parameter AND those \\(\\beta\\) parameters will be more than odd double intercepts.\nWe might examine what the family_income distributions look like across the three levels of career. We’ll do it in two plots and combine them with the patchwork syntax. The first will be overlapping densities. For the second, we’ll display the proportions of career across a discretized version of family_income in a stacked area plot.\n\n# Put the data in a tibble\nd &lt;- tibble(career = career) |&gt; \n  mutate(family_income = family_income)\n\np1 &lt;- d |&gt; \n  mutate(career = as.factor(career)) |&gt; \n  \n  ggplot(aes(x = family_income, fill = career)) +\n  geom_density(linewidth = 0, alpha = 3/4) +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[c(4, 2, 1)]) +\n  theme(legend.position = \"none\")\n  \np2 &lt;- d |&gt; \n  mutate(career = as.factor(career)) |&gt;\n  \n  mutate(fi = santoku::chop_width(family_income, width = 0.1, start = 0, labels = 1:10)) |&gt; \n  count(fi, career) |&gt; \n  group_by(fi) |&gt; \n  mutate(proportion = n / sum(n)) |&gt; \n  mutate(f = as.double(fi)) |&gt; \n  \n  ggplot(aes(x = (f - 1) / 9, y = proportion, fill = career)) +\n  geom_area() +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[c(4, 2, 1)]) +\n  xlab(\"family_income, descritized\")\n\np1 + p2\n\n\n\n\n\n\n\n\nSince Mcelreath’s simulation code in McElreath’s R code 11.59 did not contain a set.seed() line, it won’t be possible to exactly reproduce his results. Happily, though, it appears that this time the results he reported in the text to cohere reasonably well with I ran the code on my computer. They weren’t identical, but there were much closer that for m11.13 from the last section. Since things are working more smoothly, here, I’m going to jump directly to brms code.\n\nb11.14 &lt;- brm(\n  data = d, \n  family = categorical(link = logit, refcat = 3),\n  bf(career ~ 1,\n     nlf(mu1 ~ a1 + b1 * family_income),\n     nlf(mu2 ~ a2 + b2 * family_income),\n     a1 + a2 + b1 + b2 ~ 1),\n  prior = c(prior(normal(0, 1.5), class = b, nlpar = a1),\n            prior(normal(0, 1.5), class = b, nlpar = a2),\n            prior(normal(0, 1), class = b, nlpar = b1),\n            prior(normal(0, 1), class = b, nlpar = b2)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 11,\n  file = \"fits/b11.14\")\n\n\nprint(b11.14)\n\n Family: categorical \n  Links: mu1 = logit; mu2 = logit \nFormula: career ~ 1 \n         mu1 ~ a1 + b1 * family_income\n         mu2 ~ a2 + b2 * family_income\n         a1 ~ 1\n         a2 ~ 1\n         b1 ~ 1\n         b2 ~ 1\n   Data: d (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na1_Intercept    -1.28      0.26    -1.80    -0.79 1.00     2112     2261\na2_Intercept    -1.01      0.22    -1.44    -0.57 1.00     1878     2082\nb1_Intercept    -2.51      0.56    -3.60    -1.45 1.00     2263     2274\nb2_Intercept    -1.22      0.42    -2.08    -0.42 1.00     1907     2099\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCheck the PSIS-LOO.\n\nb11.14 &lt;- add_criterion(b11.14, criterion = \"loo\")\n\nloo(b11.14)\n\n\nComputed from 4000 by 500 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -330.3 17.0\np_loo         3.2  0.3\nlooic       660.6 33.9\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.2]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nNow that we actually have predictor variables with which we might estimate conventional \\(\\beta\\) parameters, we finally have more than 2 effective parameters (\\(p_\\text{LOO}\\)).\n“Again, computing implied predictions is the safest way to interpret these models. They do a great job of classifying discrete, unordered events. But the parameters are on a scale that is very hard to interpret” (p. 325). Like before, we’ll do that with fitted(). Now we have a predictor, this time we will use the newdata argument.\n\nnd &lt;- tibble(family_income = seq(from = 0, to = 1, length.out = 60))\n\nf &lt;- fitted(b11.14, newdata = nd)\n\nFirst we’ll plot the fitted probabilities for each career level across the full range of family_income values.\n\n# Wrangle\nrbind(f[, , 1],\n      f[, , 2],\n      f[, , 3]) |&gt; \n  data.frame() |&gt; \n  bind_cols(expand_grid(career = 1:3, nd)) |&gt; \n  mutate(career = str_c(\"career: \", career)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = family_income, y = Estimate,\n             ymin = Q2.5, ymax = Q97.5,\n             fill = career, color = career)) +\n  geom_ribbon(alpha = 2/3, linewidth = 0) +\n  geom_line(linewidth = 3/4) +\n  scale_x_continuous(breaks = 0:2 / 2, labels = c(0, 0.5, 1)) +\n  scale_y_continuous(\"probability\", limits = c(0, 1),\n                     breaks = 0:3 / 3, labels = c(0, 0.33, 0.67, 1)) +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[c(4, 2, 1)]) +\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[c(4, 2, 1)]) +\n  facet_wrap(~ career) +\n  theme(axis.text.y = element_text(hjust = 0),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nIf we’re willing to summarize those fitted lines by their posterior means, we could also make a model-implied version of the stacked area plot from above.\n\n# Annotation\ntext &lt;- tibble(\n  family_income = c(0.45, 0.3, 0.15),\n  proportion    = c(0.65, 0.8, 0.95),\n  label         = str_c(\"career: \", 3:1),\n  color         = c(\"a\", \"a\", \"b\"))\n\n# Wrangle\nrbind(f[, , 1],\n      f[, , 2],\n      f[, , 3]) |&gt; \n  data.frame() |&gt; \n  bind_cols(expand_grid(career = 1:3, nd)) |&gt; \n  group_by(family_income) |&gt; \n  mutate(proportion = Estimate / sum(Estimate),\n         career     = factor(career)) |&gt; \n  \n  # Plot!\n  ggplot(aes(x = family_income, y = proportion)) +\n  geom_area(aes(fill = career)) +\n  geom_text(data = text,\n            aes(label = label, color = color),\n            family = \"Times\", size = 4.25) +\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[4:3]) +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[c(4, 2, 1)]) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFor more practice fitting multinomial models with brms, check out Chapter 22 of my (2026b) translation of Kruschke’s (2015) text.\n\n11.3.2.1 Multinomial in disguise as Poisson\nHere we fit a multinomial likelihood by refactoring it to a series of Poissons. Let’s retrieve the Berkeley data.\n\ndata(UCBadmit, package = \"rethinking\")\nd &lt;- UCBadmit\nrm(UCBadmit)\n\nFit the models.\n\n# Binomial model of overall admission probability\nb11.binom &lt;- brm(\n  data = d, \n  family = binomial,\n  admit | trials(applications) ~ 1,\n  prior(normal(0, 1.5), class = Intercept),\n  iter = 2000, warmup = 1000, cores = 3, chains = 3,\n  seed = 11,\n  file = \"fits/b11.binom\")\n\n# Poisson model of overall admission rate and rejection rate\nb11.pois &lt;- brm(\n  data = d |&gt;\n    mutate(rej = reject),  # 'reject' is a reserved word\n  family = poisson,\n  mvbind(admit, rej) ~ 1,\n  prior = c(prior(normal(0, 1.5), class = Intercept, resp = admit),\n            prior(normal(0, 1.5), class = Intercept, resp = rej)),\n  iter = 2000, warmup = 1000, cores = 3, chains = 3,\n  seed = 11,\n  file = \"fits/b11.pois\")\n\nNote, the mvbind() syntax made b11.pois a multivariate Poisson model. Starting with version 2.0.0, brms supports a variety of multivariate models, which you might learn more about with Bürkner’s (2022b) vignette, Estimating multivariate models with brms. Anyway, here are the implications of b11.pois.\n\n# Extract the samples\npost &lt;- as_draws_df(b11.pois)\n# Wrangle\npost |&gt;\n  mutate(admit  = exp(b_admit_Intercept), \n         reject = exp(b_rej_Intercept)) |&gt; \n  pivot_longer(admit:reject) |&gt; \n  \n  # Plot\n  ggplot(aes(x = value, y = name, fill = name)) +\n  stat_halfeye(point_interval = median_qi, .width = 0.95,\n               color = wes_palette(\"Moonrise2\")[4]) +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[1:2]) +\n  labs(x = \"# applications\",\n       y = NULL,\n       title = \" Mean admit/reject rates across departments\") +\n  theme(axis.ticks.y = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nWe might compare the model summaries.\n\nsummary(b11.binom)$fixed\n\n           Estimate  Est.Error   l-95% CI   u-95% CI     Rhat Bulk_ESS Tail_ESS\nIntercept -0.457266 0.03031323 -0.5157915 -0.3989393 1.001358 1213.963 1589.472\n\nsummary(b11.pois)$fixed\n\n                Estimate  Est.Error l-95% CI u-95% CI     Rhat Bulk_ESS Tail_ESS\nadmit_Intercept 4.983434 0.02496957 4.934124 5.032686 1.001233 2218.621 1887.390\nrej_Intercept   5.441496 0.01899651 5.404759 5.478654 1.000388 2213.556 1606.328\n\n\nHere’s the posterior mean for the probability of admission, based on b11.binom.\n\nfixef(b11.binom)[, \"Estimate\"] |&gt;\n  plogis()\n\n[1] 0.3876346\n\n\nHappily, we get the same value within simulation error from model b11.pois.\n\nk &lt;- fixef(b11.pois) |&gt;\n  as.numeric()\n\nexp(k[1]) / (exp(k[1]) + exp(k[2]))\n\n[1] 0.3874457\n\n\nThe formula for what we just did in code is\n\\[p_\\text{admit} = \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2} = \\frac{\\exp (\\alpha_1)}{\\exp (\\alpha_1) + \\exp (\\alpha_2)}.\\]\nTo get a better appreciation on how well the two model types converge on the same solution, we might plot the full poster for admissions probability from each.\n\n# wrangle\nbind_cols(\n  as_draws_df(b11.pois) |&gt; \n    transmute(`the Poisson`  = exp(b_admit_Intercept) / (exp(b_admit_Intercept) + exp(b_rej_Intercept))),\n  as_draws_df(b11.binom) |&gt; \n    transmute(`the binomial` = plogis(b_Intercept))\n  ) |&gt; \n  pivot_longer(everything()) |&gt; \n  \n  # plot\n  ggplot(aes(x = value, y = name, fill = name)) +\n  stat_halfeye(point_interval = median_qi, .width = c(0.95, 0.5),\n               color = wes_palette(\"Moonrise2\")[4]) +\n  scale_fill_manual(values = c(wes_palette(\"Moonrise2\")[2:1])) +\n  labs(x = \"admissions probability\",\n       y = NULL,\n       title = \"Two models, same marginal posterior\") +\n  coord_cartesian(ylim = c(1.5, 2.33)) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        legend.position = \"none\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>God Spiked the Integers</span>"
    ]
  },
  {
    "objectID": "11.html#summary",
    "href": "11.html#summary",
    "title": "11  God Spiked the Integers",
    "section": "11.4 Summary",
    "text": "11.4 Summary\n\nThis chapter described some of the most common generalized linear models, those used to model counts. It is important to never convert counts to proportions before analysis, because doing so destroys information about sample size. A fundamental difficulty with these models is that parameters are on a different scale, typically log-odds (for binomial) or log-rate (for Poisson), than the outcome variable they describe. Therefore computing implied predictions is even more important than before. (p. 365)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>God Spiked the Integers</span>"
    ]
  },
  {
    "objectID": "11.html#bonus-survival-analysis",
    "href": "11.html#bonus-survival-analysis",
    "title": "11  God Spiked the Integers",
    "section": "11.5 Bonus: Survival analysis",
    "text": "11.5 Bonus: Survival analysis\nIn the middle of the thirteenth lecture of his 2019 lecture series, McElreath briefly covered continuous-time survival analysis. Sadly, the problem didn’t make it into the text. Here we’ll slip it in as a bonus section. To fully understand this section, do listen to this section of the lecture. It’s only about ten minutes.\n\n\n\n\n\n\n\n\n\nNow let’s load the AustinCats data.\n\ndata(AustinCats, package = \"rethinking\")\nd &lt;- AustinCats\nrm(AustinCats)\n\nglimpse(d)\n\nRows: 22,356\nColumns: 9\n$ id            &lt;fct&gt; A730601, A679549, A683656, A709749, A733551, A756485, A732960, A664571, A727402, A749579, A729190, A754695…\n$ days_to_event &lt;int&gt; 1, 25, 4, 41, 9, 4, 4, 5, 24, 2, 34, 27, 3, 151, 106, 4, 55, 1, 4, 30, 18, 5, 34, 1, 10, 2, 9, 7, 21, 5, 6…\n$ date_out      &lt;fct&gt; 07/08/2016 09:00:00 AM, 06/16/2014 01:54:00 PM, 07/17/2014 04:57:00 PM, 09/22/2015 12:49:00 PM, 09/01/2016…\n$ out_event     &lt;fct&gt; Transfer, Transfer, Adoption, Transfer, Transfer, Adoption, Adoption, Adoption, Adoption, Transfer, Adopti…\n$ date_in       &lt;fct&gt; 07/07/2016 12:11:00 PM, 05/22/2014 03:43:00 PM, 07/13/2014 01:20:00 PM, 08/12/2015 06:29:00 PM, 08/23/2016…\n$ in_event      &lt;fct&gt; Stray, Stray, Stray, Stray, Stray, Stray, Stray, Owner Surrender, Stray, Stray, Stray, Stray, Stray, Owner…\n$ breed         &lt;fct&gt; Domestic Shorthair Mix, Domestic Shorthair Mix, Snowshoe Mix, Domestic Shorthair Mix, Domestic Shorthair M…\n$ color         &lt;fct&gt; Blue Tabby, Black/White, Lynx Point, Calico, Brown Tabby/White, Blue Tabby, Calico, Torbie, Brown Tabby, B…\n$ intake_age    &lt;int&gt; 7, 1, 2, 12, 1, 1, 2, 24, 1, 3, 4, 12, 1, 7, 0, 12, 1, 12, 1, 24, 24, 3, 1, 24, 1, 24, 12, 4, 3, 3, 1, 4, …\n\n\nAt the moment, it doesn’t look like the rethinking package contains documentation about the AustinCats. Based on McElreath’s lecture, he downloaded them from the website of an animal shelter in Austin, TX. We have data on 22,356 cats on whether they were adopted and how long it took. The cats came in a variety of colors. Here are the first ten.\n\nd |&gt;\n  count(color) |&gt;\n  slice(1:10)\n\n                     color    n\n1                   Agouti    3\n2       Agouti/Brown Tabby    1\n3             Agouti/White    1\n4                  Apricot    1\n5                    Black 2965\n6              Black Smoke   83\n7  Black Smoke/Black Tiger    1\n8        Black Smoke/White   21\n9              Black Tabby  119\n10       Black Tabby/White   43\n\n\nMcElreath wondered whether it took longer for black cats to be adopted. If you look at the color categories, above, you’ll see the people doing the data entry were creative with their descriptions. To keep things simple, we’ll just be comparing cats for whom color == \"Black\" to all the others.\n\nd &lt;- d |&gt; \n  mutate(black = ifelse(color == \"Black\", \"black\", \"other\"))\n\nd |&gt; \n  count(black) |&gt; \n  mutate(percent = 100 * n / sum(n)) |&gt; \n  mutate(label = str_c(round(percent, digits = 1), \"%\")) |&gt; \n  \n  ggplot(aes(y = black)) +\n  geom_col(aes(x = n, fill = black)) +\n  geom_text(aes(x = n - 250, label = label),\n            color = wes_palette(\"Moonrise2\")[3], family = \"Times\", hjust = 1) +\n  scale_x_continuous(expression(italic(n)), breaks = c(0, count(d, black) |&gt; pull(n))) +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[c(4, 1)], breaks = NULL) +\n  labs(y = NULL,\n       title = \"Cat color\") +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nAnother variable we need to consider is the out_event.\n\nd |&gt; \n  count(out_event)\n\n   out_event     n\n1   Adoption 11351\n2   Censored   549\n3       Died   369\n4   Disposal     9\n5 Euthanasia   636\n6    Missing    28\n7   Transfer  9414\n\n\nHappily, most of the cats had Adoption as their out_event. For our purposes, all of the other options are the same as if they were Censored. We’ll make a new variable to indicate that.\n\nd &lt;- d |&gt; \n  mutate(adopted  = ifelse(out_event == \"Adoption\", 1, 0),\n         censored = ifelse(out_event != \"Adoption\", 1, 0))\n\nglimpse(d)\n\nRows: 22,356\nColumns: 12\n$ id            &lt;fct&gt; A730601, A679549, A683656, A709749, A733551, A756485, A732960, A664571, A727402, A749579, A729190, A754695…\n$ days_to_event &lt;int&gt; 1, 25, 4, 41, 9, 4, 4, 5, 24, 2, 34, 27, 3, 151, 106, 4, 55, 1, 4, 30, 18, 5, 34, 1, 10, 2, 9, 7, 21, 5, 6…\n$ date_out      &lt;fct&gt; 07/08/2016 09:00:00 AM, 06/16/2014 01:54:00 PM, 07/17/2014 04:57:00 PM, 09/22/2015 12:49:00 PM, 09/01/2016…\n$ out_event     &lt;fct&gt; Transfer, Transfer, Adoption, Transfer, Transfer, Adoption, Adoption, Adoption, Adoption, Transfer, Adopti…\n$ date_in       &lt;fct&gt; 07/07/2016 12:11:00 PM, 05/22/2014 03:43:00 PM, 07/13/2014 01:20:00 PM, 08/12/2015 06:29:00 PM, 08/23/2016…\n$ in_event      &lt;fct&gt; Stray, Stray, Stray, Stray, Stray, Stray, Stray, Owner Surrender, Stray, Stray, Stray, Stray, Stray, Owner…\n$ breed         &lt;fct&gt; Domestic Shorthair Mix, Domestic Shorthair Mix, Snowshoe Mix, Domestic Shorthair Mix, Domestic Shorthair M…\n$ color         &lt;fct&gt; Blue Tabby, Black/White, Lynx Point, Calico, Brown Tabby/White, Blue Tabby, Calico, Torbie, Brown Tabby, B…\n$ intake_age    &lt;int&gt; 7, 1, 2, 12, 1, 1, 2, 24, 1, 3, 4, 12, 1, 7, 0, 12, 1, 12, 1, 24, 24, 3, 1, 24, 1, 24, 12, 4, 3, 3, 1, 4, …\n$ black         &lt;chr&gt; \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\"…\n$ adopted       &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1…\n$ censored      &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0…\n\n\nHere’s what the distribution of days_to_event looks like, when grouped by our new censored variable.\n\nd |&gt; \n  mutate(censored = factor(censored)) |&gt; \n  filter(days_to_event &lt; 300) |&gt; \n  \n  ggplot(aes(x = days_to_event, y = censored)) +\n  # Let's just mark off the 50% intervals\n  stat_halfeye(.width = 0.5, fill = wes_palette(\"Moonrise2\")[2], height = 4) +\n  scale_y_discrete(NULL, labels = c(\"censored == 0\", \"censored == 1\")) +\n  coord_cartesian(ylim = c(1.5, 5.1)) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nDo note there is a very long right tail that we’ve cut off for the sake of the plot. Anyway, the point of this plot is to show that the distribution for our primary variable, days_to_event, looks very different conditional on whether the data were censored. As McElreath covered in the lecture, we definitely don’t want to loose that information by excluding the censored cases from the analysis.\nMcElreath fit his survival model using the exponential likelihood. We briefly met the exponential likelihood in Chapter 10. As McElreath wrote:\n\nIt is a fundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space. If the probability of an event is constant in time or across space, then the distribution of events tends towards exponential. The exponential distribution has maximum entropy among all non-negative continuous distributions with the same average displacement. (p. 314)\n\nIf we let \\(y\\) be a non-negative continuous variable, the probability density function for the exponential distribution is\n\\[f(y) = \\lambda e^{-\\lambda y},\\]\nwhere \\(\\lambda\\) is called the rate. The mean of the exponential distribution is the inverse of the rate\n\\[\\operatorname{E}[y] = \\frac{1}{\\lambda}.\\]\nImportantly, brms paramaterizes exponential models in terms of \\(\\operatorname{E}[y]\\). By default, it uses the log link. The is the same set-up McElreath used for rethinking in his lecture. To get a sense of how this all works, we can write our continuous-time survival model as\n\\[\\begin{align*}\n\\text{days\\_to\\_event}_i \\mid \\text{censored}_i = 0 & \\sim \\operatorname{Exponential}(\\lambda_i) \\\\\n\\text{days\\_to\\_event}_i \\mid \\text{censored}_i = 1 & \\sim \\operatorname{Exponential-CCDF}(\\lambda_i) \\\\\n\\lambda_i & = 1 / \\mu_i \\\\\n\\log \\mu_i & = \\alpha_{\\text{black}[i]} \\\\\n\\alpha & \\sim \\operatorname{Normal}(0, 1).\n\\end{align*}\\]\nThis is the same model McElreath discussed in the lecture. We’ve just renamed a couple variables. When you fit a continuous-time survival analysis with brm(), you’ll want to tell the software about how the data have been censored with help from the cens() function. For many of the models in this chapter, we used the trials() function to include the \\(n_i\\) information into our binomial models. Both trials() and cens() are members of a class of functions designed to provide supplemental information about our criterion variables to brm(). The cens() function lets us add in information about censoring. In his lecture, McElreath mentioned there can be different kinds of censoring. brms can handle variables with left, right, or interval censoring. In the case of our days_to_event data, some of the values have been right censored, which is typical in survival models. We will feed this information into the model with the formula code days_to_event | cens(censored), where censored is the name of the variable in our data that indexes the censoring. The cens() function has been set up to expect our data to be coded as either\n\n'left', 'none', 'right', and/or 'interval'; or\n-1, 0, 1, and/or 2.\n\nSince we coded our censored variable as censored = 1 and not censored = 0, we have followed the second coding scheme. For more on the topic, see the Additional response information subsection within the brmsformula section of brms reference manual (Bürkner, 2022a). Here’s how to fit our survival model with brms.\n\nb11.15 &lt;- brm(\n  data = d,\n  family = exponential,\n  days_to_event | cens(censored) ~ 0 + black,\n  prior(normal(0, 1), class = b),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 11,\n  file = \"fits/b11.15\")\n\nCheck the summary.\n\nprint(b11.15)\n\n Family: exponential \n  Links: mu = log \nFormula: days_to_event | cens(censored) ~ 0 + black \n   Data: d (Number of observations: 22356) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nblackblack     4.05      0.03     4.00     4.10 1.00     3423     2359\nblackother     3.88      0.01     3.86     3.90 1.00     4170     2660\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSince we modeled \\(\\log \\mu_i\\), we need to transform our \\(\\alpha\\) parameters back into the \\(\\lambda\\) metric using the formula\n\\[\\begin{align*}\n\\log \\mu             & = \\alpha_\\text{black}, && \\text{and} \\\\\n\\lambda              & = 1 / \\mu,             && \\text{therefore} \\\\\n\\lambda_\\text{black} & = 1 / \\exp(\\alpha_\\text{black}).\n\\end{align*}\\]\nHere are the posterior means for our two \\(\\lambda\\)’s.\n\n1 / exp(fixef(b11.15)[, -2])\n\n             Estimate       Q2.5      Q97.5\nblackblack 0.01741031 0.01834655 0.01652384\nblackother 0.02065134 0.02106793 0.02025255\n\n\nIt still might not be clear what any of this all means. To get a better sense, let’s make our version of one of the plots from McElreath’s lecture.\n\n# Annotation\ntext &lt;- tibble(\n  color = c(\"black\", \"other\"),\n  days  = c(40, 34),\n  p     = c(0.55, 0.45),\n  label = c(\"black cats\", \"other cats\"),\n  hjust = c(0, 1))\n\n# Wrangle\nf &lt;- fixef(b11.15) |&gt; \n  data.frame() |&gt; \n  rownames_to_column() |&gt; \n  mutate(color = str_remove(rowname, \"black\")) |&gt; \n  expand_grid(days = 0:100) |&gt; \n  mutate(m  = 1 - pexp(days, rate = 1 / exp(Estimate)),\n         ll = 1 - pexp(days, rate = 1 / exp(Q2.5)),\n         ul = 1 - pexp(days, rate = 1 / exp(Q97.5)))\n  \n# Plot\nf |&gt; \n  ggplot(aes(x = days)) +\n  geom_hline(yintercept = 0.5, color = wes_palette(\"Moonrise2\")[2], linetype = 3) +\n  geom_ribbon(aes(ymin = ll, ymax = ul, fill = color),\n              alpha = 1/2) +\n  geom_line(aes(y = m, color = color)) +\n  geom_text(data = text,\n            aes(y = p, label = label, hjust = hjust, color = color),\n            family = \"Times\") +\n  scale_y_continuous(\"proportion remaining\", breaks = c(0, 0.5, 1), limits = 0:1) +\n  scale_fill_manual(values = wes_palette(\"Moonrise2\")[c(4, 1)], breaks = NULL) +\n  scale_color_manual(values = wes_palette(\"Moonrise2\")[c(4, 1)], breaks = NULL) +\n  xlab(\"days to adoption\")\n\n\n\n\n\n\n\n\nMcElreath’s hypothesis is correct: Black cats are adopted a lower rates than cats of other colors. Another way to explore this model is to ask: About how many days would it take for half of the cats of a given color to be adopted? We can do this with help from the qexp() function. For example:\n\nqexp(p = 0.5, rate = 1 / exp(fixef(b11.15)[1, 1]))\n\n[1] 39.81246\n\n\nBut that’s just using one of the posterior means. Here’s that information using the full posterior distributions for our two levels of black.\n\n# Wrangle\npost &lt;- as_draws_df(b11.15) |&gt; \n  pivot_longer(starts_with(\"b_\")) |&gt; \n  mutate(color = str_remove(name, \"b_black\"),\n         days  = qexp(p = 0.5, rate = 1 / exp(value))) \n\n# Axis breaks\nmedians &lt;- group_by(post, color) |&gt; \n  summarise(med = median(days)) |&gt; \n  pull(med) |&gt; \n  round(digits = 1)\n\n# Plot\npost |&gt; \n  ggplot(aes(x = days, y = color)) +\n  stat_halfeye(.width = 0.95, fill = wes_palette(\"Moonrise2\")[2], height = 4) +\n  scale_x_continuous(\"days untill 50% are adopted\", breaks = c(30, medians, 45), \n                     labels = c(\"30\", medians, \"45\"), limits = c(30, 45)) +\n  ylab(NULL) +\n  coord_cartesian(ylim = c(1.5, 5.1)) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nThe model suggests it takes about six days longer for the half of the black cats to be adopted.\n\n11.5.1 Survival summary\nWe’ve really just scratched the surface on survival models. In addition to those which use the exponential likelihood, brms supports a variety of survival models. Some of the more popular likelihoods are the log-Normal, the gamma, and the Weibull. For details, see the Time-to-event models section of Bürkner’s (2022c) vignette, Parameterization of response distributions in brms. Starting with the release of version 2.13.5, brms now supports the Cox proportional hazards model via family = cox. If you’re tricky with your coding, you can also fit discrete-time survival models with the binomial likelihood (see here). For some examples of discrete and continuous-time survival models, you might check out my (2026a) ebook translation of Singer and Willett’s (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence, the later chapters of which provide an exhaustive introduction to survival analysis.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>God Spiked the Integers</span>"
    ]
  },
  {
    "objectID": "11.html#session-info",
    "href": "11.html#session-info",
    "title": "11  God Spiked the Integers",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] rethinking_2.42      posterior_1.6.1.9000 cmdstanr_0.9.0       ggrepel_0.9.6        GGally_2.4.0         ggdag_0.2.13        \n [7] patchwork_1.3.2      tidybayes_3.0.7      ggthemes_5.1.0       wesanderson_0.3.7    brms_2.23.0          Rcpp_1.1.0          \n[13] flextable_0.9.10     lubridate_1.9.4      forcats_1.0.1        stringr_1.6.0        dplyr_1.1.4          purrr_1.2.1         \n[19] readr_2.1.5          tidyr_1.3.2          tibble_3.3.1         ggplot2_4.0.1        tidyverse_2.0.0     \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3      shape_1.4.6.1           tensorA_0.36.2.1        rstudioapi_0.17.1       jsonlite_2.0.0         \n  [6] magrittr_2.0.4          TH.data_1.1-4           estimability_1.5.1      farver_2.1.2            rmarkdown_2.30         \n [11] ragg_1.5.0              vctrs_0.7.0             memoise_2.0.1           askpass_1.2.1           htmltools_0.5.9        \n [16] dagitty_0.3-4           distributional_0.5.0    curl_7.0.0              StanHeaders_2.36.0.9000 htmlwidgets_1.6.4      \n [21] plyr_1.8.9              sandwich_3.1-1          emmeans_1.11.2-8        zoo_1.8-14              cachem_1.1.0           \n [26] uuid_1.2-1              igraph_2.2.0            lifecycle_1.0.5         pkgconfig_2.0.3         Matrix_1.7-3           \n [31] R6_2.6.1                fastmap_1.2.0           santoku_1.1.0           digest_0.6.39           ps_1.9.1               \n [36] textshaping_1.0.4       labeling_0.4.3          timechange_0.3.0        httr_1.4.7              polyclip_1.10-7        \n [41] abind_1.4-8             compiler_4.5.1          fontquiver_0.2.1        withr_3.0.2             S7_0.2.1               \n [46] backports_1.5.0         inline_0.3.21           viridis_0.6.5           ggstats_0.11.0          QuickJSR_1.8.1         \n [51] pkgbuild_1.4.8          ggforce_0.5.0           MASS_7.3-65             openssl_2.3.4           loo_2.9.0.9000         \n [56] tools_4.5.1             zip_2.3.3               quadprog_1.5-8          glue_1.8.0              nlme_3.1-168           \n [61] grid_4.5.1              checkmate_2.3.3         reshape2_1.4.5          generics_0.1.4          gtable_0.3.6           \n [66] tzdb_0.5.0              data.table_1.17.8       hms_1.1.4               tidygraph_1.3.1         xml2_1.4.0             \n [71] utf8_1.2.6              pillar_1.11.1           ggdist_3.3.3            splines_4.5.1           tweenr_2.0.3           \n [76] lattice_0.22-7          survival_3.8-3          tidyselect_1.2.1        fontLiberation_0.1.0    knitr_1.51             \n [81] fontBitstreamVera_0.1.1 arrayhelpers_1.1-0      gridExtra_2.3           V8_8.0.1                stats4_4.5.1           \n [86] xfun_0.55               graphlayouts_1.2.2      bridgesampling_1.2-1    matrixStats_1.5.0       rstan_2.36.0.9000      \n [91] stringi_1.8.7           yaml_2.3.12             boot_1.3-31             evaluate_1.0.5          codetools_0.2-20       \n [96] officer_0.7.2           ggraph_2.2.2            gdtools_0.4.4           emo_0.0.0.9000          vembedr_0.1.5          \n[101] cli_3.6.5               RcppParallel_5.1.11-1   xtable_1.8-4            systemfonts_1.3.1       processx_3.8.6         \n[106] coda_0.19-4.1           svUnit_1.0.8            rstantools_2.5.0.9000   assertthat_0.2.1        bayesplot_1.15.0.9000  \n[111] Brobdingnag_1.2-9       viridisLite_0.4.2       mvtnorm_1.3-3           scales_1.4.0            crayon_1.5.3           \n[116] rlang_1.1.7             multcomp_1.4-29",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>God Spiked the Integers</span>"
    ]
  },
  {
    "objectID": "11.html#comments",
    "href": "11.html#comments",
    "title": "11  God Spiked the Integers",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975). Sex bias in graduate admissions: Data from Berkeley. Science, 187(4175), 398–404. https://doi.org/10.1126/science.187.4175.398\n\n\nBürkner, P.-C. (2022a). brms reference manual, Version 2.18.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2022b). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html\n\n\nBürkner, P.-C. (2022c). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nKievit, R., Frankenhuis, W. E., Waldorp, L., & Borsboom, D. (2013). Simpson’s paradox in psychological science: A practical guide. Frontiers in Psychology, 4. https://doi.org/10.3389/fpsyg.2013.00513\n\n\nKline, M. A., & Boyd, R. (2010). Population size predicts technological complexity in Oceania. Proceedings of the Royal Society B: Biological Sciences, 277(1693), 2559–2564. https://doi.org/10.1098/rspb.2010.0452\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKurz, A. S. (2026a). Applied longitudinal data analysis in brms and the tidyverse (version 0.0.4). https://solomon.quarto.pub/alda/\n\n\nKurz, A. S. (2026b). Doing Bayesian data analysis in brms and the tidyverse (Version 1.3.0). https://solomon.quarto.pub/dbda2/\n\n\nKurz, A. S. (2026c). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.4.0). https://solomon.quarto.pub/sr/\n\n\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nRam, K., & Wickham, H. (2018). wesanderson: A Wes Anderson palette generator [Manual]. https://CRAN.R-project.org/package=wesanderson\n\n\nSilk, J. B., Brosnan, S. F., Vonk, J., Henrich, J., Povinelli, D. J., Richardson, A. S., Lambeth, S. P., Mascaro, J., & Schapiro, S. J. (2005). Chimpanzees are indifferent to the welfare of unrelated group members. Nature, 437(7063), 1357–1359. https://doi.org/10.1038/nature04243\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nStan Development Team. (2022a). Stan functions reference, Version 2.31. https://mc-stan.org/docs/functions-reference/\n\n\nStan Development Team. (2022b). Stan reference manual, Version 2.31. https://mc-stan.org/docs/reference-manual/index.html\n\n\nStan Development Team. (2022c). Stan user’s guide, Version 2.31. https://mc-stan.org/docs/stan-users-guide/index.html\n\n\nTufte, E. R. (2001). The visual display of quantitative information (Second Edition). Graphics Press. https://www.edwardtufte.com/tufte/books_vdqi",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>God Spiked the Integers</span>"
    ]
  },
  {
    "objectID": "12.html",
    "href": "12.html",
    "title": "12  Monsters and Mixtures",
    "section": "",
    "text": "12.1 Over-dispersed counts\nIn this chapter we’ll cope with the problem using continuous mixture models, “models in which a linear model is attached not to the observations themselves but rather to a distribution of observations” (p. 370).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Monsters and Mixtures</span>"
    ]
  },
  {
    "objectID": "12.html#sec-Over-dispersed-counts",
    "href": "12.html#sec-Over-dispersed-counts",
    "title": "12  Monsters and Mixtures",
    "section": "",
    "text": "When counts arise from a mixture of different processes, then there may be more variation–thicker tails–than a pure count model expects. This can again lead to overly excited models. When counts are more variable than a pure process, they exhibit over-dispersion. The variance of a variable is sometimes called its dispersion. For a counting process like a binomial, the variance is a function of the same parameters as the expected value. For example, the expected value of a binomial is \\(Np\\) and its variance is \\(Np(1 - p)\\). When the observed variance exceeds this amount–after conditioning on all the predictor variables–this implies that some omitted variable is producing additional dispersion in the observed counts.\nThat isn’t necessarily bad. Such a model could still produce perfectly good inferences. But ignoring over-dispersion can also lead to all of the same problems as ignoring any predictor variable. Heterogeneity in counts can be a confound, hiding effects of interest or producing spurious inferences. (p, 370, emphasis in the original)\n\n\n\n12.1.1 Beta-binomial\n\nA beta-binomial model is a mixture of binomial distributions. It assumes that each binomial count observation has its own probability of success. We estimate the distribution of probabilities of success instead of a single probability of success. Any predictor variables describe the shape of this distribution. (p, 370, emphasis in the original)\n\nUnfortunately, we need to digress. As it turns out, there are multiple ways to parameterize the beta distribution and we’ve run square into two. In the text, McElreath described the beta distribution with two parameters, an average probability \\(\\bar p\\) and a shape parameter \\(\\theta\\). In his R code 12.1, which we’ll reproduce in a bit, he demonstrated that parameterization with the rethinking::dbeta2() function. The nice thing about this parameterization is how intuitive the pbar parameter is. If you want a beta with an average of 0.2, you set pbar = 0.2. If you want the distribution to be more or less certain, make the theta argument more or less large, respectively.\nHowever, the beta density is often defined in terms of \\(\\alpha\\) and \\(\\beta\\). If you denote the data as \\(y\\), this follows the form\n\\[\\operatorname{Beta}(y \\mid \\alpha, \\beta) = \\frac{y^{\\alpha - 1} (1 - y)^{\\beta - 1}}{\\text B (\\alpha, \\beta)},\\]\nwhich you can verify in the Continuous distributions on [0, 1] section Stan functions reference (Stan Development Team, 2022). In the formula, \\(\\text B\\) stands for the Beta function, which computes a normalizing constant, which you can learn about in the Mathematical functions chapter of the Stan functions reference. If you look at the base R dbeta() function, you’ll learn it takes two parameters, shape1 and shape2. Those uncreatively-named parameters are the same \\(\\alpha\\) and \\(\\beta\\) from the density, above. They do not correspond to the pbar and theta parameters of McEreath’s rethinking::dbeta2() function.\nMcElreath had good reason for using dbeta2(). Beta’s typical \\(\\alpha\\) and \\(\\beta\\) parameters aren’t the most intuitive to use; the parameters in McElreath’s dbeta2() are much nicer. If you dive a little deeper, it turns out you can find the mean of a beta distribution in terms of \\(\\alpha\\) and \\(\\beta\\) like this:\n\\[\\mu = \\frac{\\alpha}{\\alpha + \\beta}.\\]\nWe can talk about the spread of the distribution, sometimes called \\(\\kappa\\), in terms \\(\\alpha\\) and \\(\\beta\\) like this:\n\\[\\kappa = \\alpha + \\beta.\\]\nWith \\(\\mu\\) and \\(\\kappa\\) in hand, we can even find the \\(\\textit{SD}\\) of a beta distribution with the formula\n\\[\\sigma = \\sqrt{\\mu (1 - \\mu) / (\\kappa + 1)}.\\]\nI explicate all this because McElreath’s pbar is \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) and his theta is \\(\\kappa = \\alpha + \\beta\\), which is great news because it means that we can understand what McElreath did with his beta2() function in terms of the base R dbeta() function. This also sets us up to understand the distribution of the beta parameters used in brms::brm(). To demonstrate, let’s walk through McElreath’s R code 12.1.\nBefore we get to R code 12.1 and our version of the resulting plot, we should discuss themes. In this chapter we’ll use theme settings and a color palette from the ggthemes package.\n\nlibrary(ggthemes)\n\nWe’ll take our basic theme settings from the theme_hc() function. We’ll use the Green fields color palette, which we can inspect with the canva_pal() function and a little help from scales::show_col().\n\nscales::show_col(canva_pal(\"Green fields\")(4))\n\n\n\n\n\n\n\ncanva_pal(\"Green fields\")(4)\n\n[1] \"#919636\" \"#524a3a\" \"#fffae1\" \"#5a5f37\"\n\ncanva_pal(\"Green fields\")(4)[3]\n\n[1] \"#fffae1\"\n\n\nNow we finally get to R code 12.1.\n\nlibrary(tidyverse)\n\ntheme_set(\n  theme_hc() +\n  theme(axis.ticks.y = element_blank(),\n        plot.background = element_rect(fill = \"grey92\"))\n)\n\npbar  &lt;- 0.5\ntheta &lt;- 5\n\ntibble(x = seq(from = 0, to = 1, by = 0.01)) |&gt; \n  mutate(density = rethinking::dbeta2(x, prob = pbar, theta = theta)) |&gt; \n  \n  ggplot(aes(x = x, y = density)) +\n  geom_area(fill = canva_pal(\"Green fields\")(4)[1]) +\n  scale_x_continuous(\"probability space\", breaks = c(0, 0.5, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(expression(The~beta~distribution),\n          subtitle = expression(\"Defined in terms of \"*mu*\" (i.e., pbar) and \"*kappa*\" (i.e., theta)\"))\n\n\n\n\n\n\n\n\nIn his (2015) text, Doing Bayesian data analysis, Kruschke provided code for a convenience function that takes pbar and theta as inputs and returns the corresponding \\(\\alpha\\) and \\(\\beta\\) values. Here’s the function:\n\nbetaABfromMeanKappa &lt;- function(mean, kappa) {\n  if (mean &lt;= 0 | mean &gt;= 1) stop(\"must have 0 &lt; mean &lt; 1\")\n  if (kappa &lt;= 0) stop(\"kappa must be &gt; 0\")\n  a &lt;- mean * kappa\n  b &lt;- (1.0 - mean) * kappa\n  return(list(a = a, b = b))\n}\n\nNow we can use Kruschke’s betaABfromMeanKappa() to find the \\(\\alpha\\) and \\(\\beta\\) values corresponding to pbar and theta.\n\nbetaABfromMeanKappa(mean = pbar, kappa = theta)\n\n$a\n[1] 2.5\n\n$b\n[1] 2.5\n\n\nAnd finally, we can double check that all of this works. Here’s the same distribution but defined in terms of \\(\\alpha\\) and \\(\\beta\\).\n\ntibble(x = seq(from = 0, to = 1, by = 0.01)) |&gt; \n  mutate(density = dbeta(x, shape1 = 2.5, shape2 = 2.5)) |&gt; \n  \n  ggplot(aes(x = x, y = density)) +\n  geom_area(fill = canva_pal(\"Green fields\")(4)[4]) +\n  scale_x_continuous(\"probability space\", breaks = c(0, 0.5, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(expression(The~beta~distribution),\n          subtitle = expression(\"This time defined in terms of \"*alpha*\" and \"*beta))\n\n\n\n\n\n\n\n\nMcElreath encouraged us to “explore different values for pbar and theta” (p. 371). Here’s a grid of plots with pbar = c(0.25, 0.5, 0.75) and theta = c(5, 10, 15).\n\n# Data\ncrossing(pbar  = c(0.25, 0.5, 0.75),\n         theta = c(5, 15, 30)) |&gt; \n  expand_grid(x = seq(from = 0, to = 1, length.out = 100)) |&gt; \n  mutate(density = rethinking::dbeta2(x, prob = pbar, theta = theta),\n         mu      = str_c(\"mu == \", pbar |&gt; str_remove(\"0\")),\n         kappa   = factor(str_c(\"kappa == \", theta), \n                          levels = c(\"kappa == 30\", \"kappa == 15\", \"kappa == 5\"))) |&gt; \n  \n  # Plot\n  ggplot(aes(x = x, y = density)) +\n  geom_area(fill = canva_pal(\"Green fields\")(4)[4]) +\n  scale_x_continuous(\"probability space\", breaks = c(0, 0.5, 1), \n                     labels = c(0, 0.5, 1)) +\n  scale_y_continuous(NULL, labels = NULL) +\n  facet_grid(kappa ~ mu, labeller = label_parsed) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nIf you’d like to see how to make a similar plot in terms of \\(\\alpha\\) and \\(\\beta\\), see Chapter 6 of my (2026) ebook wherein I translated Kruschke’s text into tidyverse and brms code.\nBut remember, we’re not fitting a beta model. We’re using the beta-binomial. “We’re going to bind our linear model to \\(\\bar p\\), so that changes in predictor variables change the central tendency of the distribution” (p. 371). The statistical model we’ll be fitting follows the form\n\\[\\begin{align*}\n\\text{admit}_i & \\sim \\operatorname{BetaBinomial}(n_i, \\bar p_i, \\phi)\\\\\n\\operatorname{logit}(\\bar p_i) & = \\alpha_{\\text{gid}[i]} \\\\\n\\alpha_j                       & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\phi                           & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nHere the size, \\(n\\), is defined in the applications column in the data we’ll load in just a moment. In case you’re confused, yes, our statistical model is not quite the same as the one McElreath presented on page 371 in the text. If you look closely, we dropped all mention of \\(\\theta\\) and jumped directly to \\(\\phi\\). Instead of implementing McElreath’s \\(\\theta = \\phi + 2\\) trick, we’re going to set the lower bound for \\(\\phi\\) directly. Which brings us to the next issue:\nThe beta-binomial has historically not been officially supported by brms (see GitHub issue #144).1 However, brms versions 2.2.0 and above allow users to define custom distributions. To get all the details, you might check out Bürkner’s (2022b) vignette, Define custom response distributions with brms. Happily, Bürkner even used the beta-binomial distribution as the exemplar in the vignette.\n1 Okay, it’s actually a little more complicated. brms has officially supported family beta_binomial since version 2.17.0. For the official news release, see here. However, I still think it’s worth while to show how to make a custom likelihood with brms. But be warned that I consider this advanced. For example, I would be extremely hesitant to make a custom likelihood for my own research work. I’m just not responsible enough for that kind of power.Before we get carried away, let’s load the data and brms.\n\ndata(UCBadmit, package = \"rethinking\") \nd &lt;- UCBadmit |&gt; \n  mutate(gid = ifelse(applicant.gender == \"male\", \"1\", \"2\"))\nrm(UCBadmit)\n\nlibrary(brms)\n\nI’m not going to go into great detail explaining the ins and outs of making custom distributions for brm(). You’ve got Bürkner’s vignette for that. For our purposes, we need a few preparatory steps. First, we need to use the custom_family() function to define the name and parameters of the beta-binomial distribution for use in brm(). Second, we have to define some functions for Stan which are not defined in Stan itself. We’ll save them as stan_funs. Third, we’ll make a stanvar() statement which will allow us to pass our stan_funs to brm().\n\nbeta_binomial2 &lt;- custom_family(\n  \"beta_binomial2\", dpars = c(\"mu\", \"phi\"),\n  links = c(\"logit\", \"log\"),\n  lb = c(0, 2), ub = c(1, NA),\n  type = \"int\", vars = \"vint1[n]\"\n)\n\nstan_funs &lt;- \"\n  real beta_binomial2_lpmf(int y, real mu, real phi, int T) {\n    return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi);\n  }\n  int beta_binomial2_rng(real mu, real phi, int T) {\n    return beta_binomial_rng(T, mu * phi, (1 - mu) * phi);\n  }\n\"\n\nstanvars &lt;- stanvar(scode = stan_funs, block = \"functions\")\n\nDid you notice the lb = c(0, 2) portion of the code defining beta_binomial2()? In Bürkner’s vignette, he set the lower bound of phi to zero. Since McElreath wanted the lower bound for \\(\\phi\\) to be 2, we just set that as the default in the likelihood. We should clarify two more points:\nFirst, what McElreath referred to as the shape parameter, \\(\\theta\\), Bürkner called the precision parameter, \\(\\phi\\). In our exposition, above, we followed Kruschke’s convention and called it \\(\\kappa\\). These are all the same thing: \\(\\theta\\), \\(\\phi\\), and \\(\\kappa\\) are all the same thing. Perhaps less confusingly, what McElreath called the pbar parameter, \\(\\bar p\\), Bürkner simply refers to as \\(\\mu\\).\nSecond, we’ve become accustomed to using the y | trials() ~ ... syntax when defining our formula arguments for binomial models. Here we are replacing trials() with vint(). From Bürkner’s Define custom response distributions with brms vignette, we read:\n\nTo provide information about the number of trials (an integer variable), we are going to use the addition argument vint(), which can only be used in custom families. Similarly, if we needed to include additional vectors of real data, we would use vreal(). Actually, for this particular example, we could more elegantly apply the addition argument trials() instead of vint() as in the basic binomial model. However, since the present vignette is meant to give a general overview of the topic, we will go with the more general method.\nWe now have all components together to fit our custom beta-binomial model:\n\n\nb12.1 &lt;- brm(\n  data = d, \n  family = beta_binomial2,  # Here's our custom likelihood\n  admit | vint(applications) ~ 0 + gid,\n  prior = c(prior(normal(0, 1.5), class = b),\n            prior(exponential(1), class = phi)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  stanvars = stanvars,  # Note our `stanvars`\n  seed = 12,\n  file = \"fits/b12.01\")\n\nSuccess, our results look a lot like those in the text!\n\nprint(b12.1)\n\n Family: beta_binomial2 \n  Links: mu = logit \nFormula: admit | vint(applications) ~ 0 + gid \n   Data: d (Number of observations: 12) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ngid1    -0.45      0.40    -1.22     0.34 1.00     3100     2211\ngid2    -0.33      0.41    -1.15     0.47 1.00     3346     2229\n\nFurther Distributional Parameters:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi     3.07      0.83     2.05     4.99 1.00     2728     1886\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nJust remember that, perhaps confusingly, what McElreath’s output called theta, our brms output is calling phi. I know; this section is a lot. Keep your chin up! Here’s what the corresponding as_draws_df() data object looks like.\n\npost &lt;- as_draws_df(b12.1)\nhead(post)\n\n# A draws_df: 6 iterations, 1 chains, and 5 variables\n  b_gid1 b_gid2 phi lprior lp__\n1  -0.41  -0.49 4.4   -5.1  -71\n2  -0.37  -0.20 3.4   -4.1  -70\n3  -0.57  -0.41 3.4   -4.2  -70\n4  -0.56  -0.31 3.8   -4.5  -70\n5  -0.70  -0.83 3.8   -4.7  -71\n6  -0.81  -1.13 3.5   -4.6  -73\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nNow we can compute and summarize a contrast between the two genders, what McElreath called da.\n\nlibrary(tidybayes)\n\npost |&gt; \n  transmute(da = b_gid1 - b_gid2) |&gt; \n  mean_qi(.width = 0.89) |&gt; \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 1 × 6\n      da .lower .upper .width .point .interval\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 -0.118  -1.04  0.789   0.89 mean   qi       \n\n\nMuch like in the text, the difference between genders on admission rates is near zero with wide uncertainty intervals spanning in either direction.\nTo stay within the tidyverse while making the many thin lines in Figure 12.1.a, we’re going to need to do a bit of data processing. First, we’ll want a variable to index the rows in post (i.e., to index the posterior draws). And we’ll want to convert the b_gid2 to the \\(\\bar p\\) metric with the plogis() function. Then we’ll use slice_sample() to randomly draw a subset of the posterior draws. With the expand_grid() function, we’ll insert a dense sequence of x values ranging between 0 and 1–the parameter space of beta distribution. Finally, we’ll use pmap_dbl() to compute the density values for the rethinking::dbeta2 distribution corresponding to the unique combination of x, p_bar, and phi values in each row.\n\nset.seed(12)\n\nlines &lt;- post |&gt; \n  mutate(p_bar = plogis(b_gid2)) |&gt; \n  slice_sample(n = 100) |&gt; \n  select(.draw, p_bar, phi) |&gt; \n  expand_grid(x = seq(from = 0, to = 1, by = 0.005)) |&gt; \n  mutate(density = pmap_dbl(.l = list(x, p_bar, phi), .f = rethinking::dbeta2))\n\nstr(lines)\n\ntibble [20,100 × 5] (S3: tbl_df/tbl/data.frame)\n $ .draw  : int [1:20100] 450 450 450 450 450 450 450 450 450 450 ...\n $ p_bar  : num [1:20100] 0.524 0.524 0.524 0.524 0.524 ...\n $ phi    : num [1:20100] 2.65 2.65 2.65 2.65 2.65 ...\n $ x      : num [1:20100] 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 ...\n $ density: num [1:20100] 0 0.237 0.31 0.362 0.404 ...\n\n\nAll that was just for the thin lines. To make the thicker line for the posterior mean, we’ll get tricky with stat_function().\n\nlines |&gt; \n  ggplot(aes(x = x, y = density)) + \n  stat_function(fun = rethinking::dbeta2,\n                args = list(prob = mean(plogis(post |&gt; pull(b_gid2))),\n                            theta = mean(post |&gt; pull(phi))),\n                color = canva_pal(\"Green fields\")(4)[4], linewidth = 1.5) +\n  geom_line(aes(group = .draw),\n            alpha = 0.2, color = canva_pal(\"Green fields\")(4)[4]) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 3)) +\n  labs(x = \"probability admit\",\n       subtitle = \"distribution of female admission rates\")\n\n\n\n\n\n\n\n\nThere are other ways to do this. For ideas, check out my blog post, Make rotated Gaussians, Kruschke style.\nBefore we can do our variant of Figure 12.1.b, we’ll need to define a few more custom functions. The log_lik_beta_binomial2() and posterior_predict_beta_binomial2() functions are required for brms::predict() to work with our family = beta_binomial2 brmfit object. Similarly, posterior_epred_beta_binomial2() is required for brms::fitted() to work properly. And before all that, we need to throw in a line with the expose_functions() function. Just go with it.\n\nexpose_functions(b12.1, vectorize = TRUE)\n\n# Required to use `predict()`\nlog_lik_beta_binomial2 &lt;- function(i, prep) {\n  mu     &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  phi    &lt;- brms::get_dpar(prep, \"phi\", i = i)\n  trials &lt;- prep$data$vint1[i]\n  y      &lt;- prep$data$Y[i]\n  beta_binomial2_lpmf(y, mu, phi, trials)\n}\n\nposterior_predict_beta_binomial2 &lt;- function(i, prep, ...) {\n  mu     &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  phi    &lt;- brms::get_dpar(prep, \"phi\", i = i)\n  trials &lt;- prep$data$vint1[i]\n  beta_binomial2_rng(mu, phi, trials)\n}\n\n# Required to use `fitted()`\nposterior_epred_beta_binomial2 &lt;- function(prep) {\n  mu     &lt;- brms::get_dpar(prep, \"mu\")\n  trials &lt;- prep$data$vint1\n  trials &lt;- matrix(trials, nrow = nrow(mu), ncol = ncol(mu), byrow = TRUE)\n  mu * trials\n}\n\nWith those intermediary steps out of the way, we’re ready to make Figure 12.1.b.\n\n# The prediction intervals\npredict(b12.1) |&gt;\n  data.frame() |&gt; \n  transmute(ll = Q2.5,\n            ul = Q97.5) |&gt;\n  bind_cols(\n    # The fitted intervals\n    fitted(b12.1) |&gt; data.frame(),\n    # The original data used to fit the model\n    b12.1$data) |&gt; \n  mutate(case = 1:12) |&gt; \n  \n  # plot!\n  ggplot(aes(x = case)) +\n  geom_linerange(aes(ymin = ll / applications, \n                     ymax = ul / applications),\n                 color = canva_pal(\"Green fields\")(4)[1], \n                 linewidth = 2.5, alpha = 1/4) +\n  geom_pointrange(aes(ymin = Q2.5  / applications, \n                      ymax = Q97.5 / applications, \n                      y = Estimate / applications),\n                  color = canva_pal(\"Green fields\")(4)[4],\n                  linewidth = 1/2, shape = 1) +\n  geom_point(aes(y = admit/applications),\n             color = canva_pal(\"Green fields\")(4)[2],\n             size = 2) +\n  scale_x_continuous(breaks = 1:12) +\n  scale_y_continuous(breaks = 0:5 / 5, limits = c(0, 1)) +\n  labs(y = \"A\",\n       subtitle = \"Posterior validation check\",\n       caption = expression(italic(Note.)*\" A = admittance probability\")) +\n  theme(axis.ticks.x = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nAs in the text, the raw data are consistent with the prediction intervals. But those intervals are so incredibly wide, they’re hardly an endorsement of the model. Once we learn about hierarchical models, we’ll be able to do much better.\n\n\n12.1.2 Negative-binomial or gamma-Poisson\nRecall from the last chapter how the Poisson distribution presumes \\(\\sigma^2\\) scales with \\(\\mu\\). The negative binomial distribution relaxes this assumption and presumes “each Poisson count observation has its own rate. It estimates the shape of a gamma distribution to describe the Poisson rates across cases” (p. 373).\nIf you execute ?dgamma, you’ll see that base R will allow you to define the gamma distribution with either the shape and rate or the shape and scale. If we define gamma in terms of shape and rate, it follows the formula\n\\[\\operatorname{Gamma}(y \\mid \\alpha, \\beta) = \\frac{\\beta^\\alpha y^{\\alpha - 1} e^{-\\beta y}}{\\Gamma (\\alpha)},\\]\nwhere \\(\\alpha\\) is the shape, \\(\\beta\\) is the rate, \\(e\\) is base of the natural logarithm, and \\(\\Gamma\\) is the gamma function. It turns out the rate and scale parameters are the reciprocals of each other. Thus if you’d like to define a gamma distribution in terms of shape and scale, it would follow the formula\n\\[\\operatorname{Gamma}(y \\mid \\alpha, \\theta) = \\frac{y^{\\alpha - 1} e^{-x /\\theta}}{\\theta^\\alpha \\Gamma (\\alpha)},\\]\nwhere \\(\\alpha\\), \\(e\\), and \\(\\Gamma\\) are all as they were before and \\(\\theta\\) is the scale parameter. If that all wasn’t complicated enough, it turns out there’s one more way to define a gamma distribution. You can use the mean and shape. This would follow the formula\n\\[\\operatorname{Gamma}(y \\mid \\mu, \\alpha) = \\frac{ \\big (\\frac{\\alpha}{\\mu} \\big)^\\alpha}{\\Gamma (\\alpha)} y^{\\alpha - 1} \\exp (- \\frac{\\alpha y}{\\mu}),\\]\nwhere \\(\\alpha\\) and \\(\\Gamma\\) are still the shape and gamma function, respectively, and \\(\\mu\\) is the mean. I know, this is a lot and it probably all seems really abstract, right now. Think of this section as a reference. As you’ll see after we fit our model, you may well need it. Returning to the content in the text, we might express the gamma-Poisson (negative binomial) as\n\\[y_i \\sim \\operatorname{Gamma-Poisson}(\\mu, \\alpha),\\]\nwhere \\(\\mu\\) is the mean or rate, taking the place of \\(\\lambda\\) from the Poisson distribution, and \\(\\alpha\\) is the shape. I should warn you that this notation is a little different from the notation McElreath used in the text (p. 374), where he used \\(\\lambda\\) in place of our \\(\\mu\\) and \\(\\phi\\) in place of our \\(\\alpha\\). The meaning is really the same. The reason I have diverged from McElreath’s notation is to emphasize the connection with the walk-out, above. If you want to add injury to insult, compare both of these notations with the notation Bürkner used in his (2022d) vignette, Parameterization of response distributions in brms.\nSince this all is already a pedagogical nightmare, let’s throw in one more technical tidbit before we start fitting models. I don’t believe McElreath plainly explained this in the text, but when we talk about the gamma-Poisson model as having two parameters, the \\(\\lambda\\) parameter (a.k.a. \\(\\mu\\)) is doing double duty. As with conventional Poisson models, \\(\\lambda\\) is the mean for the criterion. What might not be as clear is that \\(\\lambda\\) is also the mean of the gamma mixture distribution. So when you want to get a sense of the overall shape of the gamma-Poisson model-implied distribution of \\(\\lambda\\) parameters—one for each variable in the data set–, the distribution is gamma with a mean of \\(\\lambda\\) and shape of \\(\\phi\\) (a.k.a. \\(\\alpha\\)). For a more detailed walk out of this, see Section 8.2 in Hilbe (2011).\nOkay, that’s enough technical background. Let’s load the Kline data and bring this down to Earth with some models.\n\ndata(Kline, package = \"rethinking\")\nd &lt;- Kline |&gt; \n  mutate(p          = rethinking::standardize(log(population)),\n         contact_id = ifelse(contact == \"high\", 2L, 1L),\n         cid        = contact)\nrm(Kline)\n\nprint(d)\n\n      culture population contact total_tools mean_TU            p contact_id  cid\n1    Malekula       1100     low          13     3.2 -1.291473310          1  low\n2     Tikopia       1500     low          22     4.7 -1.088550750          1  low\n3  Santa Cruz       3600     low          24     4.0 -0.515764892          1  low\n4         Yap       4791    high          43     5.0 -0.328773359          2 high\n5    Lau Fiji       7400    high          33     5.0 -0.044338980          2 high\n6   Trobriand       8000    high          19     4.0  0.006668287          2 high\n7       Chuuk       9200    high          40     3.8  0.098109204          2 high\n8       Manus      13000     low          28     6.6  0.324317564          1  low\n9       Tonga      17500    high          55     5.4  0.518797917          2 high\n10     Hawaii     275000     low          71     6.6  2.321008320          1  low\n\n\nIf you take a look of McElreath’s m12.2, you’ll see it’s a gamma-Poisson version of the non-linear model he fit in last chapter, m11.11. You might also recall that we had to employ somewhat complicated non-linear syntax to translate that model into brms. Instead of jumping straight into a similarly complicated gamma-Poisson version of that model, I’m going to warm us up with a simple intercept-only model of the data. The formula will be\n\\[\\begin{align*}\n\\text{total\\_tools}_i & \\sim \\operatorname{Gamma-Poisson} (\\mu, \\alpha) \\\\\n\\text{log}(\\mu) & = \\beta_0 \\\\\n\\beta_0         & \\sim \\operatorname{Normal}(3, 0.5) \\\\\n\\alpha          & \\sim \\operatorname{InvGamma}(0.4, 0.3),\n\\end{align*}\\]\nwhere we have deviated from McElreath’s convention of using \\(\\alpha\\) for the model in favor \\(\\beta_0\\). This is because brms parameterizes the gamma likelihood in terms of \\(\\mu\\) and shape and, as we discussed above, shape is typically denoted as \\(\\alpha\\). I mean, technically we could refer to the shape parameter as \\(\\psi\\) or \\(\\xi\\) or whatever, but then we’d just be abandoning one convention for another. There’s no way to win, here. Sigh. The prior for \\(\\beta_0\\) is the same one we used for the intercept way back for model b11.9. We have assigned a gamma prior for our troublesome new \\(\\alpha\\) (shape) parameter. Here’s where that prior came from.\n\nget_prior(data = d, \n          family = negbinomial,\n          total_tools ~ 1)\n\n                  prior     class coef group resp dpar nlpar lb ub tag  source\n student_t(3, 3.4, 2.5) Intercept                                      default\n    inv_gamma(0.4, 0.3)     shape                             0        default\n\n\ninv_gamma(0.4, 0.3) is the brms default for the shape parameter in this model.2 Sadly, the base-R stats package does not have convince functions for the inverse gamma distribution, but we do have access to a dinvgamma() function from the invgamma package.\n2 Back in the day, brms used to default to \\(\\operatorname{InvGamma}(0.01, 0.01)\\) in this context. Times have changed. For the details, see this issue.\nlibrary(invgamma)\n\ntibble(x = seq(from = 0, to = 14, length.out = 1001)) |&gt; \n  mutate(density = dinvgamma(x, shape = 0.4, rate = 0.3)) |&gt; \n  \n  ggplot(aes(x = x, y = density)) +\n  geom_area(color = \"transparent\", \n            fill = canva_pal(\"Green fields\")(4)[2]) +\n  scale_x_continuous(NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 13)) +\n  ggtitle(expression(brms~default~gamma^{-1}*(0.4*\", \"*0.3)~shape~prior))\n\n\n\n\n\n\n\n\nLet’s fit the model.\n\nb12.2a &lt;- brm(\n  data = d, \n  family = negbinomial,\n  total_tools ~ 1,\n  prior = c(prior(normal(3, 0.5), class = Intercept),    # beta_0\n            prior(inv_gamma(0.4, 0.3), class = shape)),  # alpha\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 12,\n  file = \"fits/b12.02a\")\n\nNotice how you use the language of family = negbinomial to fit these models with brms. Here’s the summary.\n\nprint(b12.2a)\n\n Family: negbinomial \n  Links: mu = log \nFormula: total_tools ~ 1 \n   Data: d (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     3.50      0.16     3.18     3.82 1.00     2165     1918\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     4.57      2.49     1.33    10.83 1.00     2253     1838\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe intercept is our estimate of \\(\\log \\mu\\), similar to \\(\\log \\lambda\\) from a simple Poisson model. The shape is our estimate of, well, the shape (\\(\\alpha\\)). To help us get a sense of what this model is, let’s use the brms::predict() function to return random samples of the poster predictive distribution. Because we want random samples instead of summary values, we will specify summary = F. Let’s take a look of what this returns.\n\np &lt;- predict(b12.2a, summary = F)\n\np |&gt; \n  str(give.attr = F)\n\n num [1:4000, 1:10] 44 23 27 46 56 37 33 10 33 35 ...\n\n\nBecause we have 4,000 posterior iterations, we also get back 4,000 rows. We have 10 columns, which correspond to the 10 rows (i.e., cultures) in the original data. In the next block, we’ll put convert that output to a data frame and wrangle a little before plotting the results.\n\np |&gt; \n  data.frame() |&gt; \n  set_names(d$culture) |&gt; \n  pivot_longer(everything(),\n               names_to = \"culture\",\n               values_to = \"lambda\") |&gt; \n  \n  ggplot(aes(x = lambda)) +\n  geom_density(color = \"transparent\", fill = canva_pal(\"Green fields\")(4)[2]) +\n  scale_x_continuous(expression(lambda[\"[culture]\"]), breaks = 0:2 * 100) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 210)) +\n  facet_wrap(~ culture, nrow = 2)\n\n\n\n\n\n\n\n\nBecause this model had no predictors, we have similar posterior-predictive distributions for each case in the data. It’s important, however, to be very clear of what these posterior-predictive distributions are of. They are not of the data, per se. Let’s look back at the text:\n\nA negative-binomial model, more usefully called a gamma-Poisson model, assumes that each Poisson count observation has its own rate. It estimates the shape of the gamma distribution to describe the Poisson rates across cases. (p. 373, emphasis in the original)\n\nAs a reminder, the “rate” for the Poisson distribution is just another word for the mean, also called \\(\\lambda\\). So unlike a simple Poisson model where we use the individual cases to estimate one overall \\(\\lambda\\), here we’re presuming each case has its own \\(\\lambda_i\\). There are 10 \\(\\lambda_i\\) values that generated our data and if we look at those \\(\\lambda_i\\) values on the whole, their distribution can be described with a gamma distribution. And again, this is not a gamma distribution for our data. This is a gamma distribution of the \\(\\lambda_i\\) values from the 10 separate Poisson distributions that presumably made our data.\nAfter exponentiating the intercept parameter \\((\\log \\mu)\\), here are the posterior distributions for those two gamma parameters.\n\npost &lt;- as_draws_df(b12.2a)\n\npost |&gt; \n  mutate(mu    = exp(b_Intercept),\n         alpha = shape) |&gt;\n  pivot_longer(mu:alpha, names_to = \"parameter\") |&gt; \n  \n  ggplot(aes(x = value)) +\n  geom_density(color = \"transparent\", fill = canva_pal(\"Green fields\")(4)[2]) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"posterior\",\n       title = \"Behold our gamma parameters!\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n\n\n\n\n\n\n\n\nWe might want to use these parameters estimates to visualize the model-implied gamma distribution of \\(\\lambda\\) parameters. But recall that the base R dgamma() function doesn’t take the mean. It is based on either the shape and rate or the shape andscale`. Since we already have the shape (\\(\\alpha\\)), we need a way to compute the scale or rate. Happily, we can define the scale in terms of the mean and the shape with the equation\n\\[\\theta = \\frac{\\mu}{\\alpha}.\\]\nBehold \\(\\theta\\) in a plot.\n\npost |&gt; \n  mutate(mu    = exp(b_Intercept),\n         alpha = shape) |&gt;\n  mutate(theta = mu / alpha) |&gt; \n  \n  ggplot(aes(x = theta)) +\n  geom_density(color = \"transparent\", fill = canva_pal(\"Green fields\")(4)[2]) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"posterior\",\n       subtitle = expression(We~define~the~scale~as~theta==mu/alpha)) +\n  coord_cartesian(xlim = c(0, 40))\n\n\n\n\n\n\n\n\nNow we know how to get both \\(\\alpha\\) and \\(\\theta\\) from the model, we can pump them into dgamma() to get a sense of the model-implied gamma distribution, the presumed underlying distribution of \\(\\lambda\\) values that generated the total_tools data.\n\nset.seed(12)\n\n# Wrangle to get 200 draws\npost |&gt; \n  mutate(alpha = shape,\n         theta = exp(b_Intercept) / shape) |&gt;\n  slice_sample(n = 200) |&gt; \n  expand_grid(x = 0:250)|&gt; \n  mutate(density = dgamma(x, shape = alpha, scale = theta)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = x, y = density)) +\n  geom_line(aes(group = .draw),\n            alpha = 0.1, color = canva_pal(\"Green fields\")(4)[4]) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = expression(lambda),\n       subtitle = expression(\"200 credible gamma densities for \"*lambda)) +\n  coord_cartesian(xlim = c(0, 170),\n                  ylim = c(0, 0.045))\n\n\n\n\n\n\n\n\nNow we’ve warmed up with an intercept-only gamma-Poisson, it’s time to fit a brms version of McElreath’s m12.2. Our model formula will be\n\\[\\begin{align*}\n\\text{total\\_tools}_i & \\sim \\operatorname{Gamma-Poisson} (\\mu_i, \\alpha) \\\\\n\\mu_i       & = \\exp (\\beta_{0,\\text{cid}[i]}) \\text{population}_i^{\\beta_{1,\\text{cid}[i]}} / \\gamma \\\\\n\\beta_{0,j} & \\sim \\operatorname{Normal}(1, 1) \\\\\n\\beta_{1,j} & \\sim \\operatorname{Exponential}(1) \\\\\n\\gamma      & \\sim \\operatorname{Exponential}(1) \\\\\n\\alpha      & \\sim \\operatorname{Exponential}(1),\n\\end{align*}\\]\nwhere \\(\\mu\\) and \\(\\alpha\\) and the mean and shape of the gamma distribution for the case-specific \\(\\lambda\\) parameters. Here’s how we might fit that model with brms.\n\nb12.2b &lt;- brm(\n  data = d, \n  family = negbinomial(link = \"identity\"),\n  bf(total_tools ~ exp(b0) * population^b1 / g,\n     b0 + b1 ~ 0 + cid,\n     g ~ 1,\n     nl = TRUE),\n  prior = c(prior(normal(1, 1), nlpar = b0),\n            prior(exponential(1), nlpar = b1, lb = 0),\n            prior(exponential(1), nlpar = g, lb = 0),\n            prior(exponential(1), class = shape)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 12,\n  control = list(adapt_delta = 0.85),\n  file = \"fits/b12.02b\")\n\nHere is the model summary.\n\nprint(b12.2b)\n\n Family: negbinomial \n  Links: mu = identity \nFormula: total_tools ~ exp(b0) * population^b1/g \n         b0 ~ 0 + cid\n         b1 ~ 0 + cid\n         g ~ 1\n   Data: d (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nb0_cidhigh      1.04      0.93    -0.80     2.85 1.00     2021     2213\nb0_cidlow       0.91      0.86    -0.78     2.54 1.00     2270     1994\nb1_cidhigh      0.26      0.13     0.03     0.53 1.00     1289      929\nb1_cidlow       0.25      0.10     0.07     0.45 1.00     1901     1439\ng_Intercept     1.08      0.88     0.14     3.42 1.00     1571     1631\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     3.69      1.64     1.24     7.60 1.00     2314     1739\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompute and check the PSIS-LOO estimates along with their diagnostic Pareto \\(k\\) values.\n\nb12.2b &lt;- add_criterion(b12.2b, criterion = \"loo\")\nloo(b12.2b)\n\n\nComputed from 4000 by 10 log-likelihood matrix.\n\n         Estimate  SE\nelpd_loo    -41.3 1.6\np_loo         1.2 0.2\nlooic        82.7 3.3\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.5, 1.1]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nWith the current version of brms and loo, all of the Pareto \\(k\\) values are reasonably low. This wasn’t always the case, with earlier versions of the software showing a somewhat higher \\(k\\) value for Hawaii.\nBefore we can make our version of Figure 12.2, we’ll need to reload b11.11 from last chapter. One way is with the readRDS() function.\n\nb11.11 &lt;- readRDS(\"fits/b11.11.rds\")\n\nHere we make the left panel of the figure.\n\n# The new data\nnd &lt;- distinct(d, cid) |&gt; \n  expand_grid(population = seq(from = 0, to = 300000, length.out = 100))\n\n# Compute the expected trajectories\np1 &lt;- fitted(b11.11,\n             newdata = nd,\n             probs = c(0.055, 0.945)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt;\n  \n  # Plot\n  ggplot(aes(x = population, group = cid, color = cid)) +\n  geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid),\n              stat = \"identity\",\n              alpha = 1/4, linewidth = 1/2) +\n  geom_point(data = bind_cols(d, b11.11$criteria$loo$diagnostics),\n             aes(y = total_tools, size = pareto_k),\n             alpha = 4/5) +\n  labs(y = \"total tools\",\n       subtitle = \"pure Poisson model\")\n\nNow make the right panel.\n\n# For the annotation\ntext &lt;- distinct(d, cid) |&gt; \n  mutate(population  = c(150000, 110000),\n         total_tools = c(57, 69),\n         label       = str_c(cid, \" contact\"))\n\np2 &lt;- fitted(b12.2b,\n             newdata = nd,\n             probs = c(0.055, 0.945)) |&gt;\n  data.frame() |&gt;\n  bind_cols(nd) |&gt;\n  \n  ggplot(aes(x = population, group = cid, color = cid)) +\n  geom_smooth(aes(y = Estimate, ymin = Q5.5, ymax = Q94.5, fill = cid),\n              stat = \"identity\",\n              alpha = 1/4, linewidth = 1/2) +\n  geom_point(data = bind_cols(d, b12.2b$criteria$loo$diagnostics),\n             aes(y = total_tools, size = pareto_k),\n             alpha = 4/5) +\n  geom_text(data = text,\n            aes(y = total_tools, label = label)) +\n  scale_y_continuous(NULL, labels = NULL) +\n  labs(subtitle = \"gamma-Poisson model\")\n\nCombine the two ggplots with patchwork syntax to make the full version of Figure 12.2.\n\nlibrary(patchwork)\n\n(p1 | p2) &\n  scale_x_continuous(\"population\", breaks = c(0, 50000, 150000, 250000)) &\n  scale_fill_manual(values = canva_pal(\"Green fields\")(4)[c(4, 1)]) &\n  scale_color_manual(values = canva_pal(\"Green fields\")(4)[c(4, 1)]) &\n  scale_size(range = c(2, 5)) &\n  coord_cartesian(xlim = range(d$population),\n                  ylim = range(d$total_tools)) &\n  theme(axis.ticks = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nOh man!\n\nRecall that Hawaii was a highly influential point in the pure Poisson model. It does all the work of pulling the low-contact trend down. In this new model, Hawaii is still influential, but it exerts a lot less influence on the trends. Now the high and low contact trends are much more similar, very hard to reliably distinguish. This is because the gamma-Poisson model expects rate variation, and the estimated amount of variation is quite large. Population is still strongly related to the total tools, but the influence of contact rate has greatly diminished. (p. 374)\n\nBefore we move on, let’s use predict() to generate posterior predictive distributions for each of our 10 cultures.\n\npredict(b12.2b, summary = F) |&gt; \n  data.frame() |&gt; \n  set_names(d$culture) |&gt; \n  pivot_longer(everything(),\n               names_to = \"culture\",\n               values_to = \"lambda\") |&gt; \n  left_join(d, by = \"culture\") |&gt; \n  \n  ggplot(aes(x = lambda)) +\n  stat_halfeye(point_interval = mean_qi, .width = 0.5,\n               color = canva_pal(\"Green fields\")(4)[1],\n               fill = canva_pal(\"Green fields\")(4)[2]) +\n  geom_vline(aes(xintercept = total_tools),\n             color = canva_pal(\"Green fields\")(4)[3]) +\n  scale_x_continuous(expression(lambda[\"[culture]\"]), breaks = 0:2 * 100) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 210)) +\n  facet_wrap(~ culture, nrow = 2)\n\n\n\n\n\n\n\n\nBecause we used predictors in the model, this time the posterior predictive distributions differ across the cultures. The mean and interquartile range of each distribution are marked off by the light-green dot and horizontal line below each. The vertical lines in the foreground mark off the corresponding total_tools values from the data. Recall that these distributions are not based on the total_tools values themselves, but rather are estimates of the \\(\\lambda\\) values from the underlying Poisson distributions that might have generated such total_tools values.\n\n\n12.1.3 Over-dispersion, entropy, and information criteria\n\nIn terms of model comparison using information criteria, a beta-binomial model is a binomial model, and a gamma-Poisson (negative-binomial) is a Poisson model.\nYou should not use WAIC and PSIS with these models, however, unless you are very sure of what you are doing. The reason is that while ordinary binomial and Poisson models can be aggregated and disaggregated across rows in the data, without changing any causal assumptions, the same is not true of beta-binomial and gamma-Poisson models. The reason is that a beta-binomial or gamma-Poisson likelihood applies an unobserved parameter to each row in the data. When we then go to calculate log-likelihoods, how the data are structured will determine how the beta-distributed or gamma-distributed variation enters the model. (pp. 374–375)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Monsters and Mixtures</span>"
    ]
  },
  {
    "objectID": "12.html#zero-inflated-outcomes",
    "href": "12.html#zero-inflated-outcomes",
    "title": "12  Monsters and Mixtures",
    "section": "12.2 Zero-inflated outcomes",
    "text": "12.2 Zero-inflated outcomes\n\nVery often, the things we can measure are not emissions from any pure process. Instead, they are mixtures of multiple processes. Whenever there are different causes for the same observation, then a mixture model may be useful. A mixture model uses more than one simple probability distribution to model a mixture of causes. In effect, these models use more than one likelihood for the same outcome variable.\nCount variables are especially prone to needing a mixture treatment. The reason is that a count of zero can often arise more than one way. A “zero” means that nothing happened, and nothing can happen either because the rate of events is low or rather because the process that generates events failed to get started. (p. 376, emphasis in the original)\n\n\n12.2.0.1 Rethinking: Breaking the law\nMcElreath discussed how advances in computing have made it possible for working scientists to define their own data generating models. If you’d like to dive deeper into the topic, check out Bürkner’s (2022b) vignette, Define custom response distributions with brms.\n\n\n12.2.1 Example: Zero-inflated Poisson\nDo you remember the monk data from back in Section 11.2.3? Here we simulate some more. This time we’ll work in a little alcohol.\n\n# Define parameters\nprob_drink &lt;- 0.2  # 20% of days\nrate_work  &lt;- 1    # Average 1 manuscript per day\n\n# Sample one year of production\nn &lt;- 365\n\n# Simulate days monks drink\nset.seed(365)\ndrink &lt;- rbinom(n, size = 1, prob = prob_drink)\n\n# Simulate manuscripts completed\ny &lt;- (1 - drink) * rpois(n, lambda = rate_work)\n\nWe’ll put those data in a tidy tibble before plotting.\n\nd &lt;- tibble(drink = factor(drink, levels = 1:0), \n            y     = y)\n  \nd |&gt; \n  ggplot(aes(x = y)) +\n  geom_histogram(aes(fill = drink),\n                 binwidth = 1, color = \"grey92\", linewidth = 1/10) +\n  scale_fill_manual(values = canva_pal(\"Green fields\")(4)[1:2]) +\n  xlab(\"Manuscripts completed\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWith these data, the likelihood of observing zero on y, (i.e., the likelihood zero manuscripts were completed on a given occasion) is\n\\[\\begin{align*}\n\\Pr(0 \\mid p, \\lambda) & = \\Pr(\\text{drink} \\mid p) + \\Pr(\\text{work} \\mid p) \\times \\Pr(0 \\mid \\lambda) \\\\\n                                   & = p + (1 - p) \\exp (- \\lambda).\n\\end{align*}\\]\nAnd\n\nsince the Poisson likelihood of \\(y\\) is \\(\\Pr(y \\mid \\lambda) = \\lambda^y \\exp (- \\lambda) / y!\\), the likelihood of \\(y = 0\\) is just \\(\\exp (- \\lambda)\\). The above is just the mathematics for:\n\nThe probability of observing a zero is the probability that the monks didn’t drink OR (\\(+\\)) the probability that the monks worked AND (\\(\\times\\)) failed to finish anything.\n\nAnd the likelihood of a non-zero value \\(y\\) is:\n\\[\\Pr(y \\mid y &gt; 0, p, \\lambda) = \\Pr(\\text{drink} \\mid p) (0) + \\Pr(\\text{work} \\mid p) \\Pr(y \\mid \\lambda) = (1 - p) \\frac{\\lambda^y \\exp (- \\lambda)}{y!}\\]\nSince drinking monks never produce \\(y &gt; 0\\), the expression above is just the chance the monks both work \\(1 - p\\), and finish \\(y\\) manuscripts. (pp. 377–378, emphasis in the original)\n\nSo letting \\(p\\) be the probability \\(y\\) is zero and \\(\\lambda\\) be the shape of the distribution, the zero-inflated Poisson (\\(\\operatorname{ZIPoisson}\\)) regression model might take the basic form\n\\[\\begin{align*}\ny_i & \\sim \\operatorname{ZIPoisson}(\\color{#5a5f37}{p_i}, \\color{#524a3a}{\\lambda_i})\\\\\n\\color{#5a5f37}{\\operatorname{logit}(p_i)} & \\color{#5a5f37}= \\color{#5a5f37}{\\alpha_p + \\beta_p x_i} \\\\\n\\color{#524a3a}{\\log (\\lambda_i)} & \\color{#524a3a}= \\color{#524a3a}{\\alpha_\\lambda + \\beta_\\lambda x_i},\n\\end{align*}\\]\nwhere both parameters in the likelihood, \\(p_i\\) and \\(\\lambda_i\\) might get their own statistical model, making this a special case of what Bürkner (2022a) calls distributional models. One last thing to note is that in brms, \\(p_i\\) is denoted zi. To fit a zero-inflated Poisson model with brms, make sure to specify the correct likelihood with family = zero_inflated_poisson. To use a non-default prior for zi, make sure to indicate class = zi within the prior() function.\n\nb12.3 &lt;- brm(\n  data = d, \n  family = zero_inflated_poisson,\n  y ~ 1,\n  prior = c(prior(normal(1, 0.5), class = Intercept),\n            prior(beta(2, 6), class = zi)),  # the brms default is `beta(1, 1)`\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 12,\n  file = \"fits/b12.03\")\n\n\nprint(b12.3)\n\n Family: zero_inflated_poisson \n  Links: mu = log \nFormula: y ~ 1 \n   Data: d (Number of observations: 365) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.09    -0.16     0.19 1.00     1316     1466\n\nFurther Distributional Parameters:\n   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nzi     0.23      0.06     0.12     0.34 1.00     1344     1112\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf you look at the Zero-inflated and hurdle models section of Bürkner’s (2022d) Parameterization of response distributions in brms document, you’ll see the zero-inflated Poisson is set up a little differently in brms than it is in rethinking. The difference did not influence the estimate for the intercept, \\(\\lambda\\). In both here and in the text, \\(\\lambda\\) was about zero. However, it did influence the summary of zi. Note how McElreath’s mean( inv_logit( post$ap ) ) returned 0.2241255, which seems rather close to our zi estimate of 0.233. Hopefully it’s clear that zi in brms is already in the probability metric. There’s no need to convert it. You can further confirm this by looking at the second line from the print() output, Links: mu = log; zi = identity. When there are no predictors for zi, the brms default is to use the identity link. In the text, however, McElreath used the logit link for p.\nIn the prior argument, we used beta(2, 6) for zi and also mentioned in the margin that the brms default is beta(1, 1). The beta distribution ranges from 0 to 1, making it a natural distribution to use for priors on probabilities when using the identity link. To give you a sense of what those two versions of the beta look like, let’s plot.\nA typical way to plot a beta distribution would be to use the base R dbeta() function. Let’s try a different approach, instead. The tidybayes package includes a parse_dist() function which takes the kind of string specifications you would usually include in the brms::prior() function and converts them into a format one can plot with. For example, here’s what the preparatory work would be in our case.\n\npriors &lt;- c(prior(beta(1, 1)),\n            prior(beta(2, 6)))\n\npriors |&gt; \n  parse_dist(prior)\n\n       prior class coef group resp dpar nlpar   lb   ub tag source .dist .args  .dist_obj\n1 beta(1, 1)     b                            &lt;NA&gt; &lt;NA&gt;       user  beta  1, 1 beta(1, 1)\n2 beta(2, 6)     b                            &lt;NA&gt; &lt;NA&gt;       user  beta  2, 6 beta(2, 6)\n\n\nThe first several columns look a lot like the kind of output we’d get from the brms::get_prior() function. The parse_dist() function added those last two columns. Here we put them to work by feeding them into a ggplot.\n\npriors |&gt; \n  parse_dist(prior) |&gt;\n  ggplot(aes(y = prior, dist = .dist, args = .args, fill = prior)) +\n  stat_dist_halfeye(.width = 0.95) +\n  scale_x_continuous(\"zi\", breaks = c(0, 0.5, 1)) +\n  scale_y_discrete(expand = expansion(add = 0.1)) +\n  scale_fill_manual(values = canva_pal(\"Green fields\")(4)[c(4, 1)]) +\n  ylab(NULL) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhereas the brms default is flat, our prior guided the posterior a bit toward 0. In case you were curious, we might write our statistical model for b12.3 as\n\\[\\begin{align*}\ny_i            & \\sim \\operatorname{ZIPoisson} (p, \\lambda) \\\\\np              & = \\alpha_p \\\\\n\\log \\lambda   & = \\alpha_\\lambda \\\\\n\\alpha_p       & \\sim \\operatorname{Beta}(2, 6) \\\\\n\\alpha_\\lambda & \\sim \\operatorname{Normal}(1, 0.5).\n\\end{align*}\\]\nAnyway, here’s that exponentiated \\(\\alpha_\\lambda\\).\n\nfixef(b12.3)[1, -2] |&gt;\n  exp()\n\nEstimate     Q2.5    Q97.5 \n1.022214 0.852194 1.211660 \n\n\n\n12.2.1.1 Overthinking: Zero-inflated Poisson calculations in Stan\nIf you’re curious, here’s the Stan code underlying our brms fit, b12.3.\n\nb12.3$model\n\n// generated with brms 2.23.0\nfunctions {\n  /* zero-inflated poisson log-PDF of a single response\n   * Args:\n   *   y: the response value\n   *   lambda: mean parameter of the poisson distribution\n   *   zi: zero-inflation probability\n   * Returns:\n   *   a scalar to be added to the log posterior\n   */\n  real zero_inflated_poisson_lpmf(int y, real lambda, real zi) {\n    if (y == 0) {\n      return log_sum_exp(bernoulli_lpmf(1 | zi),\n                         bernoulli_lpmf(0 | zi) +\n                         poisson_lpmf(0 | lambda));\n    } else {\n      return bernoulli_lpmf(0 | zi) +\n             poisson_lpmf(y | lambda);\n    }\n  }\n  /* zero-inflated poisson log-PDF of a single response\n   * logit parameterization of the zero-inflation part\n   * Args:\n   *   y: the response value\n   *   lambda: mean parameter of the poisson distribution\n   *   zi: linear predictor for zero-inflation part\n   * Returns:\n   *   a scalar to be added to the log posterior\n   */\n  real zero_inflated_poisson_logit_lpmf(int y, real lambda, real zi) {\n    if (y == 0) {\n      return log_sum_exp(bernoulli_logit_lpmf(1 | zi),\n                         bernoulli_logit_lpmf(0 | zi) +\n                         poisson_lpmf(0 | lambda));\n    } else {\n      return bernoulli_logit_lpmf(0 | zi) +\n             poisson_lpmf(y | lambda);\n    }\n  }\n  /* zero-inflated poisson log-PDF of a single response\n   * log parameterization for the poisson part\n   * Args:\n   *   y: the response value\n   *   eta: linear predictor for poisson distribution\n   *   zi: zero-inflation probability\n   * Returns:\n   *   a scalar to be added to the log posterior\n   */\n  real zero_inflated_poisson_log_lpmf(int y, real eta, real zi) {\n    if (y == 0) {\n      return log_sum_exp(bernoulli_lpmf(1 | zi),\n                         bernoulli_lpmf(0 | zi) +\n                         poisson_log_lpmf(0 | eta));\n    } else {\n      return bernoulli_lpmf(0 | zi) +\n             poisson_log_lpmf(y | eta);\n    }\n  }\n  /* zero-inflated poisson log-PDF of a single response\n   * log parameterization for the poisson part\n   * logit parameterization of the zero-inflation part\n   * Args:\n   *   y: the response value\n   *   eta: linear predictor for poisson distribution\n   *   zi: linear predictor for zero-inflation part\n   * Returns:\n   *   a scalar to be added to the log posterior\n   */\n  real zero_inflated_poisson_log_logit_lpmf(int y, real eta, real zi) {\n    if (y == 0) {\n      return log_sum_exp(bernoulli_logit_lpmf(1 | zi),\n                         bernoulli_logit_lpmf(0 | zi) +\n                         poisson_log_lpmf(0 | eta));\n    } else {\n      return bernoulli_logit_lpmf(0 | zi) +\n             poisson_log_lpmf(y | eta);\n    }\n  }\n  // zero-inflated poisson log-CCDF and log-CDF functions\n  real zero_inflated_poisson_lccdf(int y, real lambda, real zi) {\n    return bernoulli_lpmf(0 | zi) + poisson_lccdf(y | lambda);\n  }\n  real zero_inflated_poisson_lcdf(int y, real lambda, real zi) {\n    return log1m_exp(zero_inflated_poisson_lccdf(y | lambda, zi));\n  }\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  real&lt;lower=0,upper=1&gt; zi;  // zero-inflation probability\n}\ntransformed parameters {\n  // prior contributions to the log posterior\n  real lprior = 0;\n  lprior += normal_lpdf(Intercept | 1, 0.5);\n  lprior += beta_lpdf(zi | 2, 6);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      target += zero_inflated_poisson_log_lpmf(Y[n] | mu[n], zi);\n    }\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n}\n\n\nFor more on how to fit this model with rstan, go to my companion book here.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Monsters and Mixtures</span>"
    ]
  },
  {
    "objectID": "12.html#sec-Ordered-categorical-outcomes",
    "href": "12.html#sec-Ordered-categorical-outcomes",
    "title": "12  Monsters and Mixtures",
    "section": "12.3 Ordered categorical outcomes",
    "text": "12.3 Ordered categorical outcomes\n\nIt is very common in the social sciences, and occasional in the natural sciences, to have an outcome variable that is discrete, like a count, but in which the values merely indicate different ordered levels along some dimension. For example, if I were to ask you how much you like to eat fish,on a scale from 1 to 7, you might say 5. If I were to ask 100 people the same question, I’d end up with 100 values between 1 and 7. In modeling each outcome value, I’d have to keep in mind that these values are ordered, because 7 is greater than 6, which is greater than 5, and so on. The result is a set of ordered categories. Unlike a count, the differences in value are not necessarily equal….\nIn principle, an ordered categorical variable is just a multinomial prediction problem (page 359). But the constraint that the categories be ordered demands special treatment….\nThe conventional solution is to use a cumulative link function. The cumulative probability of a value is the probability of that value or any smaller value. In the context of ordered categories, the cumulative probability of 3 is the sum of the probabilities of 3, 2, and 1. Ordered categories by convention begin at 1, so a result less than 1 has no probability at all. By linking a linear model to cumulative probability, it is possible to guarantee the ordering of the outcomes. (p. 380, emphasis in the original)\n\n\n12.3.1 Example: Moral intuition\nLet’s get the Trolley data from rethinking (see Cushman et al., 2006).\n\ndata(Trolley, package = \"rethinking\")\nd &lt;- Trolley\nrm(Trolley)\n\nUse the dplyr::glimpse() to get a sense of the dimensions of the data.\n\nglimpse(d)\n\nRows: 9,930\nColumns: 12\n$ case      &lt;fct&gt; cfaqu, cfbur, cfrub, cibox, cibur, cispe, fkaqu, fkboa, fkbox, fkbur, fkcar, fkspe, fkswi, flboa, flcar, flche…\n$ response  &lt;int&gt; 4, 3, 4, 3, 3, 3, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 3, 3, 3, 4, 4, 5, 4, 4, 3, 4, 4, 4, 2, 4, 1, 1, 1, 7,…\n$ order     &lt;int&gt; 2, 31, 16, 32, 4, 9, 29, 12, 23, 22, 27, 19, 14, 3, 18, 15, 30, 5, 1, 13, 20, 17, 28, 10, 6, 11, 21, 26, 24, 7…\n$ id        &lt;fct&gt; 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434, 96;434…\n$ age       &lt;int&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14…\n$ male      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,…\n$ edu       &lt;fct&gt; Middle School, Middle School, Middle School, Middle School, Middle School, Middle School, Middle School, Middl…\n$ action    &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,…\n$ intention &lt;int&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,…\n$ contact   &lt;int&gt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,…\n$ story     &lt;fct&gt; aqu, bur, rub, box, bur, spe, aqu, boa, box, bur, car, spe, swi, boa, car, che, sha, swi, box, bur, pon, shi, …\n$ action2   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n\n\nThough we have 9,930 rows, we only have 331 unique individuals.\n\nd |&gt; \n  distinct(id) |&gt; \n  count()\n\n    n\n1 331\n\n\n\n\n12.3.2 Describing an ordered distribution with intercepts\nMake our version of the simple Figure 12.4 histogram of our primary variable, response.\n\np1 &lt;- d |&gt; \n  ggplot(aes(x = response, fill = after_stat(x))) +\n  geom_histogram(binwidth = 1/4, linewidth = 0) +\n  scale_x_continuous(breaks = 1:7) +\n  scale_fill_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                      high = canva_pal(\"Green fields\")(4)[1]) +\n  theme(axis.ticks = element_blank(),\n        axis.title.y = element_text(angle = 90),\n        legend.position = \"none\")\np1\n\n\n\n\n\n\n\n\nOur cumulative proportion plot, Figure 12.4.b, will require some pre-plot wrangling.\n\np2 &lt;- d |&gt;\n  count(response) |&gt;\n  mutate(pr_k     = n / nrow(d),\n         cum_pr_k = cumsum(pr_k)) |&gt; \n  \n  ggplot(aes(x = response, y = cum_pr_k, fill = response)) +\n  geom_line(color = canva_pal(\"Green fields\")(4)[2]) +\n  geom_point(color = \"grey92\", shape = 21,\n             size = 2.5, stroke = 1) +\n  scale_x_continuous(breaks = 1:7) +\n  scale_y_continuous(\"cumulative proportion\", breaks = c(0, 0.5, 1)) +\n  scale_fill_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                      high = canva_pal(\"Green fields\")(4)[1]) +\n  coord_cartesian(ylim = c(0, 1)) +\n  theme(axis.ticks = element_blank(),\n        axis.title.y = element_text(angle = 90),\n        legend.position = \"none\")\np2\n\n\n\n\n\n\n\n\n\nThen to re-describe the histogram as log-cumulative odds, we’ll need a series of intercept parameters. Each intercept will be on the log-cumulative-odds scale and stand in for the cumulative probability of each outcome. So this is just the application of the link function. The log-cumulative-odds that a response value \\(y_i\\) is equal-to-or-less-than some possible outcome value \\(k\\) is:\n\\[\\log \\frac{\\Pr(y_i \\leq k)}{1 - \\Pr(y_i \\leq k)} = \\alpha_k\\]\nwhere \\(\\alpha_k\\) is an “intercept” unique to each possible outcome value \\(k\\). (p. 383)\n\nWe can compute the \\(\\alpha_k\\) estimates directly with a little help from McElreath’s custom logit() function.\n\nlogit &lt;- function(x) log(x / (1 - x))  # Convenience function\n\nd |&gt;\n  count(response) |&gt;\n  mutate(pr_k     = n / nrow(d),\n         cum_pr_k = cumsum(n / nrow(d))) |&gt; \n  mutate(alpha = logit(cum_pr_k) |&gt; round(digits = 2))\n\n  response    n       pr_k  cum_pr_k alpha\n1        1 1274 0.12829809 0.1282981 -1.92\n2        2  909 0.09154079 0.2198389 -1.27\n3        3 1071 0.10785498 0.3276939 -0.72\n4        4 2323 0.23393756 0.5616314  0.25\n5        5 1462 0.14723061 0.7088620  0.89\n6        6 1445 0.14551863 0.8543807  1.77\n7        7 1446 0.14561934 1.0000000   NaN\n\n\nNow we plot those joints to make our version of Figure 12.4.c.\n\np3 &lt;- d |&gt;\n  count(response) |&gt;\n  mutate(cum_pr_k = cumsum(n / nrow(d))) |&gt; \n  filter(response &lt; 7) |&gt; \n  \n  # We can do the `logit()` conversion right in `ggplot() \n  ggplot(aes(x = response, y = logit(cum_pr_k), fill = response)) +\n  geom_line(color = canva_pal(\"Green fields\")(4)[2]) +\n  geom_point(colour = \"grey92\", shape = 21,\n             size = 2.5, stroke = 1) +\n  scale_x_continuous(breaks = 1:7, limits = c(1, 7)) +\n  scale_fill_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                      high = canva_pal(\"Green fields\")(4)[1]) +\n  ylab(\"log-cumulative-odds\") +\n  theme(axis.ticks = element_blank(),\n        axis.title.y = element_text(angle = 90),\n        legend.position = \"none\")\np3\n\n\n\n\n\n\n\n\nWhy not combine the three subplots with patchwork?\n\n(p1 | p2 | p3) + \n  plot_annotation(title = \"Re-describing a discrete distribution using log-cumulative-odds.\")\n\n\n\n\n\n\n\n\nThe code for Figure 12.5 is itself something of a monster.\n\n# Primary data\nd_plot &lt;- d |&gt;\n  count(response) |&gt;\n  mutate(pr_k     = n / nrow(d),\n         cum_pr_k = cumsum(n / nrow(d))) |&gt; \n  mutate(discrete_probability = ifelse(response == 1, cum_pr_k, cum_pr_k - pr_k))\n\n# Annotation\ntext &lt;- tibble(\n  text     = 1:7,\n  response = 1:7 + 0.25,\n  cum_pr_k = d_plot$cum_pr_k - 0.065) \n\nd_plot |&gt; \n  ggplot(aes(x = response, y = cum_pr_k,\n             color = cum_pr_k, fill = cum_pr_k)) +\n  geom_line(color = canva_pal(\"Green fields\")(4)[1]) +\n  geom_point(colour = \"grey92\", shape = 21,\n             size = 2.5, stroke = 1) +\n  geom_linerange(aes(ymin = 0, ymax = cum_pr_k),\n                 alpha = 1/2, color = canva_pal(\"Green fields\")(4)[1]) +\n  geom_linerange(aes(x = response + 0.025,\n                     ymin = ifelse(response == 1, 0, discrete_probability), \n                     ymax = cum_pr_k),\n                 color = \"black\") +\n  # Number annotation\n  geom_text(data = text, \n            aes(label = text),\n            size = 4) +\n  scale_x_continuous(breaks = 1:7) +\n  scale_y_continuous(\"cumulative proportion\", breaks = c(0, 0.5, 1)) +\n  scale_fill_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                      high = canva_pal(\"Green fields\")(4)[1]) +\n  scale_color_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                       high = canva_pal(\"Green fields\")(4)[1]) +\n  theme(axis.ticks = element_blank(),\n        axis.title.y = element_text(angle = 90),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nA compact way to express the formula for this first type of statistical model is\n\\[\\begin{align*}\n\\text{response}_i & \\sim \\operatorname{Categorical} (\\mathbf p) \\\\\n\\operatorname{logit}(p_k) & = \\alpha_k - \\phi \\\\\n\\phi              & = 0 \\\\\n\\alpha_k          & \\sim \\operatorname{Normal}(0, 1.5),\n\\end{align*}\\]\nwhere the \\(\\alpha_k\\) term denotes the \\(K - 1\\) intercepts (cut points or thresholds) we use to describe each possible outcome value \\(k\\) and. The mysterious looking \\(\\phi\\) term is a stand-in for the potential terms of the linear model. In the case where we have no predictors, it’s just 0. Just hold on to your hats; this will make more sense in the next section.\n\nAn ordered-logit distribution is really just a categorical distribution that takes a vector \\(\\mathbf p = \\{p_1, p_2, p_3, p_4, p_5, p_6\\}\\) of probabilities of each response value below the maximum response (7 in this example). Each response value \\(k\\) in this vector is defined by its link to an intercept parameter, \\(\\alpha_k\\). Finally, some weakly regularizing priors are placed on these intercepts. (p. 385)\n\nWhereas in rethinking::ulam() you indicate the likelihood by &lt;criterion&gt; ~ dordlogit(0 , c(&lt;the thresholds&gt;), in brms::brm() you code family = cumulative. Here’s how to fit the intercepts-only model.\n\n# Define the start values\ninit &lt;- list(`Intercept[1]` = -2,\n             `Intercept[2]` = -1,\n             `Intercept[3]` = 0,\n             `Intercept[4]` = 1,\n             `Intercept[5]` = 2,\n             `Intercept[6]` = 2.5)\n\ninit_list &lt;- list(init, init, init, init)\n\nb12.4 &lt;- brm(\n  data = d, \n  family = cumulative,\n  response ~ 1,\n  prior(normal(0, 1.5), class = Intercept),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  init = init_list,  # here we add our start values\n  file = \"fits/b12.04\")\n\nMcElreath needed to include the depth=2 argument in the rethinking::precis() function to show the threshold parameters from his m11.1stan model (R code 12.24). With a brm() fit, we just use print() or summary() as usual.\n\nprint(b12.4)\n\n Family: cumulative \n  Links: mu = logit \nFormula: response ~ 1 \n   Data: d (Number of observations: 9930) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -1.92      0.03    -1.98    -1.86 1.00     2947     2579\nIntercept[2]    -1.27      0.02    -1.31    -1.22 1.00     3944     3248\nIntercept[3]    -0.72      0.02    -0.76    -0.68 1.00     4036     3072\nIntercept[4]     0.25      0.02     0.21     0.29 1.00     4642     3212\nIntercept[5]     0.89      0.02     0.85     0.93 1.00     4673     3486\nIntercept[6]     1.77      0.03     1.71     1.82 1.00     4947     3682\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWhat McElreath’s m12.4 summary termed cutpoints[k], our brms summary termed Intercept[k]. In both cases, these are the \\(\\alpha_k\\) parameters from the formula, above (i.e., the thresholds). The summaries look like those in the text, the \\(\\widehat R\\) values are great, and both measures of effective sample size are high. The results looks good.\nWe can use the plogis() function to get these into the probability metric.\n\nb12.4 |&gt; \n  fixef() |&gt; \n  plogis() |&gt; \n  round(digits = 3)\n\n             Estimate Est.Error  Q2.5 Q97.5\nIntercept[1]    0.128     0.508 0.122 0.135\nIntercept[2]    0.220     0.506 0.212 0.228\nIntercept[3]    0.328     0.505 0.319 0.337\nIntercept[4]    0.562     0.505 0.552 0.571\nIntercept[5]    0.709     0.506 0.700 0.718\nIntercept[6]    0.854     0.507 0.847 0.861\n\n\nBut recall that the posterior \\(\\textit{SD}\\) (i.e., the ‘Est.Error’ values) are not valid using that approach. If you really care about them, you’ll need to work with the as_draws_df().\n\nas_draws_df(b12.4) |&gt; \n  mutate_at(vars(starts_with(\"b_\")), plogis) |&gt; \n  pivot_longer(starts_with(\"b_\")) |&gt; \n  group_by(name) |&gt; \n  summarise(mean = mean(value),\n            sd   = sd(value),\n            ll   = quantile(value, probs = 0.025),\n            ul   = quantile(value, probs = 0.975))\n\n# A tibble: 6 × 5\n  name            mean      sd    ll    ul\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 b_Intercept[1] 0.128 0.00335 0.122 0.135\n2 b_Intercept[2] 0.220 0.00416 0.212 0.228\n3 b_Intercept[3] 0.328 0.00467 0.319 0.337\n4 b_Intercept[4] 0.562 0.00495 0.552 0.571\n5 b_Intercept[5] 0.709 0.00457 0.700 0.718\n6 b_Intercept[6] 0.854 0.00347 0.847 0.861\n\n\nJust to confirm, those posterior means are centered right around the cum_pr_k we computed for Figure 12.4.\n\nd_plot |&gt; \n  select(response, cum_pr_k)\n\n  response  cum_pr_k\n1        1 0.1282981\n2        2 0.2198389\n3        3 0.3276939\n4        4 0.5616314\n5        5 0.7088620\n6        6 0.8543807\n7        7 1.0000000\n\n\nTo walk out our results even further, we can make b12.4-based version of Figure 12.4.c after formatting our posterior summary a little.\n\nfixef(b12.4) |&gt; \n  data.frame() |&gt; \n  rownames_to_column(\"intercept\") |&gt; \n  mutate(response = str_extract(intercept, \"\\\\d\") |&gt; as.double()) |&gt; \n  \n  ggplot(aes(x = response, y = Estimate,\n             ymin = Q2.5, ymax = Q97.5,\n             fill = response)) +\n  geom_line(color = canva_pal(\"Green fields\")(4)[2]) +\n  geom_point(colour = \"grey92\", shape = 21,\n             size = 1.5, stroke = 1) +\n  geom_linerange(color = canva_pal(\"Green fields\")(4)[2]) +\n  scale_x_continuous(breaks = 1:7, limits = c(1, 7)) +\n  scale_fill_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                      high = canva_pal(\"Green fields\")(4)[1]) +\n  ylab(\"log-cumulative-odds\") +\n  theme(axis.ticks = element_blank(),\n        axis.title.y = element_text(angle = 90),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nNow the dots are the posterior means and the vertical lines layered on top of them are their 95% posterior intervals. Given the large amount of data, the posteriors for our \\(\\alpha_k\\) parameters are rather narrow, which is expressed in tightness of those vertical lines.\n\n\n12.3.3 Adding predictor variables\nNow we define the generic linear model as \\(\\phi_i = \\beta x_i\\). Accordingly, the formula for our cumulative logit model becomes\n\\[\\begin{align*}\n\\log \\frac{\\Pr(y_i \\leq k)}{1 - \\Pr(y_i \\leq k)} & = \\alpha_k - \\phi_i \\\\\n\\phi_i & = \\beta x_i.\n\\end{align*}\\]\n\nThis form automatically ensures the correct ordering of the outcome values, while still morphing the likelihood of each individual value as the predictor \\(x_i\\) changes value. Why is the linear model \\(\\phi\\) subtracted from each intercept? Because if we decrease the log-cumulative-odds of every outcome value \\(k\\) below the maximum, this necessarily shifts probability mass upwards towards higher outcome values. So then positive values of \\(\\beta\\) mean increasing \\(x\\) also increases the mean \\(y\\). You could add \\(\\phi\\) instead like \\(\\alpha_k + \\phi_i\\). But then \\(\\beta &gt; 0\\) would indicate increasing \\(x\\) decreases the mean. (p. 386)\n\nI’m not aware that brms has an equivalent to the rethinking::dordlogit() function. So here we’ll make it by hand. The code comes from McElreath’s GitHub repo for rethinking.\n\nlogistic &lt;- function(x) {\n  p &lt;- 1 / (1 + exp(-x))\n  p &lt;- ifelse(x == Inf, 1, p)\n  p\n}\n\n# Now we get down to it\ndordlogit &lt;- function(x, phi, a, log = FALSE) {\n  a  &lt;- c(as.numeric(a), Inf)\n  p  &lt;- logistic(a[x] - phi)\n  na &lt;- c(-Inf, a)\n  np &lt;- logistic(na[x] - phi)\n  p  &lt;- p - np\n  if (log == TRUE) p &lt;- log(p)\n  p\n}\n\nThe dordlogit() function works like this:\n\npk &lt;- dordlogit(1:7, 0, fixef(b12.4)[, 1])\n\npk |&gt; \n  round(digits = 2)\n\n[1] 0.13 0.09 0.11 0.23 0.15 0.15 0.15\n\n\nNote the slight difference in how we used dordlogit() with a brm() fit summarized by fixef() than the way McElreath did with a ulam() fit summarized by coef(). McElreath just put coef(m12.4) into dordlogit(). We, however, more specifically placed fixef(b12.4)[, 1] into the function. With the [, 1] part, we specified that we were working with the posterior means (i.e., Estimate) and neglecting the other summaries (i.e., the posterior \\(\\textit{SD}\\)s and 95% intervals). If you forget to subset, chaos ensues.\nNext, as McElreath further noted on page 338, “these probabilities imply an average outcome of:”\n\nsum(pk * (1:7))\n\n[1] 4.199704\n\n\nI found that a bit abstract. Here’s the thing in a more elaborate tibble format.\n\n(\n  explicit_example &lt;- tibble(probability_of_a_response = pk) |&gt;\n    mutate(the_response = 1:7) |&gt;\n    mutate(their_product = probability_of_a_response * the_response)\n)\n\n# A tibble: 7 × 3\n  probability_of_a_response the_response their_product\n                      &lt;dbl&gt;        &lt;int&gt;         &lt;dbl&gt;\n1                    0.128             1         0.128\n2                    0.0917            2         0.183\n3                    0.108             3         0.324\n4                    0.234             4         0.935\n5                    0.147             5         0.736\n6                    0.146             6         0.873\n7                    0.146             7         1.02 \n\nexplicit_example |&gt;\n  summarise(average_outcome_value = sum(their_product))\n\n# A tibble: 1 × 1\n  average_outcome_value\n                  &lt;dbl&gt;\n1                  4.20\n\n\nNow we’ll try it by subtracting 0.5 from each.\n\n# The probabilities of a given response\npk &lt;- dordlogit(1:7, phi = 0, a = fixef(b12.4)[, 1] - 0.5)\n\npk |&gt; \n  round(digits = 2)\n\n[1] 0.08 0.06 0.08 0.21 0.16 0.18 0.22\n\n# The average rating\nsum(pk * (1:7))\n\n[1] 4.730111\n\n\nSo the rule is we subtract the linear model from each intercept. “This way, a positive \\(\\beta\\) value indicates that an increase in the predictor variable \\(x\\) results in an increase in the average response” (p. 387). Happily, even though this makes for a somewhat confusing statistical formula, we still enter our predictor terms into the brm() formula argument much the same way we always have. As to our upcoming model, we might express the statistical formula as\n\\[\\begin{align*}\n\\text{response}_i & \\sim \\operatorname{Categorical} (\\mathbf p) \\\\\n\\text{logit}(p_k)     & = \\alpha_k - \\phi_i \\\\\n\\phi_i                & = \\beta_1 \\text{action}_i + \\beta_2 \\text{contact}_i +  (\\beta_3 + \\beta_4 \\text{action}_i + \\beta_5 \\text{contact}_i) \\text{intention}_i \\\\\n\\alpha_k              & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\beta_1, \\dots, \\beta_5 & \\sim \\operatorname{Normal}(0, 0.5),\n\\end{align*}\\]\nwhere, because we have included predictors, \\(\\phi\\) is no longer set to 0. Using our skills from back in Chapter 8, we might also rewrite the linear model for \\(\\phi\\) as\n\\[\\phi_i = \\beta_1 \\text{action}_i + \\beta_2 \\text{contact}_i + \\beta_3 \\text{intention}_i + \\beta_4 (\\text{action}_i \\times \\text{intention}_i) + \\beta_5 (\\text{contact}_i \\times \\text{intention}_i).\\]\nLet’s fit the model.\n\nb12.5 &lt;- brm(\n  data = d, \n  family = cumulative,\n  response ~ 1 + action + contact + intention + intention:action + intention:contact,\n  prior = c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 0.5), class = b)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 12,\n  file = \"fits/b12.05\")\n\nThere are ways with brms to mirror how McElreath coded his phi &lt;- bA*A + bC*C + BI*I and BI &lt;- bI + bIA*A + bIC*C. Here we just used a more conventional style of syntax. Behold the summary.\n\nprint(b12.5)\n\n Family: cumulative \n  Links: mu = logit \nFormula: response ~ 1 + action + contact + intention + intention:action + intention:contact \n   Data: d (Number of observations: 9930) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]         -2.64      0.05    -2.74    -2.53 1.00     3284     2917\nIntercept[2]         -1.94      0.05    -2.03    -1.84 1.00     3326     3182\nIntercept[3]         -1.35      0.05    -1.44    -1.26 1.00     3240     3271\nIntercept[4]         -0.31      0.04    -0.40    -0.22 1.00     3162     3096\nIntercept[5]          0.36      0.04     0.27     0.45 1.00     3147     3083\nIntercept[6]          1.27      0.05     1.17     1.36 1.00     3273     3276\naction               -0.48      0.06    -0.59    -0.37 1.00     3222     3273\ncontact              -0.35      0.07    -0.48    -0.20 1.00     3259     2937\nintention            -0.29      0.06    -0.41    -0.18 1.00     3045     2762\naction:intention     -0.43      0.08    -0.59    -0.27 1.00     3045     3146\ncontact:intention    -1.23      0.10    -1.44    -1.04 1.00     3056     3137\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nFor a little variety, we’ll make our coefficient plot with a little help from the tidybayes::stat_gradientinterval() function.\n\nlabs &lt;- str_c(\"beta[\", 1:5, \"]\")\n\nas_draws_df(b12.5) |&gt; \n  select(b_action:`b_contact:intention`) |&gt; \n  set_names(labs) |&gt; \n  pivot_longer(everything()) |&gt; \n  \n  ggplot(aes(x = value, y = name)) +\n  geom_vline(xintercept = 0, alpha = 1/5, linetype = 3) +\n  stat_gradientinterval(.width = 0.5, point_size = 3/2, shape = 21, size = 1,\n                        color = canva_pal(\"Green fields\")(4)[2], \n                        fill = canva_pal(\"Green fields\")(4)[1],\n                        point_fill = canva_pal(\"Green fields\")(4)[3],) +\n  scale_x_continuous(\"marginal posterior\", breaks = -5:0 / 4) +\n  scale_y_discrete(NULL, labels = parse(text = labs)) +\n  coord_cartesian(xlim = c(-1.4, 0))\n\n\n\n\n\n\n\n\n\nAs always, this will all be easier to see if we plot the posterior predictions. There is no perfect way to plot the predictions of these log-cumulative-odds models. Why? Because each prediction is really a vector of probabilities, one for each possible outcome value. So as a predictor variable changes value, the entire vector changes. This kind of thing can be visualized in several different ways. (p. 388)\n\nOur approach to making the top panels of Figure 12.6 will start with fitted().\n\nnd &lt;- d |&gt; \n  distinct(action, contact, intention) |&gt; \n  mutate(combination = str_c(action, contact, intention, sep = \"_\"))\n\nf &lt;- fitted(b12.5,\n            newdata = nd,\n            summary = F)\n\n# What have we done?\nf |&gt; str()\n\n num [1:4000, 1:6, 1:7] 0.0899 0.0886 0.0936 0.0907 0.0942 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ : chr [1:4000] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : NULL\n  ..$ : chr [1:7] \"1\" \"2\" \"3\" \"4\" ...\n\n\nThat returned a three-dimensional array. The 4,000 rows correspond to the 4,000 post-warmup iterations. The six columns correspond to the six unique combinations of action, contact, and intention within the data (i.e., d |&gt; distinct(action, contact, intention)). The three levels of the third dimension correspond to the seven levels of our response variable. This is all the information we need to plot our posterior in the triptych in the top row of Figure 12.6. It will take a bit of tricky wrangling to get it into a useful format. Our first several steps have to do with arranging the data into a long tibble format.\n\nf &lt;- rbind(f[, , 1],\n           f[, , 2],\n           f[, , 3],\n           f[, , 4],\n           f[, , 5],\n           f[, , 6],\n           f[, , 7]) |&gt; \n  data.frame() |&gt; \n  set_names(pull(nd, combination)) |&gt; \n  mutate(response = rep(1:7, each = n() / 7),\n         draw     = rep(1:4000, times = 7)) |&gt; \n  pivot_longer(-c(draw, response),\n               names_to = c(\"action\", \"contact\", \"intention\"),\n               names_sep = \"_\",\n               values_to = \"pk\") |&gt; \n  mutate(intention = intention |&gt; as.integer())\n\n# How do the data look, now?\nglimpse(f)\n\nRows: 168,000\nColumns: 6\n$ response  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ draw      &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7,…\n$ action    &lt;chr&gt; \"0\", \"0\", \"1\", \"0\", \"1\", \"0\", \"0\", \"0\", \"1\", \"0\", \"1\", \"0\", \"0\", \"0\", \"1\", \"0\", \"1\", \"0\", \"0\", \"0\", \"1\", \"0\", …\n$ contact   &lt;chr&gt; \"1\", \"1\", \"0\", \"0\", \"0\", \"0\", \"1\", \"1\", \"0\", \"0\", \"0\", \"0\", \"1\", \"1\", \"0\", \"0\", \"0\", \"0\", \"1\", \"1\", \"0\", \"0\", …\n$ intention &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,…\n$ pk        &lt;dbl&gt; 0.08991828, 0.31044381, 0.10102966, 0.06819220, 0.19327757, 0.08614161, 0.08857309, 0.31999125, 0.10063826, 0.…\n\n\nWe’re moving pretty quickly, here. If it wasn’t apparent, the original values in the f data were in the probability metric. This is why we followed the convention from earlier in this section and named them pk. However, we need them to be in the cumulative-probability metric to make the top panels of the figure.\n\n# To order our factor levels for `facet`\nlevels &lt;- c(\"action=0, contact=0\", \"action=1, contact=0\", \"action=0, contact=1\")\n\np1 &lt;- f |&gt; \n  # Unnecessary for these plots\n  filter(response &lt; 7) |&gt; \n  # This will help us define the three panels of the triptych\n  mutate(facet = factor(str_c(\"action=\", action, \", contact=\", contact),\n                        levels = levels)) |&gt; \n  # These next three lines allow us to compute the cumulative probabilities\n  group_by(draw, facet, intention) |&gt; \n  arrange(draw, facet, intention, response) |&gt; \n  mutate(probability = cumsum(pk)) |&gt; \n  ungroup() |&gt; \n  # These next three lines are how we randomly selected 50 posterior draws\n  nest(data = -draw) |&gt; \n  slice_sample(n = 50) |&gt;\n  unnest(data) |&gt; \n  \n  # Plot!\n  ggplot(aes(x = intention, y = probability)) +\n  geom_line(aes(group = interaction(draw, response), color = probability),\n            alpha = 1/10) +\n  geom_point(data = d |&gt;  # Wrangle the original data to make the dots\n               group_by(intention, contact, action) |&gt; \n               count(response) |&gt; \n               mutate(probability = cumsum(n / sum(n)),\n                      facet = factor(str_c(\"action=\", action, \", contact=\", contact),\n                                     levels = levels)) |&gt; \n               filter(response &lt; 7),\n             color = canva_pal(\"Green fields\")(4)[2]) +\n  scale_x_continuous(\"intention\", breaks = 0:1) +\n  scale_y_continuous(breaks = c(0, 0.5, 1), limits = 0:1) +\n  scale_color_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                       high = canva_pal(\"Green fields\")(4)[1]) +\n  facet_wrap(~ facet) +\n  theme(legend.position = \"none\")\n\nWe will look at the results of our code in just a bit. For now, we’ll focus on the code for the triptych in the bottom panels of Figure 12.6. These plots will be based on predict().\n\np &lt;- predict(b12.5,\n             newdata = nd,\n             ndraws = 1000,\n             scale = \"response\",\n             summary = F)\n\np |&gt; str()\n\n num [1:1000, 1:6] 4 2 1 4 7 5 6 5 7 4 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n - attr(*, \"levels\")= chr [1:7] \"1\" \"2\" \"3\" \"4\" ...\n\n\nThis time we have a simple two-dimensional array. There are only 1,000 rows because we set ndraws = 1000. Much like with fitted(), above, the six columns correspond to the six unique combinations of our three predictor variables. With the scale = \"response\" argument, we requested our results were in the metric of the original data, which were response values ranging from 1 to 7. Compared to the last plot, the post-predict() data wrangling for this triptych is low-key. We just need the data in a long tibble format that includes a variable with which we might facet.\n\np2 &lt;- p |&gt; \n  data.frame() |&gt; \n  set_names(pull(nd, combination)) |&gt; \n  pivot_longer(everything(),\n               names_to = c(\"action\", \"contact\", \"intention\"),\n               names_sep = \"_\",\n               values_to = \"response\") |&gt; \n  mutate(facet = factor(str_c(\"action=\", action, \", contact=\", contact),\n                        levels = levels)) |&gt; \n  \n  ggplot(aes(x = response, fill = intention)) +\n  geom_bar(position = position_dodge(width = 0.4), width = 1/3) +\n  scale_x_continuous(\"response\", breaks = 1:7) +\n  scale_fill_manual(values = canva_pal(\"Green fields\")(4)[2:1]) +\n  facet_wrap(~ facet) +\n  theme(legend.position = \"none\")\n\nFinally, we’re ready to combine our two triptychs into one glorious remake of Figure 12.6.\n\n(p1 / p2) & theme(panel.background = element_rect(fill = \"grey94\"))\n\n\n\n\n\n\n\n\nJust for kicks and giggles, I’d like to make an alternative version of Figure 12.6. The triptych on the top panels did a pretty good job depicting the model in terms of the thresholds, \\(\\alpha_k\\). It’s important that we, the data analysts, have a good sense of what those are. However, I suspect many of our substantively-oriented colleagues will find themselves confused by a plot like that. In my field, people generally just want to know What’s the mean for each group? At this point, you and I know that such a question is a bit impoverished compared the to the rich output from a model like this. But if we do want to boil these analyses down to comparisons of means, McElreath has already showed us how. Look back to R code 12.20 through 12.23 (pp. 386–387). In those blocks, we multiplied the vector of probability values (pk) by their respective response values and summed, which produced an average outcome value. We can use that same approach so that our top triptych might express the results of the model in terms of means rather than parameters. All it takes is a slightly amended wrangling workflow with respect to the f data.\n\np1 &lt;- f |&gt; \n  mutate(facet = factor(str_c(\"action=\", action, \", contact=\", contact),\n                        levels = levels)) |&gt; \n  group_by(draw, facet, intention) |&gt; \n  summarise(mean_response = sum(pk * response)) |&gt; \n  ungroup() |&gt; \n  nest(data = -draw) |&gt; \n  slice_sample(n = 50) |&gt;\n  unnest(data) |&gt;\n\n  ggplot(aes(x = intention, y = mean_response)) +\n  geom_line(aes(group = draw, color = mean_response),\n            alpha = 1/10) +\n  scale_x_continuous(\"intention\", breaks = 0:1) +\n  scale_y_continuous(\"resopnse\", breaks = 1:7, limits = c(1, 7)) +\n  scale_color_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                       high = canva_pal(\"Green fields\")(4)[1]) +\n  facet_wrap(~ facet) +\n  theme(legend.position = \"none\")\n\nI really like how intuitive the histograms were in McElreath’s bottom triptych. A limitation of that approach, however, is there is no direct expression of uncertainty. To address this, we might recall that the bars in the histogram are basically just reparameterizations of the pk values already in our f data and, happily, our vector of pk values already contains the uncertainty in the posterior. One way we might express that is with the tidybayes::stat_ccdfinterval() function, which will return a bar plot where the top parts of the bars depict our uncertainty in terms of cumulative density curves.\n\np2 &lt;- f |&gt; \n  mutate(facet = factor(str_c(\"action=\", action, \", contact=\", contact),\n                        levels = levels)) |&gt; \n  \n  ggplot(aes(x = response, y = pk, fill = factor(intention))) +\n  stat_ccdfinterval(.width = 0.95, justification = 1, size = 1/4, \n                    shape = 21, point_fill = canva_pal(\"Green fields\")(4)[3], point_size = 1/3,\n                    position = \"dodge\", width = 0.75) +\n  scale_x_continuous(\"resopnse\", breaks = 1:7) +\n  scale_y_continuous(\"count\", breaks = 0:3 / 10, labels = 0:3 * 100, limits = c(0, NA)) +\n  scale_fill_manual(values = canva_pal(\"Green fields\")(4)[2:1]) +\n  facet_wrap(~ facet) +\n  theme(legend.position = \"none\")\n\nHere’s our alternative plot.\n\np1 / p2\n\n\n\n\n\n\n\n\nI suspect our stat_ccdfinterval() approach would work better in plots with fewer bars. But hopefully it gives you some ideas.\n\n12.3.3.1 Rethinking: Staring into the abyss\n\nThe plotting code for ordered logistic models is complicated, compared to that of models from previous chapters. But as models become more monstrous, so too does the code needed to compute predictions and display them. With power comes hardship. It’s better to see the guts of the machine than to live in awe or fear of it. (p. 391)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Monsters and Mixtures</span>"
    ]
  },
  {
    "objectID": "12.html#sec-Ordered-categorical-predictors",
    "href": "12.html#sec-Ordered-categorical-predictors",
    "title": "12  Monsters and Mixtures",
    "section": "12.4 Ordered categorical predictors",
    "text": "12.4 Ordered categorical predictors\n\nWe can handle ordered outcome variables using a categorical model with a cumulative link. That was the previous section. What about ordered predictor variables? We could just include them as continuous predictors like in any linear model. But this isn’t ideal. Just like with ordered outcomes, we don’t really want to assume that the distance between each ordinal value is the same. Luckily, we don’t have to. (p. 391)\n\nHere are the eight levels of edu.\n\ndistinct(d, edu)\n\n                   edu\n1        Middle School\n2    Bachelor's Degree\n3         Some College\n4      Master's Degree\n5 High School Graduate\n6      Graduate Degree\n7     Some High School\n8    Elementary School\n\n\nMcElreath defined his edu_new variable with an impressively compact couple lines of code. I’m going to take a more explicit approach with the dplyr::recode() function.\n\nd &lt;- d |&gt; \n  mutate(edu_new = recode(\n    edu,\n    \"Elementary School\"    = 1,\n    \"Middle School\"        = 2,\n    \"Some High School\"     = 3,\n    \"High School Graduate\" = 4,\n    \"Some College\"         = 5, \n    \"Bachelor's Degree\"    = 6,\n    \"Master's Degree\"      = 7,\n    \"Graduate Degree\"      = 8) |&gt; \n      as.integer())\n\n# What did we do?\nd |&gt; \n  distinct(edu, edu_new) |&gt; \n  arrange(edu_new)\n\n                   edu edu_new\n1    Elementary School       1\n2        Middle School       2\n3     Some High School       3\n4 High School Graduate       4\n5         Some College       5\n6    Bachelor's Degree       6\n7      Master's Degree       7\n8      Graduate Degree       8\n\n\nThe prior often used to handle monotonic effects is the Dirichlet distribution. The Dirichlet distribution is the multivariate extension of the beta distribution, which we met back in Section 12.1.1. Here we follow McElreath’s R code 12.32 to simulate a few draws from the Dirichlet distribution.\n\nlibrary(gtools)\nset.seed(1805)\ndelta &lt;- rdirichlet(10, alpha = rep(2, 7)) \n\nstr(delta)\n\n num [1:10, 1:7] 0.1053 0.2504 0.1917 0.1241 0.0877 ...\n\n\nPlot delta with ggplot2.\n\ndelta |&gt; \n  data.frame() |&gt;\n  set_names(1:7) |&gt; \n  mutate(row = 1:n()) |&gt; \n  pivot_longer(-row, names_to = \"index\") |&gt; \n  \n  ggplot(aes(x = index, y = value, group = row,\n             alpha = row == 3, color = row == 3)) +\n  geom_line() +\n  geom_point() +\n  scale_alpha_manual(values = c(1/3, 1)) +\n  scale_color_manual(values = canva_pal(\"Green fields\")(4)[1:2]) +\n  ylab(\"probability\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe brms package has a rdirichlet() function, too. Here we use that to make an alternative version of the plot, above.\n\nset.seed(12)\n\nbrms::rdirichlet(n = 1e4, alpha = rep(2, 7)) |&gt; \n  data.frame() |&gt; \n  set_names(1:7) |&gt; \n  pivot_longer(everything()) |&gt; \n  mutate(name  = name |&gt; as.double(),\n         alpha = str_c(\"alpha[\", name, \"]\")) |&gt; \n  \n  ggplot(aes(x = value, color = name, group = name, fill= name)) + \n  geom_density(alpha = 0.8) + \n  scale_x_continuous(\"probability\", breaks = c(0, 0.5, 1),\n                     labels = c(0, 0.5, 1), limits = c(0, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  scale_fill_gradient(low = canva_pal(\"Green fields\")(4)[2],\n                      high = canva_pal(\"Green fields\")(4)[3]) +\n  scale_color_gradient(low = canva_pal(\"Green fields\")(4)[2],\n                       high = canva_pal(\"Green fields\")(4)[3]) +\n  labs(subtitle = expression(\"Dirichlet\"*(2*\", \"*2*\", \"*2*\", \"*2*\", \"*2*\", \"*2*\", \"*2))) +\n  facet_wrap(~ alpha, labeller = label_parsed, nrow = 2) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen using brms, the issue of treating a predictor in the way McElreath covered in this section is referred to as monotonic effects. Bürkner outlined the issue in his (2022c) vignette, Estimating monotonic effects with with brms, and in his (2020) article with Emmanuel Charpentier, Modelling monotonic effects of ordinal predictors in Bayesian regression models (click here for the freely-available preprint). From the introduction in Bürkner’s vignette, we read:\n\nFor a single monotonic predictor, \\(x\\), the linear predictor term of observation \\(n\\) looks as follows:\n\\[\\eta_n = bD \\sum_{i = 1}^{x_n} \\zeta_i\\]\nThe parameter \\(b\\) can take on any real value, while \\(\\zeta\\) is a simplex, which means that it satisfies \\(\\zeta_i \\in [0, 1]\\) and \\(\\sum_{i = 1}^D \\zeta_i = 1\\) with \\(D\\) being the number of elements of \\(\\zeta\\). Equivalently, \\(D\\) is the number of categories (or highest integer in the data) minus 1, since we start counting categories from zero to simplify the notation.\n\nIn this context, \\(n\\) indexes the observations in the hypothetical data and \\(\\eta\\) denotes the linear model for some outcome \\(y\\). Unlike with rethinking, the brms syntax for fitting models with monotonic predictors is fairly simple. Just place your monotonic predictors within the mo() function and enter them into the formula.\nIn the text (p. 394), McElreath remarked it took about 20 minutes to fit this model in his computer. Using my 2023 MacBook Pro with the M2 chip, it took me just about the same amount of time. If you’re following along on your computer, get ready to wait a while when fitting a model like this.\n\nb12.6 &lt;- brm(\n  data = d, \n  family = cumulative,\n  response ~ 1 + action + contact + intention + mo(edu_new),  # Note the `mo()` syntax\n  prior = c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 1), class = b),\n            # Note the new kinds of prior statements\n            prior(normal(0, 0.143), class = b, coef = moedu_new),\n            prior(dirichlet(2, 2, 2, 2, 2, 2, 2), class = simo, coef = moedu_new1)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 12,\n  file = \"fits/b12.06\")\n\nHere’s the summary.\n\nprint(b12.6)\n\n Family: cumulative \n  Links: mu = logit \nFormula: response ~ 1 + action + contact + intention + mo(edu_new) \n   Data: d (Number of observations: 9930) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept[1]    -3.14      0.17    -3.52    -2.86 1.00     1966     2021\nIntercept[2]    -2.46      0.16    -2.83    -2.18 1.00     1976     2075\nIntercept[3]    -1.87      0.16    -2.24    -1.60 1.00     1967     2104\nIntercept[4]    -0.85      0.16    -1.22    -0.58 1.00     1976     2122\nIntercept[5]    -0.18      0.16    -0.55     0.10 1.00     1957     2064\nIntercept[6]     0.72      0.16     0.36     1.00 1.00     1977     2004\naction          -0.71      0.04    -0.79    -0.63 1.00     4027     2725\ncontact         -0.96      0.05    -1.05    -0.86 1.00     4535     3268\nintention       -0.72      0.04    -0.79    -0.65 1.00     4372     2423\nmoedu_new       -0.05      0.03    -0.11    -0.01 1.00     1959     2056\n\nMonotonic Simplex Parameters:\n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmoedu_new1[1]     0.26      0.15     0.03     0.59 1.00     2369     2284\nmoedu_new1[2]     0.14      0.09     0.02     0.36 1.00     4760     2183\nmoedu_new1[3]     0.19      0.11     0.03     0.44 1.00     4318     2272\nmoedu_new1[4]     0.16      0.09     0.03     0.39 1.00     3964     2297\nmoedu_new1[5]     0.03      0.04     0.00     0.12 1.00     2967     2482\nmoedu_new1[6]     0.09      0.06     0.01     0.25 1.00     3208     2532\nmoedu_new1[7]     0.12      0.07     0.02     0.28 1.00     4002     2620\n\nFurther Distributional Parameters:\n     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ndisc     1.00      0.00     1.00     1.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf you compare our results to those in the text, you may be surprised by how small our summary values are for moedu_new. brms and rethinking have an important difference in how they parameterize \\(\\beta_\\text{Education}\\). From page 392 in the text, McElreath explained the\n\nsum of all the \\(\\delta\\) parameters is the maximum education effect. It will be very convenient for interpretation if we call this maximum sum an ordinary coefficient like \\(\\beta_\\text{E}\\) and then let the \\(\\delta\\) parameters be fractions of it. If we also make a dummy \\(\\delta_0 = 0\\) then we can write it all very compactly. Like this:\n\\[\\phi_i = \\beta_\\text{E} \\sum_{j = 0}^{\\text{E}_i - 1} \\delta_j + \\text{other stuff}\\]\nwhere \\(\\text{E}_i\\) is the completed education level of individual \\(i\\). Now the sum of every \\(\\delta_j\\) is 1, and we can interpret the maximum education effect by looking at \\(\\beta_\\text{E}\\).\n\nThe current version of brms takes expresses \\(\\beta_\\text{E}\\) as an average effect. From Bürkner & Charpentier (2020), we read:\n\nIf the monotonic effect is used in a linear model, \\(b\\) can be interpreted as the expected average difference between two adjacent categories of \\(x\\), while \\(\\zeta_i\\) describes the expected difference between the categories \\(i\\) and \\(i - 1\\) in the form of a proportion of the overall difference between lowest and highest categories. Thus, this parameterization has an intuitive interpretation while guaranteeing the monotonicity of the effect (p. 6)\n\nTo clarify, the \\(b\\) in this section is what we’re calling \\(\\beta_\\text{E}\\) in the current example and Bürkner and Charpentier used \\(\\zeta_i\\) in place of McElreath’s \\(\\delta_j\\). The upshot of all this is that if we’d like to compare the summary of our b12.6 to the results McElreath reported for his m12.6, we’ll need to multiply our moedu_new by 7.\n\nas_draws_df(b12.6) |&gt; \n  transmute(bE = bsp_moedu_new * 7) |&gt; \n  median_qi(.width = 0.89) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 1 × 6\n     bE .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 -0.37  -0.69  -0.12   0.89 median qi       \n\n\nThis parameterization difference between brms and rethinking is also the reason why we set prior(normal(0, 0.143), class = b, coef = moedu_new) within b12.6 whereas McElreath used a \\(\\operatorname{Normal}(0, 1)\\) prior for all his \\(\\beta\\) coefficients, including for his bE. Because our moedu_new (i.e., \\(\\beta_\\text{E}\\)) is parameterized as the average of seven \\(\\delta\\) parameters, it made sense to divide our hyperparameter for \\(\\sigma\\) by 7. That is, \\(1 / 7 \\approx 0.143\\).\nThis might be a good place to practice expressing our model in formal statistical notation. Here we’ll complement McElreath’s formula from page 392 by mixing in a little notation from Bürkner & Charpentier (2020):\n\\[\\begin{align*}\n\\text{response}_i & \\sim \\operatorname{Categorical} (\\mathbf p) \\\\\n\\operatorname{logit}(p_k) & = \\alpha_k - \\phi_i \\\\\n\\phi_i & = \\beta_1 \\text{action}_i + \\beta_2 \\text{contact}_i + \\beta_3 \\text{intention}_i + \\color{#524a3a}{\\beta_4 \\operatorname{mo}(\\text{edu\\_new}_i, \\boldsymbol{\\delta})} \\\\\n\\alpha_k & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\beta_1, \\dots, \\beta_3 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\color{#524a3a}{\\beta_4} & \\color{#524a3a}\\sim \\color{#524a3a}{\\operatorname{Normal}(0, 0.143)} \\\\\n\\color{#524a3a}{\\boldsymbol{\\delta}} & \\color{#524a3a}\\sim \\color{#524a3a}{\\operatorname{Dirichlet}(2, 2, 2, 2, 2, 2, 2)},\n\\end{align*}\\]\nwhere \\(\\operatorname{mo}(x, \\boldsymbol{\\delta})\\) is an operator indicating some predictor \\(x\\) is has undergone the monotonic transform and \\(\\boldsymbol{\\delta}\\) is our vector of simplex parameters \\(\\delta_1, \\dots, \\delta_7\\). That is, \\(\\beta_4 \\operatorname{mo}(\\text{edu\\_new}_i, \\boldsymbol{\\delta})\\) is our alternative brms-oriented way of expressing McElreath’s \\(\\beta_\\text{E} \\sum_{j = 0}^{\\text{E}_i - 1} \\delta_j\\). I don’t know that one is better. 🤷🏻 At first contact, I found them both confusing.\nSince this will be our only pairs plot for this chapter, let’s use GGally::ggpairs() to make it fancy. First we customize the panels.\n\nmy_lower &lt;- function(data, mapping, ...) {\n  \n  # Get the x and y data to use the other code\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n  \n  # Compute the correlations\n  corr &lt;- cor(x, y, method = \"p\", use = \"pairwise\")\n  abs_corr &lt;- abs(corr)\n  \n  # Plot the cor value\n  ggally_text(\n    label = formatC(corr, digits = 2, format = \"f\") |&gt; str_replace(\"0.\", \".\"),\n    mapping = aes(),\n    color = canva_pal(\"Green fields\")(4)[2],\n    size = 4) +\n    scale_x_continuous(NULL, breaks = NULL) +\n    scale_y_continuous(NULL, breaks = NULL)\n}\n\nmy_diag &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_density(fill = canva_pal(\"Green fields\")(4)[1], linewidth = 0) +\n    scale_x_continuous(NULL, breaks = NULL) +\n    scale_y_continuous(NULL, breaks = NULL)\n}\n\nmy_upper &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_hex(bins = 18) +\n    scale_x_continuous(NULL, breaks = NULL) +\n    scale_y_continuous(NULL, breaks = NULL) +\n    scale_fill_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                        high = canva_pal(\"Green fields\")(4)[3]) +\n    theme(panel.background = element_rect(fill = canva_pal(\"Green fields\")(4)[2]))\n}\n\nNow we are ready to make our custom version of the pairs plot in Figure 12.8.\n\nlibrary(GGally)\n\ndelta_labels &lt;- c(\"Elem\", \"MidSch\", \"SHS\", \"HSG\", \"SCol\", \"Bach\", \"Mast\", \"Grad\")\n\nas_draws_df(b12.6) |&gt; \n  select(contains(\"simo_moedu_new1\")) |&gt; \n  set_names(str_c(delta_labels[2:8], \"~(delta[\", 1:7, \"])\")) |&gt; \n  ggpairs(upper = list(continuous = my_upper),\n          diag = list(continuous = my_diag),\n          lower = list(continuous = my_lower),\n          labeller = label_parsed) +\n  theme(strip.text = element_text(size = 8))\n\n\n\n\n\n\n\n\nHere we add a normalized version of edu_new called edu_norm.\n\nd &lt;- d |&gt; \n  mutate(edu_norm = (edu_new - 1) / 7)\n\n# What does this look like?\nd |&gt; \n  distinct(edu, edu_new, edu_norm) |&gt; \n  arrange(edu_new)\n\n                   edu edu_new  edu_norm\n1    Elementary School       1 0.0000000\n2        Middle School       2 0.1428571\n3     Some High School       3 0.2857143\n4 High School Graduate       4 0.4285714\n5         Some College       5 0.5714286\n6    Bachelor's Degree       6 0.7142857\n7      Master's Degree       7 0.8571429\n8      Graduate Degree       8 1.0000000\n\n\nNow fit the more conventional model in which we treat edu_norm as a simple continuous predictor.\n\nb12.7 &lt;- brm(\n  data = d, \n  family = cumulative,\n  response ~ 1 + action + contact + intention + edu_norm,\n  prior = c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 1), class = b)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 12,\n  file = \"fits/b12.07\")\n\n\nfixef(b12.7)[7:10, ]\n\n            Estimate  Est.Error       Q2.5       Q97.5\naction    -0.7076040 0.04003921 -0.7834841 -0.63060464\ncontact   -0.9595411 0.04927724 -1.0570859 -0.86507226\nintention -0.7199648 0.03635679 -0.7942429 -0.64744431\nedu_norm  -0.1150526 0.09117114 -0.2922633  0.06046044\n\n\nIt might be nice to get a better sense of the two models with a plot. For our approach, we’ll use fitted(). Start with b12.6.\n\nnd &lt;- tibble(edu_new   = 1:8,\n             action    = 0,\n             contact   = 0,\n             intention = 0)\n\nf &lt;- fitted(b12.6, newdata = nd) \n\nf |&gt; str()\n\n num [1:8, 1:4, 1:7] 0.042 0.0466 0.0489 0.0523 0.0552 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ : NULL\n  ..$ : chr [1:4] \"Estimate\" \"Est.Error\" \"Q2.5\" \"Q97.5\"\n  ..$ : chr [1:7] \"P(Y = 1)\" \"P(Y = 2)\" \"P(Y = 3)\" \"P(Y = 4)\" ...\n\n\nThe rows correspond to the eight educational levels. The columns are the typical summary columns. The seven levels of the third dimension are the seven levels of response. Before we plot, we’re going to need to wrangle that a little and then do the same thing all over for b12.7.\n\n# b12.6\nf12.6 &lt;- rbind(f[, , 1],\n               f[, , 2],\n               f[, , 3],\n               f[, , 4],\n               f[, , 5],\n               f[, , 6],\n               f[, , 7]) |&gt; \n  data.frame() |&gt; \n  mutate(edu      = factor(rep(1:8, times = 7)),\n         response = rep(1:7, each = 8))\n\n# b12.7\nnd &lt;- nd |&gt; \n  mutate(edu_norm  = 1:8)\n\nf &lt;- fitted(b12.7, newdata = nd) \n\nf12.7 &lt;- rbind(f[, , 1],\n               f[, , 2],\n               f[, , 3],\n               f[, , 4],\n               f[, , 5],\n               f[, , 6],\n               f[, , 7]) |&gt; \n  data.frame() |&gt; \n  mutate(edu      = factor(rep(1:8, times = 7)),\n         response = rep(1:7, each = 8))\n\nNow combine the two data objects and plot.\n\n# This will help with `scale_color_manual()`\ncolors &lt;- scales::seq_gradient_pal(\n  canva_pal(\"Green fields\")(4)[4], \n  canva_pal(\"Green fields\")(4)[3])(seq(0, 1, length.out = 8))\n\nbind_rows(f12.6, f12.7) |&gt; \n  mutate(fit = rep(c(\"b12.6 with `mo()` syntax\", \"b12.7 with conventional syntax\"), \n                   each = n() / 2)) |&gt; \n  \n  ggplot(aes(x = response, y = Estimate,\n             ymin = Q2.5, ymax = Q97.5,\n             color = edu, group = edu)) +\n  geom_pointrange(position = position_dodge(width = 3/4), size = 1/4) + \n  scale_x_continuous(breaks = 1:7) +\n  scale_y_continuous(\"probability\", limits = c(0, 0.43)) +\n  scale_color_manual(\"education\", values = colors, labels = delta_labels) +\n  facet_wrap(~ fit)   +\n  theme(legend.background = element_blank(),\n        legend.position = \"right\")\n\n\n\n\n\n\n\n\nIn case you were curious, the PSIS-LOO suggests the monotonic model (b12.6) made better sense of the data.\n\nb12.6 &lt;- add_criterion(b12.6, criterion = \"loo\")\nb12.7 &lt;- add_criterion(b12.7, criterion = \"loo\")\n\nloo_compare(b12.6, b12.7, criterion = \"loo\") |&gt; print(simplify = F)\n\n      elpd_diff se_diff  elpd_loo se_elpd_loo p_loo    se_p_loo looic    se_looic\nb12.6      0.0       0.0 -18540.4     38.1        11.0      0.1  37080.9     76.2\nb12.7     -4.6       1.8 -18545.1     38.1         9.9      0.1  37090.1     76.2\n\nmodel_weights(b12.6, b12.7, weights = \"loo\") |&gt; round(digits = 2)\n\nb12.6 b12.7 \n 0.99  0.01 \n\n\nWe might explore the monotonic effects of b12.6 in one more way. If you were reading closely along in the text, you may have noticed that “the sum of every \\(\\delta_j\\) is 1” (p. 392). When using HMC, this is true for each posterior draw. We can exploit that information to visualize the \\(\\delta_j\\) parameters in a cumulative fashion.\n\nas_draws_df(b12.6) |&gt; \n  select(contains(\"new1\")) |&gt; \n  set_names(1:7) |&gt; \n  mutate(draw = 1:n(), \n         `0`  = 0) |&gt; \n  pivot_longer(-draw, names_to = \"delta\") |&gt; \n  arrange(delta) |&gt; \n  group_by(draw) |&gt; \n  mutate(cum_sum = cumsum(value)) |&gt; \n  \n  ggplot(aes(x = delta, y = cum_sum)) +\n  stat_pointinterval(.width = 0.95, size = 1,\n                     color = canva_pal(\"Green fields\")(4)[1]) +\n  stat_pointinterval(.width = 0.5,\n                     color = canva_pal(\"Green fields\")(4)[4],\n                     point_color = canva_pal(\"Green fields\")(4)[2]) +\n  scale_x_discrete(NULL, labels = parse(text = str_c(\"delta[\", 0:7 , \"]\"))) +\n  ylab(\"cumulative sum\")\n\n\n\n\n\n\n\n\nThis is another way to show that the largest effects of education are when going from Elementary School to Middle School (\\(\\delta_0 \\rightarrow \\delta_1\\)) and when going from Some High School to High School Graduate (\\(\\delta_2 \\rightarrow \\delta_3\\)).\n\nd |&gt; \n  distinct(edu, edu_new) |&gt; \n  arrange(edu_new) |&gt; \n  mutate(`delta[j]` = edu_new - 1)\n\n                   edu edu_new delta[j]\n1    Elementary School       1        0\n2        Middle School       2        1\n3     Some High School       3        2\n4 High School Graduate       4        3\n5         Some College       5        4\n6    Bachelor's Degree       6        5\n7      Master's Degree       7        6\n8      Graduate Degree       8        7",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Monsters and Mixtures</span>"
    ]
  },
  {
    "objectID": "12.html#summary",
    "href": "12.html#summary",
    "title": "12  Monsters and Mixtures",
    "section": "12.5 Summary",
    "text": "12.5 Summary\n“This chapter introduced several new types of regression, all of which are generalizations of generalized linear models (GLMs)” (p. 397). This chapter has been a ride. If you’d like more practice with the negative-binomial model and some of the others models for categorical data we covered in this chapter and in Chapter 11, Alan Agresti covered them extensively in his (2015) text, Foundations of linear and generalized linear models. For more on different kinds of zero-inflated count models, check out Atkins et al. (2013), A tutorial on count regression and zero-altered count models for longitudinal substance use data. If you’d like to learn more about using cumulative probabilities to model ordinal data in brms, check out Bürkner and Vuorre’s (2019) Ordinal regression models in psychology: A tutorial and its repository on the Open Science Framework (https://osf.io/cu8jv/). Also check out Chapter 23 of my (2026) ebook Doing Bayesian Data Analysis in brms and the tidyverse were we model ordinal data with a series of cumulative probit models.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Monsters and Mixtures</span>"
    ]
  },
  {
    "objectID": "12.html#session-info",
    "href": "12.html#session-info",
    "title": "12  Monsters and Mixtures",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] GGally_2.4.0    gtools_3.9.5    patchwork_1.3.2 invgamma_1.2    tidybayes_3.0.7 brms_2.23.0     Rcpp_1.1.0     \n [8] lubridate_1.9.4 forcats_1.0.1   stringr_1.6.0   dplyr_1.1.4     purrr_1.2.1     readr_2.1.5     tidyr_1.3.2    \n[15] tibble_3.3.1    ggplot2_4.0.1   tidyverse_2.0.0 ggthemes_5.1.0 \n\nloaded via a namespace (and not attached):\n [1] gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7             magrittr_2.0.4         \n [6] multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          loo_2.9.0.9000          callr_3.7.6            \n[11] vctrs_0.7.0             reshape2_1.4.5          pkgconfig_2.0.3         shape_1.4.6.1           arrayhelpers_1.1-0     \n[16] crayon_1.5.3            fastmap_1.2.0           backports_1.5.0         labeling_0.4.3          utf8_1.2.6             \n[21] cmdstanr_0.9.0          rmarkdown_2.30          BH_1.90.0-1             tzdb_0.5.0              ps_1.9.1               \n[26] xfun_0.55               jsonlite_2.0.0          parallel_4.5.1          R6_2.6.1                stringi_1.8.7          \n[31] RColorBrewer_1.1-3      StanHeaders_2.36.0.9000 estimability_1.5.1      assertthat_0.2.1        rstan_2.36.0.9000      \n[36] knitr_1.51              zoo_1.8-14              bayesplot_1.15.0.9000   Matrix_1.7-3            splines_4.5.1          \n[41] timechange_0.3.0        tidyselect_1.2.1        rstudioapi_0.17.1       abind_1.4-8             yaml_2.3.12            \n[46] codetools_0.2-20        rethinking_2.42         curl_7.0.0              processx_3.8.6          pkgbuild_1.4.8         \n[51] lattice_0.22-7          plyr_1.8.9              withr_3.0.2             bridgesampling_1.2-1    S7_0.2.1               \n[56] posterior_1.6.1.9000    coda_0.19-4.1           evaluate_1.0.5          survival_3.8-3          ggstats_0.11.0         \n[61] RcppParallel_5.1.11-1   ggdist_3.3.3            RcppEigen_0.3.4.0.2     pillar_1.11.1           tensorA_0.36.2.1       \n[66] checkmate_2.3.3         stats4_4.5.1            distributional_0.5.0    generics_0.1.4          hms_1.1.4              \n[71] rstantools_2.5.0.9000   scales_1.4.0            xtable_1.8-4            glue_1.8.0              emo_0.0.0.9000         \n[76] emmeans_1.11.2-8        tools_4.5.1             hexbin_1.28.5           mvtnorm_1.3-3           grid_4.5.1             \n[81] QuickJSR_1.8.1          nlme_3.1-168            cli_3.6.5               svUnit_1.0.8            Brobdingnag_1.2-9      \n[86] V8_8.0.1                gtable_0.3.6            digest_0.6.39           TH.data_1.1-4           htmlwidgets_1.6.4      \n[91] farver_2.1.2            htmltools_0.5.9         lifecycle_1.0.5         MASS_7.3-65",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Monsters and Mixtures</span>"
    ]
  },
  {
    "objectID": "12.html#comments",
    "href": "12.html#comments",
    "title": "12  Monsters and Mixtures",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nAgresti, A. (2015). Foundations of linear and generalized linear models. John Wiley & Sons. https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034\n\n\nAtkins, D. C., Baldwin, S. A., Zheng, C., Gallop, R. J., & Neighbors, C. (2013). A tutorial on count regression and zero-altered count models for longitudinal substance use data. Psychology of Addictive Behaviors : Journal of the Society of Psychologists in Addictive Behaviors, 27(1), 166–177. https://doi.org/10.1037/a0029508\n\n\nBürkner, P.-C. (2022a). Estimating distributional models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html\n\n\nBürkner, P.-C. (2022b). Define custom response distributions with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html\n\n\nBürkner, P.-C. (2022c). Estimating monotonic effects with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_monotonic.html\n\n\nBürkner, P.-C. (2022d). Parameterization of response distributions in brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html\n\n\nBürkner, P.-C., & Charpentier, E. (2020). Modelling monotonic effects of ordinal predictors in Bayesian regression models. British Journal of Mathematical and Statistical Psychology. https://doi.org/10.1111/bmsp.12195\n\n\nBürkner, P.-C., & Vuorre, M. (2019). Ordinal regression models in psychology: A tutorial. Advances in Methods and Practices in Psychological Science, 2(1), 77–101. https://doi.org/10.1177/2515245918823199\n\n\nCushman, F., Young, L., & Hauser, M. (2006). The role of conscious reasoning and intuition in moral judgment: Testing three principles of harm. Psychological Science, 17(12), 1082–1089. https://doi.org/10.1111/j.1467-9280.2006.01834.x\n\n\nHilbe, J. M. (2011). Negative binomial regression (Second Edition). https://doi.org/10.1017/CBO9780511973420\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKurz, A. S. (2026). Doing Bayesian data analysis in brms and the tidyverse (Version 1.3.0). https://solomon.quarto.pub/dbda2/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nStan Development Team. (2022). Stan functions reference, Version 2.31. https://mc-stan.org/docs/functions-reference/",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Monsters and Mixtures</span>"
    ]
  },
  {
    "objectID": "13.html",
    "href": "13.html",
    "title": "13  Models With Memory",
    "section": "",
    "text": "13.1 Example: Multilevel tadpoles\nThese benefits include:\nI’m totally on board with this. After learning about the multilevel model, I see it everywhere. For more on the sentiment it should be the default, check out McElreath’s blog post, Multilevel regression as default.\nLet’s load the reedfrogs data (see Vonesh & Bolker, 2005) and fire up brms.\nlibrary(brms)\ndata(reedfrogs, package = \"rethinking\")\nd &lt;- reedfrogs\nrm(reedfrogs)\nGo ahead and acquaint yourself with the reedfrogs.\nlibrary(tidyverse)\n\nd |&gt;\n  glimpse()\n\nRows: 48\nColumns: 5\n$ density  &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25,…\n$ pred     &lt;fct&gt; no, no, no, no, no, no, no, no, pred, pred, pred, pred, pred, pred, pred, pred, no, no, no, no, no, no, no, no,…\n$ size     &lt;fct&gt; big, big, big, big, small, small, small, small, big, big, big, big, small, small, small, small, big, big, big, …\n$ surv     &lt;int&gt; 9, 10, 7, 10, 9, 9, 10, 9, 4, 9, 7, 6, 7, 5, 9, 9, 24, 23, 22, 25, 23, 23, 23, 21, 6, 13, 4, 9, 13, 20, 8, 10, …\n$ propsurv &lt;dbl&gt; 0.9000000, 1.0000000, 0.7000000, 1.0000000, 0.9000000, 0.9000000, 1.0000000, 0.9000000, 0.4000000, 0.9000000, 0…\nMaking the tank cluster variable is easy.\nd &lt;- d |&gt;\n  mutate(tank = 1:nrow(d))\nHere’s the formula for the un-pooled model in which each tank gets its own intercept:\n\\[\\begin{align*}\n\\text{surv}_i             & \\sim \\operatorname{Binomial}(n_i, p_i) \\\\\n\\operatorname{logit}(p_i) & = \\alpha_{\\text{tank}[i]} \\\\\n\\alpha_j                  & \\sim \\operatorname{Normal} (0, 1.5) & \\text{for } j = 1, \\dots, 48,\n\\end{align*}\\]\nwhere \\(n_i\\) is indexed by the density column. Its values are distributed like so.\nd |&gt; \n  count(density)\n\n  density  n\n1      10 16\n2      25 16\n3      35 16\nNow fit this simple aggregated binomial model much like we practiced in Section 11.1.3.\nb13.1 &lt;- brm(\n  data = d, \n  family = binomial,\n  surv | trials(density) ~ 0 + factor(tank),\n  prior(normal(0, 1.5), class = b),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 13,\n  file = \"fits/b13.01\")\nWe don’t need a depth=2 argument to discover we have 48 different intercepts. The default print() behavior will do.\nprint(b13.1)\n\n Family: binomial \n  Links: mu = logit \nFormula: surv | trials(density) ~ 0 + factor(tank) \n   Data: d (Number of observations: 48) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nfactortank1      1.72      0.75     0.36     3.28 1.00     6727     2922\nfactortank2      2.39      0.87     0.85     4.24 1.00     5009     2249\nfactortank3      0.75      0.63    -0.45     2.06 1.00     5975     2721\nfactortank4      2.41      0.90     0.84     4.40 1.00     5413     2712\nfactortank5      1.72      0.78     0.34     3.36 1.00     6346     2332\nfactortank6      1.73      0.75     0.36     3.39 1.00     5673     2882\nfactortank7      2.40      0.87     0.87     4.24 1.00     6013     2773\nfactortank8      1.71      0.76     0.32     3.29 1.00     5652     2920\nfactortank9     -0.37      0.61    -1.60     0.80 1.00     5643     2887\nfactortank10     1.70      0.75     0.39     3.31 1.00     6262     2260\nfactortank11     0.74      0.63    -0.46     2.05 1.00     6269     2696\nfactortank12     0.39      0.63    -0.82     1.64 1.00     5644     2858\nfactortank13     0.76      0.66    -0.46     2.12 1.00     5430     2848\nfactortank14     0.01      0.61    -1.16     1.22 1.00     6416     3038\nfactortank15     1.72      0.76     0.34     3.34 1.00     5830     2712\nfactortank16     1.72      0.78     0.36     3.42 1.00     5907     2726\nfactortank17     2.54      0.67     1.36     4.01 1.00     4795     2497\nfactortank18     2.14      0.61     1.05     3.44 1.00     6054     2619\nfactortank19     1.80      0.54     0.83     2.92 1.00     6042     3107\nfactortank20     3.09      0.79     1.72     4.82 1.00     5402     2768\nfactortank21     2.15      0.62     1.05     3.49 1.00     5870     2806\nfactortank22     2.14      0.57     1.12     3.36 1.00     5687     2898\nfactortank23     2.13      0.59     1.10     3.41 1.00     5338     3008\nfactortank24     1.55      0.51     0.61     2.60 1.00     6469     3086\nfactortank25    -1.11      0.45    -2.04    -0.26 1.00     5811     2806\nfactortank26     0.08      0.38    -0.65     0.81 1.00     5386     3054\nfactortank27    -1.54      0.49    -2.54    -0.63 1.00     5931     2981\nfactortank28    -0.55      0.40    -1.36     0.20 1.00     5774     2862\nfactortank29     0.07      0.40    -0.72     0.84 1.00     5880     2852\nfactortank30     1.32      0.48     0.44     2.30 1.00     5203     2097\nfactortank31    -0.72      0.42    -1.55     0.09 1.00     7194     3016\nfactortank32    -0.39      0.42    -1.23     0.40 1.00     5849     2924\nfactortank33     2.85      0.67     1.71     4.33 1.00     5415     2328\nfactortank34     2.47      0.59     1.42     3.76 1.00     5835     2769\nfactortank35     2.46      0.57     1.46     3.68 1.00     5369     2660\nfactortank36     1.91      0.49     1.02     2.97 1.00     6166     2748\nfactortank37     1.91      0.49     1.00     2.93 1.00     6123     2860\nfactortank38     3.37      0.77     2.05     5.04 1.00     5313     2355\nfactortank39     2.46      0.58     1.43     3.72 1.00     6008     2835\nfactortank40     2.16      0.53     1.21     3.32 1.00     5945     2463\nfactortank41    -1.91      0.49    -2.95    -1.02 1.00     6293     2323\nfactortank42    -0.63      0.35    -1.32     0.04 1.00     6646     3016\nfactortank43    -0.51      0.34    -1.19     0.16 1.00     5326     3035\nfactortank44    -0.39      0.33    -1.05     0.24 1.00     6226     3190\nfactortank45     0.52      0.35    -0.15     1.22 1.00     7301     2643\nfactortank46    -0.63      0.35    -1.34     0.04 1.00     5798     2948\nfactortank47     1.91      0.49     1.03     2.91 1.00     5941     2893\nfactortank48    -0.07      0.34    -0.74     0.59 1.00     7503     2958\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nThis is much like the models we’ve fit in earlier chapters using McElreath’s index approach, but on steroids. It’ll be instructive to take a look at distribution of the \\(\\alpha_j\\) parameters in density plots. We’ll plot them in both their log-odds and probability metrics.\nFor kicks and giggles, let’s use a FiveThirtyEight-like theme for this chapter’s plots. An easy way to do so is with help from the ggthemes package.\nlibrary(ggthemes) \nlibrary(tidybayes)\n\n# Change the default\ntheme_set(theme_gray() + theme_fivethirtyeight())\n\ntibble(estimate = fixef(b13.1)[, 1]) |&gt; \n  mutate(p = plogis(estimate)) |&gt; \n  pivot_longer(estimate:p) |&gt; \n  mutate(name = if_else(name == \"p\", \"expected survival probability\", \"expected survival log-odds\")) |&gt; \n  \n  ggplot(aes(x = value, fill = name)) +\n  stat_dots(size = 0) +\n  scale_y_continuous(breaks = NULL) +\n  scale_fill_manual(values = c(\"orange1\", \"orange4\")) +\n  labs(title = \"Tank-level intercepts from the no-pooling model\",\n       subtitle = \"Notice how inspecting the distributions of the posterior means can offer insights you\\nmight not get if you looked at them one at a time.\") +\n  facet_wrap(~ name, scales = \"free_x\") +\n  theme(legend.position = \"none\",\n        panel.grid = element_blank())\nEven though it seems like we can derive important insights from how the tank-level intercepts are distributed, that information is not explicitly encoded in the statistical model. Keep that in mind as we now consider the multilevel alternative. Its formula is\n\\[\\begin{align*}\n\\text{surv}_i             & \\sim \\operatorname{Binomial}(n_i, p_i) \\\\\n\\operatorname{logit}(p_i) & = \\alpha_{\\text{tank}[i]} \\\\\n\\alpha_j                  & \\sim \\operatorname{Normal}(\\color{#CD8500}{\\bar \\alpha}, \\color{#CD8500} \\sigma) \\\\\n\\color{#CD8500}{\\bar \\alpha} & \\color{#CD8500} \\sim \\color{#CD8500}{\\operatorname{Normal}(0, 1.5)} \\\\\n\\color{#CD8500} \\sigma       & \\color{#CD8500} \\sim \\color{#CD8500}{\\operatorname{Exponential}(1)},\n\\end{align*}\\]\nwhere\nWith brms, you might specify the corresponding multilevel model like this.\nb13.2 &lt;- brm(\n  data = d, \n  family = binomial,\n  surv | trials(density) ~ 1 + (1 | tank),\n  prior = c(prior(normal(0, 1.5), class = Intercept),  # alpha bar\n            prior(exponential(1), class = sd)),        # sigma\n  iter = 5000, warmup = 1000, chains = 4, cores = 4,\n  sample_prior = \"yes\",\n  seed = 13,\n  file = \"fits/b13.02\")\nThe syntax for the varying effects follows the lme4 style, ( &lt;varying parameter(s)&gt; | &lt;grouping variable(s)&gt; ). In this case (1 | tank) indicates only the intercept, 1, varies by tank. The extent to which parameters vary is controlled by the prior, prior(exponential(1), class = sd), which is parameterized in the standard deviation metric. Do note that last part. It’s common in multilevel software to model in the variance metric, instead. For technical reasons we won’t really get into until Chapter 14, Stan parameterizes this as a standard deviation.\nLet’s compute the WAIC comparisons.\nb13.1 &lt;- add_criterion(b13.1, criterion = \"waic\")\nb13.2 &lt;- add_criterion(b13.2, criterion = \"waic\")\n\nw &lt;- loo_compare(b13.1, b13.2, criterion = \"waic\")\n\nprint(w, simplify = F)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb13.2    0.0       0.0  -100.1       3.7         20.9    0.8     200.1    7.4 \nb13.1   -6.9       1.8  -107.0       2.3         25.3    1.2     214.0    4.7\nThe se_diff is small relative to the elpd_diff. If we convert the \\(\\text{elpd}\\) difference to the WAIC metric, the message stays the same.\ncbind(waic_diff = w[, 1] * -2,\n      se        = w[, 2] *  2)\n\n      waic_diff       se\nb13.2   0.00000 0.000000\nb13.1  13.89515 3.601405\nHere are the WAIC weights.\nmodel_weights(b13.1, b13.2, weights = \"waic\") |&gt; \n  round(digits = 2)\n\nb13.1 b13.2 \n    0     1\nI’m not going to show it here, but if you’d like a challenge, try comparing the models with the PSIS-LOO. You’ll get some great practice with high pareto_k values and the moment matching for problematic observations (Paananen, Piironen, et al., 2020; see Paananen, Bürkner, et al., 2020).\nBut back on track, McElreath commented on the number of effective parameters for the two models. This, recall, is listed in the column for \\(p_\\text{WAIC}\\).\nw[, \"p_waic\"]\n\n   b13.2    b13.1 \n20.94163 25.26743\nAnd indeed, even though our multilevel model (b13.2) technically had two more parameters than the conventional single-level model (b13.1), its \\(p_\\text{WAIC}\\) is substantially smaller, due to the regularizing level-2 \\(\\sigma\\) parameter. Speaking of which, let’s examine the model summary.\nprint(b13.2)\n\n Family: binomial \n  Links: mu = logit \nFormula: surv | trials(density) ~ 1 + (1 | tank) \n   Data: d (Number of observations: 48) \n  Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;\n         total post-warmup draws = 16000\n\nMultilevel Hyperparameters:\n~tank (Number of levels: 48) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.62      0.21     1.26     2.08 1.00     4231     7747\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.35      0.26     0.85     1.87 1.00     2380     4074\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\nThis time we don’t get a list of 48 separate tank-level parameters. However, we do get a description of their distribution in terms of \\(\\bar \\alpha\\) (i.e., Intercept) and \\(\\sigma\\) (i.e., sd(Intercept)). If you’d like the actual tank-level parameters, don’t worry; they’re coming in Figure 13.1. We’ll need to do a little prep work, though.\npost &lt;- as_draws_df(b13.2)\n\npost_mdn &lt;- coef(b13.2, robust = T)$tank[, , ] |&gt; \n  data.frame() |&gt; \n  bind_cols(d) |&gt;\n  mutate(post_mdn = plogis(Estimate))\n\nhead(post_mdn)\n\n   Estimate Est.Error       Q2.5    Q97.5 density pred  size surv propsurv tank  post_mdn\n1 2.0845608 0.8526477  0.6141390 4.061139      10   no   big    9      0.9    1 0.8893935\n2 2.9897777 1.0718215  1.1857960 5.561650      10   no   big   10      1.0    2 0.9521102\n3 0.9711338 0.6601700 -0.2474613 2.384460      10   no   big    7      0.7    3 0.7253454\n4 2.9724097 1.0907821  1.1761693 5.561312      10   no   big   10      1.0    4 0.9513120\n5 2.0781692 0.8624828  0.5942470 4.053401      10   no small    9      0.9    5 0.8887632\n6 2.0778971 0.8486711  0.5925602 4.056686      10   no small    9      0.9    6 0.8887363\nHere’s the ggplot2 code to reproduce Figure 13.1.\npost_mdn |&gt;\n  ggplot(aes(x = tank)) +\n  geom_hline(yintercept = plogis(median(post$b_Intercept)), linetype = 2, linewidth = 1/4) +\n  geom_vline(xintercept = c(16.5, 32.5), linewidth = 1/4, color = \"grey25\") +\n  geom_point(aes(y = propsurv), color = \"orange2\") +\n  geom_point(aes(y = post_mdn), shape = 1) +\n  annotate(geom = \"text\", \n           x = c(8, 16 + 8, 32 + 8), y = 0, \n           label = c(\"small tanks\", \"medium tanks\", \"large tanks\")) +\n  scale_x_continuous(breaks = c(1, 16, 32, 48)) +\n  scale_y_continuous(breaks = 0:5 / 5, limits = c(0, 1)) +\n  labs(title = \"Multilevel shrinkage!\",\n       subtitle = \"The empirical proportions are in orange while the model-\\nimplied proportions are the black circles. The dashed line is\\nthe model-implied average survival proportion.\") +\n  theme(panel.grid.major = element_blank())\nHere is the code for our version of Figure 13.2.a, where we visualize the model-implied population distribution of log-odds survival (i.e., the population distribution yielding all the tank-level intercepts).\n# This makes the output of `slice_sample()` reproducible\nset.seed(13)\n\np1 &lt;- post |&gt; \n  slice_sample(n = 100) |&gt; \n  expand_grid(x = seq(from = -4, to = 5, length.out = 100)) |&gt;\n  mutate(density = dnorm(x, mean = b_Intercept, sd = sd_tank__Intercept)) |&gt; \n    \n  ggplot(aes(x = x, y = density, group = .draw)) +\n  geom_line(alpha = 0.2, color = \"orange2\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Population survival distribution\",\n       subtitle = \"log-odds scale\") +\n  coord_cartesian(xlim = c(-3, 4))\nNow we make our Figure 13.2.b and then bind the two subplots with patchwork.\nset.seed(13)\n\np2 &lt;- post |&gt; \n  slice_sample(n = 8000, replace = T) |&gt; \n  mutate(sim_tanks = rnorm(n(), mean = b_Intercept, sd = sd_tank__Intercept)) |&gt; \n  \n  ggplot(aes(x = plogis(sim_tanks))) +\n  geom_density(linewidth = 0, fill = \"orange2\", adjust = 0.1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Probability of survival\",\n       subtitle = \"transformed by the inverse-logit function\")\n\nlibrary(patchwork)\n\n(p1 + p2) &\n  theme(plot.title = element_text(size = 12),\n        plot.subtitle = element_text(size = 10))\nBoth plots show different ways of expressing the model uncertainty in terms of both location \\(\\alpha\\) and scale \\(\\sigma\\).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Models With Memory</span>"
    ]
  },
  {
    "objectID": "13.html#example-multilevel-tadpoles",
    "href": "13.html#example-multilevel-tadpoles",
    "title": "13  Models With Memory",
    "section": "",
    "text": "the prior for the tank intercepts is now a function of two parameters, \\(\\bar \\alpha\\) and \\(\\sigma\\). You can say \\(\\bar \\alpha\\) like “bar alpha.” The bar means average. These two parameters inside the prior is where the “multi” in multilevel arises. The Gaussian distribution with mean \\(\\bar \\alpha\\) standard deviation \\(\\sigma\\) is the prior for each tank’s intercept. But that prior itself has priors for \\(\\bar \\alpha\\) and \\(\\sigma\\). So there are two levels in the model, each resembling a simpler model. (p. 403, emphasis in the original)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13.1.0.1 Rethinking: Varying intercepts as over-dispersion\n\nIn the previous chapter (Section 12.1), the beta-binomial and gamma-Poisson models were presented as ways for coping with over-dispersion of count data. Varying intercepts accomplish the same thing, allowing count outcomes to be over-dispersed. They accomplish this, because when each observed count gets its own unique intercept, but these intercepts are pooled through a common distribution, the predictions expect over-dispersion just like a beta-binomial or gamma-Poisson model would. Multilevel models are also mixtures. Compared to a beta-binomial or gamma-Poisson model, a binomial or Poisson model with a varying intercept on every observed outcome will often be easier to estimate and easier to extend. (p. 407, emphasis in the original)\n\n\n\n13.1.0.2 Overthinking: Prior for variance components\nYep, you can use the half-Normal distribution for your priors in brms, too. Here it is for model b13.2.\n\nb13.2b &lt;- update(\n  b13.2,\n  prior = c(prior(normal(0, 1.5), class = Intercept),\n            prior(normal(0, 1), class = sd)),\n  iter = 5000, warmup = 1000, chains = 4, cores = 4,\n  sample_prior = \"yes\",\n  seed = 13,\n  file = \"fits/b13.02b\")\n\nMcElreath mentioned how one might set a lower bound at zero for the half-Normal prior when using rethinking::ulam(). There’s no need to do so when using brms::brm(). The lower bounds for priors of class = sd are already set to zero by default.\nCheck the model summary.\n\nprint(b13.2b)\n\n Family: binomial \n  Links: mu = logit \nFormula: surv | trials(density) ~ 1 + (1 | tank) \n   Data: d (Number of observations: 48) \n  Draws: 4 chains, each with iter = 5000; warmup = 1000; thin = 1;\n         total post-warmup draws = 16000\n\nMultilevel Hyperparameters:\n~tank (Number of levels: 48) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.59      0.20     1.24     2.03 1.00     4568     8722\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.35      0.25     0.87     1.84 1.00     3673     6410\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf you’re curious how the exponential and half-Normal priors compare to one another and to their posteriors, you might just plot.\n\n# For annotation\ntext &lt;- tibble(\n  value        = c(0.5, 2.4),\n  density      = c(1, 1.85),\n  distribution = factor(c(\"prior\", \"posterior\"), levels = c(\"prior\", \"posterior\")),\n  prior        = \"Exponential(1)\")\n\n# Gather up and wrangle the prior and posterior draws\ntibble(`prior_Exponential(1)`        = prior_draws(b13.2)  |&gt; pull(sd_tank),\n       `posterior_Exponential(1)`    = as_draws_df(b13.2)  |&gt; pull(sd_tank__Intercept),\n       `prior_Half-Normal(0, 1)`     = prior_draws(b13.2b) |&gt; pull(sd_tank),\n       `posterior_Half-Normal(0, 1)` = as_draws_df(b13.2b) |&gt; pull(sd_tank__Intercept)) |&gt; \n  pivot_longer(everything(),\n               names_sep = \"_\",\n               names_to = c(\"distribution\", \"prior\")) |&gt; \n  mutate(distribution = factor(distribution, levels = c(\"prior\", \"posterior\"))) |&gt; \n  \n  # Plot\n  ggplot(aes(x = value, fill = distribution)) +\n  geom_density(linewidth = 0, alpha = 2/3, adjust = 0.25) +\n  geom_text(data = text,\n            aes(y = density, label = distribution, color = distribution)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  scale_fill_manual(NULL, values = c(\"orange4\", \"orange2\")) +\n  scale_color_manual(NULL, values = c(\"orange4\", \"orange2\")) +\n  labs(subtitle = expression(Hierarchical~sigma~parameter)) +\n  coord_cartesian(xlim = c(0, 4)) +\n  facet_wrap(~ prior) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nBy the way, this is why we set iter = 5000 and sample_prior = \"yes\" for the last two models. Neither were necessary to fit the models, but both helped us out with this plot.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Models With Memory</span>"
    ]
  },
  {
    "objectID": "13.html#varying-effects-and-the-underfittingoverfitting-trade-off",
    "href": "13.html#varying-effects-and-the-underfittingoverfitting-trade-off",
    "title": "13  Models With Memory",
    "section": "13.2 Varying effects and the underfitting/overfitting trade-off",
    "text": "13.2 Varying effects and the underfitting/overfitting trade-off\n\nVarying intercepts are just regularized estimates, but adaptively regularized by estimating how diverse the clusters are while estimating the features of each cluster. This fact is not easy to grasp….\nA major benefit of using varying effects estimates, instead of the empirical raw estimates, is that they provide more accurate estimates of the individual cluster (tank) intercepts. On average, the varying effects actually provide a better estimate of the individual tank (cluster) means. The reason that the varying intercepts provide better estimates is that they do a better job of trading off underfitting and overfitting. (p. 408)\n\nIn this section, we explicate this by contrasting three perspectives:\n\ncomplete pooling (i.e., a single-\\(\\alpha\\) model),\nno pooling (i.e., the single-level \\(\\alpha_{\\text{tank}[i]}\\) model), and\npartial pooling [i.e., the multilevel model for which \\(\\alpha_j \\sim \\operatorname{Normal} (\\bar \\alpha, \\sigma)\\)].\n\n\nTo demonstrate [the magic of the multilevel model], we’ll simulate some tadpole data. That way, we’ll know the true per-pond survival probabilities. Then we can compare the no-pooling estimates to the partial pooling estimates, by computing how close each gets to the true values they are trying to estimate. The rest of this section shows how to do such a simulation. (p. 409)\n\n\n13.2.1 The model\nThe simulation formula should look familiar.\n\\[\\begin{align*}\n\\text{surv}_i & \\sim \\operatorname{Binomial}(n_i, p_i) \\\\\n\\operatorname{logit}(p_i) & = \\alpha_{\\text{pond}[i]} \\\\\n\\alpha_j                  & \\sim \\operatorname{Normal}(\\bar \\alpha, \\sigma) \\\\\n\\bar \\alpha               & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\sigma                    & \\sim \\operatorname{Exponential}(1)\n\\end{align*}\\]\n\n\n13.2.2 Assign values to the parameters\nHere we follow along with McElreath and “assign specific values representative of the actual tadpole data” (p. 409). Because he included a set.seed() line in his R code 13.8, our results should match his exactly.\n\na_bar   &lt;-  1.5\nsigma   &lt;-  1.5\nn_ponds &lt;- 60\n\nset.seed(5005)\n\ndsim &lt;- tibble(\n  pond   = 1:n_ponds,\n  ni     = rep(c(5, 10, 25, 35), each = n_ponds / 4) |&gt; as.integer(),\n  true_a = rnorm(n = n_ponds, mean = a_bar, sd = sigma))\n\nhead(dsim)\n\n# A tibble: 6 × 3\n   pond    ni true_a\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     1     5  0.567\n2     2     5  1.99 \n3     3     5 -0.138\n4     4     5  1.86 \n5     5     5  3.91 \n6     6     5  1.95 \n\n\nMcElreath twice urged us to inspect the contents of this simulation. In addition to looking at the data with head(), we might well plot.\n\ndsim |&gt; \n  mutate(ni = factor(ni)) |&gt; \n  \n  ggplot(aes(x = true_a, y = ni)) +\n  stat_dotsinterval(fill = \"orange2\", slab_size = 0, .width = 0.5) +\n  ggtitle(\"Log-odds varying by # tadpoles per pond\") +\n  theme(plot.title = element_text(size = 14))\n\n\n\n\n\n\n\n\n\n\n13.2.3 Sumulate survivors\n\nEach pond \\(i\\) has \\(n_i\\) potential survivors, and nature flips each tadpole’s coin, so to speak, with probability of survival \\(p_i\\). This probability \\(p_i\\) is implied by the model definition, and is equal to:\n\\[p_i = \\frac{\\exp (\\alpha_i)}{1 + \\exp (\\alpha_i)}\\]\nThe model uses a logit link, and so the probability is defined by the [plogis()] function. (p. 411)\n\nAlthough McElreath shared his set.seed() number in the last section, he didn’t share it for this bit. We’ll go ahead and carry over the one from last time. However, in a moment we’ll see this clearly wasn’t the one he used here. As a consequence, our results will deviate a bit from his.\n\nset.seed(5005)\n\ndsim &lt;- dsim |&gt; \n  mutate(si = rbinom(n = n(), prob = plogis(true_a), size = ni))\n\ndsim\n\n# A tibble: 60 × 4\n    pond    ni true_a    si\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;\n 1     1     5  0.567     4\n 2     2     5  1.99      4\n 3     3     5 -0.138     3\n 4     4     5  1.86      5\n 5     5     5  3.91      5\n 6     6     5  1.95      4\n 7     7     5  1.49      4\n 8     8     5  2.52      4\n 9     9     5  2.18      3\n10    10     5  2.05      4\n# ℹ 50 more rows\n\n\n\n\n13.2.4 Compute the no-pooling estimates\nThe no-pooling estimates (i.e., \\(\\alpha_{\\text{tank}[i]}\\)) are the results of simple algebra.\n\ndsim &lt;- dsim |&gt;\n  mutate(p_nopool = si / ni)\n\ndsim\n\n# A tibble: 60 × 5\n    pond    ni true_a    si p_nopool\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n 1     1     5  0.567     4      0.8\n 2     2     5  1.99      4      0.8\n 3     3     5 -0.138     3      0.6\n 4     4     5  1.86      5      1  \n 5     5     5  3.91      5      1  \n 6     6     5  1.95      4      0.8\n 7     7     5  1.49      4      0.8\n 8     8     5  2.52      4      0.8\n 9     9     5  2.18      3      0.6\n10    10     5  2.05      4      0.8\n# ℹ 50 more rows\n\n\n“These are the same no-pooling estimates you’d get by fitting a model with a dummy variable for each pond and flat priors that induce no regularization” (p. 411). That is, these are the same kinds of estimates we got back when we fit b13.1.\n\n\n13.2.5 Compute the partial-pooling estimates\nFit the multilevel (partial-pooling) model.\n\nb13.3 &lt;- brm(\n  data = dsim, \n  family = binomial,\n  si | trials(ni) ~ 1 + (1 | pond),\n  prior = c(prior(normal(0, 1.5), class = Intercept),\n            prior(exponential(1), class = sd)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 13,\n  file = \"fits/b13.03\")\n\nHere’s our standard brms summary.\n\nprint(b13.3)\n\n Family: binomial \n  Links: mu = logit \nFormula: si | trials(ni) ~ 1 + (1 | pond) \n   Data: dsim (Number of observations: 60) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~pond (Number of levels: 60) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.50      0.20     1.15     1.93 1.00     1401     2469\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.48      0.23     1.05     1.95 1.00     1076     2151\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nI’m not aware that you can use McElreath’s depth=2 trick in brms for summary() or print(). However, you can get most of that information and more with the Stan-like summary using the $fit syntax.\n\nb13.3$fit\n\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n                        mean se_mean   sd    2.5%     25%     50%     75%   97.5% n_eff Rhat\nb_Intercept             1.48    0.01 0.23    1.05    1.32    1.47    1.63    1.95  1076 1.00\nsd_pond__Intercept      1.50    0.01 0.20    1.15    1.36    1.48    1.62    1.93  1357 1.01\nIntercept               1.48    0.01 0.23    1.05    1.32    1.47    1.63    1.95  1076 1.00\nr_pond[1,Intercept]     0.09    0.01 0.94   -1.66   -0.54    0.05    0.71    2.05  6629 1.00\nr_pond[2,Intercept]     0.09    0.01 0.94   -1.60   -0.56    0.03    0.70    2.07  7378 1.00\nr_pond[3,Intercept]    -0.70    0.01 0.88   -2.39   -1.27   -0.73   -0.14    1.12  7412 1.00\nr_pond[4,Intercept]     1.13    0.01 1.09   -0.79    0.35    1.07    1.81    3.54  6255 1.00\nr_pond[5,Intercept]     1.14    0.01 1.15   -0.86    0.36    1.03    1.86    3.66  7031 1.00\nr_pond[6,Intercept]     0.08    0.01 0.95   -1.65   -0.57    0.05    0.66    2.10  6449 1.00\nr_pond[7,Intercept]     0.11    0.01 0.97   -1.68   -0.55    0.06    0.71    2.16  5907 1.00\nr_pond[8,Intercept]     0.09    0.01 0.95   -1.61   -0.58    0.06    0.73    1.99  5613 1.00\nr_pond[9,Intercept]    -0.68    0.01 0.86   -2.32   -1.24   -0.70   -0.13    1.12  5743 1.00\nr_pond[10,Intercept]    0.09    0.01 0.94   -1.63   -0.55    0.06    0.67    2.12  7382 1.00\nr_pond[11,Intercept]    1.13    0.01 1.15   -0.86    0.31    1.03    1.85    3.59  5931 1.00\nr_pond[12,Intercept]   -1.36    0.01 0.84   -3.01   -1.91   -1.37   -0.82    0.27  7206 1.00\nr_pond[13,Intercept]    1.14    0.01 1.14   -0.86    0.35    1.06    1.89    3.47  6121 1.00\nr_pond[14,Intercept]    0.08    0.01 0.93   -1.62   -0.56    0.03    0.66    2.01  6706 1.00\nr_pond[15,Intercept]    1.12    0.01 1.14   -0.91    0.31    1.07    1.85    3.51  7435 1.00\nr_pond[16,Intercept]   -0.85    0.01 0.64   -2.05   -1.29   -0.87   -0.43    0.47  4031 1.00\nr_pond[17,Intercept]   -1.97    0.01 0.66   -3.37   -2.40   -1.95   -1.53   -0.68  5017 1.00\nr_pond[18,Intercept]   -1.23    0.01 0.64   -2.47   -1.66   -1.22   -0.79    0.06  4629 1.00\nr_pond[19,Intercept]   -1.24    0.01 0.66   -2.56   -1.68   -1.24   -0.79    0.05  5152 1.00\nr_pond[20,Intercept]   -0.43    0.01 0.70   -1.77   -0.91   -0.45    0.00    1.01  5574 1.00\nr_pond[21,Intercept]   -1.59    0.01 0.63   -2.84   -2.01   -1.58   -1.17   -0.34  4659 1.00\nr_pond[22,Intercept]    0.66    0.01 0.86   -0.88    0.07    0.60    1.18    2.55  5591 1.00\nr_pond[23,Intercept]    1.54    0.01 1.08   -0.38    0.77    1.44    2.20    4.02  5819 1.00\nr_pond[24,Intercept]   -0.85    0.01 0.65   -2.09   -1.29   -0.87   -0.44    0.48  4425 1.00\nr_pond[25,Intercept]    0.03    0.01 0.73   -1.27   -0.46    0.01    0.49    1.58  5744 1.00\nr_pond[26,Intercept]    0.66    0.01 0.86   -0.90    0.07    0.62    1.21    2.48  6059 1.00\nr_pond[27,Intercept]    0.04    0.01 0.77   -1.39   -0.48    0.01    0.52    1.65  5419 1.00\nr_pond[28,Intercept]   -0.44    0.01 0.70   -1.72   -0.93   -0.46    0.03    0.96  5242 1.00\nr_pond[29,Intercept]   -0.84    0.01 0.64   -2.08   -1.28   -0.86   -0.43    0.44  5404 1.00\nr_pond[30,Intercept]   -0.43    0.01 0.71   -1.75   -0.91   -0.44    0.04    0.97  4563 1.00\nr_pond[31,Intercept]    1.41    0.01 0.82   -0.02    0.83    1.36    1.90    3.22  5761 1.00\nr_pond[32,Intercept]    0.54    0.01 0.61   -0.59    0.12    0.51    0.91    1.80  4727 1.00\nr_pond[33,Intercept]    1.40    0.01 0.75    0.05    0.86    1.35    1.89    3.00  4635 1.00\nr_pond[34,Intercept]   -0.45    0.01 0.50   -1.43   -0.78   -0.45   -0.12    0.54  3287 1.00\nr_pond[35,Intercept]   -0.98    0.01 0.46   -1.87   -1.28   -0.97   -0.68   -0.08  2724 1.00\nr_pond[36,Intercept]    2.12    0.01 0.99    0.47    1.42    2.03    2.70    4.34  4609 1.00\nr_pond[37,Intercept]   -3.38    0.01 0.61   -4.68   -3.75   -3.34   -2.96   -2.28  3598 1.00\nr_pond[38,Intercept]   -2.08    0.01 0.47   -3.03   -2.38   -2.08   -1.76   -1.14  3120 1.00\nr_pond[39,Intercept]   -0.98    0.01 0.47   -1.87   -1.30   -0.98   -0.67   -0.03  3074 1.00\nr_pond[40,Intercept]    2.09    0.01 0.94    0.47    1.46    2.00    2.67    4.14  5770 1.00\nr_pond[41,Intercept]    2.10    0.01 0.95    0.51    1.43    2.00    2.68    4.20  4919 1.00\nr_pond[42,Intercept]    0.54    0.01 0.61   -0.55    0.11    0.50    0.93    1.84  3953 1.00\nr_pond[43,Intercept]   -1.75    0.01 0.46   -2.66   -2.06   -1.74   -1.45   -0.88  2981 1.00\nr_pond[44,Intercept]   -0.63    0.01 0.47   -1.52   -0.97   -0.64   -0.31    0.31  2922 1.00\nr_pond[45,Intercept]   -2.63    0.01 0.51   -3.69   -2.95   -2.60   -2.29   -1.70  3416 1.00\nr_pond[46,Intercept]   -1.44    0.01 0.40   -2.23   -1.71   -1.44   -1.18   -0.66  2546 1.00\nr_pond[47,Intercept]    2.32    0.01 0.92    0.75    1.69    2.23    2.88    4.42  4424 1.00\nr_pond[48,Intercept]    2.33    0.01 0.92    0.77    1.71    2.24    2.87    4.39  4950 1.00\nr_pond[49,Intercept]    0.14    0.01 0.50   -0.81   -0.20    0.13    0.46    1.14  3564 1.00\nr_pond[50,Intercept]    0.14    0.01 0.49   -0.78   -0.19    0.12    0.45    1.14  3190 1.00\nr_pond[51,Intercept]    0.14    0.01 0.48   -0.76   -0.18    0.14    0.47    1.11  3041 1.00\nr_pond[52,Intercept]   -1.45    0.01 0.40   -2.24   -1.72   -1.45   -1.18   -0.67  2401 1.00\nr_pond[53,Intercept]    0.14    0.01 0.48   -0.77   -0.18    0.12    0.46    1.12  3074 1.00\nr_pond[54,Intercept]    2.31    0.01 0.90    0.77    1.69    2.23    2.84    4.32  5291 1.00\nr_pond[55,Intercept]   -0.88    0.01 0.42   -1.68   -1.16   -0.88   -0.61   -0.07  2819 1.00\nr_pond[56,Intercept]    1.69    0.01 0.76    0.31    1.17    1.63    2.15    3.28  4509 1.00\nr_pond[57,Intercept]   -0.77    0.01 0.43   -1.61   -1.05   -0.77   -0.48    0.08  2568 1.00\nr_pond[58,Intercept]    1.20    0.01 0.64    0.07    0.74    1.18    1.62    2.57  5191 1.00\nr_pond[59,Intercept]    0.35    0.01 0.50   -0.60    0.01    0.33    0.68    1.35  3766 1.00\nr_pond[60,Intercept]    1.21    0.01 0.65    0.03    0.75    1.17    1.63    2.60  4938 1.00\nlprior                 -3.32    0.01 0.28   -3.93   -3.48   -3.30   -3.13   -2.85  1361 1.00\nlp__                 -185.98    0.25 7.55 -202.04 -190.92 -185.50 -180.69 -172.22   908 1.00\n\nSamples were drawn using NUTS(diag_e) at Fri Jan  9 13:10:10 2026.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nAs an aside, notice how this summary still reports the old-style n_eff values, rather than the updated Bulk_ESS and Tail_ESS values. I suspect this will change sometime soon. In the meantime, here’s a thread on the Stan Forums featuring members of the Stan team discussing how.\nLet’s get ready for the diagnostic plot of Figure 13.3. First we add the partially-pooled estimates, as summarized by their posterior means, to the dsim data. Then we compute error values.\n\n# We could have included this step in the block of code below, if we wanted to\np_partpool &lt;- coef(b13.3)$pond[, , ] |&gt; \n  data.frame() |&gt;\n  transmute(p_partpool = plogis(Estimate))\n\ndsim &lt;- dsim |&gt;\n  bind_cols(p_partpool) |&gt; \n  mutate(p_true = plogis(true_a)) |&gt;\n  mutate(nopool_error   = abs(p_nopool   - p_true),\n         partpool_error = abs(p_partpool - p_true)) |&gt; \n  mutate(ni_factor = factor(ni, \n                            levels = c(5, 10, 25, 35),\n                            labels = c(\"tiny (5)\", \"small (10)\", \"medium (25)\", \"large (35)\")))\n\ndsim |&gt; \n  glimpse()\n\nRows: 60\nColumns: 10\n$ pond           &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29…\n$ ni             &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, …\n$ true_a         &lt;dbl&gt; 0.56673123, 1.99002317, -0.13775688, 1.85676651, 3.91208800, 1.95414869, 1.48963805, 2.52407196, 2.178280…\n$ si             &lt;int&gt; 4, 4, 3, 5, 5, 4, 4, 4, 3, 4, 5, 2, 5, 4, 5, 6, 3, 5, 5, 7, 4, 9, 10, 6, 8, 9, 8, 7, 6, 7, 24, 22, 24, 18…\n$ p_nopool       &lt;dbl&gt; 0.80, 0.80, 0.60, 1.00, 1.00, 0.80, 0.80, 0.80, 0.60, 0.80, 1.00, 0.40, 1.00, 0.80, 1.00, 0.60, 0.30, 0.5…\n$ p_partpool     &lt;dbl&gt; 0.8270110, 0.8277081, 0.6861836, 0.9314136, 0.9317488, 0.8261827, 0.8303546, 0.8279099, 0.6888539, 0.8274…\n$ p_true         &lt;dbl&gt; 0.6380086, 0.8797456, 0.4656151, 0.8649196, 0.9803934, 0.8758983, 0.8160239, 0.9258122, 0.8982820, 0.8857…\n$ nopool_error   &lt;dbl&gt; 0.161991419, 0.079745589, 0.134384860, 0.135080387, 0.019606594, 0.075898310, 0.016023939, 0.125812219, 0…\n$ partpool_error &lt;dbl&gt; 0.189002439, 0.052037481, 0.220568419, 0.066494026, 0.048644576, 0.049715648, 0.014330707, 0.097902333, 0…\n$ ni_factor      &lt;fct&gt; tiny (5), tiny (5), tiny (5), tiny (5), tiny (5), tiny (5), tiny (5), tiny (5), tiny (5), tiny (5), tiny …\n\n\nHere is our code for Figure 13.3. The extra data processing for dfline is how we get the values necessary for the horizontal summary lines.\n\ndfline &lt;- dsim |&gt;\n  select(ni_factor, nopool_error:partpool_error) |&gt;\n  pivot_longer(nopool_error:partpool_error) |&gt;\n  group_by(name, ni_factor) |&gt;\n  summarise(mean_error = mean(value)) |&gt;\n  mutate(x    = c( 1, 16, 31, 46),\n         xend = c(15, 30, 45, 60))\n  \ndsim |&gt; \n  ggplot(aes(x = pond)) +\n  geom_point(aes(y = nopool_error), color = \"orange2\") +\n  geom_point(aes(y = partpool_error), shape = 1) +\n  geom_segment(data = dfline, \n               aes(x = x, xend = xend, \n                   y = mean_error, yend = mean_error),\n               color = rep(c(\"orange2\", \"black\"), each = 4),\n               linetype = rep(1:2, each = 4)) +\n  scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60)) +\n  labs(y = \"absolute error\",\n       title = \"Estimate error by model type\",\n       subtitle = \"The horizontal axis displays pond number. The vertical axis measures\\nthe absolute error in the predicted proportion of survivors, compared to\\nthe true value used in the simulation. The higher the point, the worse\\nthe estimate. No-pooling shown in orange. Partial pooling shown in black.\\nThe orange and dashed black lines show the average error for each kind\\nof estimate, across each initial density of tadpoles (pond size).\") +\n  facet_wrap(~ ni_factor, nrow = 1, scales = \"free_x\") +\n  theme(panel.grid.major = element_blank(),\n        plot.subtitle = element_text(size = 10))\n\n\n\n\n\n\n\n\nIf you wanted to quantify the difference in simple summaries, you might execute something like this.\n\ndsim |&gt;\n  select(ni, nopool_error:partpool_error) |&gt;\n  pivot_longer(-ni) |&gt;\n  group_by(name) |&gt;\n  summarise(mean_error   = mean(value) |&gt; round(digits = 3),\n            median_error = median(value) |&gt; round(digits = 3))\n\n# A tibble: 2 × 3\n  name           mean_error median_error\n  &lt;chr&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n1 nopool_error        0.059        0.042\n2 partpool_error      0.054        0.033\n\n\nAlthough many years of work in statistics have shown that partially pooled estimates are better, on average, this is not always the case. Our results are an example of this. McElreath addressed this directly:\n\nBut there are some cases in which the no-pooling estimates are better. These exceptions often result from ponds with extreme probabilities of survival. The partial pooling estimates shrink such extreme ponds towards the mean, because few ponds exhibit such extreme behavior. But sometimes outliers really are outliers. (p. 414)\n\nI originally learned about the multilevel in order to work with longitudinal data. In that context, I found the basic principles of a multilevel structure quite intuitive. The concept of partial pooling, however, took me some time to wrap my head around. If you’re struggling with this, be patient and keep chipping away.\nWhen McElreath lectured on this topic in 2015, he traced partial pooling to statistician Charles M. Stein. Efron and Morris (1977) wrote the now classic paper, Stein’s paradox in statistics, which does a nice job breaking down why partial pooling can be so powerful. One of the primary examples they used in the paper was of 1970 batting average data. If you’d like more practice seeing how partial pooling works–or if you just like baseball–, check out my blog post, Stein’s paradox and what partial pooling can do for you.\n\n13.2.5.1 Overthinking: Repeating the pond simulation\nWithin the brms workflow, we can reuse a compiled model with update(). But first, we’ll simulate new data.\n\na_bar   &lt;- 1.5\nsigma   &lt;- 1.5\nn_ponds &lt;- 60\n\nset.seed(1999)  # For new data, set a new seed\n\nnew_dsim &lt;- tibble(\n  pond   = 1:n_ponds,\n  ni     = rep(c(5, 10, 25, 35), each = n_ponds / 4) |&gt; as.integer(),\n  true_a = rnorm(n = n_ponds, mean = a_bar, sd = sigma)) |&gt; \n  mutate(si = rbinom(n = n(), prob = plogis(true_a), size = ni)) |&gt; \n  mutate(p_nopool = si / ni)\n\nglimpse(new_dsim)\n\nRows: 60\nColumns: 5\n$ pond     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, …\n$ ni       &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 25, 25…\n$ true_a   &lt;dbl&gt; 2.5990087, 1.4432554, 3.3045137, 3.7047030, 1.7005354, 2.2797409, 0.6759270, -0.2784119, -0.2209196, 3.2411130,…\n$ si       &lt;int&gt; 4, 4, 5, 4, 4, 4, 2, 4, 3, 5, 4, 5, 2, 2, 5, 10, 8, 10, 10, 9, 10, 9, 5, 10, 10, 6, 7, 7, 8, 6, 10, 22, 5, 25, …\n$ p_nopool &lt;dbl&gt; 0.80, 0.80, 1.00, 0.80, 0.80, 0.80, 0.40, 0.80, 0.60, 1.00, 0.80, 1.00, 0.40, 0.40, 1.00, 1.00, 0.80, 1.00, 1.0…\n\n\nFit the new model.\n\nb13.3_new &lt;- update(\n  b13.3,\n  newdata = new_dsim,\n  chains = 4, cores = 4,\n  seed = 13,\n  file = \"fits/b13.03_new\")\n\n\nprint(b13.3_new)\n\n Family: binomial \n  Links: mu = logit \nFormula: si | trials(ni) ~ 1 + (1 | pond) \n   Data: new_dsim (Number of observations: 60) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~pond (Number of levels: 60) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.33      0.19     1.00     1.75 1.00     1383     2118\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.64      0.21     1.25     2.06 1.00     1601     2273\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWhy not plot the first simulation versus the second one?\n\nbind_rows(as_draws_df(b13.3),\n          as_draws_df(b13.3_new)) |&gt;\n  mutate(model = rep(c(\"b13.3\", \"b13.3_new\"), each = n() / 2)) |&gt; \n  ggplot(aes(x = b_Intercept, y = sd_pond__Intercept)) +\n  stat_density_2d(geom = \"raster\", \n                  aes(fill = after_stat(density)), \n                  contour = F, n = 200) +\n  geom_vline(xintercept = a_bar, color = \"orange3\", linetype = 3) +\n  geom_hline(yintercept = sigma, color = \"orange3\", linetype = 3) +\n  scale_fill_gradient(low = \"grey25\", high = \"orange3\") +\n  ggtitle(\"Our simulation posteriors contrast a bit\",\n          subtitle = expression(alpha*\" is on the x and \"*sigma*\" is on the y, both in log-odds. The dotted lines intersect at the true values.\")) +\n  coord_cartesian(xlim = c(0.7, 2),\n                  ylim = c(0.8, 1.9)) +\n  facet_wrap(~ model, ncol = 2) +\n  theme(legend.position = \"none\",\n        panel.grid.major = element_blank())\n\n\n\n\n\n\n\n\nIf you’d like the stanfit portion of your brm() object, subset with $fit. Take b13.3, for example. You might check out its structure via b13.3$fit |&gt; str(). Here’s the actual Stan code.\n\nb13.3$fit@stanmodel\n\nS4 class stanmodel 'anon_model' coded as follows:\n// generated with brms 2.23.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  array[N] int Y;  // response variable\n  array[N] int trials;  // number of trials\n  // data for group-level effects of ID 1\n  int&lt;lower=1&gt; N_1;  // number of grouping levels\n  int&lt;lower=1&gt; M_1;  // number of coefficients per level\n  array[N] int&lt;lower=1&gt; J_1;  // grouping indicator per observation\n  // group-level predictor values\n  vector[N] Z_1_1;\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n}\nparameters {\n  real Intercept;  // temporary intercept for centered predictors\n  vector&lt;lower=0&gt;[M_1] sd_1;  // group-level standard deviations\n  array[M_1] vector[N_1] z_1;  // standardized group-level effects\n}\ntransformed parameters {\n  vector[N_1] r_1_1;  // actual group-level effects\n  // prior contributions to the log posterior\n  real lprior = 0;\n  r_1_1 = (sd_1[1] * (z_1[1]));\n  lprior += normal_lpdf(Intercept | 0, 1.5);\n  lprior += exponential_lpdf(sd_1 | 1);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    // initialize linear predictor term\n    vector[N] mu = rep_vector(0.0, N);\n    mu += Intercept;\n    for (n in 1:N) {\n      // add more terms to the linear predictor\n      mu[n] += r_1_1[J_1[n]] * Z_1_1[n];\n    }\n    target += binomial_logit_lpmf(Y | trials, mu);\n  }\n  // priors including constants\n  target += lprior;\n  target += std_normal_lpdf(z_1[1]);\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept;\n} \n\n\nFor more on how to fit this model with rstan, go to my companion book here.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Models With Memory</span>"
    ]
  },
  {
    "objectID": "13.html#sec-More-than-one-type-of-cluster",
    "href": "13.html#sec-More-than-one-type-of-cluster",
    "title": "13  Models With Memory",
    "section": "13.3 More than one type of cluster",
    "text": "13.3 More than one type of cluster\n“We can use and often should use more than one type of cluster in the same model” (p. 415).\n\n13.3.0.1 Rethinking: Cross-classification and hierarchy\n\nThe kind of data structure in data(chimpanzees) is usually called a cross-classified multilevel model. It is cross-classified, because actors are not nested within unique blocks. If each chimpanzee had instead done all of his or her pulls on a single day, within a single block, then the data structure would instead be hierarchical. However, the model specification would typically be the same. So the model structure and code you’ll see below will apply both to cross-classified designs and hierarchical designs. (p. 415, emphasis in the original)\n\n\n\n13.3.1 Multilevel chimpanzees\nThe initial multilevel update from model b11.4 from Section 11.1.1 follows the statistical formula\n\\[\\begin{align*}\n\\text{left\\_pull}_i         & \\sim \\operatorname{Binomial}(n_i = 1, p_i) \\\\\n\\operatorname{logit} (p_i) & = \\alpha_{\\text{actor}[i]}  + \\color{#CD8500}{\\gamma_{\\text{block}[i]}} + \\beta_{\\text{treatment}[i]} \\\\\n\\beta_j  & \\sim \\operatorname{Normal}(0, 0.5) \\;\\;\\; , \\text{for } j = 1, \\dots, 4 \\\\\n\\alpha_j & \\sim \\operatorname{Normal}(\\bar \\alpha, \\sigma_\\alpha) \\;\\;\\; , \\text{for } j = 1, \\dots, 7 \\\\\n\\color{#CD8500}{\\gamma_j} & \\color{#CD8500} \\sim \\color{#CD8500}{\\operatorname{Normal}(0, \\sigma_\\gamma) \\;\\;\\; , \\text{for } j = 1, \\dots, 6} \\\\\n\\bar \\alpha   & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\sigma_\\alpha & \\sim \\operatorname{Exponential}(1) \\\\\n\\color{#CD8500}{\\sigma_\\gamma} & \\color{#CD8500} \\sim \\color{#CD8500}{\\operatorname{Exponential}(1)}.\n\\end{align*}\\]\n⚠️ WARNING ⚠️\nI am so sorry, but we are about to head straight into a load of confusion. If you follow along linearly in the text, we won’t have the language to parse this all out until Section 13.4. In short, our difficulties will have to do with what are called the centered and the non-centered parameterizations for multilevel models. For the next several models in the text, McElreath used the centered parameterization. As we’ll learn in Section 13.4, this often causes problems when you use Stan to fit your multilevel models. Happily, the solution to those problems is often the non-centered parameterization, which is well known among the Stan team. This issue is so well known, in fact, that Bürkner only supports the non-centered parameterization with brms (see here). To my knowledge, there is no easy way around this. In the long run, this is a good thing. Your brms models will likely avoid some of the problems McElreath highlighted in this part of the text. In the short term, this also means that our results will not completely match up with those in the text. If you really want to reproduce McElreath’s models m13.4 through m13.6, you’ll have to fit them with the rethinking package or directly in Stan. Our models b13.4 through b13.6 will be the non-centered brms alternatives. Either way, the models make the same predictions, but the nuts and bolts and gears we’ll use to construct our multilevel golems will look a little different. With all that in mind, here’s how we might express our statistical model using the non-centered parameterization more faithful to the way it will be expressed with brms::brm():\n\\[\\begin{align*}\n\\text{left\\_pull}_i         & \\sim \\operatorname{Binomial}(n_i = 1, p_i) \\\\\n\\operatorname{logit} (p_i) & = \\bar \\alpha + \\beta_{\\text{treatment}[i]} + \\color{#CD8500}{z_{\\text{actor}[i]} \\sigma_\\alpha + x_{\\text{block}[i]} \\sigma_\\gamma} \\\\\n\\bar \\alpha   & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\beta_j       & \\sim \\operatorname{Normal}(0, 0.5) \\;\\;\\; , \\text{for } j = 1, \\dots, 4 \\\\\n\\color{#CD8500}{z_j} & \\color{#CD8500}\\sim \\color{#CD8500}{\\operatorname{Normal}(0, 1)} \\\\\n\\color{#CD8500}{x_j} & \\color{#CD8500}\\sim \\color{#CD8500}{\\operatorname{Normal}(0, 1)} \\\\\n\\sigma_\\alpha & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_\\gamma & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nIf you jump ahead to Section 13.4.2, you’ll see this is just re-write of the formula on the top of page 424. For now, let’s load the data.\n\ndata(chimpanzees, package = \"rethinking\")\nd &lt;- chimpanzees\nrm(chimpanzees)\n\nWrangle and view.\n\nd &lt;- d |&gt; \n  mutate(actor     = factor(actor),\n         block     = factor(block),\n         treatment = factor(1 + prosoc_left + 2 * condition))\n\nglimpse(d)\n\nRows: 504\nColumns: 9\n$ actor        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ recipient    &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ condition    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ block        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,…\n$ trial        &lt;int&gt; 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56,…\n$ prosoc_left  &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,…\n$ chose_prosoc &lt;int&gt; 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ pulled_left  &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ treatment    &lt;fct&gt; 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1,…\n\n\nEven when using the non-centered parameterization, McElreath’s m13.4 is a bit of an odd model to translate into brms syntax. To my knowledge, it can’t be done with conventional syntax. But we can fit the model with careful use of the non-linear syntax, which might look like this.\n\nb13.4 &lt;- brm(\n  data = d, \n  family = binomial,\n  bf(pulled_left | trials(1) ~ a + b,\n     a ~ 1 + (1 | actor) + (1 | block), \n     b ~ 0 + treatment,\n     nl = TRUE),\n  prior = c(prior(normal(0, 0.5), nlpar = b),\n            prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = a),\n            prior(exponential(1), class = sd, group = actor, nlpar = a),\n            prior(exponential(1), class = sd, group = block, nlpar = a)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 13,\n  file = \"fits/b13.04\")\n\nThe b ~ 0 + treatment part of the formula is our expression of what we wrote above as \\(\\beta_{\\text{treatment}[i]}\\). There’s a lot going on with the a ~ 1 + (1 | actor) + (1 | block) part of the formula. The initial 1 outside of the parenthesis is \\(\\bar \\alpha\\). The (1 | actor) and (1 | block) parts correspond to \\(z_{\\text{actor}[i]} \\sigma_\\alpha\\) and \\(x_{\\text{block}[i]} \\sigma_\\gamma\\), respectively.\nCheck the trace plots.\n\nlibrary(bayesplot)\n\ncolor_scheme_set(\"orange\")\n\nas_draws_df(b13.4) |&gt; \n  mcmc_trace(pars = vars(b_a_Intercept:`r_block__a[6,Intercept]`),\n             facet_args = list(ncol = 4), \n             linewidth = 0.15) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThey all look fine. In the text (e.g., page 416), McElreath briefly mentioned warnings about divergent transitions. We didn’t get any warnings like that. Keep following along and you’ll soon learn why.\nHere’s a look at the summary when using print().\n\nprint(b13.4)\n\n Family: binomial \n  Links: mu = logit \nFormula: pulled_left | trials(1) ~ a + b \n         a ~ 1 + (1 | actor) + (1 | block)\n         b ~ 0 + treatment\n   Data: d (Number of observations: 504) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~actor (Number of levels: 7) \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(a_Intercept)     2.02      0.65     1.08     3.61 1.00     1396     2247\n\n~block (Number of levels: 6) \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(a_Intercept)     0.21      0.18     0.01     0.66 1.00     1700     1525\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_Intercept      0.66      0.73    -0.80     2.15 1.01     1044     1758\nb_treatment1    -0.13      0.30    -0.69     0.45 1.00     2548     3040\nb_treatment2     0.40      0.30    -0.16     0.97 1.00     2740     2522\nb_treatment3    -0.48      0.30    -1.05     0.11 1.00     2723     2820\nb_treatment4     0.28      0.30    -0.29     0.86 1.00     2711     2626\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWhen you use the (1 | &lt;group&gt;) syntax within brm(), the group-specific parameters are not shown with print(). You only get the hierarchical \\(\\sigma_\\text{&lt;group&gt;}\\) summaries, shown here as the two rows for sd(a_Intercept). However, you can get a summary of all the parameters with the posterior_summary() function.\n\nposterior_summary(b13.4) |&gt; round(digits = 2)\n\n                        Estimate Est.Error    Q2.5   Q97.5\nb_a_Intercept               0.66      0.73   -0.80    2.15\nb_b_treatment1             -0.13      0.30   -0.69    0.45\nb_b_treatment2              0.40      0.30   -0.16    0.97\nb_b_treatment3             -0.48      0.30   -1.05    0.11\nb_b_treatment4              0.28      0.30   -0.29    0.86\nsd_actor__a_Intercept       2.02      0.65    1.08    3.61\nsd_block__a_Intercept       0.21      0.18    0.01    0.66\nr_actor__a[1,Intercept]    -1.02      0.74   -2.51    0.40\nr_actor__a[2,Intercept]     4.05      1.37    1.95    7.44\nr_actor__a[3,Intercept]    -1.31      0.74   -2.82    0.17\nr_actor__a[4,Intercept]    -1.32      0.74   -2.83    0.17\nr_actor__a[5,Intercept]    -1.01      0.75   -2.55    0.49\nr_actor__a[6,Intercept]    -0.07      0.74   -1.58    1.38\nr_actor__a[7,Intercept]     1.45      0.78   -0.08    2.96\nr_block__a[1,Intercept]    -0.17      0.22   -0.73    0.13\nr_block__a[2,Intercept]     0.04      0.18   -0.30    0.45\nr_block__a[3,Intercept]     0.05      0.18   -0.28    0.48\nr_block__a[4,Intercept]     0.01      0.19   -0.37    0.42\nr_block__a[5,Intercept]    -0.03      0.18   -0.43    0.33\nr_block__a[6,Intercept]     0.11      0.20   -0.18    0.58\nlprior                     -6.34      1.18   -9.19   -4.54\nlp__                     -286.68      3.71 -294.62 -280.28\n\n\nWe might make the coefficient plot of Figure 13.4.a with bayesplot::mcmc_plot().\n\nb13.4 |&gt; \n  mcmc_plot(variable = c(\"^r_\", \"^b_\", \"^sd_\"), regex = T) +\n  theme(axis.text.y = element_text(hjust = 0))\n\n\n\n\n\n\n\n\nFor a little more control, we might switch to a tidybayes-oriented approach.\n\n# Extract the posterior draws\npost &lt;- as_draws_df(b13.4) \n\n# Rhis is all stylistic fluff\nlevels &lt;- c(\n  \"sd_block__a_Intercept\", \"sd_actor__a_Intercept\", \n  \"b_a_Intercept\", \n  str_c(\"r_block__a[\", 6:1, \",Intercept]\"), \n  str_c(\"r_actor__a[\", 7:1, \",Intercept]\"), \n  str_c(\"b_b_treatment\", 4:1))\n\ntext &lt;- tibble(\n  x     = posterior_summary(b13.4, probs = c(0.055, 0.955))[\"r_actor__a[2,Intercept]\", c(3, 1)],\n  y     = c(13.5, 16.5),\n  label = c(\"89% CI\", \"mean\"),\n  hjust = c(0.5, 0))\n\narrow &lt;- tibble(\n  x    = posterior_summary(b13.4, probs = c(0.055, 0.955))[\"r_actor__a[2,Intercept]\", c(3, 1)] + c(-0.3, 0.2),\n  xend = posterior_summary(b13.4, probs = c(0.055, 0.955))[\"r_actor__a[2,Intercept]\", c(3, 1)],\n  y    = c(14, 16),\n  yend = c(14.8, 15.35))\n\n# Here's the main event\npost |&gt; \n  pivot_longer(b_a_Intercept:`r_block__a[6,Intercept]`)|&gt; \n  mutate(name = factor(name, levels = levels)) |&gt; \n  \n  ggplot(aes(x = value, y = name)) +\n  stat_pointinterval(point_interval = mean_qi, .width = 0.89, \n                     point_fill = \"blue\", point_size = 2, shape = 21, size = 1) +\n  geom_text(data = text,\n            aes(x = x, y = y, label = label, hjust = hjust)) +\n  geom_segment(data = arrow,\n               aes(x = x, xend = xend,\n                   y = y, yend = yend),\n               arrow = arrow(length = unit(0.15, \"cm\"))) +\n  theme(axis.text.y = element_text(hjust = 0),\n        panel.grid.major.y = element_line(linetype = 3))\n\n\n\n\n\n\n\n\nRegardless of whether we use a bayesplot- or tidybayes-oriented workflow, a careful look at our coefficient plots will show the parameters are a little different from those McElreath reported. Again, this is because of the subtle differences between our non-centered parameterization and McElreath’s centered parameterization. This will all make more sense in Section 13.4.\nNow use post to compare the group-level \\(\\sigma\\) parameters as in Figure 13.4.b.\n\npost |&gt;\n  pivot_longer(starts_with(\"sd\")) |&gt; \n  \n  ggplot(aes(x = value, fill = name)) +\n  geom_density(adjust = 2/3, alpha = 3/4, linewidth = 0, show.legend = F) +\n  annotate(geom = \"text\", x = 0.67, y = 2, color = \"orange4\", label = \"block\") +\n  annotate(geom = \"text\", x = 2.725, y = 0.5, color = \"orange1\", label = \"actor\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  scale_fill_manual(values = str_c(\"orange\", c(1, 4))) +\n  ggtitle(expression(sigma[\"&lt;group&gt;\"])) +\n  coord_cartesian(xlim = c(0, 4))\n\n\n\n\n\n\n\n\nSince both the coefficient plots and the density plots indicate there is much more variability among the actor parameters than in the block parameters, we might fit a model that ignores the variation among the levels of block.\n\nb13.5 &lt;- brm(\n  data = d, \n  family = binomial,\n  bf(pulled_left | trials(1) ~ a + b,\n     a ~ 1 + (1 | actor), \n     b ~ 0 + treatment,\n     nl = TRUE),\n  prior = c(prior(normal(0, 0.5), nlpar = b),\n            prior(normal(0, 1.5), class = b, coef = Intercept, nlpar = a),\n            prior(exponential(1), class = sd, group = actor, nlpar = a)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 13,\n  file = \"fits/b13.05\")\n\nWe might compare our models by their WAIC estimates.\n\nb13.4 &lt;- add_criterion(b13.4, criterion = \"waic\")\nb13.5 &lt;- add_criterion(b13.5, criterion = \"waic\")\n\nloo_compare(b13.4, b13.5, criterion = \"waic\") |&gt; \n  print(simplify = F)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb13.5    0.0       0.0  -265.7       9.6          8.6    0.4     531.4   19.2 \nb13.4   -0.3       0.8  -266.0       9.7         10.5    0.5     532.0   19.3 \n\nmodel_weights(b13.4, b13.5, weights = \"waic\") |&gt; \n  round(digits = 2)\n\nb13.4 b13.5 \n 0.43  0.57 \n\n\nThe two models yield nearly-equivalent WAIC estimates. Just as in the text, our p_waic column shows the models differ by about 2 effective parameters due to the shrinkage from the multilevel partial pooling. Yet recall what McElreath wrote:\n\nThere is nothing to gain here by selecting either model. The comparison of the two models tells a richer story… Since this is an experiment, there is nothing to really select. The experimental design tells us the relevant causal model to inspect. (pp. 418–419)\n\n\n\n13.3.2 Even more clusters\nWe can extend partial pooling to the treatment conditions, too. With brms, it will be more natural to revert to the conventional formula syntax.\n\nb13.6 &lt;- brm(\n  data = d,\n  family = binomial,\n  pulled_left | trials(1) ~ 1 + (1 | actor) + (1 | block) + (1 | treatment),\n  prior = c(prior(normal(0, 1.5), class = Intercept),\n            prior(exponential(1), class = sd)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 13,\n  file = \"fits/b13.06\")\n\nRecall that with brms, we don’t have a coeftab() like with McElreath’s rethinking. For us, one approach would be to compare the relevent rows from fixef(b13.4) to the relevant elements from ranef(b13.6).\n\ntibble(parameter = str_c(\"b[\", 1:4, \"]\"),\n       `b13.4`   = fixef(b13.4)[2:5, 1],\n       `b13.6`   = ranef(b13.6)$treatment[, 1, \"Intercept\"]) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 4 × 3\n  parameter b13.4 b13.6\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 b[1]      -0.13 -0.12\n2 b[2]       0.4   0.38\n3 b[3]      -0.48 -0.45\n4 b[4]       0.28  0.27\n\n\nLike in the text, “these are not identical, but they are very close” (p. 419). We might compare the group-level \\(\\sigma\\) parameters with a plot.\n\nas_draws_df(b13.6) |&gt; \n  pivot_longer(starts_with(\"sd\")) |&gt; \n  mutate(group = str_remove(name, \"sd_\") |&gt; str_remove(\"__Intercept\")) |&gt; \n  mutate(parameter = str_c(\"sigma[\", group,\"]\")) |&gt; \n  \n  ggplot(aes(x = value, y = parameter)) +\n  stat_halfeye(.width = 0.95,\n               adjust = 0.1, fill = \"orange\", size = 1) +\n  scale_y_discrete(labels = ggplot2:::parse_safe, expand = expansion(add = 0.1)) +\n  labs(subtitle = \"The variation among treatment levels is small, but the\\nvariation among the levels of block is still the smallest.\") +\n  theme(axis.text.y = element_text(hjust = 0))\n\n\n\n\n\n\n\n\nAmong the three \\(\\sigma_\\text{&lt;group&gt;}\\) parameters, \\(\\sigma_\\text{block}\\) is the smallest. Now we’ll compare b13.6 to the last two models with the WAIC.\n\nb13.6 &lt;- add_criterion(b13.6, criterion = \"waic\")\n\nloo_compare(b13.4, b13.5, b13.6, criterion = \"waic\") |&gt; \n  print(simplify = F)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nb13.5    0.0       0.0  -265.7       9.6          8.6    0.4     531.4   19.2 \nb13.4   -0.3       0.8  -266.0       9.7         10.5    0.5     532.0   19.3 \nb13.6   -0.8       0.8  -266.4       9.6         10.8    0.5     532.9   19.3 \n\nmodel_weights(b13.4, b13.5, b13.6, weights = \"loo\") |&gt; \n  round(digits = 2)\n\nb13.4 b13.5 b13.6 \n 0.33  0.45  0.21 \n\n\nThe models show little difference “on purely predictive criteria. This is the typical result, when each cluster (each treatment here) has a lot of data to inform its parameters” (p. 419). Unlike in the text, we didn’t have a problem with divergent transitions. We’ll see why in the next section.\nBefore we move on, this section just hints at a historical software difficulty. In short, it’s not uncommon to have a theory-based model that includes multiple sources of clustering (i.e., requiring many ( &lt;varying parameter(s)&gt; | &lt;grouping variable(s)&gt; ) parts in the model formula). This can make for all kinds of computational difficulties and result in software error messages, inadmissible solutions, and so on. One of the practical solutions to difficulties like these has been to simplify the statistical models by removing some of the clustering terms. Even though such simpler models were not the theory-based ones, at least they yielded solutions. Nowadays, Stan (via brms or otherwise) is making it easier to fit the full theoretically-based model. To learn more about this topic, check out this nice blog post by Michael Frank, Mixed effects models: Is it time to go Bayesian by default?. Make sure to check out the discussion in the comments section, which includes all-stars like Bürkner and Douglas Bates. You can get more context for the issue from Barr et al. (2013), Random effects structure for confirmatory hypothesis testing: Keep it maximal.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Models With Memory</span>"
    ]
  },
  {
    "objectID": "13.html#sec-Divergent-transitions-and-non-centered-priors",
    "href": "13.html#sec-Divergent-transitions-and-non-centered-priors",
    "title": "13  Models With Memory",
    "section": "13.4 Divergent transitions and non-centered priors",
    "text": "13.4 Divergent transitions and non-centered priors\nAlthough we did not get divergent transitions warnings in from our last few models the way McElreath did with his, the issues is still relevant for brms.\n\nOne of the best things about Hamiltonian Monte Carlo is that it provides internal checks of efficiency and accuracy. One of these checks comes free, arising from the constraints on the physics simulation. Recall that HMC simulates the frictionless flow of a particle on a surface. In any given transition, which is just a single flick of the particle, the total energy at the start should be equal to the total energy at the end. That’s how energy in a closed system works. And in a purely mathematical system, the energy is always conserved correctly. It’s just a fact about the physics.\nBut in a numerical system, it might not be. Sometimes the total energy is not the same at the end as it was at the start. In these cases, the energy is divergent. How can this happen? It tends to happen when the posterior distribution is very steep in some region of parameter space. Steep changes in probability are hard for a discrete physics simulation to follow. When that happens, the algorithm notices by comparing the energy at the start to the energy at the end. When they don’t match, it indicates numerical problems exploring that part of the posterior distribution.\nDivergent transitions are rejected. They don’t directly damage your approximation of the posterior distribution. But they do hurt it indirectly, because the region where divergent transitions happen is hard to explore correctly. (p. 420, emphasis in the original)\n\nTwo primary ways to handle divergent transitions are by increasing the adapt_delta parameter, which we’ve already done a few times in previous chapters, or reparameterizing the model. As McElreath will cover in a bit, switching from the centered to the non-centered parameterization will often work when using multilevel models.\n\n13.4.1 The Devil’s Funnel\nMcElreath posed a joint distribution\n\\[\\begin{align*}\nv & \\sim \\operatorname{Normal}(0, 3) \\\\\nx & \\sim \\operatorname{Normal}(0, \\exp(v)),\n\\end{align*}\\]\nwhere the scale of \\(x\\) depends on another variable, \\(v\\). In R code 13.26, McElreath then proposed fitting the following model with rethinking::ulam().\n\nm13.7 &lt;- ulam(\n  data = list(N = 1),\n  alist(\n    v ~ normal(0, 3),\n    x ~ normal(0, exp(v))\n  ), \n  chains = 4 \n)\n\nI’m not aware that you can do something like this with brms. If you think I’m in error, please share your solution. We can at least get a sense of the model by simulating from the joint distribution and plotting.\n\nset.seed(13)\n\ntibble(v = rnorm(1e3, mean = 0, sd = 3)) |&gt; \n  mutate(x = rnorm(1e3, mean = 0, sd = exp(v))) |&gt; \n  \n  ggplot(aes(x = x)) +\n  geom_histogram(binwidth = 1, fill = \"orange2\") +\n  annotate(geom = \"text\",\n           x = -100, y = 490, hjust = 0,\n           label = expression(italic(v)%~%Normal(0, 3))) +\n  annotate(geom = \"text\",\n           x = -100, y = 440, hjust = 0,\n           label = expression(italic(x)%~%Normal(0, exp(italic(v))))) +\n  coord_cartesian(xlim = c(-100, 100)) +\n  scale_y_continuous(breaks = NULL)\n\n\n\n\n\n\n\n\nThe distribution looks something like a Student-\\(t\\) with a very low \\(\\nu\\) parameter. We can express the joint likelihood of \\(p(v, x)\\) as\n\\[p(v, x) = p(x \\mid v)\\ p(v).\\]\nHere that is in a plot.\n\n# Define the parameter space\nparameter_space &lt;- seq(from = -4, to = 4, length.out = 200)\n\n# Simulate\ncrossing(v = parameter_space,\n         x = parameter_space) |&gt; \n  mutate(likelihood_v = dnorm(v, mean = 0, sd = 3),\n         likelihood_x = dnorm(x, mean = 0, sd = exp(v))) |&gt; \n  mutate(joint_likelihood = likelihood_v * likelihood_x) |&gt; \n  \n  # Plot\n  ggplot(aes(x = x, y = v, fill = joint_likelihood)) +\n  geom_raster(interpolate = T) +\n  scale_fill_viridis_c(option = \"B\") +\n  labs(subtitle = \"Centered parameterization\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThis ends up as a version of McElreath’s Figure 13.5.a.\n\nAt low values of \\(v\\), the distribution of \\(x\\) contracts around zero. This forms a very steep valley that the Hamiltonian particle needs to explore. Steep surfaces are hard to simulate, because the simulation is not actually continuous. It happens in discrete steps. If the steps are too big, the simulation will overshoot. (p. 421)\n\nTo avoid the divergent transitions than can arise from steep valleys like this, we can switch from our original formula to a non-centered parameterization, such as:\n\\[\\begin{align*}\nv & \\sim \\operatorname{Normal}(0, 3) \\\\\nz & \\sim \\operatorname{Normal}(0, 1) \\\\\nx & = z \\exp(v),\n\\end{align*}\\]\nwhere \\(x\\) is now the product of two independent distributions, \\(v\\) and \\(z\\). With this parameterization, we can express the joint likelihood \\(p(v, z)\\) as\n\\[p(v, z) = p(z)\\ p(v),\\]\nwhere \\(p(z)\\) is not conditional on \\(v\\) and \\(p(v)\\) is not conditional on \\(z\\). Here’s what that looks like in a plot.\n\n# Simulate\ncrossing(v = parameter_space,\n         z = parameter_space / 2) |&gt; \n  mutate(likelihood_v = dnorm(v, mean = 0, sd = 3),\n         likelihood_z = dnorm(z, mean = 0, sd = 1)) |&gt; \n  mutate(joint_likelihood = likelihood_v * likelihood_z) |&gt; \n  \n  # Plot\n  ggplot(aes(x = z, y = v, fill = joint_likelihood)) +\n  geom_raster(interpolate = T) +\n  scale_fill_viridis_c(option = \"B\") +\n  labs(subtitle = \"Non-centered parameterization\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThis is our version of the right-hand panel of McElreath’s Figure 13.5. No nasty funnel–just a friendly glowing likelihood orb.\n\n\n13.4.2 Non-centered chimpanzees\nAt the top of the section, McElreath reported the rethinking::ulam() default is to set adapt_delta = 0.95. Readers should be aware that the brms::brm() default is adapt_delta = 0.80. A consequence of this difference is rethinking::ulam() will tend to take smaller step sizes than brms::brm(), at the cost of slower exploration of the posterior. I don’t know that one is inherently better than the other. They’re just defaults.\nRecall that due to how brms only supports the non-centered parameterization, we have already fit our version of McElreath’s m13.4nc. We called it b13.4. Here is the model summary, again.\n\nprint(b13.4)\n\n Family: binomial \n  Links: mu = logit \nFormula: pulled_left | trials(1) ~ a + b \n         a ~ 1 + (1 | actor) + (1 | block)\n         b ~ 0 + treatment\n   Data: d (Number of observations: 504) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~actor (Number of levels: 7) \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(a_Intercept)     2.02      0.65     1.08     3.61 1.00     1396     2247\n\n~block (Number of levels: 6) \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(a_Intercept)     0.21      0.18     0.01     0.66 1.00     1700     1525\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_Intercept      0.66      0.73    -0.80     2.15 1.01     1044     1758\nb_treatment1    -0.13      0.30    -0.69     0.45 1.00     2548     3040\nb_treatment2     0.40      0.30    -0.16     0.97 1.00     2740     2522\nb_treatment3    -0.48      0.30    -1.05     0.11 1.00     2723     2820\nb_treatment4     0.28      0.30    -0.29     0.86 1.00     2711     2626\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBecause we only fit this model using the non-centered parameterization, we won’t be able to fully reproduce McElreath’s Figure 13.6. But we can still plot our effective sample sizes. Recall that unlike the way rethinking only reports n_eff, brms now reports both Bulk_ESS and Tail_ESS (see Vehtari et al., 2019). At the moment, brms does not offer a convenience function that allows users to collect those values in a data frame. However you can do so with help from the posterior package. For our purposes, the function of interest is summarise_draws(), which will take the output from as_draws_df() as input.\n\nlibrary(posterior)\n\nas_draws_df(b13.4) |&gt; \n  summarise_draws()\n\n# A tibble: 22 × 10\n   variable                  mean median    sd   mad      q5     q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;                    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 b_a_Intercept            0.656  0.640 0.733 0.682 -0.530   1.87    1.01    1044.    1758.\n 2 b_b_treatment1          -0.133 -0.136 0.295 0.303 -0.596   0.365   1.00    2548.    3040.\n 3 b_b_treatment2           0.397  0.394 0.297 0.305 -0.0728  0.883   1.00    2740.    2522.\n 4 b_b_treatment3          -0.475 -0.477 0.297 0.293 -0.963   0.0243  1.00    2723.    2820.\n 5 b_b_treatment4           0.281  0.274 0.300 0.304 -0.204   0.784   1.00    2711.    2626.\n 6 sd_actor__a_Intercept    2.02   1.92  0.648 0.555  1.19    3.28    1.00    1396.    2247.\n 7 sd_block__a_Intercept    0.213  0.175 0.177 0.146  0.0176  0.540   1.00    1700.    1525.\n 8 r_actor__a[1,Intercept] -1.02  -1.00  0.735 0.684 -2.25    0.166   1.01    1037.    1539.\n 9 r_actor__a[2,Intercept]  4.05   3.86  1.37  1.19   2.21    6.49    1.00    2053.    2673.\n10 r_actor__a[3,Intercept] -1.31  -1.29  0.745 0.692 -2.53   -0.133   1.00    1024.    1654.\n# ℹ 12 more rows\n\n\nNote how the last three columns are the rhat, the ess_bulk, and the ess_tail. Here we summarize those two effective sample size columns in a scatter plot similar to Figure 13.6, but based only on our b13.4, which used the non-centered parameterization.\n\nas_draws_df(b13.4) |&gt; \n  summarise_draws() |&gt; \n\n  ggplot(aes(x = ess_bulk, y = ess_tail)) +\n  geom_abline(linetype = 2) +\n  geom_point(color = \"blue\") +\n  xlim(0, 4700) +\n  ylim(0, 4700) +\n  ggtitle(\"Effective sample size summaries for b13.4\",\n          subtitle = \"ess_bulk is on the x and ess_tail is on the y\") +\n  theme(plot.subtitle = element_text(size = 10),\n        plot.title = element_text(size = 11.5),\n        plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\nBoth measures of effective sample size are fine.\n\nSo should we always use the non-centered parameterization? No. Sometimes the centered form is better. It could even be true that the centered form is better for one cluster in a model while the non-centered form is better for another cluster in the same model. It all depends upon the details. Typically, a cluster with low variation, like the blocks in m13.4, will sample better with a non-centered prior. And if you have a large number of units inside a cluster, but not much data for each unit, then the non-centered is also usually better. But being able to switch back and forth as needed is very useful. (p. 425)\n\nI won’t argue with McElreath, here. But if you run into a situation where you’d like to use the centered parameterization, you will have to use rethinking or fit your model directly in Stan. brms won’t support you, there.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Models With Memory</span>"
    ]
  },
  {
    "objectID": "13.html#multilevel-posterior-predictions",
    "href": "13.html#multilevel-posterior-predictions",
    "title": "13  Models With Memory",
    "section": "13.5 Multilevel posterior predictions",
    "text": "13.5 Multilevel posterior predictions\n\nEvery model is a merger of sense and nonsense. When we understand a model, we can find its sense and control its nonsense. But as models get more complex, it is very difficult to impossible to understand them just by inspecting tables of posterior means and intervals. Exploring implied posterior predictions helps much more….\nThe introduction of varying effects does introduce nuance, however.\nFirst, we should no longer expect the model to exactly retrodict the sample, because adaptive regularization has as its goal to trade off poorer fit in sample for better inference and hopefully better fit out of sample. That is what shrinkage does for us. Of course, we should never be trying to really retrodict the sample. But now you have to expect that even a perfectly good model fit will differ from the raw data in a systematic way.\nSecond, “prediction” in the context of a multilevel model requires additional choices. If we wish to validate a model against the specific clusters used to fit the model, that is one thing. But if we instead wish to compute predictions for new clusters, other than the ones observed in the sample, that is quite another. We’ll consider each of these in turn, continuing to use the chimpanzees model from the previous section. (p. 426)\n\n\n13.5.1 Posterior prediction for same clusters\nLike McElreath did in the text, we’ll do this two ways. Recall we use brms::fitted() in place of rethinking::link().\n\nchimp &lt;- 2\n\nnd &lt;- d |&gt; \n  distinct(treatment) |&gt; \n  mutate(actor = chimp,\n         block = 1)\n\nlabels &lt;- c(\"R/N\", \"L/N\", \"R/P\", \"L/P\")\n\nf &lt;- fitted(b13.4, newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  mutate(treatment = factor(treatment, labels = labels))\n\nf\n\n   Estimate  Est.Error      Q2.5     Q97.5 treatment actor block\n1 0.9787165 0.02072611 0.9236727 0.9994928       R/N     2     1\n2 0.9872600 0.01265721 0.9534633 0.9997033       L/N     2     1\n3 0.9707194 0.02773709 0.8949614 0.9992587       R/P     2     1\n4 0.9856789 0.01408381 0.9474881 0.9996992       L/P     2     1\n\n\nHere are the empirical probabilities computed directly from the data (i.e., the no-pooling model).\n\nchimp_2_d &lt;- d |&gt; \n  filter(actor == chimp) |&gt; \n  group_by(treatment) |&gt; \n  summarise(prob = mean(pulled_left)) |&gt; \n  ungroup() |&gt; \n  mutate(treatment = factor(treatment, labels = labels))\n\nchimp_2_d\n\n# A tibble: 4 × 2\n  treatment  prob\n  &lt;fct&gt;     &lt;dbl&gt;\n1 R/N           1\n2 L/N           1\n3 R/P           1\n4 L/P           1\n\n\nMcElreath didn’t show the corresponding plot in the text. It might look like this.\n\nf |&gt;\n  # If you want to use `geom_line()` or `geom_ribbon()` with a factor on the x-axis,\n  # you need to code something like `group = 1` in `aes()`\n  ggplot(aes(x = treatment, y = Estimate, group = 1)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = \"orange1\") +\n  geom_line(color = \"blue\") +\n  geom_point(data = chimp_2_d,\n             aes(y = prob),\n             color = \"grey25\") +\n  ggtitle(\"Chimp #2\",\n          subtitle = \"The posterior mean and 95%\\nintervals are the blue line\\nand orange band, respectively.\\nThe empirical means are\\nthe charcoal dots.\") +\n  coord_cartesian(ylim = c(0.75, 1)) +\n  theme(plot.subtitle = element_text(size = 10))\n\n\n\n\n\n\n\n\nDo note how severely we’ve restricted the \\(y\\)-axis range. But okay, now let’s do things by hand. We’ll need to extract the posterior draws and look at the structure of the data.\n\npost &lt;- as_draws_df(b13.4)\n\nglimpse(post)\n\nRows: 4,000\nColumns: 25\n$ b_a_Intercept             &lt;dbl&gt; 0.48479855, 1.10261828, 1.32414097, 0.61872027, 0.68580252, 1.82688159, 1.80901729, 0.45746781…\n$ b_b_treatment1            &lt;dbl&gt; -0.6187074919, -0.3437486176, 0.0804279731, -0.1390937124, -0.6857561819, -0.0848117344, -0.36…\n$ b_b_treatment2            &lt;dbl&gt; 0.1383289310, 0.3904019777, 0.8951058613, 0.7557796742, -0.0395914687, 0.4286274552, 0.2005432…\n$ b_b_treatment3            &lt;dbl&gt; -0.30576782, -0.91277692, -0.36259023, -0.42106581, -0.59420320, -0.84186871, -0.34092465, -0.…\n$ b_b_treatment4            &lt;dbl&gt; 0.146705426, 0.079166983, 0.624069604, 0.666658493, -0.115511484, 0.477890020, -0.137705935, 0…\n$ sd_actor__a_Intercept     &lt;dbl&gt; 1.6082309, 1.8679855, 2.0615517, 1.9206350, 2.1149926, 1.4807693, 1.2881741, 0.8924760, 1.7975…\n$ sd_block__a_Intercept     &lt;dbl&gt; 0.074252910, 0.042437439, 0.194326596, 0.285374450, 0.038305713, 0.113925096, 0.171021798, 0.0…\n$ `r_actor__a[1,Intercept]` &lt;dbl&gt; -0.66821389, -1.57806473, -1.58218761, -1.18241690, -0.62033699, -1.99190528, -1.45665994, -0.…\n$ `r_actor__a[2,Intercept]` &lt;dbl&gt; 4.105414, 3.308461, 4.612588, 3.230853, 3.729072, 2.707005, 2.705277, 2.154973, 4.305520, 2.22…\n$ `r_actor__a[3,Intercept]` &lt;dbl&gt; -0.9143057, -1.2672703, -2.2543003, -1.4780342, -0.9916124, -2.1684133, -2.0672630, -1.4486659…\n$ `r_actor__a[4,Intercept]` &lt;dbl&gt; -1.1743512, -1.7493985, -2.2260619, -1.3978979, -0.8812848, -2.6113931, -2.1385626, -1.6201558…\n$ `r_actor__a[5,Intercept]` &lt;dbl&gt; -0.45998857, -1.24893388, -1.85653184, -1.55547356, -0.34322624, -1.73311239, -1.98902409, -1.…\n$ `r_actor__a[6,Intercept]` &lt;dbl&gt; -0.01434108, -0.05977793, -1.17631349, -0.07773770, 0.09858990, -1.21145376, -1.04737339, -0.5…\n$ `r_actor__a[7,Intercept]` &lt;dbl&gt; 1.6095004, 1.1279427, 0.6975524, 1.3489876, 1.7953747, 0.4297830, 0.2385079, 1.1316119, 0.6575…\n$ `r_block__a[1,Intercept]` &lt;dbl&gt; 0.059474740, 0.027987751, -0.412821873, -0.463421465, 0.041109921, -0.280710383, -0.174172714,…\n$ `r_block__a[2,Intercept]` &lt;dbl&gt; -0.1268652689, -0.0881593460, 0.4228474840, 0.6300555403, -0.0732821086, 0.0897455161, -0.0486…\n$ `r_block__a[3,Intercept]` &lt;dbl&gt; 0.0640009759, -0.0256565570, 0.1511196316, -0.0563593557, 0.0045955296, -0.1136581086, -0.0103…\n$ `r_block__a[4,Intercept]` &lt;dbl&gt; -0.097988227, 0.005712426, -0.026642650, 0.081390947, -0.014328158, 0.045720819, -0.021047199,…\n$ `r_block__a[5,Intercept]` &lt;dbl&gt; 0.068321777, 0.042586015, -0.126076880, 0.078935454, -0.030331945, 0.129920020, -0.226338339, …\n$ `r_block__a[6,Intercept]` &lt;dbl&gt; -0.0353906554, -0.0095662871, 0.1492559810, 0.3984556679, -0.0432118567, -0.0878467838, 0.4374…\n$ lprior                    &lt;dbl&gt; -4.996182, -6.628174, -7.530316, -6.943208, -6.161883, -6.820002, -5.026128, -7.037665, -5.663…\n$ lp__                      &lt;dbl&gt; -289.2636, -287.2589, -286.9593, -285.7644, -286.7215, -291.4669, -291.5642, -299.4945, -289.0…\n$ .chain                    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ .iteration                &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,…\n$ .draw                     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,…\n\n\nMcElreath didn’t show what his R code 13.33 dens( post$a[,5] ) would look like. But here’s our analogue.\n\npost |&gt;\n  transmute(actor_5 = `r_actor__a[5,Intercept]`) |&gt; \n  \n  ggplot(aes(x = actor_5)) +\n  geom_density(linewidth = 0, fill = \"blue\") +\n  scale_y_continuous(breaks = NULL) +\n  ggtitle(\"Chimp #5's density\")\n\n\n\n\n\n\n\n\nAnd because we made the density only using the r_actor__a[5,Intercept] values (i.e., we didn’t add b_Intercept to them), the density is in a deviance-score metric.\nMcElreath built his own link() function in R code 13.34. With this particular model, it will be easiest for us to just work directly with post.\n\nf &lt;- post |&gt; \n  pivot_longer(b_b_treatment1:b_b_treatment4) |&gt; \n  mutate(fitted = plogis(b_a_Intercept + value + `r_actor__a[1,Intercept]` + `r_block__a[1,Intercept]`)) |&gt; \n  mutate(treatment = factor(str_remove(name, \"b_b_treatment\"),\n                            labels = labels)) |&gt; \n  select(name:treatment)\n\nf\n\n# A tibble: 16,000 × 4\n   name             value fitted treatment\n   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    \n 1 b_b_treatment1 -0.619   0.322 R/N      \n 2 b_b_treatment2  0.138   0.504 L/N      \n 3 b_b_treatment3 -0.306   0.394 R/P      \n 4 b_b_treatment4  0.147   0.506 L/P      \n 5 b_b_treatment1 -0.344   0.312 R/N      \n 6 b_b_treatment2  0.390   0.486 L/N      \n 7 b_b_treatment3 -0.913   0.204 R/P      \n 8 b_b_treatment4  0.0792  0.409 L/P      \n 9 b_b_treatment1  0.0804  0.357 R/N      \n10 b_b_treatment2  0.895   0.556 L/N      \n# ℹ 15,990 more rows\n\n\nNow we’ll summarize those values and compute their empirical analogues directly from the data.\n\n(\n  f &lt;- f |&gt;\n    group_by(treatment) |&gt;\n    tidybayes::mean_qi(fitted)\n)\n\n# A tibble: 4 × 7\n  treatment fitted .lower .upper .width .point .interval\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 R/N        0.343  0.198  0.497   0.95 mean   qi       \n2 L/N        0.467  0.289  0.631   0.95 mean   qi       \n3 R/P        0.272  0.149  0.421   0.95 mean   qi       \n4 L/P        0.439  0.269  0.607   0.95 mean   qi       \n\n# The empirical summaries\nchimp &lt;- 5\n\n(\n  chimp_5_d &lt;-d |&gt; \n    filter(actor == chimp) |&gt; \n    group_by(treatment) |&gt; \n    summarise(prob = mean(pulled_left)) |&gt; \n    ungroup() |&gt; \n    mutate(treatment = factor(treatment, labels = labels))\n)\n\n# A tibble: 4 × 2\n  treatment  prob\n  &lt;fct&gt;     &lt;dbl&gt;\n1 R/N       0.333\n2 L/N       0.556\n3 R/P       0.278\n4 L/P       0.5  \n\n\nOkay, let’s see how good we are at retrodicting the pulled_left probabilities for actor == 5.\n\nf |&gt;\n  ggplot(aes(x = treatment, y = fitted, group = 1)) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), \n              fill = \"orange1\") +\n  geom_line(color = \"blue\") +\n  geom_point(data = chimp_5_d,\n             aes(y = prob),\n             color = \"grey25\") +\n  ggtitle(\"Chimp #5\",\n          subtitle = \"This plot is like the last except\\nwe did more by hand.\") +\n  coord_cartesian(ylim = 0:1) +\n  theme(plot.subtitle = element_text(size = 10))\n\n\n\n\n\n\n\n\nNot bad.\n\n\n13.5.2 Posterior prediction for new clusters\nBy average actor, McElreath referred to a chimp with an intercept exactly at the population mean \\(\\bar \\alpha\\). Given our non-centered parameterization for b13.4, this means we’ll leave out the random effects for actor. Since we’re predicting what might happen in new experimental blocks, we’ll leave out the random effects for block, too. When doing this by hand, the workflow is much like is was before, just with fewer columns added together within the first mutate() line.\n\nf &lt;- post |&gt; \n  pivot_longer(b_b_treatment1:b_b_treatment4) |&gt; \n  mutate(fitted = plogis(b_a_Intercept + value)) |&gt; \n  mutate(treatment = factor(str_remove(name, \"b_b_treatment\"),\n                            labels = labels)) |&gt; \n  select(name:treatment) |&gt;\n  group_by(treatment) |&gt;\n  # Note we're using 80% intervals\n  mean_qi(fitted, .width = 0.8)\n\nf\n\n# A tibble: 4 × 7\n  treatment fitted .lower .upper .width .point .interval\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 R/N        0.615  0.406  0.808    0.8 mean   qi       \n2 L/N        0.721  0.542  0.877    0.8 mean   qi       \n3 R/P        0.540  0.326  0.743    0.8 mean   qi       \n4 L/P        0.699  0.509  0.864    0.8 mean   qi       \n\n\nMake Figure 13.7.a.\n\np1 &lt;- f |&gt;\n  ggplot(aes(x = treatment, y = fitted, group = 1)) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), \n              fill = \"orange1\") +\n  geom_line(color = \"blue\") +\n  ggtitle(\"Average actor\") +\n  coord_cartesian(ylim = 0:1) +\n  theme(plot.title = element_text(hjust = 0.5, size = 14))\n\np1\n\n\n\n\n\n\n\n\nIf we want to depict the variability across the chimps, we need to include sd_actor__a_Intercept into the calculations. In the first block of code, below, we simulate a bundle of new intercepts defined by\n\\[\\text{simulated chimpanzees} \\sim \\operatorname{Normal}(\\bar \\alpha, \\sigma_\\alpha).\\]\nAs before, we are also averaging over block.\n\nset.seed(13)\n\nf &lt;- post |&gt; \n  # Simulated chimpanzees\n  mutate(a_sim = rnorm(n(), mean = b_a_Intercept, sd = sd_actor__a_Intercept)) |&gt; \n  pivot_longer(b_b_treatment1:b_b_treatment4) |&gt; \n  mutate(fitted = plogis(a_sim + value)) |&gt; \n  mutate(treatment = factor(str_remove(name, \"b_b_treatment\"),\n                            labels = labels)) |&gt; \n  group_by(treatment) |&gt;\n  # Note we're using 80% intervals\n  mean_qi(fitted, .width = 0.8)\n\nf\n\n# A tibble: 4 × 7\n  treatment fitted .lower .upper .width .point .interval\n  &lt;fct&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 R/N        0.573 0.103   0.961    0.8 mean   qi       \n2 L/N        0.649 0.160   0.977    0.8 mean   qi       \n3 R/P        0.523 0.0745  0.946    0.8 mean   qi       \n4 L/P        0.633 0.144   0.974    0.8 mean   qi       \n\n\nBehold Figure 13.7.b.\n\np2 &lt;- f |&gt;\n  ggplot(aes(x = treatment, y = fitted, group = 1)) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), \n              fill = \"orange1\") +\n  geom_line(color = \"blue\") +\n  ggtitle(\"Marginal of actor\") +\n  coord_cartesian(ylim = 0:1) +\n  theme(plot.title = element_text(hjust = 0.5, size = 14))\n\np2\n\n\n\n\n\n\n\n\nThe big difference between this workflow and the last is now we start of by marking off the rows in post with an iter index and then use slice_sample() to randomly sample 100 posterior rows. We also omit the group_by() and mean_qi() lines at the end.\n\n# How many simulated chimps would you like?\nn_chimps &lt;- 100\n\nset.seed(13)\n\nf &lt;- post |&gt; \n  slice_sample(n = n_chimps) |&gt; \n  # Simulated chimpanzees\n  mutate(a_sim = rnorm(n(), mean = b_a_Intercept, sd = sd_actor__a_Intercept)) |&gt; \n  pivot_longer(b_b_treatment1:b_b_treatment4) |&gt; \n  mutate(fitted = plogis(a_sim + value)) |&gt; \n  mutate(treatment = str_remove(name, \"b_b_treatment\") |&gt; \n           factor(labels = labels)) |&gt; \n  select(.draw:treatment)\n\nf\n\n# A tibble: 400 × 6\n   .draw  a_sim name             value fitted treatment\n   &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;    \n 1  1496 -4.54  b_b_treatment1  0.242  0.0134 R/N      \n 2  1496 -4.54  b_b_treatment2  0.851  0.0243 L/N      \n 3  1496 -4.54  b_b_treatment3  0.0924 0.0115 R/P      \n 4  1496 -4.54  b_b_treatment4  0.664  0.0202 L/P      \n 5  3843  0.475 b_b_treatment1 -0.834  0.411  R/N      \n 6  3843  0.475 b_b_treatment2 -0.261  0.553  L/N      \n 7  3843  0.475 b_b_treatment3 -0.999  0.372  R/P      \n 8  3843  0.475 b_b_treatment4 -0.0949 0.594  L/P      \n 9   960  2.21  b_b_treatment1 -0.594  0.834  R/N      \n10   960  2.21  b_b_treatment2  0.303  0.925  L/N      \n# ℹ 390 more rows\n\n\nMake Figure 13.7.c.\n\np3 &lt;- f |&gt;\n  ggplot(aes(x = treatment, y = fitted, group = .draw)) +\n  geom_line(alpha = 1/2, color = \"orange3\") +\n  ggtitle(\"100 simulated actors\") +\n  coord_cartesian(ylim = 0:1) +\n  theme(plot.title = element_text(hjust = 0.5, size = 14))\n\np3\n\n\n\n\n\n\n\n\nFor the finale, we’ll combine the three plots with patchwork.\n\nlibrary(patchwork)\n\np1 | p2 | p3\n\n\n\n\n\n\n\n\n\n13.5.2.1 Bonus: Let’s use fitted() this time\nWe just made those plots using various wrangled versions of post, the data frame returned by as_draws_df(b.13.4). If you followed along closely, part of what made that a great exercise is that it forced you to consider what the various vectors in post meant with respect to the model formula. But it’s also handy to see how to do that from a different perspective. So in this section, we’ll repeat that process by relying on the fitted() function, instead. We’ll go in the same order, starting with the average actor.\n\nnd &lt;- distinct(d, treatment)\n\n(\n  f &lt;- fitted(b13.4,\n              newdata = nd,\n              re_formula = NA,\n              probs = c(0.1, 0.9)) |&gt; \n    data.frame() |&gt; \n    bind_cols(nd) |&gt; \n    mutate(treatment = factor(treatment, labels = labels))\n)\n\n   Estimate Est.Error       Q10       Q90 treatment\n1 0.6148421 0.1542169 0.4061214 0.8076083       R/N\n2 0.7206447 0.1343735 0.5415473 0.8766787       L/N\n3 0.5401932 0.1600461 0.3258537 0.7426736       R/P\n4 0.6986064 0.1398025 0.5087745 0.8641619       L/P\n\n\nYou should notice a few things. Since b13.4 is a cross-classified multilevel model, it had three predictors: treatment, block, and actor. However, our nd data only included the first of those three. The reason fitted() permitted that was because we set re_formula = NA. When you do that, you tell fitted() to ignore group-level effects (i.e., focus only on the fixed effects). This was our fitted() version of ignoring the r_ vectors returned by as_draws_df(). Here’s the plot.\n\np4 &lt;- f |&gt;\n  ggplot(aes(x = treatment, y = Estimate, group = 1)) +\n  geom_ribbon(aes(ymin = Q10, ymax = Q90), \n              fill = \"blue\") +\n  geom_line(color = \"orange1\") +\n  ggtitle(\"Average actor\") +\n  coord_cartesian(ylim = 0:1) +\n  theme(plot.title = element_text(hjust = 0.5, size = 14))\n\np4\n\n\n\n\n\n\n\n\nFor marginal of actor, we can continue using the same nd data. This time we’ll be sticking with the default re_formula setting, which will accommodate the multilevel nature of the model. However, we’ll also be adding allow_new_levels = T and sample_new_levels = \"gaussian\". The former will allow us to marginalize across the specific actors and blocks in our data and the latter will instruct fitted() to use the multivariate normal distribution implied by the random effects. It’ll make more sense why I say multivariate normal by the end of Chapter 14. For now, just go with it.\n\n(\n  f &lt;- fitted(b13.4,\n              newdata = nd,\n              probs = c(0.1, 0.9),\n              allow_new_levels = T,\n              sample_new_levels = \"gaussian\") |&gt; \n    data.frame() |&gt; \n    bind_cols(nd) |&gt; \n    mutate(treatment = factor(treatment, labels = labels))\n)\n\n   Estimate Est.Error        Q10       Q90 treatment\n1 0.5816443 0.3179784 0.09565353 0.9656763       R/N\n2 0.6549381 0.3046262 0.15049058 0.9799405       L/N\n3 0.5325079 0.3226019 0.06762004 0.9541468       R/P\n4 0.6392903 0.3080312 0.13923280 0.9779486       L/P\n\n\nHere’s our fitted()-based marginal of actor plot.\n\np5 &lt;- f |&gt;\n  ggplot(aes(x = treatment, y = Estimate, group = 1)) +\n  geom_ribbon(aes(ymin = Q10, ymax = Q90), \n              fill = \"blue\") +\n  geom_line(color = \"orange1\") +\n  ggtitle(\"Marginal of actor\") +\n  coord_cartesian(ylim = 0:1) +\n  theme(plot.title = element_text(hjust = 0.5, size = 14))\n\np5\n\n\n\n\n\n\n\n\nWe’ll have to amend our workflow a bit to make a fitted() version of the third panel. First we redefine our nd data and execute the fitted() code.\n\n# How many simulated chimps would you like?\nn_chimps &lt;- 100\n\nnd &lt;- distinct(d, treatment) |&gt; \n  # Define 100 new actors\n  expand_grid(actor = str_c(\"new\", 1:n_chimps)) |&gt; \n  arrange(actor, treatment) |&gt; \n  # This adds a row number, which will come in handy, later\n  mutate(row = 1:n())\n\n# Fitted\nset.seed(13)\n\nf &lt;- fitted(b13.4,\n            newdata = nd,\n            allow_new_levels = T,\n            sample_new_levels = \"gaussian\",\n            summary = F,\n            ndraws = n_chimps)\n\nOur f object will need a lot of wrangling. Before I walk out the wrangling steps, we should reiterate what McElreath originally did in the text (pp. 429–430). He based the new actors on the deviation scores from post$sigma_a. That was the first working line in his R code 13.38. In the remaining lines in that code block, he used the model formula to compute the actor-level trajectories. Then in his plot code in R code 13.39, he just used the first 100 rows from that output.\nIn our fitted() code, above, we saved a little time and computer memory by setting ndraws = n_chimps, which equaled 100. That’s functionally the same as when McElreath used the first 100 posterior draws in the plot. A difficulty for us is the way brms::fitted() returns the output, the 100 new levels of actor and the four levels of treatment are confounded in the 400 columns. In the code block, below, the data.frame() through left_join() lines are meant to disentangle those two. After that, we’ll make an actor_number variable, which which we’ll filter the data such that the first row returned by fitted() is only assigned to the new actor #1, the second row is only assigned to the new actor #2, and so on. The result is that we have 100 new simulated actors, each of which corresponds to a different iteration of the posterior draws from the fixed effects.1\n1 The fitted() version of the code for the third panel is cumbersome. Indeed, this in one of those cases where it seems more straightforward to work directly with the as_draws_df() output, rather than with fitted(). The workflow in this section from previous editions of this ebook was more streamlined and superficially seemed to work. However, fellow researcher Ladislas Nalborczyk kindly pointed out I was taking 100 draws from one new simulated actor, rather than one simulated draw from 100 new levels of actor. To my knowledge, if you want 100 new levels of actor AND want each one to be from a different posterior iteration, you’ll need a lot of post-processing code when working with fitted().\np6 &lt;- f |&gt;\n  data.frame() |&gt; \n  # Name the columns by the `row` values in `nd`\n  set_names(pull(nd, row)) |&gt; \n  # Add a draw index\n  mutate(draw = row_number()) |&gt; \n  # Make it long\n  pivot_longer(-draw, names_to = \"row\") |&gt; \n  mutate(row = as.double(row)) |&gt; \n  # Add the new data\n  left_join(nd, by = \"row\") |&gt; \n  # Extract the numbers from the names of the new actors\n  mutate(actor_number = str_extract(actor, \"\\\\d+\") |&gt; as.double()) |&gt; \n  # Only keep the posterior iterations that match the `actor_number` values\n  filter(actor_number == draw) |&gt; \n  # Add the `treatment` labels\n  mutate(treatment = factor(treatment, labels = labels)) |&gt; \n  \n  # Plot!\n  ggplot(aes(x = treatment, y = value, group = actor)) +\n  geom_line(alpha = 1/2, color = \"blue\") +\n  ggtitle(\"100 simulated actors\") +\n  theme(plot.title = element_text(hjust = 0.5, size = 14))\n\np6\n\n\n\n\n\n\n\n\nHere they are altogether.\n\np4 | p5 | p6\n\n\n\n\n\n\n\n\n\n\n\n13.5.3 Post-stratification\nIf you have estimates \\(p_i\\) for each relevant demographic category \\(i\\), the post-stratified prediction for the whole population just re-weights these estimates using the number of individuals \\(N_i\\) in each category with the formula\n\\[\\frac{\\sum_i N_i p_i}{\\sum_i N_i}.\\]\nYou can find a more comprehensive introduction to post-stratification in Chapter 17 of Gelman et al. (2020). Within the multilevel context, this is approach is called multilevel regression and post-stratification (MRP, pronounced “Mister P”). Gelman is a long-time advocate for MRP (e.g., Gelman & Little, 1997; Park et al., 2004). He mentions MRP a lot in his blog (e.g., here, here, here, here, here, here, here, here).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Models With Memory</span>"
    ]
  },
  {
    "objectID": "13.html#summary-bonus-post-stratification-in-an-example",
    "href": "13.html#summary-bonus-post-stratification-in-an-example",
    "title": "13  Models With Memory",
    "section": "13.6 Summary Bonus: Post-stratification in an example",
    "text": "13.6 Summary Bonus: Post-stratification in an example\nThough I was excited to see McElreath introduce MRP, I was disappointed he did not work through an example. Happily, MRP tutorials have been popping up all over the place online. In this bonus section, we’ll draw heavily from the great blog post from demographer Monica Alexander, Analyzing name changes after marriage using a non-representative survey. From the introduction of her post, we read:\n\nRecently on Twitter, sociologist Phil Cohen put out a survey asking people about their decisions to change their name (or not) after marriage. The response was impressive - there are currently over 5,000 responses. Thanks to Phil, the data from the survey are publicly available and downloadable here for anyone to do their own analysis.\nHowever, there’s an issue with using the raw data without lots of caveats: the respondents are not very representative of the broader population, and in particular tend to have a higher education level and are younger than average….\nThis is a very common problem for social scientists: trying to come up with representative estimates using non-representative data. In this post I’ll introduce one particular technique of trying to do this: multilevel regression and post-stratification (MRP). In particular, I’ll use data from the marital name change survey to estimate the proportion of women in the US who kept their maiden name after marriage.\n\n\n13.6.1 Meet the data\nAlexander used two data sources in her example. As alluded to in the block quote, above, she used a subset of the data from Cohen’s Twitter poll. She derived her post-stratification weights from the 2017 5-year ACS data from IPUMS-USA, which provides U.S. census data for research use. Alexander provided some of her data wrangling code in her post and her full R code is available on her GitHub repo, marriage-name-change. For the sake of space, I downloaded the data, wrangled them similarly to how they were used in her blog, and saved the tidied data as external files in my data folder on GitHub. You can download them from there.\nLoad the data.\n\nload(\"data/mrp_data_ch13.rds\")\n\nglimpse(d)\n\nRows: 4,413\nColumns: 5\n$ kept_name      &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, …\n$ state_name     &lt;chr&gt; \"ohio\", \"virginia\", \"new york\", \"rhode island\", \"illinois\", \"north carolina\", \"iowa\", \"texas\", \"south dak…\n$ age_group      &lt;chr&gt; \"50\", \"35\", \"35\", \"55\", \"35\", \"25\", \"35\", \"35\", \"35\", \"35\", \"40\", \"35\", \"30\", \"30\", \"45\", \"45\", \"30\", \"40…\n$ decade_married &lt;chr&gt; \"1979\", \"1999\", \"2009\", \"1999\", \"2009\", \"2009\", \"1999\", \"2009\", \"1999\", \"2009\", \"2009\", \"2009\", \"2009\", \"…\n$ educ_group     &lt;chr&gt; \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", \"&gt;BA\", …\n\nglimpse(cell_counts)\n\nRows: 6,058\nColumns: 5\n$ state_name     &lt;chr&gt; \"alabama\", \"alabama\", \"alabama\", \"alabama\", \"alabama\", \"alabama\", \"alaska\", \"alaska\", \"alaska\", \"alaska\",…\n$ age_group      &lt;chr&gt; \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25…\n$ decade_married &lt;chr&gt; \"1999\", \"2009\", \"1999\", \"2009\", \"1999\", \"2009\", \"1999\", \"2009\", \"1999\", \"2009\", \"1999\", \"2009\", \"1999\", \"…\n$ educ_group     &lt;chr&gt; \"&lt;BA\", \"&lt;BA\", \"&gt;BA\", \"&gt;BA\", \"BA\", \"BA\", \"&lt;BA\", \"&lt;BA\", \"&gt;BA\", \"&gt;BA\", \"BA\", \"BA\", \"&lt;BA\", \"&lt;BA\", \"&gt;BA\", \"&gt;BA…\n$ n              &lt;dbl&gt; 19012, 37488, 959, 5319, 2986, 14261, 3320, 7001, 159, 435, 341, 2660, 23279, 45477, 1105, 6229, 3994, 16…\n\n\nOur primary data file, which contains the survey responses to whether women changed their names after marriage, is d. Our criterion variable will be kept_name, which is dummy coded 0 = “no” 1 = “yes.” We have four grouping variables:\n\nage_group, which ranges from 25 to 75 and is discretized such that 25 = [25, 30), 30 = [30, 35), and so on;\ndecade_married, which ranges from 1979 to 2019 and is discretized such that 1979 = [1979, 1989), 1989 = [1989, 1999), and so on;\neduc_group, which is coded as &lt;BA = no bachelor’s degree, BA = bachelor’s degree, and &gt;BA = above a bachelor’s degree; and\nstate_name, which includes the names of the 50 US states, the District of Columbia, and Puerto Rico.\n\nThe cell_counts data contains the relevant information from the US census. The first four columns, state_name, age_group, decade_married, and educ_group are the same demographic categories from the survey data. The fifth column, n, has the counts of women falling within those categories from the US census. There were 6,058 unique combinations of the demographic categories represented in the census data.\n\ncell_counts |&gt; count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  6058\n\n\nWe can use a histogram to get a sense of how those counts vary.\n\ncell_counts |&gt; \n  ggplot(aes(x = n)) +\n  geom_histogram(binwidth = 2000, fill = \"blue\") +\n  scale_x_continuous(breaks = 0:3 * 100000, labels = c(0, \"100K\", \"200K\", \"300K\"))\n\n\n\n\n\n\n\n\nThough some of the categories are large with an excess of 100,000 persons in them, many are fairly small. It seems unlikely that the women who participated in Cohen’s Twitter poll fell into these categories in the same proportions. This is where post-stratification will help.\n\n\n13.6.2 Settle the MR part of MRP\nLike in the earlier examples in this chapter, we will model the data with multilevel logistic regression. Alexander fit her model with brms and kept things simple by using default priors. Here we’ll continue on with McElreath’s recommendations and use weakly regularizing priors. Though I am no expert on the topic of women’s name-changing practices following marriage, my layperson’s sense is that most do not keep their maiden name after they marry. I’m not quite sure what the proportion might be, but I’d like my \\(\\bar \\alpha\\) prior to tend closer to 0 than to 1. Recall that the \\(\\bar \\alpha\\) for a multilevel logistic model is typically a Gaussian set on the log-odds scale. If we were to use \\(\\operatorname{Normal}(-1, 1)\\), here’s what that would look like when converted back to the probability metric.\n\nset.seed(13)\n\ntibble(n = rnorm(1e6, mean = -1, sd = 1)) |&gt; \n  mutate(p = plogis(n)) |&gt; \n  \n  ggplot(aes(x = p)) +\n  geom_histogram(binwidth = 0.02, boundary = 0, fill = \"blue\") +\n  scale_y_continuous(breaks = NULL)\n\n\n\n\n\n\n\n\nTo my eye, this looks like a good place to start. Feel free to experiment with different priors on your end. As to the hierarchical \\(\\sigma_\\text{&lt;group&gt;}\\) priors, we will continue our practice of setting them to \\(\\operatorname{Exponential}(1)\\). Here’s how to fit the model.\n\nb13.7 &lt;- brm(\n  data = d,\n  family = binomial,\n  kept_name | trials(1) ~ 1 + (1 | age_group) + (1 | decade_married) + (1 | educ_group) + (1 | state_name),\n  prior = c(prior(normal(-1, 1), class = Intercept),\n            prior(exponential(1), class = sd)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 13,\n  control = list(adapt_delta = 0.95),\n  file = \"fits/b13.07\")\n\nNote how, like Alexander did in the blog, we had to adjust the adept_delta setting to stave off a few divergent transitions. In my experience, this is common when your hierarchical grouping variables have few levels. Our decade_married has five levels and educ_group has only four. Happily, brms::brm() came through in the end. You can see by checking the summary.\n\nprint(b13.7)\n\n Family: binomial \n  Links: mu = logit \nFormula: kept_name | trials(1) ~ 1 + (1 | age_group) + (1 | decade_married) + (1 | educ_group) + (1 | state_name) \n   Data: d (Number of observations: 4373) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~age_group (Number of levels: 11) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.15      0.30     0.71     1.85 1.00     1289     2195\n\n~decade_married (Number of levels: 5) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.00      0.39     0.50     1.94 1.00     1757     2559\n\n~educ_group (Number of levels: 4) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.91      0.47     0.36     2.19 1.00     1813     2602\n\n~state_name (Number of levels: 52) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.25      0.06     0.15     0.38 1.00     1180     2246\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.73      0.62    -1.96     0.47 1.00     1489     2364\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nEven with 4,373 cases in the data, the uncertainty around \\(\\bar \\alpha\\) is massive, -0.73 [-1.96, 0.47], suggesting a lot of the action is lurking in the \\(\\sigma_\\text{&lt;group&gt;}\\) parameters. It might be easier to compare the \\(\\sigma_\\text{&lt;group&gt;}\\) parameters with an interval plot.\n\nas_draws_df(b13.7) |&gt; \n  select(starts_with(\"sd_\")) |&gt; \n  set_names(str_c(\"sigma[\", c(\"age\", \"decade~married\", \"education\", \"state\"), \"]\")) |&gt; \n  pivot_longer(everything()) |&gt; \n  group_by(name) |&gt;\n  median_qi(.width = seq(from = 0.5, to = 0.9, by = 0.1)) |&gt;\n  \n  ggplot(aes(x = value, xmin = .lower, xmax = .upper, y = reorder(name, value))) +\n  geom_interval(aes(alpha = .width), \n                color = \"orange3\") +\n  scale_y_discrete(labels = ggplot2:::parse_safe) +\n  scale_alpha_continuous(\"CI width\", range = c(0.7, 0.15)) +\n  xlim(0, NA) +\n  theme(axis.text.y = element_text(hjust = 0),\n        panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\nIt seems the largest share of the variation is to be found among the age groups. Since there was relatively less variation across states, we can expect more aggressive regularization along those lines.\n\n\n13.6.3 Post-stratify to put the P in MRP\nIn her post, Alexander contrasted the MRP results with the empirical proportions from the Twitter survey in a series of four plots, one for each of the four grouping variables. We will take a slightly different approach. For simplicity, we will only focus on the results for age_group and state. However, we will examine the results for each using three estimation methods: the empirical proportions, the naïve results from the multilevel model, and the MRP estimates.\n\n13.6.3.1 Estimates by age group\nTo warm up, here is the plot of the empirical proportions for kept_name, by age_group.\n\nlevels &lt;- c(\"raw data\", \"multilevel\", \"MRP\")\n\n# Compute the proportions from the data\np1 &lt;- d |&gt; \n  group_by(age_group, kept_name) |&gt;\n  summarise(n = n()) |&gt; \n  group_by(age_group) |&gt; \n  mutate(prop = n / sum(n),\n         type = factor(\"raw data\", levels = levels)) |&gt; \n  filter(kept_name == 1, age_group &lt; 80, age_group &gt; 20) |&gt;\n\n  # Plot\n  ggplot(aes(x = prop, y = age_group)) + \n  geom_point() +\n  scale_x_continuous(breaks = c(0, 0.5, 1), limits = 0:1) +\n  facet_wrap(~ type)\n\np1\n\n\n\n\n\n\n\n\nWe’ll combine that plot with the next two, in a bit. I just wanted to give a preview of what we’re doing. The second plot will showcase the typical multilevel estimates for the same. The most straightforward way to do this with brms is with the fitted() function. We’ll use the re_formula argument to average over the levels of all grouping variables other than age_group. Relatedly, we’ll feed in the unique levels of age_group into the newdata argument. Then we just wrangle and plot.\n\nnd &lt;- distinct(d, age_group) |&gt; arrange(age_group)\n\np2 &lt;- fitted(\n  b13.7,\n  re_formula = ~ (1 | age_group),\n  newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  mutate(prop = Estimate,\n         type = factor(\"multilevel\", levels = levels)) |&gt; \n  \n  ggplot(aes(x = prop, xmin = Q2.5, xmax = Q97.5, y = age_group)) + \n  geom_pointrange(color = \"blue2\", linewidth = 0.8, size = 1/4) +\n  scale_x_continuous(breaks = c(0, 0.5, 1), limits = c(0, 1)) +\n  scale_y_discrete(labels = NULL) +\n  facet_wrap(~ type)\n\nWe will take a look at the multilevel coefficient plot in just a bit. Now we turn our focus to computing the MRP estimates. As a first step, we’ll follow Alexander’s lead and add a prop column to the cell_counts data, which will give us the proportions of the combinations of the other three demographic categories, within each level of age_group. We’ll save the results as age_prop.\n\nage_prop &lt;- cell_counts |&gt; \n  group_by(age_group) |&gt; \n  mutate(prop = n / sum(n)) |&gt; \n  ungroup()\n\nage_prop\n\n# A tibble: 6,058 × 6\n   state_name age_group decade_married educ_group     n      prop\n   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 alabama    25        1999           &lt;BA        19012 0.00414  \n 2 alabama    25        2009           &lt;BA        37488 0.00816  \n 3 alabama    25        1999           &gt;BA          959 0.000209 \n 4 alabama    25        2009           &gt;BA         5319 0.00116  \n 5 alabama    25        1999           BA          2986 0.000650 \n 6 alabama    25        2009           BA         14261 0.00310  \n 7 alaska     25        1999           &lt;BA         3320 0.000723 \n 8 alaska     25        2009           &lt;BA         7001 0.00152  \n 9 alaska     25        1999           &gt;BA          159 0.0000346\n10 alaska     25        2009           &gt;BA          435 0.0000947\n# ℹ 6,048 more rows\n\n\nThese results are then fed into the newdata argument within the add_predicted_draws() function, which we’ll save as p.\n\np &lt;- add_predicted_draws(\n  b13.7, \n  newdata = age_prop |&gt; \n    filter(age_group &gt; 20, \n           age_group &lt; 80, \n           decade_married &gt; 1969),\n  allow_new_levels = T)\n\nglimpse(p)\n\nRows: 24,232,000\nColumns: 11\nGroups: state_name, age_group, decade_married, educ_group, n, prop, .row [6,058]\n$ state_name     &lt;chr&gt; \"alabama\", \"alabama\", \"alabama\", \"alabama\", \"alabama\", \"alabama\", \"alabama\", \"alabama\", \"alabama\", \"alaba…\n$ age_group      &lt;chr&gt; \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25\", \"25…\n$ decade_married &lt;chr&gt; \"1999\", \"1999\", \"1999\", \"1999\", \"1999\", \"1999\", \"1999\", \"1999\", \"1999\", \"1999\", \"1999\", \"1999\", \"1999\", \"…\n$ educ_group     &lt;chr&gt; \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", \"&lt;BA\", …\n$ n              &lt;dbl&gt; 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, 19012, …\n$ prop           &lt;dbl&gt; 0.004137905, 0.004137905, 0.004137905, 0.004137905, 0.004137905, 0.004137905, 0.004137905, 0.004137905, 0…\n$ .row           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ .chain         &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ .iteration     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ .draw          &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29…\n$ .prediction    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nThe tidybayes::add_predicted_draws() function is somewhat analogous to brms::predict(). It allowed us to compute the posterior predictions from our model, given the levels of the predictors we fed into newdata. The results were returned in a tidy data format, including the levels of the predictors from the newdata argument. Because there were 6,058 unique predictor values and 4,000 posterior draws, this produced a 24,232,000-row data frame. The posterior predictions are in the .prediction column on the end. Since we used a binomial regression model, we got a series of 0’s and 1’s.\nNext comes the MRP magic. If we group the results by age_group and .draw, we can sum the product of the posterior predictions and the weights, which will leave us with 4,000 stratified posterior draws for each of the 11 levels of age_group. This is the essence of the post-stratification equation McElreath presented in Section 13.5.3,\n\\[\\frac{\\sum_i N_i p_i}{\\sum_i N_i}.\\]\nWe will follow Alexander and call these summary values kept_name_predict. We then complete the project by grouping by age_group and summarizing each stratified posterior predictive distribution by its mean and 95% interval.\n\np &lt;- p |&gt; \n  group_by(age_group, .draw) |&gt; \n  summarise(kept_name_predict = sum(.prediction * prop)) |&gt; \n  group_by(age_group) |&gt; \n  mean_qi(kept_name_predict)\n\np\n\n# A tibble: 11 × 7\n   age_group kept_name_predict .lower .upper .width .point .interval\n   &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n 1 25                    0.175 0.0921  0.273   0.95 mean   qi       \n 2 30                    0.182 0.116   0.263   0.95 mean   qi       \n 3 35                    0.218 0.148   0.297   0.95 mean   qi       \n 4 40                    0.244 0.171   0.326   0.95 mean   qi       \n 5 45                    0.279 0.203   0.368   0.95 mean   qi       \n 6 50                    0.301 0.218   0.399   0.95 mean   qi       \n 7 55                    0.324 0.231   0.427   0.95 mean   qi       \n 8 60                    0.438 0.327   0.555   0.95 mean   qi       \n 9 65                    0.560 0.416   0.704   0.95 mean   qi       \n10 70                    0.515 0.291   0.762   0.95 mean   qi       \n11 75                    0.226 0.0485  0.505   0.95 mean   qi       \n\n\nNow we are finally ready to plot our MRP estimates and combine the three subplots into a coherent whole with patchwork syntax.\n\n# MRP plot\np3 &lt;- p |&gt;\n  mutate(type = factor(\"MRP\", levels = levels)) |&gt; \n\n  ggplot(aes(x = kept_name_predict, xmin = .lower, xmax = .upper, y = age_group)) + \n  geom_pointrange(color = \"orange2\", linewidth = 0.8, size = 1/4) +\n  scale_x_continuous(breaks = c(0, 0.5, 1), limits = 0:1) +\n  scale_y_discrete(labels = NULL) +\n  facet_wrap(~ type)\n\n# Combine, entitle, and display\n(p1 | p2 | p3) +\n  plot_annotation(title = \"Proportion of women keeping name after marriage, by age\",\n                  subtitle = \"Proportions are on the x-axis and age groups are on the y-axis.\")\n\n\n\n\n\n\n\n\nBoth multilevel and MRP estimates tended to be a little lower than the raw proportions, particularly for women in the younger age groups. Alexander mused this was “likely due to the fact that the survey has an over-sample of highly educated women, who are more likely to keep their name.” The MRP estimates were more precise than the multilevel predictions, which averaged across the grouping variables other than age. All three estimates show something of an inverted U-shape curve across age, which Alexander noted “is consistent with past observations that there was a peak in name retention in the 80s and 90s.”\n\n\n13.6.3.2 Estimates by US state\nNow we turn out attention to variation across states. The workflow, here, will only deviate slightly from what we just did. This time, of course, we will be grouping the estimates by state_name instead of by age_group. The other notable difference is since we’re plotting data clustered by US states, it might be fun to show the results in a map format. Alexander used the geom_statebins() function from the statebins package (Rudis, 2020). I thought the results were pretty cool, we will do the same. To give you a sense of what we’re building, here’s the plot of the empirical proportions.\n\nlibrary(statebins)\n\np1 &lt;- d |&gt;\n  group_by(state_name, kept_name) |&gt;\n  summarise(n = n()) |&gt;\n  group_by(state_name) |&gt;\n  mutate(prop = n/sum(n)) |&gt;\n  filter(kept_name == 1,\n         state_name != \"puerto rico\") |&gt; \n  mutate(type = factor(\"raw data\", levels = levels),\n         statename = str_to_title(state_name)) |&gt;\n  \n  ggplot(aes(fill = prop, state = statename)) + \n  geom_statebins(lbl_size = 2.5, border_size = 1/4, radius = grid::unit(2, \"pt\")) +\n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  scale_fill_viridis_c(\"proportion\\nkeeping\\nname\", option = \"B\", limits = c(0, 0.8)) +\n  facet_wrap(~ type) +\n  theme(legend.position = \"none\")\n\np1\n\n\n\n\n\n\n\n\nFor the naïve multilevel estimates, we’ll continue using fitted().\n\nnd &lt;- distinct(d, state_name)\n\np2 &lt;- fitted(b13.7,\n             newdata = nd,\n             re_formula = ~ (1 | state_name),\n             allow_new_levels = T) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  filter(state_name != \"puerto rico\") |&gt; \n  mutate(prop = Estimate,\n         type = factor(\"multilevel\", levels = levels),\n         statename = str_to_title(state_name)) |&gt; \n  \n  ggplot(aes(fill = prop, state = statename)) + \n  geom_statebins(lbl_size = 2.5, border_size = 1/4, radius = grid::unit(2, \"pt\")) +\n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  scale_fill_viridis_c(\"proportion\\nkeeping\\nname\", option = \"B\", limits = c(0, 0.8)) +\n  facet_wrap(~ type)\n\nIn preparation for the MRP estimates, we’ll first wrangle cell_counts, this time grouping by state_name before computing the weights.\n\nstate_prop &lt;- cell_counts |&gt; \n  group_by(state_name) |&gt; \n  mutate(prop = n/sum(n))  |&gt; \n  ungroup()\n\nstate_prop\n\n# A tibble: 6,058 × 6\n   state_name age_group decade_married educ_group     n     prop\n   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1 alabama    25        1999           &lt;BA        19012 0.0187  \n 2 alabama    25        2009           &lt;BA        37488 0.0369  \n 3 alabama    25        1999           &gt;BA          959 0.000945\n 4 alabama    25        2009           &gt;BA         5319 0.00524 \n 5 alabama    25        1999           BA          2986 0.00294 \n 6 alabama    25        2009           BA         14261 0.0141  \n 7 alaska     25        1999           &lt;BA         3320 0.0225  \n 8 alaska     25        2009           &lt;BA         7001 0.0474  \n 9 alaska     25        1999           &gt;BA          159 0.00108 \n10 alaska     25        2009           &gt;BA          435 0.00295 \n# ℹ 6,048 more rows\n\n\nNow we’ll feed those state_prop values into add_predicted_draws(), wrangle, and plot the MRP plot in one step.\n\np3 &lt;- add_predicted_draws(\n  b13.7,\n  newdata = state_prop |&gt; \n    filter(age_group &gt; 20, \n           age_group &lt; 80, \n           decade_married &gt; 1969),\n  allow_new_levels = T) |&gt;\n  group_by(state_name, .draw) |&gt; \n  summarise(kept_name_predict = sum(.prediction * prop)) |&gt; \n  group_by(state_name) |&gt; \n  mean_qi(kept_name_predict) |&gt; \n  mutate(prop      = kept_name_predict,\n         type      = factor(\"MRP\", levels = levels),\n         statename = str_to_title(state_name)) |&gt; \n  \n  ggplot(aes(fill = kept_name_predict, state = statename)) + \n  geom_statebins(lbl_size = 2.5, border_size = 1/4, radius = grid::unit(2, \"pt\")) +\n  scale_x_continuous(breaks = NULL) +\n  scale_y_continuous(breaks = NULL) +\n  scale_fill_viridis_c(\"proportion\\nkeeping\\nname\", option = \"B\", limits = c(0, 0.8)) +\n  facet_wrap(~ type) +\n  theme(legend.position = \"none\")\n\nWe’re finally ready to combine our three panels into one grand plot.\n\n(p1 | p2 | p3) +\n  plot_annotation(title = \"Proportion off women keeping name after marriage, by state\",\n                  theme = theme(plot.margin = margin(0.2, 0, 0.01, 0, \"cm\")))\n\n\n\n\n\n\n\n\nRemember how small the posterior for \\(\\sigma_\\text{state}\\) was relative to the other \\(\\sigma_\\text{&lt;group&gt;}\\) posteriors? We said that would imply more aggressive regularization across states. You can really see that regularization in the panels showing the multilevel and MRP estimates. They are much more uniform than the proportions from the raw data, which are all over the place. This is why you use multilevel models and/or stratify. When you divide the responses up at the state level, the proportions get jerked all around due to small and unrepresentative samples. Even with the regularization from the multilevel partial pooling, you can still see some interesting differences in the multilevel and MRP panels. Both suggest women keep their maiden names in relatively low proportions in Utah and relatively high proportions in New York. For those acquainted with American culture, this shouldn’t be a great surprise.\n\n\n\n13.6.4 Wrap this MRP up\nInterested readers should practice exploring the MRP estimates by the other two grouping variables, educ_group and decate_married. Both contain interesting results. Also, there are many other great free resources for learning about MRP.\n\nTim Mastny showed how to use MRP via brms with data of US state level opinions for gay marriage in his blog post, MRP Using brms and tidybayes.\nRohan Alexander showed how to fit political poling data with both lme4 and brms in his post, Getting started with MRP.\nLauren Kennedy and Andrew Gelman have a (2021) paper called Know your population and know your model: Using model-based regression and post-stratification to generalize findings beyond the observed sample, which shows how to use brms to apply MRP to Big Five personality data. Here’s a link to the preprint.\n\nFor a more advanced application, check out the paper by Kolczynska, Bürkner, Kennedy, and Vehtari (2020), which combines MRP with a model with ordinal outcomes (recall Section 12.3). Their supplemental material, which includes their R code, lives at https://osf.io/dz4y7/. With all this good stuff, it seems we have an embarrassment of riches when it comes to brms and MRP! To wrap this section up, we’ll give Monica Alexander the last words:\n\nMRP is probably most commonly used in political analysis to reweight polling data, but it is a useful technique for many different survey responses. Many modeling extensions are possible. For example, the multilevel regression need not be limited to just using random effects, as was used here, and other model set ups could be investigated. MRP is a relatively easy and quick way of trying to get more representative estimates out of non-representative data, while giving you a sense of the uncertainty around the estimates (unlike traditional post-stratification).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Models With Memory</span>"
    ]
  },
  {
    "objectID": "13.html#session-info",
    "href": "13.html#session-info",
    "title": "13  Models With Memory",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] statebins_1.4.0       posterior_1.6.1.9000  bayesplot_1.15.0.9000 patchwork_1.3.2       tidybayes_3.0.7      \n [6] ggthemes_5.1.0        lubridate_1.9.4       forcats_1.0.1         stringr_1.6.0         dplyr_1.1.4          \n[11] purrr_1.2.1           readr_2.1.5           tidyr_1.3.2           tibble_3.3.1          ggplot2_4.0.1        \n[16] tidyverse_2.0.0       brms_2.23.0           Rcpp_1.1.0           \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1        svUnit_1.0.8            viridisLite_0.4.2       farver_2.1.2            loo_2.9.0.9000         \n [6] S7_0.2.1                fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39          \n[11] timechange_0.3.0        estimability_1.5.1      lifecycle_1.0.5         StanHeaders_2.36.0.9000 survival_3.8-3         \n[16] magrittr_2.0.4          compiler_4.5.1          rlang_1.1.7             tools_4.5.1             utf8_1.2.6             \n[21] yaml_2.3.12             knitr_1.51              emo_0.0.0.9000          labeling_0.4.3          bridgesampling_1.2-1   \n[26] htmlwidgets_1.6.4       pkgbuild_1.4.8          curl_7.0.0              plyr_1.8.9              RColorBrewer_1.1-3     \n[31] multcomp_1.4-29         abind_1.4-8             withr_3.0.2             grid_4.5.1              stats4_4.5.1           \n[36] xtable_1.8-4            inline_0.3.21           emmeans_1.11.2-8        scales_1.4.0            MASS_7.3-65            \n[41] cli_3.6.5               mvtnorm_1.3-3           crayon_1.5.3            rmarkdown_2.30          generics_0.1.4         \n[46] RcppParallel_5.1.11-1   rstudioapi_0.17.1       reshape2_1.4.5          tzdb_0.5.0              rstan_2.36.0.9000      \n[51] splines_4.5.1           assertthat_0.2.1        parallel_4.5.1          matrixStats_1.5.0       vctrs_0.7.0            \n[56] V8_8.0.1                Matrix_1.7-3            sandwich_3.1-1          jsonlite_2.0.0          hms_1.1.4              \n[61] arrayhelpers_1.1-0      ggdist_3.3.3            glue_1.8.0              codetools_0.2-20        distributional_0.5.0   \n[66] stringi_1.8.7           gtable_0.3.6            QuickJSR_1.8.1          quadprog_1.5-8          pillar_1.11.1          \n[71] htmltools_0.5.9         Brobdingnag_1.2-9       R6_2.6.1                evaluate_1.0.5          lattice_0.22-7         \n[76] backports_1.5.0         rstantools_2.5.0.9000   coda_0.19-4.1           gridExtra_2.3           nlme_3.1-168           \n[81] checkmate_2.3.3         xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Models With Memory</span>"
    ]
  },
  {
    "objectID": "13.html#comments",
    "href": "13.html#comments",
    "title": "13  Models With Memory",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001\n\n\nEfron, B., & Morris, C. (1977). Stein’s paradox in statistics. Scientific American, 236(5), 119–127. https://doi.org/10.1038/scientificamerican0577-119\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGelman, A., & Little, T. C. (1997). Postratification into many categories using hierarchical logistic regression. Survey Methodology, 23, 127–135. https://stat.columbia.edu/~gelman/research/published/poststrat3.pdf\n\n\nKennedy, L., & Gelman, A. (2021). Know your population and know your model: Using model-based regression and poststratification to generalize findings beyond the observed sample. Psychological Methods, 26(5), 547–558. https://doi.org/10.1037/met0000362\n\n\nKolczynska, M., Bürkner, P.-C., Kennedy, L., & Vehtari, A. (2020). Trust in state institutions in Europe, 1989-2019. SocArXiv. https://doi.org/10.31235/osf.io/3v5g7\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nPaananen, T., Bürkner, P.-C., Vehtari, A., & Gabry, J. (2020). Avoiding model refits in leave-one-out cross-validation with moment matching. https://CRAN.R-project.org/package=loo/vignettes/loo2-moment-matching.html\n\n\nPaananen, T., Piironen, J., Bürkner, P.-C., & Vehtari, A. (2020). Implicitly adaptive importance sampling. http://arxiv.org/abs/1906.08850\n\n\nPark, D. K., Gelman, A., & Bafumi, J. (2004). Bayesian multilevel estimation with poststratification: State-level estimates from national polls. Political Analysis, 12(4), 375–385. https://www.jstor.org/stable/25791784\n\n\nRudis, B. (2020). statebins: Create united states uniform cartogram heatmaps [Manual]. https://CRAN.R-project.org/package=statebins\n\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved for assessing convergence of MCMC. https://arxiv.org/abs/1903.08008?\n\n\nVonesh, J. R., & Bolker, B. M. (2005). Compensatory larval responses shift trade-offs associated with predator-induced hatching plasticity. Ecology, 86(6), 1580–1591. https://doi.org/10.1890/04-0535",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Models With Memory</span>"
    ]
  },
  {
    "objectID": "14.html",
    "href": "14.html",
    "title": "14  Adventures in Covariance",
    "section": "",
    "text": "14.1 Varying slopes by construction",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Adventures in Covariance</span>"
    ]
  },
  {
    "objectID": "14.html#varying-slopes-by-construction",
    "href": "14.html#varying-slopes-by-construction",
    "title": "14  Adventures in Covariance",
    "section": "",
    "text": "How should the robot pool information across intercepts and slopes? By modeling the joint population of intercepts and slopes, which means by modeling their covariance. In conventional multilevel models, the device that makes this possible is a joint multivariate Gaussian distribution for all of the varying effects, both intercepts and slopes. So instead of having two independent Gaussian distributions of intercepts and of slopes, the robot can do better by assigning a two-dimensional Gaussian distribution to both the intercepts (first dimension) and the slopes (second dimension). (p. 437)\n\n\n14.1.0.1 Rethinking: Why Gaussian?\n\nThere is no reason the multivariate distribution of intercepts and slopes must be Gaussian. But there are both practical and epistemological justifications. On the practical side, there aren’t many multivariate distributions that are easy to work with. The only common ones are multivariate Gaussian and multivariate Student-t distributions. On the epistemological side, if all we want to say about these intercepts and slopes is their means, variances, and covariances, then the maximum entropy distribution is multivariate Gaussian. (p. 437)\n\nAs it turns out, brms does currently allow users to use the multivariate Student-\\(t\\) distribution in this way. For details, check out this discussion from the brms GitHub repository. Bürkner’s exemplar syntax from his comment on May 13, 2018, was y ~ x + (x | gr(g, dist = \"student\")). I haven’t experimented with this, but if you do, do consider sharing how it went.\n\n\n14.1.1 Simulate the population\nIf you follow this section closely, it’s a great template for simulating multilevel code for any of your future projects. You might think of this as an alternative to a frequentist power analysis. Vourre has done some nice work along these lines, I have a blog series on Bayesian power analysis, and Kruschke covered the topic in Chapter 13 of his (2015) text.\n\na       &lt;-  3.5  # Average morning wait time\nb       &lt;- -1    # Average difference afternoon wait time\nsigma_a &lt;-  1    # SD in intercepts\nsigma_b &lt;-  0.5  # SD in slopes\nrho     &lt;- -0.7  # Correlation between intercepts and slopes\n\n# The next three lines of code simply combine the terms, above\nmu     &lt;- c(a, b)\n\ncov_ab &lt;- sigma_a * sigma_b * rho\nsigma  &lt;- matrix(c(sigma_a^2, cov_ab, \n                   cov_ab, sigma_b^2), ncol = 2)\n\nIt’s common to refer to a covariance matrix as \\(\\mathbf \\Sigma\\). The mathematical notation for those last couple lines of code is\n\\[\n\\mathbf \\Sigma = \\begin{bmatrix} \\sigma_\\alpha^2 & \\sigma_\\alpha \\sigma_\\beta \\rho \\\\ \\sigma_\\alpha \\sigma_\\beta \\rho & \\sigma_\\beta^2 \\end{bmatrix}.\n\\]\nAnyway, if you haven’t used the matrix() function before, you might get a sense of the elements like so.\n\nmatrix(1:4, nrow = 2, ncol = 2)\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nThis next block of code will finally yield our café data.\n\nlibrary(tidyverse)\n\nsigmas &lt;- c(sigma_a, sigma_b)          # SDs\nrho    &lt;- matrix(c(1, rho,             # Correlation matrix\n                   rho, 1), nrow = 2)\n\n# Now matrix multiply to get covariance matrix\nsigma &lt;- diag(sigmas) %*% rho %*% diag(sigmas)\n\n# How many cafes would you like?\nn_cafes &lt;- 20\n\nset.seed(5)  # Used to replicate example\n\nvary_effects &lt;- MASS::mvrnorm(n_cafes, mu, sigma) |&gt; \n  data.frame() |&gt; \n  set_names(\"a_cafe\", \"b_cafe\")\n\nhead(vary_effects)\n\n    a_cafe     b_cafe\n1 4.223962 -1.6093565\n2 2.010498 -0.7517704\n3 4.565811 -1.9482646\n4 3.343635 -1.1926539\n5 1.700971 -0.5855618\n6 4.134373 -1.1444539\n\n\nLet’s make sure we’re keeping this all straight. a_cafe = our café-specific intercepts; b_cafe = our café-specific slopes. These aren’t the actual data, yet. But at this stage, it might make sense to ask What’s the distribution of a_cafe and b_cafe? Our variant of Figure 14.2 contains the answer.\nFor our plots in this chapter, we’ll make our own custom ggplot2 theme. The color palette will come from the “pearl_earring” palette of the dutchmasters package (Thoen, 2022). You can learn more about the original painting, Vermeer’s (1665) Girl with a Pearl Earring, here.\n\nlibrary(dutchmasters)\n\ndutchmasters$pearl_earring\n\n        red(lips)              skin      blue(scarf1)      blue(scarf2)      white(colar)       gold(dress)      gold(dress2) \n        \"#A65141\"         \"#E7CDC2\"         \"#80A0C7\"         \"#394165\"         \"#FCF9F0\"         \"#B1934A\"         \"#DCA258\" \nblack(background)      grey(scarf3)    yellow(scarf4)                   \n        \"#100F14\"         \"#8B9DAF\"         \"#EEDA9D\"         \"#E8DCCF\" \n\nscales::show_col(dutchmasters$pearl_earring)\n\n\n\n\n\n\n\n\nWe’ll name our custom theme theme_pearl_earring(). I cobbled together this approach to defining a custom ggplot2 theme with help from\n\nChapter 19 of Wichkam’s (2016) ggplot2: Elegant graphics for data analysis;\nSection 4.6 of Peng, Kross, and Anderson’s (2017) Mastering Software Development in R;\nLea Waniek’s blog post, Custom themes in ggplot2, and\nJoey Stanley’s blog post of the same name, Custom themes in ggplot2.\n\n\ntheme_pearl_earring &lt;- function(light_color = \"#E8DCCF\", \n                                dark_color = \"#100F14\", \n                                my_family = \"Courier\",\n                                ...) {\n  \n  theme(line = element_line(color = light_color),\n        text = element_text(color = light_color, family = my_family),\n        axis.line = element_blank(),\n        axis.text = element_text(color = light_color),\n        axis.ticks = element_line(color = light_color),\n        legend.background = element_rect(fill = dark_color, color = \"transparent\"),\n        legend.key = element_rect(fill = dark_color, color = \"transparent\"),\n        panel.background = element_rect(fill = dark_color, color = light_color),\n        panel.grid = element_blank(),\n        plot.background = element_rect(fill = dark_color, color = dark_color),\n        strip.background = element_rect(fill = dark_color, color = \"transparent\"),\n        strip.text = element_text(color = light_color, family = my_family),\n        ...)\n  \n}\n\n# Now set `theme_pearl_earring()` as the default theme\ntheme_set(theme_pearl_earring())\n\nNote how our custom theme_pearl_earing() function has a few adjustable parameters. Feel free to play around with alternative settings to see how they work. If we just use the defaults as we have defined them, here is our Figure 14.2.\n\nvary_effects |&gt; \n  ggplot(aes(x = a_cafe, y = b_cafe)) +\n  geom_point(color = \"#80A0C7\") +\n  geom_rug(color = \"#8B9DAF\", linewidth = 1/7)\n\n\n\n\n\n\n\n\nAgain, these are not “data.” Figure 14.2 shows a distribution of parameters. Here’s their Pearson’s correlation coefficient.\n\ncor(vary_effects$a_cafe, vary_effects$b_cafe)\n\n[1] -0.5721537\n\n\nSince there are only 20 rows in our vary_effects simulation, it shouldn’t be a surprise that the Pearson’s correlation is a bit off from the population value of \\(\\rho = -.7\\). If you rerun the simulation with n_cafes &lt;- 200, the Pearson’s correlation is much closer to the data generating value.\n\n\n14.1.2 Simulate observations\nHere we put those simulated parameters to use and simulate actual data from them.\n\nn_visits &lt;- 10\nsigma    &lt;-  0.5  # SD within cafes\n\nset.seed(22)  # Used to replicate example\n\nd &lt;- vary_effects |&gt; \n  mutate(cafe = 1:n_cafes) |&gt; \n  expand_grid(visit = 1:n_visits) |&gt; \n  mutate(afternoon = rep(0:1, times = n() / 2)) |&gt; \n  mutate(mu = a_cafe + b_cafe * afternoon) |&gt; \n  mutate(wait = rnorm(n = n(), mean = mu, sd = sigma)) |&gt; \n  select(cafe, everything())\n\nWe might peek at the data.\n\nd |&gt;\n  glimpse()\n\nRows: 200\nColumns: 7\n$ cafe      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,…\n$ a_cafe    &lt;dbl&gt; 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 4.223962, 2.010498, …\n$ b_cafe    &lt;dbl&gt; -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1.6093565, -1…\n$ visit     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6,…\n$ afternoon &lt;int&gt; 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,…\n$ mu        &lt;dbl&gt; 4.223962, 2.614606, 4.223962, 2.614606, 4.223962, 2.614606, 4.223962, 2.614606, 4.223962, 2.614606, 2.010498, …\n$ wait      &lt;dbl&gt; 3.9678929, 3.8571978, 4.7278755, 2.7610133, 4.1194827, 3.5436522, 4.1909492, 2.5332235, 4.1240321, 2.7648868, …\n\n\nNow we’ve finally simulated our data, we are ready to make our version of Figure 14.1, from way back on page 436.\n\nd |&gt;\n  mutate(afternoon = ifelse(afternoon == 0, \"M\", \"A\"),\n         day       = rep(rep(1:5, each = 2), times = n_cafes)) |&gt;\n  filter(cafe %in% c(3, 5)) |&gt;\n  mutate(cafe = str_c(\"café #\", cafe)) |&gt; \n  \n  ggplot(aes(x = visit, y = wait, group = day)) +\n  geom_point(aes(color = afternoon), size = 2) +\n  geom_line(color = \"#8B9DAF\") +\n  scale_x_continuous(NULL, breaks = 1:10, labels = rep(c(\"M\", \"A\"), times = 5)) +\n  scale_y_continuous(\"wait time in minutes\", limits = c(0, NA)) +\n  scale_color_manual(values = c(\"#80A0C7\", \"#EEDA9D\")) +\n  facet_wrap(~ cafe, ncol = 1) +\n  theme_pearl_earring(axis.ticks.x = element_blank(),\n                      legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n14.1.2.1 Rethinking: Simulation and misspecification\n\nIn this exercise, we are simulating data from a generative process and then analyzing that data with a model that reflects exactly the correct structure of that process. But in the real world, we’re never so lucky. Instead we are always forced to analyze data with a model that is misspecified: The true data-generating process is different than the model. Simulation can be used however to explore misspecification. Just simulate data from a process and then see how a number of models, none of which match exactly the data-generating process, perform. And always remember that Bayesian inference does not depend upon data-generating assumptions, such as the likelihood, being true. (p. 441)\n\n\n\n\n14.1.3 The varying slopes model\nThe statistical formula for our varying intercepts and slopes café model follows the form\n\\[\\begin{align*}\n\\text{wait}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i         & = \\alpha_{\\text{café}[i]} + \\beta_{\\text{café}[i]} \\text{afternoon}_i \\\\\n\\begin{bmatrix} \\alpha_\\text{café} \\\\ \\beta_\\text{café} \\end{bmatrix} & \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}, \\mathbf \\Sigma \\end{pmatrix} \\\\\n\\mathbf \\Sigma     & = \\begin{bmatrix} \\sigma_\\alpha & 0 \\\\ 0 & \\sigma_\\beta \\end{bmatrix} \\mathbf R \\begin{bmatrix} \\sigma_\\alpha & 0 \\\\ 0 & \\sigma_\\beta \\end{bmatrix} \\\\\n\\alpha        & \\sim \\operatorname{Normal}(5, 2) \\\\\n\\beta         & \\sim \\operatorname{Normal}(-1, 0.5) \\\\\n\\sigma        & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_\\alpha & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_\\beta  & \\sim \\operatorname{Exponential}(1) \\\\\n\\mathbf R     & \\sim \\operatorname{LKJcorr}(2),\n\\end{align*}\\]\nwhere \\(\\mathbf \\Sigma\\) is the covariance matrix and \\(\\mathbf R\\) is the corresponding correlation matrix, which we might more fully express as\n\\[\\mathbf R = \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}.\\]\nAnd according to our prior, \\(\\mathbf R\\) is distributed as \\(\\operatorname{LKJcorr}(2)\\). We’ll use rethinking::rlkjcorr() to get a better sense of what that even is.\n\nlibrary(rethinking)\n\nn_sim &lt;- 1e4\n\nset.seed(14)\nr_1 &lt;- rlkjcorr(n_sim, K = 2, eta = 1) |&gt;\n  data.frame()\n\nset.seed(14)\nr_2 &lt;- rlkjcorr(n_sim, K = 2, eta = 2) |&gt;\n  data.frame()\n\nset.seed(14)\nr_4 &lt;- rlkjcorr(n_sim, K = 2, eta = 4) |&gt;\n  data.frame()\n\nHere are the \\(\\text{LKJcorr}\\) distributions of Figure 14.3.\n\n# For annotation\ntext &lt;- tibble(\n  x     = c(0.83, 0.625, 0.45),\n  y     = c(0.56, 0.750, 1.07),\n  label = c(\"eta = 1\", \"eta = 2\", \"eta = 4\"))\n\n# Plot\nr_1 |&gt; \n  ggplot(aes(x = X2)) +\n  geom_density(adjust = 1/2, alpha = 2/3, color = \"transparent\", fill = \"#394165\") +\n  geom_density(data = r_2,\n               adjust = 1/2, alpha = 2/3, color = \"transparent\", fill = \"#DCA258\") +\n  geom_density(data = r_4,\n               adjust = 1/2, alpha = 2/3, color = \"transparent\", fill = \"#FCF9F0\") +\n  geom_text(data = text,\n            aes(x = x, y = y, label = label),\n            color = \"#A65141\", family = \"Courier\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"correlation\",\n       title = expression(LKJcorr(eta)))\n\n\n\n\n\n\n\n\nAs it turns out, the shape of the LKJ is sensitive to both \\(\\eta\\) and the \\(K\\) dimensions of the correlation matrix. Our simulations only considered the shapes for when \\(K = 2\\). We can use a combination of the parse_dist() and stat_dist_halfeye() functions from the tidybayes package to derive analytic solutions for different combinations of \\(\\eta\\) and \\(K\\).\n\nlibrary(tidybayes)\n\ncrossing(k   = 2:5,\n         eta = 1:4) |&gt; \n  mutate(prior = str_c(\"lkjcorr_marginal(\", k, \", \", eta, \")\"),\n         strip = str_c(\"K==\", k)) |&gt; \n  parse_dist(prior) |&gt;\n  \n  ggplot(aes(y = eta, dist = .dist, args = .args)) +\n  stat_dist_halfeye(.width = c(0.5, 0.95),\n                    color = \"#FCF9F0\", fill = \"#A65141\") +\n  scale_x_continuous(expression(rho), breaks = c(-1, -0.5, 0, 0.5, 1),\n                     labels = c(\"-1\", \"-.5\", \"0\", \".5\", \"1\"), limits = c(-1, 1)) +\n  scale_y_continuous(expression(eta), breaks = 1:4) +\n  ggtitle(expression(\"Marginal correlation for the LKJ prior relative to K and \"*eta)) +\n  facet_wrap(~ strip, labeller = label_parsed, ncol = 4)\n\n\n\n\n\n\n\n\nTo learn more about this plotting method, check out Kay’s (2020) Marginal distribution of a single correlation from an LKJ distribution. To get a better intuition what that plot means, check out the illuminating blog post by Stephen Martin, Is the LKJ(1) prior uniform? “Yes”.\nOkay, let’s get ready to model and switch out rethinking for brms.\n\ndetach(package:rethinking, unload = T)\nlibrary(brms)\n\nAs defined above, our first model has both varying intercepts and afternoon slopes. I should point out that the (1 + afternoon | cafe) syntax specifies that we’d like brm() to fit the random effects for 1 (i.e., the intercept) and the afternoon slope as correlated. Had we wanted to fit a model in which they were orthogonal, we’d have coded (1 + afternoon || cafe).\n\nb14.1 &lt;- brm(\n  data = d, \n  family = gaussian,\n  wait ~ 1 + afternoon + (1 + afternoon | cafe),\n  prior = c(prior(normal(5, 2), class = Intercept),\n            prior(normal(-1, 0.5), class = b),\n            prior(exponential(1), class = sd),\n            prior(exponential(1), class = sigma),\n            prior(lkj(2), class = cor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 867530,\n  file = \"fits/b14.01\")\n\nWith Figure 14.4, we assess how the posterior for the correlation of the random effects compares to its prior.\n\npost &lt;- as_draws_df(b14.1)\n\npost |&gt;\n  ggplot() +\n  geom_density(data = r_2, aes(x = X2),\n               alpha = 3/4, color = \"transparent\", fill = \"#EEDA9D\") +\n  geom_density(aes(x = cor_cafe__Intercept__afternoon),\n               alpha = 9/10, color = \"transparent\", fill = \"#A65141\") +\n  annotate(geom = \"text\", \n           x = c(-0.15, 0), y = c(2.21, 0.85), \n           label = c(\"posterior\", \"prior\"), \n           color = c(\"#A65141\", \"#EEDA9D\"), family = \"Courier\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(x = \"correlation\",\n       subtitle = \"Correlation between intercepts\\nand slopes, prior and posterior\")\n\n\n\n\n\n\n\n\nMcElreath then depicted multidimensional shrinkage by plotting the posterior mean of the varying effects compared to their raw, unpooled estimated. With brms, we can get the cafe-specific intercepts and afternoon slopes with coef(), which returns a three-dimensional list.\n\n# coef(b14.1) |&gt; glimpse()\ncoef(b14.1)\n\n$cafe\n, , Intercept\n\n   Estimate Est.Error     Q2.5    Q97.5\n1  4.213297 0.1978596 3.829642 4.590972\n2  2.157338 0.2013847 1.769423 2.549774\n3  4.378303 0.2049147 3.978473 4.786580\n4  3.243736 0.1985901 2.843128 3.621809\n5  1.878760 0.2058461 1.473869 2.280121\n6  4.263000 0.1978386 3.858712 4.639112\n7  3.615683 0.1994118 3.222665 4.007438\n8  3.947060 0.1967307 3.561294 4.339305\n9  3.979611 0.1976517 3.597588 4.364261\n10 3.558939 0.2000770 3.172513 3.955428\n11 1.932402 0.2053137 1.528221 2.331512\n12 3.836842 0.2022665 3.429331 4.222623\n13 3.891585 0.2014402 3.506540 4.280477\n14 3.176418 0.1942546 2.791193 3.557988\n15 4.450167 0.2060186 4.037785 4.851168\n16 3.390435 0.2063019 2.981886 3.792157\n17 4.216905 0.2013102 3.827670 4.611066\n18 5.745353 0.2054073 5.337968 6.155167\n19 3.244080 0.2066967 2.842267 3.636957\n20 3.739302 0.1970768 3.360855 4.137549\n\n, , afternoon\n\n     Estimate Est.Error       Q2.5      Q97.5\n1  -1.1555017 0.2568967 -1.6603627 -0.6388167\n2  -0.9014608 0.2655887 -1.4199688 -0.3626409\n3  -1.9416692 0.2732649 -2.4709132 -1.4135964\n4  -1.2340735 0.2579623 -1.7488369 -0.7370762\n5  -0.1342429 0.2704266 -0.6536181  0.4017659\n6  -1.3025528 0.2560035 -1.8104277 -0.8086929\n7  -1.0223715 0.2641168 -1.5501771 -0.5117426\n8  -1.6305479 0.2662107 -2.1559578 -1.1222900\n9  -1.2991796 0.2632013 -1.8216574 -0.7910278\n10 -0.9515703 0.2630617 -1.4607206 -0.4360276\n11 -0.4279579 0.2753179 -0.9847493  0.1043620\n12 -1.1761338 0.2669048 -1.6873338 -0.6666824\n13 -1.8149738 0.2710414 -2.3536671 -1.2954402\n14 -0.9432858 0.2579568 -1.4528844 -0.4381805\n15 -2.1901761 0.2855867 -2.7617111 -1.6327983\n16 -1.0467058 0.2693629 -1.5738346 -0.5120496\n17 -1.2246217 0.2678882 -1.7545433 -0.6888909\n18 -1.0276014 0.2841628 -1.5786265 -0.4645198\n19 -0.2607186 0.2790455 -0.8029477  0.2769846\n20 -1.0669509 0.2613099 -1.5783545 -0.5341076\n\n\nHere’s the code to extract the relevant elements from the coef() list, convert them to a tibble, and add the cafe index.\n\n# With this line we select each of the 20 cafe's posterior mean (i.e., Estimate)\n# for both `Intercept` and `afternoon`\npartially_pooled_params &lt;- coef(b14.1)$cafe[ , 1, 1:2] |&gt;\n  data.frame() |&gt;                 # Convert the two vectors to a data frame\n  rename(Slope = afternoon) |&gt;\n  mutate(cafe = row_number()) |&gt;  # Add the `cafe` index\n  select(cafe, everything())      # Simply moving `cafe` to the leftmost position\n\nLike McElreath, we’ll compute the unpooled estimates directly from the data.\n\n# Compute unpooled estimates directly from data\nun_pooled_params &lt;- d |&gt;\n  # With these two lines, we compute the mean value for each cafe's \n  # wait time in the morning and then the afternoon\n  group_by(afternoon, cafe) |&gt;\n  summarise(mean = mean(wait)) |&gt;\n  ungroup() |&gt;  # Ungrouping lets us to alter the grouping variable `afternoon`\n  mutate(afternoon = ifelse(afternoon == 0, \"Intercept\", \"Slope\")) |&gt;\n  spread(key = afternoon, value = mean) |&gt;  # Use `spread()` just like above\n  mutate(Slope = Slope - Intercept)         # Finally, here's our slope!\n\n# Here we combine the partially-pooled and unpooled means into a single \n# data object, which will make plotting easier.\n# `bind_rows()` will stack the second tibble below the first\nparams &lt;- bind_rows(partially_pooled_params, un_pooled_params) |&gt;\n  # Index whether the estimates are pooled\n  mutate(pooled = rep(c(\"partially\", \"not\"), each = n() / 2)) \n\n# Here's a glimpse at what we've been working for\nparams |&gt;\n  slice(c(1:5, 36:40))\n\n      cafe Intercept       Slope    pooled\n1        1  4.213297 -1.15550174 partially\n2        2  2.157338 -0.90146084 partially\n3        3  4.378303 -1.94166924 partially\n4        4  3.243736 -1.23407346 partially\n5        5  1.878760 -0.13424290 partially\n...6    16  3.373496 -1.02563866       not\n...7    17  4.236192 -1.22236910       not\n...8    18  5.755987 -0.87660383       not\n...9    19  3.121060  0.01441784       not\n...10   20  3.728481 -1.03811567       not\n\n\nFinally, here’s our code for Figure 14.5.a, showing shrinkage in two dimensions.\n\np1 &lt;- ggplot(data = params, aes(x = Intercept, y = Slope)) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 1/10, linewidth = 0) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 2/10, linewidth = 0) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 3/10, linewidth = 0) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 4/10, linewidth = 0) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 5/10, linewidth = 0) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 6/10, linewidth = 0) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 7/10, linewidth = 0) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 8/10, linewidth = 0) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 9/10, linewidth = 0) +\n  stat_ellipse(geom = \"polygon\", type = \"norm\", alpha = 1/20, fill = \"#E7CDC2\", level = 0.99, linewidth = 0) +\n  geom_point(aes(group = cafe, color = pooled)) +\n  geom_line(aes(group = cafe), linewidth = 1/4) +\n  scale_color_manual(\"Pooled?\", values = c(\"#80A0C7\", \"#A65141\")) +\n  coord_cartesian(xlim = range(params$Intercept),\n                  ylim = range(params$Slope))\n\np1\n\n\n\n\n\n\n\n\nLearn more about stat_ellipse(), here. Let’s prep for Figure 14.5.b.\n\n# Retrieve the partially-pooled estimates with `coef()`\npartially_pooled_estimates &lt;- coef(b14.1)$cafe[ , 1, 1:2] |&gt;\n  # Convert the two vectors to a data frame\n  data.frame() |&gt;\n  # The Intercept is the wait time for morning (i.e., `afternoon == 0`)\n  rename(morning = Intercept) |&gt;\n  # `afternoon` wait time is the `morning` wait time plus the afternoon slope\n  mutate(afternoon = morning + afternoon,\n         cafe      = row_number()) |&gt;  # Add the `cafe` index\n  select(cafe, everything()) \n\n# Compute unpooled estimates directly from data\nun_pooled_estimates &lt;- d |&gt;\n  # As above, these two lines compute each cafe's mean wait value by time of day\n  group_by(afternoon, cafe) |&gt; \n  summarise(mean = mean(wait)) |&gt;\n  # Ungrouping lets us alter the grouping variable, `afternoon`\n  ungroup() |&gt; \n  mutate(afternoon = ifelse(afternoon == 0, \"morning\", \"afternoon\")) |&gt;\n  # This separates out the values into `morning` and `afternoon` columns\n  spread(key = afternoon, value = mean)\n\nestimates &lt;- bind_rows(partially_pooled_estimates, un_pooled_estimates) |&gt;\n  mutate(pooled = rep(c(\"partially\", \"not\"), each = n() / 2))\n\nThe code for Figure 14.5.b.\n\np2 &lt;- ggplot(data = estimates, aes(x = morning, y = afternoon)) +\n  # Nesting `stat_ellipse()` within `mapply()` is a less redundant way to produce the \n  # ten-layered semitransparent ellipses we did with ten lines of `stat_ellipse()` \n  # functions in the previous plot\n  mapply(function(level) {\n    stat_ellipse(geom  = \"polygon\", type = \"norm\",\n                 alpha = 1/20, fill = \"#E7CDC2\",\n                 level = level, linewidth = 0)\n    }, \n    # Enter the levels here\n    level = c(1:9 / 10, 0.99)) +\n  geom_point(aes(group = cafe, color = pooled)) +\n  geom_line(aes(group = cafe), linewidth = 1/4) +\n  scale_color_manual(\"Pooled?\", values = c(\"#80A0C7\", \"#A65141\")) +\n  labs(x = \"morning wait (mins)\",\n       y = \"afternoon wait (mins)\") +\n  coord_cartesian(xlim = range(estimates$morning),\n                  ylim = range(estimates$afternoon))\n\nHere we combine the two subplots together with patchwork syntax.\n\nlibrary(patchwork)\n\n(p1 + theme(legend.position = \"none\")) + \n  p2 + \n  plot_annotation(title = \"Shrinkage in two dimensions\")\n\n\n\n\n\n\n\n\n\nWhat I want you to appreciate in this plot is that shrinkage on the parameter scale naturally produces shrinkage where we actually care about it: on the outcome scale. And it also implies a population of wait times, shown by the [semitransparent ellipses]. That population is now positively correlated–cafés with longer morning waits also tend to have longer afternoon waits. They are popular, after all. But the population lies mostly below the dashed line where the waits are equal. You’ll wait less in the afternoon, on average. (p. 446)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Adventures in Covariance</span>"
    ]
  },
  {
    "objectID": "14.html#advanced-varying-slopes",
    "href": "14.html#advanced-varying-slopes",
    "title": "14  Adventures in Covariance",
    "section": "14.2 Advanced varying slopes",
    "text": "14.2 Advanced varying slopes\nIn Section 13.3 we saw that data can be considered cross-classified if they have multiple grouping factors. We used the chipanzees data in that section and we only considered cross-classification by single intercepts. Turns out cross-classified models can be extended further. Let’s load and wrangle those data.\n\ndata(chimpanzees, package = \"rethinking\")\nd &lt;- chimpanzees\nrm(chimpanzees)\n\n# Wrangle\nd &lt;- d |&gt; \n  mutate(actor     = factor(actor),\n         block     = factor(block),\n         treatment = factor(1 + prosoc_left + 2 * condition),\n         # This will come in handy, later\n         labels    = factor(treatment,\n                            levels = 1:4,\n                            labels = c(\"r/n\", \"l/n\", \"r/p\", \"l/p\")))\n\nglimpse(d)\n\nRows: 504\nColumns: 10\n$ actor        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ recipient    &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ condition    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ block        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,…\n$ trial        &lt;int&gt; 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56,…\n$ prosoc_left  &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,…\n$ chose_prosoc &lt;int&gt; 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ pulled_left  &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ treatment    &lt;fct&gt; 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1,…\n$ labels       &lt;fct&gt; r/n, r/n, l/n, r/n, l/n, l/n, l/n, l/n, r/n, r/n, r/n, l/n, r/n, l/n, r/n, l/n, l/n, r/n, l/n, r/n, r/n, r/…\n\n\nIf I’m following along correctly with the text, McElreath’s m14.2 uses the centered parameterization. Recall from the last chapter that brms only supports the non-centered parameterization. Happily, McElreath’s m14.3 appears to use the non-centered parameterization. Thus, we’ll skip making a b14.2 and jump directly into making a b14.3. I believe one could describe the statistical model as\n\\[\\begin{align*}\n\\text{left\\_pull}_i & \\sim \\operatorname{Binomial}(n_i = 1, p_i) \\\\\n\\operatorname{logit} (p_i) & = \\gamma_{\\text{treatment}[i]} + \\alpha_{\\text{actor}[i], \\text{treatment}[i]} + \\beta_{\\text{block}[i], \\text{treatment}[i]} \\\\\n\\gamma_j & \\sim \\operatorname{Normal}(0, 1), \\;\\;\\; \\text{for } j = 1, \\dots, 4 \\\\\n\\begin{bmatrix} \\alpha_{j, 1} \\\\ \\alpha_{j, 2} \\\\ \\alpha_{j, 3} \\\\ \\alpha_{j, 4} \\end{bmatrix} & \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma_\\text{actor} \\end{pmatrix} \\\\\n\\begin{bmatrix} \\beta_{j, 1} \\\\ \\beta_{j, 2} \\\\ \\beta_{j, 3} \\\\ \\beta_{j, 4} \\end{bmatrix} & \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf \\Sigma_\\text{block} \\end{pmatrix} \\\\\n\\mathbf \\Sigma_\\text{actor} & = \\mathbf{S_\\alpha R_\\alpha S_\\alpha} \\\\\n\\mathbf \\Sigma_\\text{block} & = \\mathbf{S_\\beta R_\\beta S_\\beta} \\\\\n\\sigma_{\\alpha, [1]}, \\dots, \\sigma_{\\alpha, [4]} & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_{\\beta, [1]}, \\dots, \\sigma_{\\beta, [4]}   & \\sim \\operatorname{Exponential}(1) \\\\\n\\mathbf R_\\alpha & \\sim \\operatorname{LKJ}(2) \\\\\n\\mathbf R_\\beta  & \\sim \\operatorname{LKJ}(2).\n\\end{align*}\\]\nIn this model, we have four population-level intercepts, \\(\\gamma_1, \\dots, \\gamma_4\\), one for each of the four levels of treatment. There are two higher-level grouping variables, actor and block, making this a cross-classified model.\nThe term \\(\\alpha_{\\text{actor}[i], \\text{treatment}[i]}\\) is meant to convey that each of the treatment effects can vary by actor. The first line containing the \\(\\operatorname{MVNormal}(\\cdot)\\) operator indicates the actor-level deviations from the population-level estimates for \\(\\gamma_j\\) follow the multivariate normal distribution where the four means are set to zero (i.e., they are deviations) and their spread around those zeros are controlled by \\(\\Sigma_\\text{actor}\\). In the first line below the last line containing \\(\\operatorname{MVNormal}(\\cdot)\\), we learn that \\(\\Sigma_\\text{actor}\\) can be decomposed into two terms, \\(\\mathbf S_\\alpha\\) and \\(\\mathbf R_\\alpha\\). It may not yet be clear by the notation, but \\(\\mathbf S_\\alpha\\) is a \\(4 \\times 4\\) matrix,\n\\[\n\\mathbf S_\\alpha = \\begin{bmatrix} \\sigma_{\\alpha, [1]} & 0 & 0 & 0 \\\\ 0 & \\sigma_{\\alpha, [2]} & 0 & 0 \\\\ 0 & 0 & \\sigma_{\\alpha, [3]} & 0 \\\\ 0 & 0 & 0 & \\sigma_{\\alpha, [4]} \\end{bmatrix}.\n\\]\nIn a similar way, \\(\\mathbf R_\\alpha\\) is a \\(4 \\times 4\\) matrix,\n\\[\n\\mathbf R_\\alpha = \\begin{bmatrix} 1 & \\rho_{\\alpha, [1, 2]} & \\rho_{\\alpha, [1, 3]} & \\rho_{\\alpha, [1, 4]} \\\\ \\rho_{\\alpha, [2, 1]} & 1 & \\rho_{\\alpha, [2, 3]} & \\rho_{\\alpha, [2, 4]} \\\\ \\rho_{\\alpha, [3, 1]} & \\rho_{\\alpha, [3, 2]} & 1 & \\rho_{\\alpha, [3, 4]} \\\\ \\rho_{\\alpha, [4, 1]} & \\rho_{\\alpha, [4, 2]} & \\rho_{\\alpha, [4, 3]} & 1 \\end{bmatrix}.\n\\]\nThe same overall pattern holds true for \\(\\beta_{\\text{block}[i], \\text{treatment}[i]}\\) and the associated \\(\\beta\\) parameters connected to the block grouping variable. All the population parameters \\(\\sigma_{\\alpha, [1]}, \\dots, \\sigma_{\\alpha, [4]}\\) and \\(\\sigma_{\\beta, [1]}, \\dots, \\sigma_{\\beta, [4]}\\) have individual \\(\\operatorname{Exponential}(1)\\) priors. The two \\(\\mathbf R_{&lt; \\cdot &gt;}\\) matrices have the priors \\(\\operatorname{LKJ}(2)\\).\nI know; this is a lot. This all takes time to grapple with. Here’s how to fit such a model with brms.\n\nb14.3 &lt;- brm(\n  data = d, \n  family = binomial,\n  pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor) + (0 + treatment | block),\n  prior = c(prior(normal(0, 1), class = b),\n            prior(exponential(1), class = sd, group = actor),\n            prior(exponential(1), class = sd, group = block),\n            prior(lkj(2), class = cor, group = actor),\n            prior(lkj(2), class = cor, group = block)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,  \n  seed = 4387510,\n  file = \"fits/b14.03\")\n\nHappily, we got no warnings about divergent transitions. Since it’s been a while, we’ll use bayesplot::mcmc_rank_overlay() to examine the primary parameters with a trank plot.\n\nlibrary(bayesplot)\n\n# Give the parameters fancy names\nnames &lt;- c(\n  str_c(\"treatment[\", 1:4, \"]\"), \n  str_c(\"sigma['actor[\", 1:4, \"]']\"), \n  str_c(\"sigma['block[\", 1:4, \"]']\"), \n  str_c(\"rho['actor:treatment[\", c(1, 1:2, 1:3), \",\", rep(2:4, times = 1:3), \"]']\"), \n  str_c(\"rho['block:treatment[\", c(1, 1:2, 1:3), \",\", rep(2:4, times = 1:3), \"]']\"), \n  \"Chain\")\n\n# Wrangle\nas_draws_df(b14.3) |&gt; \n  select(b_treatment1:`cor_block__treatment3__treatment4`, .chain) |&gt; \n  set_names(names) |&gt; \n  \n  # Plot\n  mcmc_rank_overlay() +\n  scale_x_continuous(NULL, breaks = 0:4 * 1e3, labels = c(0, str_c(1:4, \"K\"))) +\n  scale_color_manual(values = c(\"#80A0C7\", \"#B1934A\", \"#A65141\", \"#EEDA9D\")) +\n  coord_cartesian(ylim = c(30, NA)) +\n  ggtitle(\"McElreath would love this trank plot.\") +\n  facet_wrap(~ parameter, labeller = label_parsed, ncol = 4) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nBecause we only fit a non-centered version of the model, we aren’t able to make a faithful version of McElreath’s Figure 14.6. However, we can still use posterior::summarise_draws() to help make histograms of the two kinds of effective sample sizes for our b14.3.\n\nlibrary(posterior)\n\nas_draws_df(b14.3) |&gt; \n  summarise_draws() |&gt; \n  pivot_longer(starts_with(\"ess\")) |&gt; \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 250, color = \"#DCA258\", fill = \"#EEDA9D\") +\n  xlim(0, NA) +\n  facet_wrap(~ name)\n\n\n\n\n\n\n\n\nHere is a summary of the model parameters.\n\nprint(b14.3)\n\n Family: binomial \n  Links: mu = logit \nFormula: pulled_left | trials(1) ~ 0 + treatment + (0 + treatment | actor) + (0 + treatment | block) \n   Data: d (Number of observations: 504) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~actor (Number of levels: 7) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(treatment1)                 1.39      0.50     0.69     2.58 1.00     2163     2656\nsd(treatment2)                 0.91      0.40     0.34     1.91 1.00     2580     2609\nsd(treatment3)                 1.83      0.57     0.99     3.25 1.00     2820     2923\nsd(treatment4)                 1.58      0.61     0.75     3.07 1.00     3035     2829\ncor(treatment1,treatment2)     0.42      0.29    -0.20     0.87 1.00     2540     2791\ncor(treatment1,treatment3)     0.53      0.25    -0.06     0.90 1.00     2558     3083\ncor(treatment2,treatment3)     0.48      0.27    -0.11     0.89 1.00     2732     2900\ncor(treatment1,treatment4)     0.44      0.27    -0.16     0.87 1.00     2757     2733\ncor(treatment2,treatment4)     0.44      0.28    -0.18     0.88 1.00     3169     3223\ncor(treatment3,treatment4)     0.57      0.24     0.00     0.92 1.00     3078     3302\n\n~block (Number of levels: 6) \n                           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(treatment1)                 0.40      0.33     0.02     1.24 1.00     2141     2318\nsd(treatment2)                 0.46      0.35     0.02     1.34 1.00     1724     2046\nsd(treatment3)                 0.30      0.26     0.01     0.97 1.00     3124     2322\nsd(treatment4)                 0.47      0.37     0.02     1.39 1.00     2078     2544\ncor(treatment1,treatment2)    -0.08      0.37    -0.74     0.62 1.00     3633     3014\ncor(treatment1,treatment3)    -0.01      0.38    -0.71     0.70 1.00     6210     3184\ncor(treatment2,treatment3)    -0.03      0.38    -0.71     0.70 1.00     4125     2916\ncor(treatment1,treatment4)     0.06      0.37    -0.65     0.74 1.00     4038     2875\ncor(treatment2,treatment4)     0.05      0.37    -0.65     0.72 1.00     3795     3075\ncor(treatment3,treatment4)     0.02      0.38    -0.70     0.71 1.00     3686     3623\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntreatment1     0.22      0.50    -0.75     1.23 1.00     1848     2448\ntreatment2     0.65      0.42    -0.21     1.48 1.00     1985     2617\ntreatment3    -0.02      0.58    -1.16     1.14 1.00     2720     2621\ntreatment4     0.68      0.53    -0.38     1.75 1.00     2733     2936\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLike McElreath explained on page 450, our b14.3 has 76 parameters:\n\n4 average treatment effects, as listed in the ‘Population-Level Effects’ section;\n7 \\(\\times\\) 4 = 28 varying effects on actor, as indicated in the ‘~actor:treatment (Number of levels: 7)’ header multiplied by the four levels of treatment;\n6 \\(\\times\\) 4 = 24 varying effects on block, as indicated in the ‘~block:treatment (Number of levels: 6)’ header multiplied by the four levels of treatment;\n8 standard deviations listed in the eight rows beginning with sd(; and\n12 free correlation parameters listed in the eight rows beginning with cor(.\n\nCompute the WAIC estimate.\n\nb14.3 &lt;- add_criterion(b14.3, criterion = \"waic\")\n\nwaic(b14.3)\n\n\nComputed from 4000 by 504 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -272.8  9.9\np_waic        27.2  1.5\nwaic         545.5 19.8\n\n1 (0.2%) p_waic estimates greater than 0.4. We recommend trying loo instead. \n\n\nLike the \\(p_\\text{WAIC}\\), our brms version of the model has about 27 effective parameters. Now we’ll get a better sense of the model with a posterior predictive check in the form of our version of Figure 14.7. McElreath described his R code 14.22 as “a big chunk of code” (p. 451). I’ll leave up to the reader to decide whether our big code chunk is any better.\n\n# For annotation\ntext &lt;- distinct(d, labels) |&gt; \n  mutate(actor = 1,\n         prop  = c(0.07, 0.8, 0.08, 0.795))\n\nnd &lt;- d |&gt; \n  distinct(actor, condition, labels, prosoc_left, treatment) |&gt; \n  mutate(block = 5)\n\n# Compute and wrangle the posterior predictions\nfitted(b14.3, newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  # Add the empirical proportions\n  left_join(\n    d |&gt;\n      group_by(actor, treatment) |&gt;\n      mutate(proportion = mean(pulled_left)) |&gt; \n      distinct(actor, treatment, proportion),\n    by = c(\"actor\", \"treatment\")\n  ) |&gt; \n  mutate(condition = factor(condition)) |&gt; \n  \n  # Plot!\n  ggplot(aes(x = labels)) +\n  geom_hline(yintercept = 0.5, alpha = 1/2, color = \"#E8DCCF\", linetype = 2) +\n  # Empirical proportions\n  geom_line(aes(y = proportion, group = prosoc_left),\n            color = \"#394165\", linewidth = 1/4) +\n  geom_point(aes(y = proportion, shape = condition),\n             color = \"#394165\", fill = \"#100F14\", size = 2.5, show.legend = F) + \n  # Posterior predictions\n  geom_line(aes(y = Estimate, group = prosoc_left),\n            color = \"#80A0C7\", linewidth = 3/4) +\n  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5, shape = condition),\n                  color = \"#80A0C7\", fill = \"#100F14\", linewidth = 1/3, size = 2/3, show.legend = F) + \n  # Annotation for the conditions\n  geom_text(data = text,\n            aes(y = prop, label = labels), \n            color = \"#DCA258\", family = \"Courier\", size = 3) +\n  scale_x_discrete(NULL, breaks = NULL) +\n  scale_y_continuous(\"proportion left lever\", breaks = 0:2 / 2, labels = c(\"0\", \".5\", \"1\")) +\n  scale_shape_manual(values = c(21, 19)) +\n  labs(subtitle = \"Posterior predictions, in light blue, against the raw data, in dark blue, for\\nmodel b14.3, the cross-classified varying effects model.\") +\n  facet_wrap(~ actor, nrow = 1, labeller = label_both)\n\n\n\n\n\n\n\n\n\nThese chimpanzees simply did not behave in any consistently different way in the partner treatments. The model we’ve used here does have some advantages, though. Since it allows for some individuals to differ in how they respond to the treatments, it could reveal a situation in which a treatment has no effect on average, even though some of the individuals respond strongly. That wasn’t the case here. But often we are more interested in the distribution of responses than in the average response, so a model that estimates the distribution of treatment effects is very useful. (p. 452)\n\nFor more practice with models of this kind, check out my blog post, Multilevel models and the index-variable approach.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Adventures in Covariance</span>"
    ]
  },
  {
    "objectID": "14.html#instruments-and-causal-designs",
    "href": "14.html#instruments-and-causal-designs",
    "title": "14  Adventures in Covariance",
    "section": "14.3 Instruments and causal designs",
    "text": "14.3 Instruments and causal designs\n\nOf course sometimes it won’t be possible to close all of the non-causal paths or rule of unobserved confounds. What can be done in that case? More than nothing. If you are lucky, there are ways to exploit a combination of natural experiments and clever modeling that allow causal inference even when non-causal paths cannot be closed. (p. 455)\n\n\n14.3.1 Instrumental variables\nSay were are interested in the causal impact of education \\(E\\) on wages \\(W\\), \\(E \\rightarrow W\\). Further imagine there is some unmeasured variable \\(U\\) that has causal relations with both, \\(E \\leftarrow U \\rightarrow W\\), creating a backdoor path. We might use good old ggdag to plot the DAG.\n\nlibrary(ggdag)\n\ndag_coords &lt;- tibble(\n  name = c(\"E\", \"U\", \"W\"),\n  x    = c(1, 2, 3),\n  y    = c(1, 2, 1))\n\nBefore we make the plot, we’ll make a custom theme, theme_pearl_dag(), to streamline our DAG plots.\n\ntheme_pearl_dag &lt;- function(...) {\n  theme_pearl_earring() +\n    theme_dag() + \n    theme(panel.background = element_rect(fill = \"#100F14\"),\n          ...)\n}\n\ndagify(E ~ U,\n       W ~ E + U,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"U\"),\n                 fill = \"#FCF9F0\", shape = 21, size = 6, stroke = 2, show.legend = F) +\n  geom_dag_text(color = \"#100F14\", family = \"Courier\") +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(\"#EEDA9D\", \"#A65141\")) +\n  theme_pearl_dag()\n\n\n\n\n\n\n\n\nInstrumental variables will solve some of the difficulties we have in not being able to condition on \\(U\\). Here we’ll call our instrumental variable \\(Q\\). In the terms of the present example, the instrumental variable has the qualities that\n\n\\(Q\\) is independent of \\(U\\),\n\\(Q\\) is not independent of \\(E\\), and\n\\(Q\\) can only influence \\(W\\) through \\(E\\) (i.e., the effect of \\(Q\\) on \\(W\\) is fully mediated by \\(E\\)).\n\nThere is what this looks like in a DAG.\n\ndag_coords &lt;- tibble(\n  name = c(\"Q\", \"E\", \"U\", \"W\"),\n  x    = c(0, 1, 2, 3),\n  y    = c(2, 1, 2, 1))\n\ndagify(E ~ Q + U,\n       W ~ E + U,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"U\"),\n                 fill = \"#FCF9F0\", shape = 21, size = 6, stroke = 2, show.legend = F) +\n  geom_dag_text(color = \"#100F14\", family = \"Courier\") +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(\"#EEDA9D\", \"#A65141\")) +\n  theme_pearl_dag()\n\n\n\n\n\n\n\n\nSadly, our condition that \\(Q\\) can only influence \\(W\\) through \\(E\\)–often called the exclusion restriction–generally cannot be tested. Given \\(U\\) is unmeasured, by definition, we also cannot test that \\(Q\\) is independent of \\(U\\). These are model assumptions.\nLet’s simulate data based on Angrist & Keueger (1991) to get a sense of how this works.\n\n# Make a standardizing function\nstandardize &lt;- function(x) {\n  (x - mean(x)) / sd(x)\n}\n\n# Simulate\nset.seed(73) \n\nn &lt;- 500\n\ndat_sim &lt;- tibble(u_sim = rnorm(n, mean = 0, sd = 1),\n                  q_sim = sample(1:4, size = n, replace = T)) |&gt; \n  mutate(e_sim = rnorm(n, mean = u_sim + q_sim, sd = 1)) |&gt; \n  mutate(w_sim = rnorm(n, mean = u_sim + 0 * e_sim, sd = 1)) |&gt; \n  mutate(w = standardize(w_sim),\n         e = standardize(e_sim),\n         q = standardize(q_sim))\n\ndat_sim\n\n# A tibble: 500 × 7\n     u_sim q_sim e_sim   w_sim       w       e      q\n     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.145      1 1.51   0.216   0.173  -0.575  -1.36 \n 2  0.291      1 0.664  0.846   0.584  -1.09   -1.36 \n 3  0.0938     3 2.44  -0.664  -0.402  -0.0185  0.428\n 4 -0.127      3 4.09  -0.725  -0.442   0.978   0.428\n 5 -0.847      4 2.62  -1.24   -0.780   0.0939  1.32 \n 6  0.141      4 3.54  -0.0700 -0.0146  0.651   1.32 \n 7  1.54       2 3.65   1.88    1.26    0.714  -0.464\n 8  2.74       3 4.91   2.52    1.67    1.48    0.428\n 9  1.55       3 4.18   0.624   0.439   1.04    0.428\n10  0.462      1 0.360  0.390   0.286  -1.27   -1.36 \n# ℹ 490 more rows\n\n\n\\(Q\\) in this context is like quarter in the school year, but inversely scaled such that larger numbers indicate more quarters. In this simulation, we have set the true effect of education on wages–\\(E \\rightarrow W\\)–to be zero. Any univariate association is through the confounding variable \\(U\\). Also, \\(Q\\) has no direct effect on \\(W\\) or \\(U\\), but it does have a causal relation with \\(E\\), which is \\(Q \\rightarrow E \\leftarrow U\\). First we fit the univariable model corresponding to \\(E \\rightarrow W\\).\n\nb14.4 &lt;- brm(\n  data = dat_sim,\n  family = gaussian,\n  w ~ 1 + e,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,  \n  seed = 14,\n  file = \"fits/b14.04\")\n\n\nprint(b14.4)\n\n Family: gaussian \n  Links: mu = identity \nFormula: w ~ 1 + e \n   Data: dat_sim (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.04    -0.08     0.08 1.00     4366     3289\ne             0.40      0.04     0.32     0.48 1.00     3866     2712\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.92      0.03     0.87     0.98 1.00     4561     2935\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBecause we have not conditioned on \\(U\\), then model suggests a moderately large spurious causal relation for \\(E \\rightarrow W\\). Now see what happens when we also condition directly on \\(Q\\), as in \\(Q \\rightarrow W \\leftarrow E\\).\n\nb14.5 &lt;- brm(\n  data = dat_sim,\n  family = gaussian,\n  w ~ 1 + e + q,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,  \n  seed = 14,\n  file = \"fits/b14.05\")\n\n\nprint(b14.5)\n\n Family: gaussian \n  Links: mu = identity \nFormula: w ~ 1 + e + q \n   Data: dat_sim (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.04    -0.07     0.07 1.00     3652     3159\ne             0.64      0.05     0.54     0.73 1.00     3623     2952\nq            -0.41      0.05    -0.50    -0.31 1.00     3444     2824\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.86      0.03     0.81     0.91 1.00     4070     3007\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHoly smokes that’s a mess. This model suggests both \\(E\\) and \\(Q\\) have moderate to strong causal effects on \\(W\\), even though we know neither do based on the true data-generating model. Like McElreath said, “bad stuff happens” when we condition on an instrumental variable this way.\n\nThere is no backdoor path through \\(Q\\), as you can see. But there is a non-causal path from \\(Q\\) to \\(W\\) through \\(U\\): \\(Q \\rightarrow E \\leftarrow U \\rightarrow W\\). This is a non-causal path, because changing \\(Q\\) doesn’t result in any change in \\(W\\) through this path. But since we are conditioning on \\(E\\) in the same model, and \\(E\\) is a collider of \\(Q\\) and \\(U\\), the non-causal path is open. This confounds the coefficient on \\(Q\\). It won’t be zero, because it’ll pick up the association between \\(U\\) and \\(W\\). And then, as a result, the coefficient on \\(E\\) can get even more confounded. Used this way, an instrument like \\(Q\\) might be called a bias amplifier. (p. 456, emphasis in the original)\n\nThe statistical solution to this mess is to express the data-generating DAG as a multivariate statistical model following the form\n\\[\\begin{align*}\n\\begin{bmatrix} W_i \\\\ E_i \\end{bmatrix} & \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} \\mu_{\\text W,i} \\\\ \\mu_{\\text E,i} \\end{bmatrix}, \\color{#A65141}{\\mathbf \\Sigma} \\end{pmatrix} \\\\\n\\mu_{\\text W,i} & = \\alpha_\\text W + \\beta_\\text{EW} E_i \\\\\n\\mu_{\\text E,i} & = \\alpha_\\text E + \\beta_\\text{QE} Q_i \\\\\n\\color{#A65141}{\\mathbf\\Sigma} & \\color{#A65141}= \\color{#A65141}{\\begin{bmatrix} \\sigma_\\text W & 0 \\\\ 0  & \\sigma_\\text E \\end{bmatrix} \\mathbf R \\begin{bmatrix} \\sigma_\\text W & 0 \\\\ 0  & \\sigma_\\text E \\end{bmatrix}} \\\\\n\\color{#A65141}{\\mathbf R} & \\color{#A65141}= \\color{#A65141}{\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}} \\\\\n\\alpha_\\text W \\text{ and } \\alpha_\\text E & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_\\text{EW} \\text{ and } \\beta_\\text{QE} & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma_\\text W \\text{ and } \\sigma_\\text E & \\sim \\operatorname{Exponential}(1) \\\\\n\\rho & \\sim \\operatorname{LKJ}(2).\n\\end{align*}\\]\nYou might not remember, but we’ve actually fit a model like this before. It was b5.3_A from way back in Section 5.1.5.3. The big difference between that earlier model and this one is whereas the former did not include a residual correlation, \\(\\rho\\), this one will. Thus, this time we will make sure to set set_rescor(TRUE) in the formula. Within brms parlance, priors for residual correlations are of class = rescor.\n\ne_model &lt;- bf(e ~ 1 + q)\nw_model &lt;- bf(w ~ 1 + e)\n\nb14.6 &lt;- brm(\n  data = dat_sim, \n  family = gaussian,\n  e_model + w_model + set_rescor(TRUE),\n  prior = c(\n    # E model\n    prior(normal(0, 0.2), class = Intercept, resp = e),\n    prior(normal(0, 0.5), class = b, resp = e),\n    prior(exponential(1), class = sigma, resp = e),\n    \n    # W model\n    prior(normal(0, 0.2), class = Intercept, resp = w),\n    prior(normal(0, 0.5), class = b, resp = w),\n    prior(exponential(1), class = sigma, resp = w),\n    \n    # Rho\n    prior(lkj(2), class = rescor)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  file = \"fits/b14.06\")\n\n\nprint(b14.6)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity\n         mu = identity \nFormula: e ~ 1 + q \n         w ~ 1 + e \n   Data: dat_sim (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ne_Intercept     0.00      0.03    -0.07     0.07 1.00     2899     3024\nw_Intercept     0.00      0.04    -0.09     0.09 1.00     2969     2621\ne_q             0.59      0.04     0.52     0.66 1.00     2560     2739\nw_e            -0.05      0.08    -0.20     0.09 1.00     1823     2100\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_e     0.81      0.03     0.76     0.86 1.00     3579     2982\nsigma_w     1.02      0.05     0.94     1.12 1.00     1949     2084\n\nResidual Correlations: \n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nrescor(e,w)     0.54      0.05     0.43     0.64 1.00     1822     2135\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow the parameter for \\(E \\rightarrow W\\), w_e, is just where it should be–near zero. The residual correlation between \\(E\\) and \\(Q\\), rescor(e,w), is positive and large in magnitude, indicating their common influence from the unmeasured variable \\(U\\). Next we’ll take McElreath’s direction to “adjust the simulation and try other scenarios” (p. 459) by adjusting the causal relations, as in his R code 14.28.\n\nset.seed(73) \n\nn &lt;- 500\n\ndat_sim &lt;- tibble(u_sim = rnorm(n, mean = 0, sd = 1),\n                  q_sim = sample(1:4, size = n, replace = T)) |&gt; \n  mutate(e_sim = rnorm(n, mean = u_sim + q_sim, sd = 1)) |&gt; \n  mutate(w_sim = rnorm(n, mean = -u_sim + 0.2 * e_sim, sd = 1)) |&gt; \n  mutate(w = standardize(w_sim),\n         e = standardize(e_sim),\n         q = standardize(q_sim))\n\ndat_sim\n\n# A tibble: 500 × 7\n     u_sim q_sim e_sim  w_sim       w       e      q\n     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 -0.145      1 1.51   0.809  0.248  -0.575  -1.36 \n 2  0.291      1 0.664  0.396 -0.0563 -1.09   -1.36 \n 3  0.0938     3 2.44  -0.364 -0.615  -0.0185  0.428\n 4 -0.127      3 4.09   0.347 -0.0922  0.978   0.428\n 5 -0.847      4 2.62   0.976  0.370   0.0939  1.32 \n 6  0.141      4 3.54   0.357 -0.0852  0.651   1.32 \n 7  1.54       2 3.65  -0.466 -0.690   0.714  -0.464\n 8  2.74       3 4.91  -1.98  -1.80    1.48    0.428\n 9  1.55       3 4.18  -1.64  -1.55    1.04    0.428\n10  0.462      1 0.360 -0.461 -0.686  -1.27   -1.36 \n# ℹ 490 more rows\n\n\nWe’ll use update() to avoid re-compiling the models.\n\nb14.4x &lt;- update(\n  b14.4,\n  newdata = dat_sim,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  file = \"fits/b14.04x\")\n\nb14.6x &lt;- update(\n  b14.6,\n  newdata = dat_sim,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14, \n  file = \"fits/b14.06x\")\n\nJust for kicks, let’s examine the results with a coefficient plot.\n\ntext &lt;- tibble(\n  Estimate = c(fixef(b14.4x)[2, 3], fixef(b14.6x)[4, 4]),\n  y        = c(4.35, 3.65),\n  hjust    = c(0, 1),\n  fit      = c(\"b14.4x\", \"b14.6x\"))\n\nbind_rows(\n  # b_14.4x\n  posterior_summary(b14.4x)[1:3, ] |&gt; \n    data.frame() |&gt; \n    mutate(param = c(\"alpha[W]\", \"beta[EW]\", \"sigma[W]\"),\n           fit = \"b14.4x\"),\n  # b_14.6x\n  posterior_summary(b14.6x)[1:7, ] |&gt;  \n    data.frame() |&gt; \n    mutate(param = c(\"alpha[E]\", \"alpha[W]\", \"beta[QE]\", \"beta[EW]\", \"sigma[E]\", \"sigma[W]\", \"rho\"),\n           fit = \"b14.6x\")) |&gt; \n  mutate(param = factor(param,\n                        levels = c(\"rho\", \"sigma[W]\", \"sigma[E]\", \"beta[EW]\", \"beta[QE]\", \"alpha[W]\", \"alpha[E]\"))) |&gt;\n  \n  ggplot(aes(x = param, y = Estimate, color = fit)) +\n  geom_hline(yintercept = 0, alpha = 1/4, color = \"#E8DCCF\") +\n  geom_pointrange(aes(ymin = Q2.5, ymax = Q97.5),\n                  position = position_dodge(width = 0.5), size = 1/4) +\n  geom_text(data = text,\n            aes(x = y, label = fit, hjust = hjust)) +\n  scale_x_discrete(NULL, labels = ggplot2:::parse_safe) +\n  scale_color_manual(NULL, values = c(\"#E7CDC2\", \"#A65141\")) +\n  ylab(\"marginal posterior\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank(),\n        legend.position = \"none\")\n\n\n\n\n\n\n\n\nWith the help from b14.6x, we found “that \\(E\\) and \\(W\\) have a negative correlation in their residual variance, because the confound positively influences one and negatively influences the other” (p. 459).\nOne can use the dagitty() and instrumentalVariables() functions from the dagitty package to first define a DAG and then query whether there are instrumental variables for a given exposure and outcome.\n\nlibrary(dagitty)\n\ndagIV &lt;- dagitty(\"dag{Q -&gt; E &lt;- U -&gt; W &lt;- E}\")\n\ninstrumentalVariables(dagIV, exposure = \"E\", outcome = \"W\")\n\n Q\n\n\n\nThe hardest thing about instrumental variables is believing in any particular instrument. If you believe in your DAG, they are easy to believe. But should you believe in your DAG?…\nIn general, it is not possible to statistically prove whether a variable is a good instrument. As always, we need scientific knowledge outside of the data to make sense of the data. (p. 460)\n\n\n14.3.1.1 Rethinking: Two-stage worst squares\n“The instrumental variable model is often discussed with an estimation procedure known as two-stage least squares (2SLS)” (p. 460, emphasis in the original). For a nice introduction to instrumental variables via 2SLS, see this practical introduction, and also the slides and video-lecture files, from the great Andrew Heiss. For the clinical researchers in the room, Kristoffer Magnusson has a nice brms-based blog post on the topic called Confounded dose-response effects of treatment adherence: fitting Bayesian instrumental variable models using brms.\n\n\n\n14.3.2 Other designs\n\nThere are potentially many ways to find natural experiments. Not all of them are strictly instrumental variables. But they can provide theoretically correct designs for causal inference, if you can believe the assumptions. Let’s consider two more.\nIn addition to the backdoor criterion you met in Chapter 6, there is something called the front-door criterion. (p. 460, emphasis in the original)\n\nTo get a sense of the front-door criterion, consider the following DAG with observed variables \\(X\\), \\(Y\\), and \\(Z\\) and an unobserved variable, \\(U\\).\n\ndag_coords &lt;- tibble(\n  name = c(\"X\", \"Z\", \"U\", \"Y\"),\n  x    = c(1, 2, 2, 3),\n  y    = c(1, 1, 2, 1))\n\ndagify(X ~ U,\n       Z ~ X,\n       Y ~ U + Z,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"U\"),\n                 fill = \"#FCF9F0\", shape = 21, size = 6, stroke = 2, show.legend = F) +\n  geom_dag_text(color = \"#100F14\", family = \"Courier\") +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(\"#EEDA9D\", \"#A65141\")) +\n  theme_pearl_dag()\n\n\n\n\n\n\n\n\n\nWe are interested, as usual, in the causal influence of \\(X\\) on \\(Y\\). But there is an unobserved confound \\(U\\), again as usual. It turns out that, if we can find a perfect mediator \\(Z\\), then we can possibly estimate the causal effect of \\(X\\) on \\(Y\\). It isn’t crazy to think that causes are mediated by other causes. Everything has a mechanism. \\(Z\\) in the DAG above is such a mechanism. If you have a believable \\(Z\\) variable, then the causal effect of \\(X\\) on \\(Y\\) is estimated by expressing the generative model as a statistical model, similar to the instrumental variable example before. (p. 461)\n\nMcElreath’s second example is the regression discontinuity approach. If you have a time series where the variable of interest is measured before and after some relevant intervention variable, you can estimate intercepts and slopes before and after the intervention, the cutoff. However,\n\nin practice, one trend is fit for individuals above the cutoff and another to those below the cutoff. Then an estimate of the causal effect is the average difference between individuals just above and just below the cutoff. While the difference near the cuttoff is of interest, the entire function influences this difference. So some care is needed in choosing functions for the overall relationship between the exposure and the outcome. (p. 461)\n\nMcElreath’s not kidding about the need for care when fitting regression discontinuity models. Gleman’s blog is littered with awful examples (e.g., here, here, here, here, here). See also Gelman and Imbens’ (2019) paper, Why high-order polynomials should not be used in regression discontinuity designs, or Nick HK’s informative tweet on how this applies to autocorrelated data.1\n1 We’ll briefly cover autoregressive models in Section 16.4.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Adventures in Covariance</span>"
    ]
  },
  {
    "objectID": "14.html#sec-Social-relations-as",
    "href": "14.html#sec-Social-relations-as",
    "title": "14  Adventures in Covariance",
    "section": "14.4 Social relations as correlated varying effects",
    "text": "14.4 Social relations as correlated varying effects\nIt looks like brms is not set up to fit a model like this, at this time. See the Social relations model (SRM) thread on the Stan Forums and issue #502 on the brms GitHub repo for details. In short, the difficulty is brms is not set up to allow covariances among distinct random effects with the same levels and it looks like this will not change any time soon. So, in this section we will fit the model with rethinking, but still use ggplot2 and friends in the post processing.\nLet’s load the kl_dyads data (Koster & Leckie, 2014).\n\nlibrary(rethinking)\ndata(KosterLeckie)\n\nTake a look at the data.\n\nkl_dyads |&gt; glimpse()\n\nRows: 300\nColumns: 13\n$ hidA    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ hidB    &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 3, 4, 5, 6, 7, 8, 9, 10,…\n$ did     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3…\n$ giftsAB &lt;int&gt; 0, 6, 2, 4, 8, 2, 1, 0, 10, 1, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 1, 2, 0, 3, 0, 43, 5, 0, 1, 0, 1, 1, 2…\n$ giftsBA &lt;int&gt; 4, 31, 5, 2, 2, 1, 2, 1, 110, 0, 0, 6, 11, 0, 1, 4, 0, 2, 0, 7, 0, 13, 0, 2, 3, 1, 1, 0, 1, 10, 4, 1, 0, 0, 2, 0…\n$ offset  &lt;dbl&gt; 0.000, -0.003, -0.019, 0.000, -0.003, 0.000, 0.000, 0.000, -0.186, 0.000, -0.471, -0.019, -0.011, -0.014, -0.019…\n$ drel1   &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ drel2   &lt;int&gt; 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ drel3   &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0…\n$ drel4   &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1…\n$ dlndist &lt;dbl&gt; -2.790, -2.817, -1.886, -1.892, -3.499, -1.853, -1.475, -1.644, -1.897, -2.379, -2.200, -2.117, -2.830, -2.492, …\n$ dass    &lt;dbl&gt; 0.000, 0.044, 0.025, 0.011, 0.022, 0.071, 0.046, 0.003, 0.552, 0.018, 0.004, 0.004, 0.036, 0.006, 0.014, 0.000, …\n$ d0125   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n# kl_households |&gt; glimpse()\n\n“The variables hidA and hidB tell us the household IDs in each dyad, and did is a unique dyad ID number” (p. 462). To get a sense of the interrelation among those three ID variables, we’ll make a tile plot.\n\nkl_dyads |&gt; \n  ggplot(aes(x = hidA, y = hidB, label = did)) +\n  geom_tile(aes(fill = did),\n            show.legend = F) +\n  geom_text(size = 2.25, family = \"Courier\") +\n  geom_vline(xintercept = 0:24 + 0.5, color = \"#394165\", linewidth = 1/5) +\n  geom_hline(yintercept = 1:25 + 0.5, color = \"#394165\", linewidth = 1/5) +\n  scale_fill_gradient(low = \"#DCA258\", high = \"#EEDA9D\", limits = c(1, NA)) +\n  scale_x_continuous(breaks = 1:24) +\n  scale_y_continuous(breaks = 2:25) +\n  theme(axis.text = element_text(size = 9),\n        axis.ticks = element_blank())\n\n\n\n\n\n\n\n\nThe orange/yellow gradient fill is a little silly, but I couldn’t stop myself. Here is our version of Figure 14.8, the bivariate distribution of dyadic gifts, collapsing across dyads.\n\nkl_dyads |&gt; \n  ggplot(aes(x = giftsAB, y = giftsBA)) +\n  geom_hex(bins = 70) +\n  geom_abline(color = \"#DCA258\", linetype = 3) +\n  scale_x_continuous(\"gifts household A to household B\", limits = c(0, 113)) +\n  scale_y_continuous(\"gifts from B to A\", limits = c(0, 113)) +\n  scale_fill_gradient(low = \"#E7CDC2\", high = \"#A65141\", limits = c(1, NA)) +\n  ggtitle(\"Distribution of dyadic gifts\") +\n  coord_equal()\n\n\n\n\n\n\n\n\nHere’s the overall Pearson’s correlation coefficient, collapsing across grouping levels.\n\nkl_dyads |&gt; \n  summarise(rho = cor(giftsAB, giftsBA))\n\n        rho\n1 0.2391549\n\n\nHowever, it would be a mistake to take this correlation seriously. It is a disentangled mixture of various kinds of associations, none of which are guaranteed to be even close to \\(r = 0.24\\). Remember this as we move along with the analyses and let the consequences burn a methodological mark into your soul.\n\nkl_data &lt;- list(\n  N            = nrow(kl_dyads),\n  N_households = max(kl_dyads$hidB), \n  did          = kl_dyads$did,\n  hidA         = kl_dyads$hidA,\n  hidB         = kl_dyads$hidB,\n  giftsAB      = kl_dyads$giftsAB, \n  giftsBA      = kl_dyads$giftsBA)\n\nm14.7 &lt;- ulam( \n  alist(\n    giftsAB ~ poisson(lambdaAB),\n    giftsBA ~ poisson(lambdaBA),\n    log(lambdaAB) &lt;- a + gr[hidA, 1] + gr[hidB, 2] + d[did, 1] , \n    log(lambdaBA) &lt;- a + gr[hidB, 1] + gr[hidA, 2] + d[did, 2] , \n    a ~ normal(0, 1),\n    \n    # `gr` matrix of varying effects\n    vector[2]:gr[N_households] ~ multi_normal(0, Rho_gr, sigma_gr), \n    Rho_gr ~ lkj_corr(4),\n    sigma_gr ~ exponential(1),\n    \n    # Dyad effects\n    transpars&gt; matrix[N,2]:d &lt;-\n      compose_noncentered(rep_vector(sigma_d, 2), L_Rho_d, z), \n    matrix[2,N]:z ~ normal(0, 1),\n    cholesky_factor_corr[2]:L_Rho_d ~ lkj_corr_cholesky(8), \n    sigma_d ~ exponential(1),\n    \n    # Compute correlation matrix for dyads\n    gq&gt; matrix[2, 2]:Rho_d &lt;&lt;- Chol_to_Corr(L_Rho_d)\n  ), \n  data = kl_data, \n  chains = 4, cores = 4, iter = 2000)\n\nWe don’t get a lot of information from the default precis() output for this model, but at least we’ll get a summary for \\(\\alpha\\).\n\nprecis(m14.7)\n\n\n\n        mean   sd 5.5% 94.5% rhat ess_bulk\na       0.54 0.16 0.27   0.8    1   680.38\nsigma_d 1.10 0.06 1.01   1.2    1  1217.32\n\n\nOne of the interesting things about this model is we only have one \\(\\alpha\\) parameter for two criterion variables. This makes \\(\\alpha\\) like the grand mean of counts. Here is a focused look at the precis() output when you set depth = 3.\n\nprecis(m14.7, depth = 3, pars = c(\"Rho_gr\", \"sigma_gr\"))\n\n\n\n             mean   sd  5.5% 94.5% rhat ess_bulk\nRho_gr[1,1]  1.00 0.00  1.00  1.00   NA       NA\nRho_gr[2,1] -0.41 0.20 -0.70 -0.08 1.00  1326.90\nRho_gr[1,2] -0.41 0.20 -0.70 -0.08 1.00  1326.90\nRho_gr[2,2]  1.00 0.00  1.00  1.00   NA       NA\nsigma_gr[1]  0.83 0.14  0.64  1.06 1.00  2267.45\nsigma_gr[2]  0.42 0.09  0.30  0.58 1.01   942.52\n\n\nThese are the posterior summaries for the part of the model McElreath defined in the middle of page 463,\n\\[\n\\begin{bmatrix} g_i \\\\ r_i \\end{bmatrix} \\sim \\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_g^2 & \\sigma_g \\sigma_r \\rho_{gr} \\\\ \\sigma_g \\sigma_r \\rho_{rg} & \\sigma_r^2 \\end{bmatrix} \\end{pmatrix},\n\\]\nthe population of household effects. But as per usual with Stan, the variance parameters are expressed in a \\(\\sigma\\) metric. Also, rethinking::precis() returned the summary for \\(\\rho_{gr}\\) in matrix form,\n\\[\n\\begin{bmatrix} 1 & \\rho_{gr} \\\\ \\rho_{rg} & 1 \\end{bmatrix},\n\\]\nwhere \\(\\rho_{gr} = \\rho_{rg}\\). The correlation is negative. We might view \\(\\sigma_g\\), \\(\\sigma_r\\), and their correlation in a plot. First we extract the posterior draws with extract.samples().\n\npost &lt;- extract.samples(m14.7)\n\nNow we plot.\n\ntibble(`sigma[italic(g)]`          = post$sigma_gr[, 1],\n       `sigma[italic(r)]`          = post$sigma_gr[, 2],\n       `rho[italic(g)][italic(r)]` = post$Rho_gr[, 2, 1]) |&gt; \n  pivot_longer(everything()) |&gt; \n  \n  ggplot(aes(x = value, y = name, fill = name)) +\n  geom_vline(xintercept = 0, alpha = 1/3, color = \"#FCF9F0\") +\n  stat_halfeye(.width = 0.89, color = \"#FCF9F0\", height = 1.5) + \n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  scale_fill_manual(values = c(\"#80A0C7\", \"#EEDA9D\", \"#DCA258\")) +\n  xlab(\"marginal posterior\") +\n  coord_cartesian(ylim = c(1.5, 3.9)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n“This implies that individuals who give more across all dyads tend to receive less[, and ] clear evidence that rates of giving are more variable than rates of receiving” (p. 465). McElreath suggested we “try plot(exp(g[,1]),exp(r[,1])) for example to show the posterior distribution of giving/receiving for household number 1” (p. 465). Here’s a tidyverse version of that plot.\n\ng &lt;- sapply(1:25, function(i) post$a + post$gr[, i, 1]) \nr &lt;- sapply(1:25, function(i) post$a + post$gr[, i, 2]) \n\ntibble(g = exp(g[, 1]),\n       r = exp(r[, 1])) |&gt; \n  ggplot(aes(x = g, y = r)) +\n  geom_abline(alpha = 1/3, color = \"#FCF9F0\", linetype = 2) + # white \"#FCF9F0\" # gold \"#B1934A\"\n  geom_point(alpha = 1/3, color = \"#B1934A\", size = 1/4) +\n  stat_ellipse(type = \"norm\", color = \"#80A0C7\", level = 0.5, linewidth = 1/2) +\n  stat_ellipse(type = \"norm\", color = \"#80A0C7\", level = 0.9, linewidth = 1/2) +\n  labs(x = expression(giving[italic(i)==1]),\n       y = expression(receiving[italic(i)==1])) +\n  coord_equal(xlim = c(0, 5),\n              ylim = c(0, 5))\n\n\n\n\n\n\n\n\nThe gold dots are the bivariate posterior draws and the two blue ellipses mark off the 50% and 90% intervals, presuming a bivariate Gaussian distribution. Here’s a programmatic way to make our version of Figure 14.9.a.\n\nd_fig &lt;- rbind(exp(g), exp(r)) |&gt; \n  data.frame() |&gt; \n  set_names(1:25) |&gt; \n  mutate(parameter = rep(c(\"g\", \"r\"), each = n() / 2),\n         iter      = rep(1:4000, times = 2)) |&gt; \n  pivot_longer(-c(parameter, iter), names_to = \"household\") |&gt; \n  pivot_wider(names_from = parameter, values_from = value) |&gt; \n  group_by(household) |&gt; \n  mutate(mu_g = mean(g),\n         mu_r = mean(r)) |&gt; \n  nest(data = c(\"g\", \"r\", \"iter\")) \n\nd_fig |&gt; \n  ggplot(aes(group = household)) +\n  geom_abline(alpha = 1/3, color = \"#FCF9F0\", linetype = 2) +\n  stat_ellipse(data = d_fig |&gt; \n                 unnest(data),\n               aes(x = g, y = r),\n               type = \"norm\", alpha = 1/2, color = \"#80A0C7\", level = 0.5, linewidth = 1/2) +\n  geom_point(aes(x = mu_g, y = mu_r),\n             color = \"#DCA258\") +\n  labs(x = \"generalized giving\",\n       y = \"generalized receiving\") +\n  coord_equal(xlim = c(0, 8.5),\n              ylim = c(0, 8.5))\n\n\n\n\n\n\n\n\nHere is a look at the covariance matrix for the dyadic effects, the posterior summaries for the part of the model McElreath defined in the middle of page 463 as,\n\\[\n\\begin{bmatrix} d_{ij} \\\\ d_{ji} \\end{bmatrix} \\sim \\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} \\sigma_d^2 & \\sigma_d^2 \\rho_d \\\\ \\sigma_d^2 \\rho_d & \\sigma_d^2 \\end{bmatrix} \\end{pmatrix},\n\\]\nwhere there is only one standard deviation parameter \\(\\sigma_d\\) because the labels for each dyad are arbitrary. Here they are in the precis() output.\n\nprecis(m14.7, depth = 3, pars = c(\"Rho_d\", \"sigma_d\"))\n\n                mean         sd      5.5%     94.5%     rhat ess_bulk\nRho_d[1,1] 1.0000000 0.00000000 1.0000000 1.0000000       NA       NA\nRho_d[2,1] 0.8820644 0.03275936 0.8254698 0.9296409 1.002855 1110.151\nRho_d[1,2] 0.8820644 0.03275936 0.8254698 0.9296409 1.002855 1110.151\nRho_d[2,2] 1.0000000 0.00000000 1.0000000 1.0000000       NA       NA\nsigma_d    1.1015718 0.05885262 1.0123212 1.1988938 1.004155 1217.318\n\n\nHere they are in a plot.\n\ntibble(`sigma[italic(d)]` = post$sigma_d,\n       `rho[italic(d)]`   = post$Rho_d[, 2, 1]) |&gt; \n  pivot_longer(everything()) |&gt; \n  \n  ggplot(aes(x = value, y = name, fill = name)) +\n  geom_vline(xintercept = 0, alpha = 1/3, color = \"#FCF9F0\") +\n  stat_halfeye(.width = 0.89, color = \"#FCF9F0\") + \n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  scale_fill_manual(values = c(\"#A65141\", \"#B1934A\")) +\n  xlab(\"marginal posterior\") +\n  coord_cartesian(ylim = c(1.5, 2)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n“The correlation here is positive and strong. And there is more variation among dyads than there is among household in giving rates” (p. 467). Now make the right hand panel of Figure 14.9.\n\ntibble(dy1 = apply(post$d[, , 1], 2, mean),\n       dy2 = apply(post$d[, , 2], 2, mean)) |&gt; \n  \n  ggplot(aes(x = dy1, y = dy2)) +\n  geom_abline(alpha = 1/3, color = \"#FCF9F0\", linetype = 2) +\n  geom_vline(xintercept = 0, alpha = 1/3, color = \"#FCF9F0\", linetype = 2) +\n  geom_hline(yintercept = 0, alpha = 1/3, color = \"#FCF9F0\", linetype = 2) +\n  geom_point(alpha = 1/2, color = \"#8B9DAF\", size = 1/2) +\n  geom_text(x = mean(post$d[, 1, 1]),\n            y = mean(post$d[, 1, 2]),\n            label = \"1\",\n            color = \"#EEDA9D\", family = \"Courier\") +\n  labs(x = \"household A in dyad\",\n       y = \"household B in dyad\") +\n  coord_equal(xlim = c(-2, 3.5),\n              ylim = c(-2, 3.5))\n\n\n\n\n\n\n\n\nAs McElreath pointed out, each dot is the posterior mean for one of the 300 levels of did. Do you see the yellow #1 toward the middle? It’s marking off the posterior mean for did == 1. To give a better sense of the uncertainty in each of the levels of did, here is the full bivariate distribution for did == 1.\n\ntibble(dy1 = post$d[, 1, 1],\n       dy2 = post$d[, 1, 2]) |&gt; \n  \n  ggplot(aes(x = dy1, y = dy2)) +\n  geom_abline(alpha = 1/3, color = \"#FCF9F0\", linetype = 2) +\n  geom_vline(xintercept = 0, alpha = 1/3, color = \"#FCF9F0\", linetype = 2) +\n  geom_hline(yintercept = 0, alpha = 1/3, color = \"#FCF9F0\", linetype = 2) +\n  geom_point(color = \"#8B9DAF\", alpha = 1/3, size = 1/4) +\n  stat_ellipse(type = \"norm\", level = 0.5, color = \"#EEDA9D\", linewidth = 1/2) +\n  stat_ellipse(type = \"norm\", level = 0.9, color = \"#EEDA9D\", linewidth = 1/2) +\n  labs(x = expression(\"household A in dyad\"[italic(i)==1]),\n       y = expression(\"household B in dyad\"[italic(i)==1])) +\n  coord_equal(xlim = c(-2, 3.5),\n              ylim = c(-2, 3.5))\n\n\n\n\n\n\n\n\nThe two yellow ellipses mark off the 50% and 90% intervals, again presuming a bivariate Gaussian distribution for the posterior.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Adventures in Covariance</span>"
    ]
  },
  {
    "objectID": "14.html#continuous-categories-and-the-gaussian-process",
    "href": "14.html#continuous-categories-and-the-gaussian-process",
    "title": "14  Adventures in Covariance",
    "section": "14.5 Continuous categories and the Gaussian process",
    "text": "14.5 Continuous categories and the Gaussian process\n\nThere is a way to apply the varying effects approach to continuous categories… The general approach is known as Gaussian process regression. This name is unfortunately wholly uninformative about what it is for and how it works.\nWe’ll proceed to work through a basic example that demonstrates both what it is for and how it works. The general purpose is to define some dimension along which cases differ. This might be individual differences in age. Or it could be differences in location. Then we measure the distance between each pair of cases. What the model then does is estimate a function for the covariance between pairs of cases at different distances. This covariance function provides one continuous category generalization of the varying effects approach. (p. 468, emphasis in the original)\n\n\n14.5.1 Example: Spatial autocorrelation in Oceanic tools\nWe start by loading the matrix of geographic distances.\n\n# Load the distance matrix\nlibrary(rethinking)\ndata(islandsDistMatrix)\n\n# Display (measured in thousands of km)\nd_mat &lt;- islandsDistMatrix\ncolnames(d_mat) &lt;- c(\"Ml\", \"Ti\", \"SC\", \"Ya\", \"Fi\", \"Tr\", \"Ch\", \"Mn\", \"To\", \"Ha\")\n\nround(d_mat, 1)\n\n            Ml  Ti  SC  Ya  Fi  Tr  Ch  Mn  To  Ha\nMalekula   0.0 0.5 0.6 4.4 1.2 2.0 3.2 2.8 1.9 5.7\nTikopia    0.5 0.0 0.3 4.2 1.2 2.0 2.9 2.7 2.0 5.3\nSanta Cruz 0.6 0.3 0.0 3.9 1.6 1.7 2.6 2.4 2.3 5.4\nYap        4.4 4.2 3.9 0.0 5.4 2.5 1.6 1.6 6.1 7.2\nLau Fiji   1.2 1.2 1.6 5.4 0.0 3.2 4.0 3.9 0.8 4.9\nTrobriand  2.0 2.0 1.7 2.5 3.2 0.0 1.8 0.8 3.9 6.7\nChuuk      3.2 2.9 2.6 1.6 4.0 1.8 0.0 1.2 4.8 5.8\nManus      2.8 2.7 2.4 1.6 3.9 0.8 1.2 0.0 4.6 6.7\nTonga      1.9 2.0 2.3 6.1 0.8 3.9 4.8 4.6 0.0 5.0\nHawaii     5.7 5.3 5.4 7.2 4.9 6.7 5.8 6.7 5.0 0.0\n\n\nIf you wanted to use color to more effectively visualize the values in the matrix, you might do something like this.\n\nd_mat |&gt;\n  data.frame() |&gt; \n  rownames_to_column(\"row\") |&gt; \n  gather(column, distance, -row) |&gt; \n  mutate(column = factor(column, levels = colnames(d_mat)),\n         row    = factor(row,    levels = rownames(d_mat)) |&gt; fct_rev(),\n         label  = formatC(distance, format = 'f', digits = 2)) |&gt;\n  \n  ggplot(aes(x = column, y = row)) + \n  geom_raster(aes(fill = distance)) + \n  geom_text(aes(label = label),\n            color = \"#100F14\", family = \"Courier\", size = 3) +\n  scale_x_discrete(NULL, expand = c(0, 0), position = \"top\") +\n  scale_y_discrete(NULL, expand = c(0, 0)) +\n  scale_fill_gradient(low = \"#FCF9F0\", high = \"#A65141\") +\n  theme_pearl_earring(axis.text.y = element_text(hjust = 0)) +\n  theme(axis.ticks = element_blank())\n\n\n\n\n\n\n\n\nFigure 14.10 shows the “shape of the function relating distance to the covariance \\(\\mathbf K_{ij}\\).”\n\ntibble(x       = seq(from = 0, to = 4, by = 0.01),\n       linear  = exp(-1 * x),\n       squared = exp(-1 * x^2)) |&gt;\n  \n  ggplot(aes(x = x)) +\n  geom_line(aes(y = linear),\n            color = \"#B1934A\", linetype = 2) +\n  geom_line(aes(y = squared),\n            color = \"#DCA258\") +\n  scale_x_continuous(\"distance\", expand = c(0, 0)) +\n  scale_y_continuous(\"correlation\", breaks = c(0, 0.5, 1))\n\n\n\n\n\n\n\n\nNow load the primary data.\n\ndata(Kline2)  # Load the ordinary data, now with coordinates\n\nd &lt;- Kline2 |&gt;\n  mutate(society = 1:10)\n\nrm(Kline2)\n\nd |&gt; glimpse()\n\nRows: 10\nColumns: 10\n$ culture     &lt;fct&gt; Malekula, Tikopia, Santa Cruz, Yap, Lau Fiji, Trobriand, Chuuk, Manus, Tonga, Hawaii\n$ population  &lt;int&gt; 1100, 1500, 3600, 4791, 7400, 8000, 9200, 13000, 17500, 275000\n$ contact     &lt;fct&gt; low, low, low, high, high, high, high, low, high, low\n$ total_tools &lt;int&gt; 13, 22, 24, 43, 33, 19, 40, 28, 55, 71\n$ mean_TU     &lt;dbl&gt; 3.2, 4.7, 4.0, 5.0, 5.0, 4.0, 3.8, 6.6, 5.4, 6.6\n$ lat         &lt;dbl&gt; -16.3, -12.3, -10.7, 9.5, -17.7, -8.7, 7.4, -2.1, -21.2, 19.9\n$ lon         &lt;dbl&gt; 167.5, 168.8, 166.0, 138.1, 178.1, 150.9, 151.6, 146.9, -175.2, -155.6\n$ lon2        &lt;dbl&gt; -12.5, -11.2, -14.0, -41.9, -1.9, -29.1, -28.4, -33.1, 4.8, 24.4\n$ logpop      &lt;dbl&gt; 7.003065, 7.313220, 8.188689, 8.474494, 8.909235, 8.987197, 9.126959, 9.472705, 9.769956, 12.524526\n$ society     &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n\n\n👋 Heads up: The brms package is capable of handling a variety of Gaussian process models using the gp() function. As we will see throughout this section, this method will depart in important ways from how McElreath fits Gaussian process models with rethinking. Due in large part to these differences, this section and its analogue in the first edition of Statistical rethinking (McElreath, 2015) baffled me, at first. Happily, fellow enthusiasts Louis Bliard and Richard Torkar reached out and helped me hammer this section out behind the scenes. The method to follow is due in large part to their efforts. 🤝\nThe brms::gp() function takes a handful of arguments. The first and most important argument, ..., accepts the names of one or more predictors from the data. When fitting a spatial Gaussian process of this kind, we’ll enter in the latitude and longitude data for each of levels of culture. This will be an important departure from the text. For his m14.8, McElreath directly entered in the Dmat distance matrix data into ulam(). In so doing, he defined \\(D_{ij}\\), the matrix of distances between each of the societies. When using brms, we instead estimate the distance matrix from the latitude and longitude variables.\nBefore we practice fitting a Gaussian process with the brms::gp() function, we’ll first need to think a little bit about our data. McElreath’s Dmat measured the distances in thousands of km. However, the lat and lon2 variables in the data above are in decimal degrees, which means they need to be transformed to keep our model in the same metric as McElreath’s. Turns out that one decimal degree is 111.32km (at the equator). Thus, we can turn both lat and lon2 into 1,000 km units by multiplying each by 0.11132. Here’s the conversion.\n\nd &lt;- d |&gt; \n  mutate(lat_adj  = lat  * 0.11132,\n         lon2_adj = lon2 * 0.11132)\n\nd |&gt; \n  select(culture, lat, lon2, lat_adj:lon2_adj)\n\n      culture   lat  lon2   lat_adj  lon2_adj\n1    Malekula -16.3 -12.5 -1.814516 -1.391500\n2     Tikopia -12.3 -11.2 -1.369236 -1.246784\n3  Santa Cruz -10.7 -14.0 -1.191124 -1.558480\n4         Yap   9.5 -41.9  1.057540 -4.664308\n5    Lau Fiji -17.7  -1.9 -1.970364 -0.211508\n6   Trobriand  -8.7 -29.1 -0.968484 -3.239412\n7       Chuuk   7.4 -28.4  0.823768 -3.161488\n8       Manus  -2.1 -33.1 -0.233772 -3.684692\n9       Tonga -21.2   4.8 -2.359984  0.534336\n10     Hawaii  19.9  24.4  2.215268  2.716208\n\n\nNote that because this conversion is valid at the equator, it is only an approximation for latitude and longitude coordinates for our island societies.\nNow we’ve scaled our two spatial variables, the basic way to use them in a brms Gaussian process is including gp(lat_adj, lon2_adj) into the formula argument within the brm() function. Note however that one of the default gp() settings is scale = TRUE, which scales predictors so that the maximum distance between two points is 1. We don’t want this for our example, so we will set scale = FALSE instead.\nOur Gaussian process model is an extension of the non-linear model from Section 11.2.1.1, b11.11. Thus our model here will also use the non-linear syntax. Here’s how we might use brms to fit our amended non-centered version of McElreath’s m14.8.\n\nb14.8 &lt;- brm(\n  data = d, \n  family = poisson(link = \"identity\"),\n  bf(total_tools ~ exp(a) * population^b / g,\n     a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE),\n     b + g ~ 1,\n     nl = TRUE),\n  prior = c(prior(normal(0, 1), nlpar = a),\n            prior(exponential(1), nlpar = b, lb = 0),\n            prior(exponential(1), nlpar = g, lb = 0),\n            prior(inv_gamma(2.874624, 2.941204), class = lscale, coef = gplat_adjlon2_adj, nlpar = a),\n            prior(exponential(1), class = sdgp, coef = gplat_adjlon2_adj, nlpar = a)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  sample_prior = T,\n  file = \"fits/b14.08\")\n\nCheck the results.\n\nprint(b14.8)\n\n Family: poisson \n  Links: mu = identity \nFormula: total_tools ~ exp(a) * population^b/g \n         a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE)\n         b ~ 1\n         g ~ 1\n   Data: d (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGaussian Process Hyperparameters:\n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsdgp(a_gplat_adjlon2_adj)       0.48      0.31     0.15     1.33 1.00     1184     1975\nlscale(a_gplat_adjlon2_adj)     1.66      0.94     0.51     4.11 1.00     1158     1785\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_Intercept     0.31      0.87    -1.44     1.95 1.00     2731     2632\nb_Intercept     0.26      0.08     0.10     0.43 1.00     1656     1551\ng_Intercept     0.65      0.63     0.06     2.44 1.00     2013     2258\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe posterior_summary() function will return a summary that looks more like the one in the text.\n\nposterior_summary(b14.8)[1:15, ] |&gt; round(digits = 2)\n\n                            Estimate Est.Error  Q2.5 Q97.5\nb_a_Intercept                   0.31      0.87 -1.44  1.95\nb_b_Intercept                   0.26      0.08  0.10  0.43\nb_g_Intercept                   0.65      0.63  0.06  2.44\nsdgp_a_gplat_adjlon2_adj        0.48      0.31  0.15  1.33\nlscale_a_gplat_adjlon2_adj      1.66      0.94  0.51  4.11\nzgp_a_gplat_adjlon2_adj[1]     -0.48      0.76 -2.02  0.93\nzgp_a_gplat_adjlon2_adj[2]      0.44      0.87 -1.31  2.14\nzgp_a_gplat_adjlon2_adj[3]     -0.63      0.71 -2.00  0.86\nzgp_a_gplat_adjlon2_adj[4]      0.96      0.68 -0.30  2.39\nzgp_a_gplat_adjlon2_adj[5]      0.20      0.76 -1.32  1.66\nzgp_a_gplat_adjlon2_adj[6]     -1.03      0.77 -2.47  0.52\nzgp_a_gplat_adjlon2_adj[7]      0.16      0.72 -1.35  1.51\nzgp_a_gplat_adjlon2_adj[8]     -0.21      0.85 -1.84  1.52\nzgp_a_gplat_adjlon2_adj[9]      0.43      0.93 -1.51  2.17\nzgp_a_gplat_adjlon2_adj[10]    -0.41      0.79 -1.99  1.15\n\n\nLet’s focus on our three non-linear parameters, first. Happily, both our b_b_Intercept and b_g_Intercept summaries look a lot like those for McElreath’s b and g, respectively. Our b_a_Intercept might look distressingly small, but that’s just because of how we parameterized our model. It’s actually very close to McElreath’s a after you exponentiate.\n\nfixef(b14.8, probs = c(0.055, 0.945))[\"a_Intercept\", c(1, 3:4)] |&gt; \n  exp() |&gt; \n  round(digits = 2)\n\nEstimate     Q5.5    Q94.5 \n    1.37     0.35     5.34 \n\n\nOur Gaussian process parameters are different from McElreath’s. From the gp section of the brms reference manual (Bürkner, 2022a), we learn the brms parameterization follows the form\n\\[k(x_{i},x_{j}) = sdgp^2 \\exp \\big (-||x_i - x_j||^2 / (2 lscale^2) \\big ),\\]\nwhere \\(k(x_{i},x_{j})\\) is the same as McElreath’s \\(\\mathbf K_{ij}\\) and \\(||x_i - x_j||^2\\) is the Euclidean distance, the same as McElreath’s \\(D_{ij}^2\\). Thus we could also express the brms parameterization as\n\\[\\mathbf K_{ij} = sdgp^2 \\exp \\big (-D_{ij}^2 / (2 lscale^2) \\big ),\\]\nwhich is much closer to McElreath’s\n\\[\\mathbf K_{ij} = \\eta^2 \\exp \\big (-\\rho^2 D_{ij}^2 \\big ) + \\delta_{ij} \\sigma^2\\]\nOn page 470, McElreath explained that the final \\(\\delta_{ij} \\sigma^2\\) term is mute with the Oceanic societies data. Thus we won’t consider it further. This reduces McElreath’s equation to\n\\[\\mathbf K_{ij} = \\eta^2 \\exp \\big (-\\rho^2 D_{ij}^2 \\big ).\\]\nImportantly, what McElreath called \\(\\eta\\), Bürkner called \\(sdgp\\). While McElreath estimated \\(\\eta^2\\), brms simply estimated \\(sdgp\\). So we’ll have to square our sdgp(a_gplat_adjlon2_adj) before it’s on the same scale as etasq in the text. Here it is.\n\npost &lt;- as_draws_df(b14.8) |&gt; \n  mutate(etasq = sdgp_a_gplat_adjlon2_adj^2)\n\npost |&gt; \n  mean_hdi(etasq, .width = 0.89) |&gt; \n  mutate_if(is.double, round, digits = 3)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\nWarning: Dropping 'draws_df' class as required metadata was removed.\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n# A tibble: 3 × 6\n  etasq .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.325  0      0.619   0.89 mean   hdi      \n2 0.325  0.649  0.675   0.89 mean   hdi      \n3 0.325  0.754  0.756   0.89 mean   hdi      \n\n\nThough our posterior is a little bit larger than McElreath’s, we’re in the ballpark. You may have noticed that in our model brm() code, above, we just went with the flow and kept the exponential(1) prior on sdgp. The brms default would have been student_t(3, 0, 15.6).\nNow look at the denominator of the inner part of Bürkner’s equation, \\(2 lscale^2\\). This appears to be the brms equivalent to McElreath’s \\(\\rho^2\\). Or at least it’s what we’ve got. Anyway, also note that McElreath estimated \\(\\rho^2\\) directly as rhosq. If I’m doing the algebra correctly, we might expect\n\\[\\begin{align*}\n\\rho^2 & = 1/(2 \\cdot lscale^2) & \\text{and thus} \\\\\nlscale & = \\sqrt{1 / (2 \\cdot \\rho^2)}.\n\\end{align*}\\]\nTo get a sense of this relation, it might be helpful to plot.\n\np1 &lt;- tibble(`rho^2` = seq(from = 0, to = 11, by = 0.01)) |&gt; \n  mutate(lscale = sqrt(1 / (2 * `rho^2`))) |&gt;\n  \n  ggplot(aes(x = `rho^2`, y = lscale)) +\n  geom_hline(yintercept = 0, color = \"#FCF9F0\", linewidth = 1/4, linetype = 2) +\n  geom_vline(xintercept = 0, color = \"#FCF9F0\", linewidth = 1/4, linetype = 2) +\n  geom_line(color = \"#A65141\") +\n  xlab(expression(rho^2)) +\n  coord_cartesian(xlim = c(0, 10),\n                  ylim = c(0, 10))\n\np2 &lt;- tibble(lscale = seq(from = 0, to = 11, by = 0.01)) |&gt; \n  mutate(`rho^2` = 1 / (2 * lscale^2)) |&gt;\n  \n  ggplot(aes(x = lscale, y = `rho^2`)) +\n  geom_hline(yintercept = 0, color = \"#FCF9F0\", linewidth = 1/4, linetype = 2) +\n  geom_vline(xintercept = 0, color = \"#FCF9F0\", linewidth = 1/4, linetype = 2) +\n  geom_line(color = \"#80A0C7\") +\n  ylab(expression(rho^2)) +\n  coord_cartesian(xlim = c(0, 10),\n                  ylim = c(0, 10))\n\np1 + p2\n\n\n\n\n\n\n\n\nThe two aren’t quite inverses of one another, but the overall pattern is when one is large, the other is small. Now we have a sense of how they compare and how to covert one to the other, let’s see how our posterior for \\(lscale\\) looks when we convert it to the scale of McElreath’s \\(\\rho^2\\).\n\npost &lt;- post |&gt; \n  mutate(rhosq = 1 / (2 * lscale_a_gplat_adjlon2_adj^2))\n\npost |&gt; \n  mean_hdi(rhosq, .width = 0.89) |&gt; \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 1 × 6\n  rhosq .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 0.421   0.01  0.916   0.89 mean   hdi      \n\n\nThis is about a third of the size of the McElreath’s \\(\\rho^2 = 1.31, 89 \\text{% HDI } [0.08, 4.41]\\). The plot deepens. If you look back, you’ll see we used a very different prior for lscale. Here it is: inv_gamma(2.874624, 2.941204). Use get_prior() to discover where that came from.\n\nget_prior(\n  data = d, \n  family = poisson(link = \"identity\"),\n  bf(total_tools ~ exp(a) * population^b / g,\n     a  ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE),\n     b + g ~ 1,\n     nl = TRUE))\n\n                         prior  class              coef group resp dpar nlpar lb ub tag       source\n                        (flat)      b                                       a                default\n                        (flat)      b         Intercept                     a           (vectorized)\n                        (flat) lscale                                       a  0             default\n inv_gamma(2.874624, 2.941204) lscale gplat_adjlon2_adj                     a  0             default\n         student_t(3, 0, 15.6)   sdgp                                       a  0             default\n         student_t(3, 0, 15.6)   sdgp gplat_adjlon2_adj                     a  0        (vectorized)\n                        (flat)      b                                       b                default\n                        (flat)      b         Intercept                     b           (vectorized)\n                        (flat)      b                                       g                default\n                        (flat)      b         Intercept                     g           (vectorized)\n\n\nThat is, we used the brms default prior for \\(lscale\\). In a GitHub exchange, Bürkner pointed out that brms uses special priors for \\(lscale\\) parameters based on Michael Betancourt’s (2017) vignette, Robust Gaussian processes in Stan. We can use the dinvgamma() function from the well-named invgamma package (Kahle & Stamey, 2017) to get a sense of what that prior looks like.\n\ntibble(lscale = seq(from = 0.01, to = 9, by = 0.01)) |&gt; \n  mutate(density = invgamma::dinvgamma(lscale, shape = 2.874624, rate = 2.941204)) |&gt; \n  \n  ggplot(aes(x = lscale, y = density)) +\n  geom_area(fill = \"#80A0C7\") +\n  annotate(geom = \"text\", \n           x = 4.75, y = 0.75,\n           label = \"inverse gamma(2.874624, 2.941204)\",\n           color = \"#8B9DAF\", family = \"Courier\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  coord_cartesian(xlim = c(0, 8))\n\n\n\n\n\n\n\n\nAnyways, let’s make the subplots for our version of Figure 14.11 to get a sense of what this all means. Start with the left panel, the prior predictive distribution for the covariance.\n\n# For `slice_sample()`\nset.seed(14)\n\n# Wrangle\np1 &lt;- prior_draws(b14.8) |&gt; \n  transmute(draw  = 1:n(),\n            etasq = sdgp_a^2,\n            rhosq = 1 / (2 * lscale_a__1_gplat_adjlon2_adj^2)) |&gt; \n  slice_sample(n = 100) |&gt;\n  expand_grid(x = seq(from = 0, to = 10, by = 0.05)) |&gt; \n  mutate(covariance = etasq * exp(-rhosq * x^2)) |&gt; \n  \n  # plot\n  ggplot(aes(x = x, y = covariance)) +\n  geom_line(aes(group = draw),\n            alpha = 1/4, color = \"#EEDA9D\", linewidth = 1/4) +\n  scale_x_continuous(\"distance (thousand km)\", \n                     breaks = 0:5 * 2, expand = c(0, 0)) +\n  coord_cartesian(xlim = c(0, 10),\n                  ylim = c(0, 2)) +\n  labs(subtitle = \"Gaussian process prior\")\n\nNow make the right panel, the posterior distribution.\n\n# For `slice_sample()`\nset.seed(14)\n\n# Wrangle\np2 &lt;- post |&gt; \n  mutate(etasq = sdgp_a_gplat_adjlon2_adj^2,\n         rhosq = 1 / (2 * lscale_a_gplat_adjlon2_adj^2)) |&gt; \n  slice_sample(n = 50) |&gt; \n  expand_grid(x = seq(from = 0, to = 10, by = 0.05)) |&gt; \n  mutate(covariance = etasq * exp(-rhosq * x^2)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = x, y = covariance)) +\n  geom_line(aes(group = .draw),\n            alpha = 1/4, color = \"#EEDA9D\", linewidth = 1/4) +\n  stat_function(fun = function(x) mean(post$sdgp_a_gplat_adjlon2_adj)^2 *\n                  exp(-(1 / (2 * mean(post$lscale_a_gplat_adjlon2_adj)^2)) * x^2),\n                color = \"#DCA258\", linewidth = 1) +\n  scale_x_continuous(\"distance (thousand km)\",\n                     breaks = 0:5 * 2, expand = c(0, 0)) +\n  coord_cartesian(xlim = c(0, 10),\n                  ylim = c(0, 2)) +\n  labs(subtitle = \"Gaussian process posterior\")\n\nCombine the two with patchwork.\n\np1 | p2\n\n\n\n\n\n\n\n\nThough the Gaussian process parameters from our brms parameterization looked different from McElreath’s, they resulted in a similar decline in spatial covariance.\nLet’s finish this up and “push the parameters back through the function for \\(\\mathbf{K}\\), the covariance matrix” (p. 473).\n\n# Compute posterior median covariance among societies\nk &lt;- matrix(0, nrow = 10, ncol = 10)\nfor (i in 1:10)\n    for (j in 1:10)\n        k[i, j] &lt;- median(post$etasq) * \n  exp(-median(post$rhosq) * islandsDistMatrix[i, j]^2)\n\ndiag(k) &lt;- median(post$etasq) + 0.01\n\nk |&gt; round(2)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,] 0.17 0.15 0.14 0.00 0.11 0.06 0.01 0.02 0.07  0.00\n [2,] 0.15 0.17 0.15 0.00 0.11 0.06 0.02 0.03 0.06  0.00\n [3,] 0.14 0.15 0.17 0.00 0.09 0.08 0.03 0.04 0.05  0.00\n [4,] 0.00 0.00 0.00 0.17 0.00 0.04 0.09 0.08 0.00  0.00\n [5,] 0.11 0.11 0.09 0.00 0.17 0.01 0.00 0.00 0.14  0.00\n [6,] 0.06 0.06 0.08 0.04 0.01 0.17 0.07 0.13 0.00  0.00\n [7,] 0.01 0.02 0.03 0.09 0.00 0.07 0.17 0.11 0.00  0.00\n [8,] 0.02 0.03 0.04 0.08 0.00 0.13 0.11 0.17 0.00  0.00\n [9,] 0.07 0.06 0.05 0.00 0.14 0.00 0.00 0.00 0.17  0.00\n[10,] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.17\n\n\nWe’ll continue to follow suit and change these to a correlation matrix.\n\n# Convert to correlation matrix\nrho &lt;- round(cov2cor(k), 2)\n\n# Add row/col names for convenience\ncolnames(rho) &lt;- c(\"Ml\", \"Ti\", \"SC\", \"Ya\", \"Fi\", \"Tr\", \"Ch\", \"Mn\", \"To\", \"Ha\")\nrownames(rho) &lt;- colnames(rho)\n\nrho |&gt; round(digits = 2)\n\n     Ml   Ti   SC   Ya   Fi   Tr   Ch   Mn   To Ha\nMl 1.00 0.89 0.86 0.01 0.65 0.35 0.08 0.15 0.41  0\nTi 0.89 1.00 0.92 0.01 0.65 0.36 0.13 0.17 0.37  0\nSC 0.86 0.92 1.00 0.03 0.53 0.47 0.19 0.25 0.27  0\nYa 0.01 0.01 0.03 1.00 0.00 0.22 0.53 0.50 0.00  0\nFi 0.65 0.65 0.53 0.00 1.00 0.08 0.02 0.02 0.82  0\nTr 0.35 0.36 0.47 0.22 0.08 1.00 0.43 0.79 0.03  0\nCh 0.08 0.13 0.19 0.53 0.02 0.43 1.00 0.66 0.00  0\nMn 0.15 0.17 0.25 0.50 0.02 0.79 0.66 1.00 0.01  0\nTo 0.41 0.37 0.27 0.00 0.82 0.03 0.00 0.01 1.00  0\nHa 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  1\n\n\nHere are those correlations in a plot.\n\nrho |&gt;\n  data.frame() |&gt; \n  mutate(row = d$culture) |&gt; \n  pivot_longer(-row, values_to = \"distance\") |&gt; \n  mutate(column = factor(name, levels = colnames(d_mat)),\n         row    = factor(row, levels = rownames(d_mat)) |&gt; fct_rev(),\n         label  = formatC(distance, format = 'f', digits = 2) |&gt; str_replace(\"0.\", \".\")) |&gt;\n  # Omit this line to keep the diagonal of 1's\n  filter(distance != 1) |&gt; \n  \n  ggplot(aes(x = column, y = row)) + \n  geom_raster(aes(fill = distance)) + \n  geom_text(aes(label = label),\n            size = 2.75, family = \"Courier\", color = \"#100F14\") +\n  scale_x_discrete(NULL, position = \"top\", expand = c(0, 0)) +\n  scale_y_discrete(NULL, expand = c(0, 0)) +\n  scale_fill_gradient(expression(rho), low = \"#FCF9F0\", high = \"#A65141\", limits = c(0, 1)) +\n  theme_pearl_earring(axis.text.y = element_text(hjust = 0)) +\n  theme(axis.ticks = element_blank())\n\n\n\n\n\n\n\n\nThe correlations in our rho matrix look a little higher than those in the text (p. 474). Before we move on to the next plot, let’s consider psize. If you really want to scale the points in Figure 14.12.a like McElreath did, you can make the psize variable in a tidyverse sort of way as follows. However, if you compare the psize method and the default ggplot2 method using just logpop, you’ll see the difference is negligible. In that light, I’m going to be lazy and just use logpop in my plots.\n\nd |&gt; \n  transmute(psize = logpop / max(logpop)) |&gt; \n  transmute(psize = exp(psize * 1.5) - 2)\n\n       psize\n1  0.3134090\n2  0.4009582\n3  0.6663711\n4  0.7592196\n5  0.9066890\n6  0.9339560\n7  0.9834797\n8  1.1096138\n9  1.2223112\n10 2.4816891\n\n\nAs far as I can figure, you still have to get rho into a tidy data frame before feeding it into ggplot2. Here’s my attempt at doing so.\n\ntidy_rho &lt;- rho |&gt;\n  data.frame() |&gt; \n  rownames_to_column() |&gt; \n  bind_cols(d |&gt; select(culture, logpop, total_tools, lon2, lat)) |&gt; \n  pivot_longer(Ml:Ha,\n               names_to = \"colname\", \n               values_to = \"correlation\") |&gt;\n  mutate(group = str_c(pmin(rowname, colname), pmax(rowname, colname))) |&gt;\n  select(rowname, colname, group, culture, everything())  \n\nhead(tidy_rho)\n\n# A tibble: 6 × 9\n  rowname colname group culture  logpop total_tools  lon2   lat correlation\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;fct&gt;     &lt;dbl&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 Ml      Ml      MlMl  Malekula   7.00          13 -12.5 -16.3        1   \n2 Ml      Ti      MlTi  Malekula   7.00          13 -12.5 -16.3        0.89\n3 Ml      SC      MlSC  Malekula   7.00          13 -12.5 -16.3        0.86\n4 Ml      Ya      MlYa  Malekula   7.00          13 -12.5 -16.3        0.01\n5 Ml      Fi      FiMl  Malekula   7.00          13 -12.5 -16.3        0.65\n6 Ml      Tr      MlTr  Malekula   7.00          13 -12.5 -16.3        0.35\n\n\nOkay, here’s the code for our version of Figure 14.12.a.\n\nlibrary(ggrepel)\n\np1 &lt;- tidy_rho |&gt;       \n  ggplot(aes(x = lon2, y = lat)) +\n  geom_point(data = d, \n             aes(size = logpop), color = \"#DCA258\") +\n  geom_line(aes(group = group, alpha = correlation^2),\n            color = \"#EEDA9D\") +\n  geom_text_repel(data = d, aes(label = culture), \n                  color = \"#FCF9F0\", family = \"Courier\", point.padding = 0.2, seed = 14, size = 2.75) +\n  scale_alpha_continuous(range = c(0, 1)) +\n  labs(x = \"longitude\",\n       y = \"latitude\",\n       subtitle = \"Among societies in geographic space\\n\") +\n  coord_cartesian(xlim = range(d$lon2),\n                  ylim = range(d$lat)) +\n  theme(legend.position = \"none\")\n\nHere’s our the code for our version of Figure 14.12.b.\n\n# Compute the average posterior predictive relationship between \n# log population and total tools, summarized by the median and 80% interval\nf &lt;- post |&gt; \n  expand_grid(logpop = seq(from = 6, to = 14, length.out = 30)) |&gt;\n  mutate(population = exp(logpop)) |&gt; \n  mutate(lambda = exp(b_a_Intercept) * population^b_b_Intercept / b_g_Intercept) |&gt;\n  group_by(logpop) |&gt; \n  median_qi(lambda, .width = 0.8)\n\n# Plot\np2 &lt;- tidy_rho |&gt; \n  ggplot(aes(x = logpop)) +\n  geom_smooth(data = f,\n              aes(y = lambda, ymin = .lower, ymax = .upper),\n              stat = \"identity\",\n              alpha = 0.5, color = \"#100F14\", fill = \"#394165\", linewidth = 1.1) +\n  geom_point(data = d, \n             aes(y = total_tools, size = logpop), \n             color = \"#DCA258\") +\n  geom_line(aes(y = total_tools, group = group, alpha = correlation^2),\n            color = \"#EEDA9D\") +\n  geom_text_repel(data = d, \n                  aes(y = total_tools, label = culture), \n                  color = \"#FCF9F0\", family = \"Courier\", point.padding = 0.2, seed = 14, size = 2.75) +\n  scale_alpha_continuous(range = c(0, 1)) +\n  labs(x = \"log population\",\n       y = \"total tools\",\n       subtitle = \"Shown against the relation between\\ntotal tools and log pop\") +\n  coord_cartesian(xlim = range(d$logpop),\n                  ylim = range(d$total_tools)) +\n  theme(legend.position = \"none\")\n\nNow we combine them to make the full version of Figure 14.12.\n\np1 + p2 + \n  plot_annotation(title = \"Posterior median correlations\")\n\n\n\n\n\n\n\n\nAs expressed by the intensity of the colors of those connecting lines, our correlations are a bit more pronounced than those in the text, making for a more densely webbed plot. It is still the case that the correlations among Malekula, Tikopia, and Santa Cruz are the most pronounced. The next two notable correlations are between Manus and Trobriand and between Lau Fiji and Tonga.\n\n14.5.1.0.1 Rethinking: Dispersion by other names\nMcElreath remarked it might be a good idea to fit an alternative of this model using the gamma-Poisson likelihood. Let’s take him up on the challenge. Remember that gamma-Poisson models are also referred to as negative binomial models. When using brms, you specify this using family = negbinomial.\n\nb14.8_nb &lt;- brm(\n  data = d, \n  family = negbinomial(link = \"identity\"),\n  bf(total_tools ~ exp(a) * population^b / g,\n     a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE),\n     b + g ~ 1,\n     nl = TRUE),\n  prior = c(prior(normal(0, 1), nlpar = a),\n            prior(exponential(1), nlpar = b, lb = 0),\n            prior(exponential(1), nlpar = g, lb = 0),\n            prior(inv_gamma(2.874624, 2.941204), class = lscale, coef = gplat_adjlon2_adj, nlpar = a),\n            prior(exponential(1), class = sdgp, coef = gplat_adjlon2_adj, nlpar = a),\n            # Default prior\n            prior(inv_gamma(0.4, 0.3), class = shape)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  control = list(adapt_delta = 0.95),\n  file = \"fits/b14.08_nb\")\n\nCheck the summary.\n\nprint(b14.8_nb)\n\n Family: negbinomial \n  Links: mu = identity \nFormula: total_tools ~ exp(a) * population^b/g \n         a ~ 1 + gp(lat_adj, lon2_adj, scale = FALSE)\n         b ~ 1\n         g ~ 1\n   Data: d (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGaussian Process Hyperparameters:\n                            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsdgp(a_gplat_adjlon2_adj)       0.40      0.31     0.02     1.19 1.00      836     1333\nlscale(a_gplat_adjlon2_adj)     1.69      1.21     0.50     4.59 1.00     1937     2298\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\na_Intercept     0.33      0.87    -1.44     1.97 1.00     2891     1919\nb_Intercept     0.27      0.09     0.10     0.44 1.00     1876     1273\ng_Intercept     0.69      0.64     0.06     2.35 1.00     2585     2084\n\nFurther Distributional Parameters:\n           Estimate       Est.Error l-95% CI  u-95% CI Rhat Bulk_ESS Tail_ESS\nshape 4472764831.05 268871223158.15     4.12 128492.12 1.00      833     1163\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThis resulted in a slightly less pronounced correlation matrix.\n\npost &lt;- as_draws_df(b14.8_nb)\n\n# Compute posterior median covariance among societies\nk &lt;- matrix(0, nrow = 10, ncol = 10)\nfor (i in 1:10)\n    for (j in 1:10)\n        k[i, j] &lt;- median(post$sdgp_a_gplat_adjlon2_adj^2) * \n  exp(-islandsDistMatrix[i, j]^2 / (2 * median(post$lscale_a_gplat_adjlon2_adj)^2))\n\ndiag(k) &lt;- median(post$sdgp_a_gplat_adjlon2_adj^2) + 0.01\n\n# Convert to correlation matrix\nrho &lt;- round(cov2cor(k), 2)\n\n# Add row/col names for convenience\ncolnames(rho) &lt;- c(\"Ml\", \"Ti\", \"SC\", \"Ya\", \"Fi\", \"Tr\", \"Ch\", \"Mn\", \"To\", \"Ha\")\nrownames(rho) &lt;- colnames(rho)\n\nrho |&gt; round(2)\n\n     Ml   Ti   SC   Ya   Fi   Tr   Ch   Mn   To Ha\nMl 1.00 0.86 0.83 0.01 0.61 0.31 0.06 0.12 0.37  0\nTi 0.86 1.00 0.89 0.01 0.61 0.32 0.10 0.14 0.33  0\nSC 0.83 0.89 1.00 0.02 0.49 0.42 0.16 0.21 0.23  0\nYa 0.01 0.01 0.02 1.00 0.00 0.18 0.48 0.46 0.00  0\nFi 0.61 0.61 0.49 0.00 1.00 0.06 0.01 0.02 0.79  0\nTr 0.31 0.32 0.42 0.18 0.06 1.00 0.39 0.76 0.02  0\nCh 0.06 0.10 0.16 0.48 0.01 0.39 1.00 0.62 0.00  0\nMn 0.12 0.14 0.21 0.46 0.02 0.76 0.62 1.00 0.00  0\nTo 0.37 0.33 0.23 0.00 0.79 0.02 0.00 0.00 1.00  0\nHa 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  1\n\n\nLike before, we’ll view them in a plot.\n\nrho |&gt;\n  data.frame() |&gt; \n  mutate(row = d$culture) |&gt; \n  pivot_longer(-row, values_to = \"distance\") |&gt; \n  mutate(column = factor(name, levels = colnames(d_mat)),\n         row    = factor(row, levels = rownames(d_mat)) |&gt; fct_rev(),\n         label  = formatC(distance, format = 'f', digits = 2) |&gt; str_replace(\"0.\", \".\")) |&gt;\n  # Omit this line to keep the diagonal of 1's\n  filter(distance != 1) |&gt; \n  \n  ggplot(aes(x = column, y = row)) + \n  geom_raster(aes(fill = distance)) + \n  geom_text(aes(label = label),\n            color = \"#100F14\", family = \"Courier\", size = 2.75) +\n  scale_x_discrete(NULL, expand = c(0, 0), position = \"top\") +\n  scale_y_discrete(NULL, expand = c(0, 0)) +\n  scale_fill_gradient(expression(rho), low = \"#FCF9F0\", high = \"#A65141\", limits = c(0, 1)) +\n  theme_pearl_earring(axis.text.y = element_text(hjust = 0)) +\n  theme(axis.ticks = element_blank())\n\n\n\n\n\n\n\n\nOn the whole, the correlation medians appear a just little bit lower than from the Poisson model. Now let’s make a gamma-Poisson alternative to Figure 14.12.\n\n# Tidy up rho\ntidy_rho &lt;- rho |&gt;\n  data.frame() |&gt; \n  rownames_to_column() |&gt; \n  bind_cols(d |&gt; select(culture, logpop, total_tools, lon2, lat)) |&gt; \n  pivot_longer(Ml:Ha,\n               names_to = \"colname\", \n               values_to = \"correlation\") |&gt;\n  mutate(group = str_c(pmin(rowname, colname), pmax(rowname, colname))) \n\n# Left panel\np1 &lt;- tidy_rho |&gt;       \n  ggplot(aes(x = lon2, y = lat)) +\n  geom_point(data = d, \n             aes(size = logpop), color = \"#DCA258\") +\n  geom_line(aes(group = group, alpha = correlation^2),\n            color = \"#EEDA9D\") +\n  geom_text_repel(data = d, aes(label = culture), \n                  color = \"#FCF9F0\", family = \"Courier\", point.padding = 0.2, seed = 14, size = 2.75) +\n  scale_alpha_continuous(range = c(0, 1)) +\n  labs(x = \"longitude\",\n       y = \"latitude\",\n       subtitle = \"Among societies in geographic space\\n\") +\n  coord_cartesian(xlim = range(d$lon2),\n                  ylim = range(d$lat)) +\n  theme(legend.position = \"none\")\n\n# Compute the average posterior predictive relationship between \n# log population and total tools, summarized by the median and 80% interval\nf &lt;- post |&gt; \n  expand_grid(logpop = seq(from = 6, to = 14, length.out = 30)) |&gt;\n  mutate(population = exp(logpop)) |&gt; \n  mutate(lambda = exp(b_a_Intercept) * population^b_b_Intercept / b_g_Intercept) |&gt;\n  group_by(logpop) |&gt; \n  median_qi(lambda, .width = 0.8)\n\n# Right panel\np2 &lt;- tidy_rho |&gt; \n  ggplot(aes(x = logpop)) +\n  geom_smooth(data = f,\n              aes(y = lambda, ymin = .lower, ymax = .upper),\n              stat = \"identity\",\n              alpha = 0.5, color = \"#100F14\", fill = \"#394165\", linewidth = 1.1) +\n  geom_point(data = d, \n             aes(y = total_tools, size = logpop), \n             color = \"#DCA258\") +\n  geom_line(aes(y = total_tools, group = group, alpha = correlation^2),\n            color = \"#EEDA9D\") +\n  geom_text_repel(data = d, \n                  aes(y = total_tools, label = culture), \n                  color = \"#FCF9F0\", family = \"Courier\", point.padding = 0.2, seed = 14, size = 2.75) +\n  scale_alpha_continuous(range = c(0, 1)) +\n  labs(x = \"log population\",\n       y = \"total tools\",\n       subtitle = \"Shown against the relation between\\ntotal tools and log pop\") +\n  coord_cartesian(xlim = range(d$logpop),\n                  ylim = range(d$total_tools)) +\n  theme(legend.position = \"none\")\n\n# Combine, entitle, and display\np1 + p2 + \n  plot_annotation(title = \"Posterior median correlations based on the gamma-Poisson model\") \n\n\n\n\n\n\n\n\nThere is very little difference. Finish off by comparing the two models with the WAIC.\n\nb14.8    &lt;- add_criterion(b14.8,    criterion = \"waic\")\nb14.8_nb &lt;- add_criterion(b14.8_nb, criterion = \"waic\")\n\nloo_compare(b14.8, b14.8_nb, criterion = \"waic\") |&gt; print(simplify = F)\n\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic  se_waic\nb14.8      0.0       0.0   -33.7       1.4          3.9    0.7      67.3   2.8  \nb14.8_nb  -2.4       0.4   -36.1       1.7          3.8    0.7      72.2   3.4  \n\n\nThe WAIC comparison suggests we gain little by switching to the gamma-Poisson. If anything, we may have overfit.\n\n\n\n14.5.2 Example: Phylogenetic distance\n\nConsider as an example the causal influence of group size (\\(G\\)) on brain size (\\(B\\)). Hypotheses connecting these variables are popular, because primates (including humans) are unusual in both. Most primates live in social groups. Most mammals do not. Second, primates have relatively large brains. There is a family of hypotheses linking these two features. Suppose for example that group living, whatever its cause, could select for larger brains, because once you live with others, a larger brain helps to cope with the complexity of cooperation and manipulation. (p. 477)\n\nThere are also many potential confounds, which we’ll call \\(U\\). Also imagine there are two time points, indicated by subscripts 1 and 2. Here is the basic DAG.\n\ndag_coords &lt;- tibble(\n  name = c(\"G1\", \"B1\", \"U1\", \"G2\", \"B2\", \"U2\"),\n  x    = rep(1:2, each = 3),\n  y    = rep(3:1, times = 2))\n\ndagify(G2 ~ G1 + U1,\n       B2 ~ G1 + B1 + U1,\n       U2 ~ U1,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"G1\", \"B1\"), \"a\",\n                        ifelse(name %in% c(\"G2\", \"B2\"), \"b\", \"c\"))) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 fill = \"#FCF9F0\", shape = 21, size = 7, stroke = 2, show.legend = F) +\n  geom_dag_text(color = \"#100F14\", family = \"Courier\", parse = T,\n                label = c(expression(B[1]), expression(G[1]), expression(U[1]), \n                          expression(B[2]), expression(G[2]), expression(U[2]))) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(\"#80A0C7\", \"#EEDA9D\", \"#A65141\")) +\n  theme_pearl_dag()\n\n\n\n\n\n\n\n\nHowever, this will not be our model. Rather, we’ll consider this one.\n\ndag_coords &lt;- tibble(\n  name = c(\"G\", \"U\", \"M\", \"P\", \"B\"),\n  x    = c(0, 1, 1, 2, 2),\n  y    = c(3, 1, 2, 1, 3))\n\ndagify(G ~ U + M,\n       U ~ P,\n       M ~ U,\n       B ~ G + M + U,\n       coords = dag_coords) |&gt;\n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = name == \"U\"),\n                 fill = \"#FCF9F0\", shape = 21, size = 6, stroke = 2, show.legend = F) +\n  geom_dag_text(color = \"#100F14\", family = \"Courier\", parse = T) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(\"#EEDA9D\", \"#A65141\")) +\n  theme_pearl_dag()\n\n\n\n\n\n\n\n\n\nThere’s a lot going on here, but we can take it one piece at a time. Again, we’re interested in \\(G \\rightarrow B\\). There is one confound we know for sure, body mass (\\(M\\)). It possibly influences both \\(G\\) and \\(B\\). So we’ll include that in the model. The unobserved confounds \\(U\\) could potentially influence all three variables. Finally, we let the phylogenetic relationships (\\(P\\)) influence \\(U\\). How is \\(P\\) causal? If we traveled back in time and delayed a split between two species, it could influence the expected differences in their traits. So it is really the timing of the split that is causal, not the phylogeny. Of course \\(P\\) may also influence \\(G\\) and \\(B\\) and \\(M\\) directly. But those arrows aren’t our concern right now, so I’ve omitted them for clarity. (p. 478)\n\nPhylogenetic regression models, which “use some function of phylogenetic distance to model the covariation among species” (p. 478) attempt to grapple with all this. To see how, load the primates data and its phylogeny (see Street et al., 2017).\n\ndata(Primates301, package = \"rethinking\") \ndata(Primates301_nex)\n\nWhen working within the ggplot2 framework, one can plot a phylogeny with help from the ggtree package (Yu et al., 2017, 2018; Yu, 2020a, 2020b). Here’s a basic plot for our version of Figure 14.13.\n\nlibrary(ggtree)\n\nPrimates301_nex |&gt;\n  ggtree(layout = \"circular\", color = \"#394165\", size = 1/4) + \n  geom_tiplab(color = \"#100F14\", size = 5/3)\n\n\n\n\n\n\n\n\nLet’s format the data.\n\nd &lt;- Primates301 |&gt; \n  mutate(name = as.character(name)) |&gt; \n  drop_na(group_size, body, brain) |&gt; \n  mutate(m = log(body) |&gt; standardize(),\n         b = log(brain) |&gt; standardize(),\n         g = log(group_size) |&gt; standardize())\n\nglimpse(d)\n\nRows: 151\nColumns: 19\n$ name                &lt;chr&gt; \"Allenopithecus_nigroviridis\", \"Alouatta_belzebul\", \"Alouatta_caraya\", \"Alouatta_guariba\", \"Alouatta…\n$ genus               &lt;fct&gt; Allenopithecus, Alouatta, Alouatta, Alouatta, Alouatta, Alouatta, Alouatta, Aotus, Aotus, Arctocebus…\n$ species             &lt;fct&gt; nigroviridis, belzebul, caraya, guariba, palliata, pigra, seniculus, azarai, trivirgatus, calabarens…\n$ subspecies          &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ spp_id              &lt;int&gt; 1, 3, 4, 5, 6, 7, 9, 10, 18, 22, 23, 25, 26, 28, 29, 32, 33, 34, 40, 41, 46, 49, 50, 51, 52, 53, 54,…\n$ genus_id            &lt;int&gt; 1, 3, 3, 3, 3, 3, 3, 4, 4, 6, 7, 7, 7, 8, 8, 10, 11, 11, 13, 14, 14, 14, 14, 15, 15, 15, 15, 16, 16,…\n$ social_learning     &lt;int&gt; 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 17, 5, 0, 0, 1, NA, NA, 1, 0…\n$ research_effort     &lt;int&gt; 6, 15, 45, 37, 79, 25, 82, 22, 58, 1, 12, 58, 30, 10, 6, 24, 11, 8, 43, 16, 161, NA, 36, 13, 249, 60…\n$ brain               &lt;dbl&gt; 58.02, 52.84, 52.63, 51.70, 49.88, 51.13, 55.22, 20.67, 16.85, 6.92, 117.02, 105.09, 103.85, 9.86, 7…\n$ body                &lt;dbl&gt; 4655, 6395, 5383, 5175, 6250, 8915, 5950, 1205, 989, 309, 8167, 7535, 8280, 1207, 801, 6728, 3165, 2…\n$ group_size          &lt;dbl&gt; 40.00, 7.40, 8.90, 7.40, 13.10, 5.50, 7.90, 4.10, 3.15, 1.00, 14.50, 42.00, 20.00, 2.00, 3.00, 3.20,…\n$ gestation           &lt;dbl&gt; NA, NA, 185.92, NA, 185.42, 185.92, 189.90, NA, 133.47, 133.74, 138.20, 226.37, 228.18, 136.15, NA, …\n$ weaning             &lt;dbl&gt; 106.15, NA, 323.16, NA, 495.60, NA, 370.04, 229.69, 76.21, 109.26, NA, 816.35, 805.41, 149.15, NA, 6…\n$ longevity           &lt;dbl&gt; 276.0, NA, 243.6, NA, 300.0, 240.0, 300.0, NA, 303.6, 156.0, 336.0, 327.6, 453.6, NA, NA, NA, 324.0,…\n$ sex_maturity        &lt;dbl&gt; NA, NA, 1276.72, NA, 1578.42, NA, 1690.22, NA, 736.60, 298.91, NA, 2104.57, 2104.57, NA, NA, 2689.08…\n$ maternal_investment &lt;dbl&gt; NA, NA, 509.08, NA, 681.02, NA, 559.94, NA, 209.68, 243.00, NA, 1042.72, 1033.59, 285.30, NA, 867.63…\n$ m                   &lt;dbl&gt; 0.36958768, 0.57601585, 0.46403740, 0.43842259, 0.56110778, 0.79196303, 0.52913339, -0.50888297, -0.…\n$ b                   &lt;dbl&gt; 0.4039485, 0.3285765, 0.3253670, 0.3109981, 0.2821147, 0.3020630, 0.3640841, -0.4278779, -0.5925603,…\n$ g                   &lt;dbl&gt; 1.397272713, 0.003132082, 0.155626096, 0.003132082, 0.475005322, -0.242029791, 0.057151751, -0.48473…\n\n\nOur first model, which we might call the naïve model, explores the conditional relations of \\(\\log(\\text{body mass})\\) and \\(\\log(\\text{group size})\\) on \\(\\log(\\text{brain size})\\) without accounting for phylogenetic relationships.\n\nb14.9 &lt;- brm(\n  data = d,\n  family = gaussian,\n  b ~ 1 + m + g,\n  prior = c(prior(normal(0, 1), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  file = \"fits/b14.09\")\n\nCheck the summary of the naïve model.\n\nprint(b14.9)\n\n Family: gaussian \n  Links: mu = identity \nFormula: b ~ 1 + m + g \n   Data: d (Number of observations: 151) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.00      0.02    -0.03     0.04 1.00     3964     2812\nm             0.89      0.02     0.85     0.94 1.00     3163     2807\ng             0.12      0.02     0.08     0.17 1.00     3314     2647\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.22      0.01     0.19     0.24 1.00     4013     2656\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf you want \\(\\sigma\\) in the \\(\\sigma^2\\) metric, you can square that by hand.\n\nas_draws_df(b14.9) |&gt; \n  mutate(sigma_sq = sigma^2) |&gt; \n  mean_qi(sigma_sq) |&gt; \n  mutate_if(is.double, round, digits = 2)\n\n# A tibble: 1 × 6\n  sigma_sq .lower .upper .width .point .interval\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1     0.05   0.04   0.06   0.95 mean   qi       \n\n\nThe oldest and most conservative way to include information about phylogenetic relationships is with a Brownian motion model.\n\nBrownian motion just means Gaussian random walks. If species traits drift randomly with respect to one another after speciation, then the covariance between a pair of species ends up being linearly related to the phylogenetic branch distance between them–the further apart, the less covariance, as a proportion of distance. (p. 481)\n\nWe’ll use functions from the ape package (Emmanuel Paradis et al., 2022; E. Paradis & Schliep, 2019) to make the covariance matrix (V) and the distance matrix (Dmat).\n\nlibrary(ape)\n\nspp_obs &lt;- d$name\n\ntree_trimmed &lt;- keep.tip(Primates301_nex, spp_obs)\nRbm &lt;- corBrownian(phy = tree_trimmed)\n\nV &lt;- vcv(Rbm)\nDmat &lt;- cophenetic( tree_trimmed )\n\nHere’s the distance by covariance scatter plot McElreath alluded to but did not show in the text.\n\nfull_join(\n  Dmat |&gt; \n    as_tibble(rownames = \"row\") |&gt; \n    pivot_longer(-row,\n                 names_to = \"col\",\n                 values_to = \"distance\"),\n  V |&gt; \n    as_tibble(rownames = \"row\") |&gt; \n    pivot_longer(-row,\n                 names_to = \"col\",\n                 values_to = \"covariance\"),\n  by = c(\"row\", \"col\")) |&gt; \n  \n  ggplot(aes(x = distance, y = covariance)) +\n  geom_point(alpha = 1/10, color = \"#80A0C7\") +\n  labs(x = \"phylogenetic distance\", \n       y = \"covariance\",\n       subtitle = \"These variables are the\\ninverse of one another.\")\n\n\n\n\n\n\n\n\nMcElreath suggested executing image(V) and image(Dmat) to plot heat maps of each matrix. We’ll have to work a little harder to make decent-looking head maps within our tidyverse workflow.\n\n# Headmap of `Dmat`\np1 &lt;- Dmat |&gt; \n  as_tibble(rownames = \"row\") |&gt; \n  pivot_longer(-row,\n               names_to = \"col\",\n               values_to = \"distance\") |&gt;\n  \n  ggplot(aes(x = col, y = row, fill = distance)) +\n  geom_tile() +\n  scale_x_discrete(NULL, breaks = NULL) +\n  scale_y_discrete(NULL, breaks = NULL) +\n  scale_fill_gradient(low = \"#100F14\", high = \"#EEDA9D\") +\n  theme(legend.position = \"top\")\n\n# Headmap of `V`\np2 &lt;- V |&gt; \n  as_tibble(rownames = \"row\") |&gt; \n  pivot_longer(-row,\n               names_to = \"col\",\n               values_to = \"covariance\") |&gt; \n  \n  ggplot(aes(x = col, y = row, fill = covariance)) +\n  geom_tile() +\n  scale_x_discrete(NULL, breaks = NULL) +\n  scale_y_discrete(NULL, breaks = NULL) +\n  scale_fill_gradient(low = \"#100F14\", high = \"#EEDA9D\") +\n  theme(legend.position = \"top\")\n\n# Combine, entitle, and display\n(p1 | p2) + plot_annotation(subtitle = \"Again, distance is the inverse of covariance.\")\n\n\n\n\n\n\n\n\nWithin the brms paradigm, one inserts a known covariance matrix into a model using the fcor() function. For any longer-term brms users, fcor() is the replacement for the now-depreciated cor_fixed() function. Along with fcor(), one tells brms about the data with the data2 function. Here’s how to use it for our version of McElreath’s m14.10.\n\nR &lt;- V[spp_obs, spp_obs] / max(V)\n\nb14.10 &lt;- brm(\n  data = d,\n  data2 = list(R = R),\n  family = gaussian,\n  b ~ 1 + m + g + fcor(R),\n  prior = c(prior(normal(0, 1), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  file = \"fits/b14.10\")\n\nCheck the summary of the Brownian model.\n\nprint(b14.10)\n\n Family: gaussian \n  Links: mu = identity \nFormula: b ~ 1 + m + g + fcor(R) \n   Data: d (Number of observations: 151) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.19      0.16    -0.52     0.12 1.00     4252     2997\nm             0.70      0.04     0.63     0.77 1.00     4253     2619\ng            -0.01      0.02    -0.05     0.03 1.00     3981     3083\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.40      0.02     0.36     0.45 1.00     4190     2918\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSince our residual variance is still in the \\(\\sigma\\) metric, it will be easier to compare it to McElreath’s sigma_sq parameter after transforming the posterior samples.\n\nas_draws_df(b14.10) |&gt; \n  mutate(sigma_sq = sigma^2) |&gt; \n  mean_hdi(sigma_sq, .width = 0.89) |&gt; \n  mutate_if(is.double, round, 2)\n\n# A tibble: 1 × 6\n  sigma_sq .lower .upper .width .point .interval\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1     0.16   0.13   0.19   0.89 mean   hdi      \n\n\nMcElreath introduced the Ornstein–Uhlenbeck (OU) process as an alternative to the Brownian motion model. The OU model proposes the covariance between two species \\(i\\) and \\(j\\) is\n\\[K(i, j) = \\eta^2 \\exp(\\rho^2 D_{ij}),\\]\nwhere, in our case, \\(D_{ij}\\) is the distance matrix we’ve saved above as Dmat. Sadly for us, brms only supports the exponentiated-quadratic kernel for Gaussian process models, at this time. However, the Ornstein–Uhlenbeck kernel is one of the alternative kernels Bürkner has on his to-do list (see GitHub issue #234). To follow along with McElreath, we will have to use rethinking or raw Stan. Our approach will be the former. First we make the dat_list.\n\ndat_list &lt;- list(\n  N_spp = nrow(d),\n  M     = standardize(log(d$body)),\n  B     = standardize(log(d$brain)),\n  G     = standardize(log(d$group_size)), Imat = diag(nrow(d)),\n  V     = V[spp_obs, spp_obs],\n  R     = V[spp_obs, spp_obs] / max(V[spp_obs, spp_obs]),\n  Dmat  = Dmat[spp_obs, spp_obs] / max(Dmat))\n\nNow fit the OU model with rethinking::ulam().\n\nm14.11 &lt;- ulam( \n  alist(\n    B ~ multi_normal(mu, SIGMA),\n    mu &lt;- a + bM * M + bG * G,\n    matrix[N_spp,N_spp]: SIGMA &lt;- cov_GPL1(Dmat, etasq, rhosq, 0.01), \n    a ~ normal(0, 1),\n    c(bM,bG) ~ normal(0, 0.5),\n    etasq ~ half_normal(1, 0.25),\n    rhosq ~ half_normal(3, 0.25)\n  ), \n  data = dat_list, \n  chains = 4, cores = 4)\n\nHappily, our results are much like those in the text.\n\nprecis(m14.11)\n\n\n\n       mean   sd  5.5% 94.5% rhat ess_bulk\na     -0.06 0.08 -0.18  0.06    1  2085.50\nbG     0.05 0.02  0.01  0.09    1  2096.53\nbM     0.83 0.03  0.79  0.88    1  2301.19\netasq  0.03 0.01  0.03  0.05    1  1792.25\nrhosq  2.80 0.24  2.41  3.18    1  1759.88\n\n\nNext we extract the posterior draws with extract.samples().\n\npost &lt;- extract.samples(m14.11)\n\nTo get a sense of the covariance function implied by etasq and rhosq, here we’ll plot the posterior against the prior for our tidyverse variant of Figure 14.14.\n\nset.seed(14)\n\nd_fig &lt;- left_join(\n  # Posterior\n  post |&gt; \n    data.frame() |&gt; \n    slice_sample(n = 30) |&gt; \n    mutate(draw = 1:n()) |&gt; \n    select(draw, etasq, rhosq) |&gt; \n    expand_grid(d_seq = seq(from = 0, to = 1, length.out = 50)),\n  # Prior\n  tibble(eta = abs(rnorm(1e5, mean = 1, sd = 0.25)),\n         rho = abs(rnorm(1e5, mean = 3, sd = 0.25))) |&gt; \n    expand_grid(d_seq = seq(from = 0, to = 1, length.out = 50)) |&gt; \n    mutate(k = eta * exp(-rho * d_seq)) |&gt; \n    group_by(d_seq) |&gt; \n    mean_hdi(k, .width = 0.89),\n  # Join them\n  by = \"d_seq\")\n\n# Plot!\nd_fig |&gt; \n  ggplot(aes(x = d_seq)) +\n  geom_line(aes(y = etasq * exp(-rhosq * d_seq), group = draw),\n            color = \"#80A0C7\", alpha = 1/2) +\n  geom_lineribbon(data = d_fig |&gt; filter(draw == 1),\n            aes(y = k, ymin = .lower, ymax = .upper),\n            color = \"#A65141\", fill = \"#E7CDC2\") +\n  annotate(geom = \"text\",\n           x = c(0.2, 0.5), y = c(0.1, 0.5),\n           label = c(\"posterior\", \"prior\"),\n           color = c(\"#80A0C7\", \"#E7CDC2\"),\n           family = \"Courier\") +\n  labs(x = \"phylogenetic distance\", \n       y = \"covariance\") +\n  ylim(0, 1.5)\n\n\n\n\n\n\n\n\n\nThere just isn’t a lot of phylogenetic covariance for brain sizes, at least according to this model and these data. As a result, the phylogenetic distance doesn’t completely explain away the association between group size and brain size, as it did in the Brownian motion model. (p. 485)\n\nFor a thorough discussion on how to fit phylogenetic models with brms, particularly within the multilevel paradigm, see Bürkner’s (2022d) vignette, Estimating phylogenetic multilevel models with brms.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Adventures in Covariance</span>"
    ]
  },
  {
    "objectID": "14.html#summary-bonus-multilevel-growth-models-and-the-melsm",
    "href": "14.html#summary-bonus-multilevel-growth-models-and-the-melsm",
    "title": "14  Adventures in Covariance",
    "section": "14.6 Summary Bonus: Multilevel growth models and the MELSM",
    "text": "14.6 Summary Bonus: Multilevel growth models and the MELSM\nTo this point in the chapter and most of the text, the data have largely had a cross-sectional feel. In fairness, we did incorporate an element of time with the café example from model b14.1 by looking at the differences between mornings and evenings. However, even then we collapsed across longer time spans, such as days, weeks, months, and so on. One of the two goals of this bonus section to provide a brief introduction multilevel models designed to express change over time. The particular brand of multilevel models we’ll focus on are often called multilevel growth models. Though we will focus on simple linear models, this basic framework can be generalized along many lines. The second goal is to build on our appreciation of covariance structures by introducing a class of multilevel models designed to investigate variation in variation called the mixed-effects location scale models (MELSM). For our final model, we get a little fancy and fit a multivariate MELSM.\n\n14.6.1 Borrow some data\nAll the models in this bonus section are based on the paper by Donald R. Williams, Martin, et al. (2021), Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity (you can find the preprint here). Williams and colleagues’ data and supporting scripts are available in the example_analyses folder from their OSF project at https://osf.io/3bmdh/. You can also download the data from the data folder of this ebook’s GitHub repo.\nLoad the data.\n\n# Load the data from GitHub\nload(url(\"https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/raw/master/data/m_melsm_dat.rda\"))\n\n# Add a scaled time variable\ndat &lt;- dat |&gt; \n  mutate(day01 = (day - 2) / max((day - 2)))\n\n# Take a look\nglimpse(dat)\n\nRows: 13,033\nColumns: 9\n$ P_A.std   &lt;dbl&gt; 1.74740876, -0.23109384, 0.34155950, 0.45664827, -0.23484069, 1.12785344, 1.11272629, 0.55245938, 1.52371746, …\n$ day       &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 25, 26, 27, 28, 29, 30, 33, 34, 35…\n$ P_A.lag   &lt;dbl&gt; 0.7478597, 1.4674156, -0.3772641, 0.1286055, 0.3292090, 1.4107233, 0.7696644, 0.7304159, 1.0444039, 0.6519189,…\n$ N_A.lag   &lt;dbl&gt; 0.25399356, -0.85363386, 0.96144592, -0.19620339, -0.16047348, -0.90365575, -0.77502804, -0.31053913, -0.68213…\n$ steps.pm  &lt;dbl&gt; 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, 0.955171, …\n$ steps.pmd &lt;dbl&gt; 0.59955783, -0.39471676, -1.51935866, -1.34423352, 0.41759695, -0.32310425, 0.37641976, 0.31766495, -0.4459848…\n$ record_id &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ N_A.std   &lt;dbl&gt; -0.73357975, 0.53856559, 0.60161616, 0.27807249, 0.54674641, 0.05660701, -0.08417053, 0.10458994, -0.48557629,…\n$ day01     &lt;dbl&gt; 0.00000000, 0.01020408, 0.02040816, 0.03061224, 0.04081633, 0.05102041, 0.06122449, 0.07142857, 0.08163265, 0.…\n\n\nThese data are from 193 participants.\n\ndistinct(dat, record_id) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1   193\n\n\nParticipants were asked to complete self-report ratings once a day for a few months. People varied by how many days they participated in the study, with number of days ranging from 8 to 99 and a median of 74.\n\ndat |&gt; \n  count(record_id) |&gt; \n  summarise(median = median(n),\n            min = min(n),\n            max = max(n))\n\n# A tibble: 1 × 3\n  median   min   max\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     74     8    99\n\n\nHere is a plot of that distribution.\n\ndat |&gt; \n  count(record_id) |&gt; \n  \n  ggplot(aes(x = n)) +\n  geom_bar(fill = \"#B1934A\") +\n  scale_x_continuous(\"number of days\", limits = c(0, NA)) +\n  theme_pearl_earring()\n\n\n\n\n\n\n\n\nOur primary variables of interest were taken from the Positive and Negative Affect Schedule (PANAS, Watson et al., 1988), which is widely used in certain areas of psychology to measure mood or emotion. In this study, participants completed the PANAS once a day by endorsing the extent to which they experienced various positive (e.g., excited, inspired) and negative (e.g., upset, afraid) emotional states. These responses are summed into two composite scores: positive affect (PA) and negative affect (NA). In the current data, the standardized versions of these scores are in the P_A.std and N_A.std columns, respectively. To get a sense of what these look like, here are the daily N_A.std scores from a random sample of 16 participants.\n\nset.seed(14)\n\ndat |&gt; \n  nest(data = c(P_A.std, day, P_A.lag, N_A.lag, steps.pm, steps.pmd, N_A.std, day01)) |&gt; \n  slice_sample(n = 16) |&gt; \n  unnest(data) |&gt; \n  \n  ggplot(aes(x = day, y = N_A.lag)) +\n  geom_line(color = \"#80A0C7\") +\n  geom_point(color = \"#FCF9F0\", size = 1/2) +\n  ylab(\"negative affect (standardized)\") +\n  facet_wrap(~ record_id)\n\n\n\n\n\n\n\n\n\n\n14.6.2 Conventional multilevel growth model\nIn the social sciences, a typical way to analyze data like these is with a multilevel growth model in which participants vary in their intercepts (starting point) and time slopes (change over time). In the sample of the data, above, it looks like most participants have fairly constant levels of NA over time (i.e., near-zero slopes) but some (e.g., #128 and 147) show some evidence of systemic decreases in NA (i.e., negative slopes). There is also some variation in starting points, though most of the participants in this subset of the data seemed to have endorsed relatively low levels of NA both at baseline and throughout the study.\nWe want a model that can capture all these kinds of variation. Eventually, we will fit a model that accounts for both PA and NA. But to keep things simple while we’re warming up, we will restrict our focus to NA. If we let \\(\\text{NA}_{ij}\\) be the standardized NA score for the \\(i\\)th participant on the \\(j\\)th day, our first Bayesian multilevel growth model will follow the form\n\\[\\begin{align*}\n\\text{NA}_{ij} & \\sim \\operatorname{Normal}(\\mu_{ij}, \\sigma) \\\\\n\\mu_{ij}       & = \\beta_0 + \\beta_1 \\text{time}_{ij} + \\color{#A65141}{u_{0i} + u_{1i} \\text{time}_{ij}} \\\\\n\\sigma & = \\sigma_\\epsilon \\\\\n\\color{#A65141}{\\begin{bmatrix} u_{0i} \\\\ u_{1i} \\end{bmatrix}} & \\color{#A65141}\\sim \\color{#A65141}{\\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf S \\mathbf R \\mathbf S \\end{pmatrix}} \\\\\n\\mathbf S & = \\begin{bmatrix} \\sigma_0 & 0 \\\\ 0 & \\sigma_1 \\end{bmatrix} \\\\\n\\mathbf R & = \\begin{bmatrix} 1 & \\rho_{12} \\\\ \\rho_{21} & 1 \\end{bmatrix} \\\\\n\\beta_0   & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_1   & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma_0 \\text{ and } \\sigma_1 & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_\\epsilon & \\sim \\operatorname{Exponential}(1) \\\\\n\\mathbf R & \\sim \\operatorname{LKJ}(2),\n\\end{align*}\\]\nwhere \\(\\beta_0\\) is the intercept (i.e., starting point) and \\(u_{0i}\\) captures variations in that intercept across participants. Similarly, \\(\\beta_1\\) is the slope depicting linear change in \\(\\text{NA}\\) across time and \\(u_{1i}\\) captures variations in that linear change across participants. The \\(u_{0i}\\) and \\(u_{1i}\\) parameters are modeled as multivariate normal with zero means (i.e., they are deviations from the population parameters) and standard deviations \\(\\sigma_0\\) and \\(\\sigma_1\\). We express the correlation between those two group-level \\(\\sigma\\) parameters with \\(\\mathbf R\\), the symmetric correlation matrix. Here we just have one correlation, \\(\\rho_{21}\\), which is the same as \\(\\rho_{12}\\). Finally, variation not accounted for by the other parameters is captured by the single parameter \\(\\sigma_\\epsilon\\), which is often just called \\(\\epsilon\\).\nWe have two variables measuring time in these data. The day variable measures time by integers, ranging from 2 to 100. To make it a little easier to set the priors and fit the model with Stan, we have a rescaled version of the variable, day01, which ranges from 0 to 1. In this way, \\(\\beta_0\\) is the value for the first day in the data set and \\(\\beta_1\\) is the expected change by the end of the collection (i.e., the 100th day). For consistency, we are largely following McElreath’s weakly-regularizing approach to priors.\nYou may have noticed my statistical notation differs a bit from McElreath’s, here. It follows a blend of sensibilities from McElreath, Williams, and from the notation I used in my (2026) translation](https://solomon.quarto.pub/alda/) of Singer and Willett’s (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence. I hope it’s clear.\nHere is how to fit our simple multilevel growth model with brms.\n\nb14.12 &lt;- brm(\n  data = dat,\n  family = gaussian,\n  N_A.std ~ 1 + day01 + (1 + day01 | record_id),\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 1), class = b),\n            prior(exponential(1), class = sd),\n            prior(exponential(1), class = sigma),\n            prior(lkj(2), class = cor)),\n  iter = 3000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  file = \"fits/b14.12\")\n\nCheck the summary.\n\nprint(b14.12)\n\n Family: gaussian \n  Links: mu = identity \nFormula: N_A.std ~ 1 + day01 + (1 + day01 | record_id) \n   Data: dat (Number of observations: 13033) \n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~record_id (Number of levels: 193) \n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)            0.78      0.04     0.70     0.86 1.00     1741     3350\nsd(day01)                0.65      0.05     0.57     0.75 1.00     3211     5311\ncor(Intercept,day01)    -0.34      0.08    -0.49    -0.18 1.00     3101     4725\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.03      0.05    -0.07     0.14 1.01      847     1547\nday01        -0.16      0.06    -0.26    -0.05 1.00     2872     4656\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.61      0.00     0.60     0.62 1.00    13236     5452\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHopefully it makes sense that the population-level intercept (\\(\\beta_0\\)) is near zero. It would be odd if it wasn’t given these are standardized data. The coefficient for day01 (\\(\\beta_1\\)) is mildly negative, suggesting an overall trend for participants to endorse lower NA scores over time.\nThe two group-level \\(\\sigma\\) parameters are fairly large given the scale of the data. They suggest participants varied quite a bit in terms of both intercepts and slopes. They also have a moderate negative correlation, suggesting that participants with higher intercepts tended to have more negative slopes.\nTo get a sense of the model, we’ll plot the posterior means for each participants’ fitted trajectory across time (thin lines), along with the population-average trajectory (thick line).\n\nnd &lt;- dat |&gt; \n  distinct(record_id, day01)\n\nfitted(b14.12, newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = day01, y = Estimate, group = record_id)) +\n  geom_line(alpha = 1/3, color = \"#8B9DAF\", linewidth = 1/3) +\n  geom_segment(x = 0, xend = 1,\n               y = fixef(b14.12)[1, 1],\n               yend = fixef(b14.12)[1, 1] + fixef(b14.12)[2, 1],\n               color = \"#80A0C7\", linewidth = 3) +\n  scale_x_continuous(breaks = c(0, 0.5, 1)) +\n  ylab(\"negative affect (standardized)\")\n\n\n\n\n\n\n\n\nIf you look back up to the model summary from before the plot, the one parameter we didn’t focus on was the lone sigma parameter at the bottom. That’s our \\(\\sigma_\\epsilon\\), which captures the variation in NA not accounted for by the intercepts, slopes, and their correlation. An important characteristic of the conventional multilevel growth model is that \\(\\sigma_\\epsilon\\) does not vary across persons, occasions, or other variables. To give a sense of why this might not be the best assumption, let’s take a focused look at the model implied trajectories for two participants. Here we will take cues from some Figure 4.8 from way back in Section 4.4.3.5. We will plot the original data atop both the fitted lines and their 95% intervals, which expresses the mean structure, along with the 95% posterior predictive interval, which expresses the uncertainty of the \\(\\sigma_\\epsilon\\) parameter.\n\nnd &lt;- dat |&gt; \n  filter(record_id %in% c(30, 115)) |&gt; \n  select(record_id, N_A.std, day01)\n\nbind_cols(\n  # `fitted`\n  fitted(b14.12, newdata = nd) |&gt; \n    data.frame(),\n  # `predict`\n  predict(b14.12, newdata = nd) |&gt; \n    data.frame() |&gt; \n    select(Q2.5:Q97.5) |&gt; \n    set_names(\"p_lower\", \"p_upper\")\n) |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = day01)) +\n  geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              alpha = 1/2, color = \"#8B9DAF\", fill = \"#8B9DAF\", linewidth = 1/2) +\n  geom_ribbon(aes(ymin = p_lower, ymax = p_upper),\n              alpha = 1/2, fill = \"#8B9DAF\") +\n  geom_point(aes(y = N_A.std),\n             color = \"#8B9DAF\") +\n  scale_x_continuous(breaks = c(0, 0.5, 1)) +\n  ylab(\"negative affect (standardized)\") +\n  facet_wrap(~ record_id)\n\n\n\n\n\n\n\n\nBecause of our fixed \\(\\sigma_\\epsilon\\) parameter, the 95% posterior predictive interval is the same width for both participants. Yet look how closely participant the data points for participant 30 cluster not only within the center region of the posterior prediction intervals, but also almost completely within the 95% interval for the fitted line. In contrast, notice how much more spread out the data points for participant 115 are, and how many of them extend well beyond the posterior predictive interval. This difference in variability is ignored by conventional growth models. However, there’s no reason we can’t adjust our model to capture this kind of person-level variability, too. Enter the MELSM.\n\n\n14.6.3 Learn more about your data with the MELSM\nMixed-effects location scale models (MELSMs) have their origins in the work of Donald Hedeker and colleagues (Hedeker et al., 2008, 2012). Rast et al. (2012) showcased an early application of the framework to the BUGS/JAGS software. More recently Philippe Rast and colleagues (particularly Donald Williams) have adapted this approach for use within the Stan/brms ecosystem (Donald R. Williams et al., 2019; Donald R. Williams et al., 2020, 2022; Donald R. Williams, Mulder, et al., 2021).\nWithin the brms framework, MELSMs apply a distributional modeling approach (see Bürkner, 2022b) to the multilevel growth model. Not only are parameters from the mean structure allowed to vary across groups, but parameters applied to \\(\\sigma\\) are allowed to vary across groups, too. Do you remember the little practice model b10.1 from Section 10.2.2? We simulated Gaussian data for two groups with the same mean parameter but different parameters for \\(\\sigma\\). If you check back in that section, you’ll see that the brms default was to model \\(\\log \\sigma\\) in that case. This is smart because when you define a model for \\(\\sigma\\), you want to use a link function that ensures the predictions will be stay at zero and above. The MELSM approach of Hedeker, Rast, Williams and friends applies this logic to \\(\\sigma_\\epsilon\\) in multilevel growth models. However, not only can \\(\\sigma_\\epsilon\\) vary across groups in a fixed-effects sort of way, we can use multilevel partial pooling, too.\nTo get a sense of what this looks like, we’ll augment our previous model, but this time allowing \\(\\sigma_\\epsilon\\) to vary across participants. You might express the updated statistical model as\n\\[\\begin{align*}\n\\text{NA}_{ij} & \\sim \\operatorname{Normal}(\\mu_{ij}, \\color{#A65141}{\\sigma_{i}})\\\\\n\\mu_{ij} & = \\beta_0 + \\beta_1 \\text{time}_{ij} + u_{0i} + u_{1i} \\text{time}_{ij} \\\\\n\\color{#A65141}{\\log (\\sigma_i )} & \\color{#A65141}= \\color{#A65141}{\\eta_0 + u_{2i}} \\\\\n\\begin{bmatrix} u_{0i} \\\\ u_{1i} \\\\ \\color{#A65141}{u_{2i}} \\end{bmatrix} & \\sim \\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf S \\mathbf R \\mathbf S \\end{pmatrix} \\\\\n\\mathbf S & = \\begin{bmatrix} \\sigma_0 & 0 & 0 \\\\ 0 & \\sigma_1 & 0 \\\\ 0 & 0 & \\sigma_2 \\end{bmatrix} \\\\\n\\mathbf R & = \\begin{bmatrix} 1 & \\rho_{12} & \\rho_{13} \\\\ \\rho_{21} & 1 & \\rho_{23} \\\\ \\rho_{31} & \\rho_{32} & 1 \\end{bmatrix} \\\\\n\\beta_0   & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_1 \\text{and } \\eta_0 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma_0,\\dots, \\sigma_2     & \\sim \\operatorname{Exponential}(1) \\\\\n\\mathbf R & \\sim \\operatorname{LKJ}(2).\n\\end{align*}\\]\nIn the opening likelihood statement from the prior model, we simply set \\(\\text{NA}_{ij} \\sim \\operatorname{Normal}\\begin{pmatrix} \\mu_{ij}, \\sigma \\end{pmatrix}\\). For our first MELSM, we now refer to \\(\\sigma_i\\), meaning the levels of variation not accounted for by the mean structure can vary across participants (hence the \\(i\\) subscript). Two lines down, we see the formula for \\(\\log \\begin{pmatrix} \\sigma_i \\end{pmatrix}\\) contains population-level intercept, \\(\\eta_0\\), and participant-specific deviations around that parameter, \\(u_{2i}\\). In the next three lines, the plot deepens. We see that all three participant-level deviations, \\(u_{0i},\\dots,u_{2i}\\) are multivariate normal with means set to zero and variation expressed in the parameters \\(\\sigma_0,\\dots,\\sigma_2\\) of the \\(\\mathbf S\\) matrix. In the \\(\\mathbf R\\) matrix, we now have three correlation parameters, with \\(\\rho_{31}\\) and \\(\\rho_{32}\\) allowing us to assess the correlations among individual differences in variability and individual differences in starting points and change over time, respectively. Let’s fit the model.\n\nb14.13 &lt;- brm(\n  data = dat,\n  family = gaussian,\n  bf(N_A.std ~ 1 + day01 + (1 + day01 |i| record_id),\n     sigma ~ 1 + (1 |i| record_id)),\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 1), class = b),\n            prior(exponential(1), class = sd),\n            \n            prior(normal(0, 1), class = Intercept, dpar = sigma),\n            prior(exponential(1), class = sd, dpar = sigma),\n            \n            prior(lkj(2), class = cor)),\n  iter = 3000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  file = \"fits/b14.13\")\n\nWe should note a few things about the brm() syntax. First, because we modeled both \\(\\mu_{ij}\\) and \\(\\sigma_i\\), we nested both model formulas within the bf() function. Second, because the brms default is to use the log link when modeling \\(\\sigma_i\\), there was no need to explicitly set it that way in the family line. However, we could have if we wanted to. Third, notice our use of the |i| syntax within the parentheses in the formula lines. If we had used the conventional | syntax, that would have not allowed our \\(u_{2i}\\) parameters to correlate with \\(u_{0i}\\) and \\(u_{1i}\\) from the mean structure. It would have effectively set \\(\\rho_{31} = \\rho_{32} = 0\\). Finally, notice how within the prior() functions, we explicitly referred to those for the new \\(\\sigma\\) structure with the dpar = sigma operator.\nOkay, time to check the model summary.\n\nprint(b14.13)\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: N_A.std ~ 1 + day01 + (1 + day01 | i | record_id) \n         sigma ~ 1 + (1 | i | record_id)\n   Data: dat (Number of observations: 13033) \n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~record_id (Number of levels: 193) \n                               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                      0.76      0.04     0.69     0.84 1.00      697     1505\nsd(day01)                          0.60      0.04     0.52     0.69 1.01     1215     2668\nsd(sigma_Intercept)                0.69      0.03     0.63     0.77 1.01      704     1320\ncor(Intercept,day01)              -0.33      0.08    -0.48    -0.17 1.01      725     1717\ncor(Intercept,sigma_Intercept)     0.61      0.05     0.52     0.69 1.00      896     1870\ncor(day01,sigma_Intercept)        -0.09      0.08    -0.25     0.06 1.01      672     1514\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           0.03      0.05    -0.08     0.13 1.02      258      655\nsigma_Intercept    -0.79      0.05    -0.88    -0.69 1.01      429     1096\nday01              -0.16      0.05    -0.26    -0.06 1.00      749     1809\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe ‘sigma_Intercept’ lines in the ‘Population-Level Effects’ section is the summary for our \\(\\eta_0\\) parameter. To get a sense of what this means out of the log space, just exponentiate.\n\nfixef(b14.13)[\"sigma_Intercept\", -2] |&gt; exp()\n\n Estimate      Q2.5     Q97.5 \n0.4558183 0.4133110 0.5021022 \n\n\nTo get a sense of the variation in that parameter across participants [i.e., \\(\\exp(\\eta_0 + u_{2i})\\)], it’s best to plot.\n\ncoef(b14.13)$record_id[, , \"sigma_Intercept\"] |&gt; \n  exp() |&gt; \n  data.frame() |&gt; \n  arrange(Estimate) |&gt; \n  mutate(rank = 1:n()) |&gt; \n  \n  ggplot(aes(x = rank, y = Estimate, ymin = Q2.5, ymax = Q97.5)) +\n  geom_pointrange(color = \"#EEDA9D\", linewidth = 0.4, size = 1/20) +\n  scale_x_continuous(\"participants ranked by posterior mean\", breaks = NULL) +\n  ylab(expression(exp(eta[0]+italic(u)[2][italic(i)])))\n\n\n\n\n\n\n\n\nLooks like there’s a lot of variation in a parameter that was formerly fixed across participants as a single value \\(\\sigma_\\epsilon\\). Here’s what this looks like in terms of our posterior predictive distributions for participants 30 and 115, from before.\n\nnd &lt;- dat |&gt; \n  filter(record_id %in% c(30, 115)) |&gt; \n  select(record_id, N_A.std, day01)\n\nbind_cols(\n  fitted(b14.13, newdata = nd) |&gt; \n    data.frame(),\n  predict(b14.13, newdata = nd) |&gt; \n    data.frame() |&gt; \n    select(Q2.5:Q97.5) |&gt; \n    set_names(\"p_lower\", \"p_upper\")) |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = day01)) +\n  geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              alpha = 1/2, color = \"#8B9DAF\", fill = \"#8B9DAF\", linewidth = 1/2) +\n  geom_ribbon(aes(ymin = p_lower, ymax = p_upper),\n              alpha = 1/2, fill = \"#8B9DAF\") +\n  geom_point(aes(y = N_A.std),\n             color = \"#8B9DAF\") +\n  scale_x_continuous(breaks = c(0, 0.5, 1)) +\n  ylab(\"negative affect (standardized)\") +\n  facet_wrap(~ record_id)\n\n\n\n\n\n\n\n\nThat’s a big improvement. Let’s expand our skill set. Within the MELSM paradigm, one can use multiple variables to model participant-specific variation. Here we’ll add in our time variable.\n\\[\\begin{align*}\n\\text{NA}_{ij} & \\sim \\operatorname{Normal}(\\mu_{ij}, \\color{#A65141}{\\sigma_{ij}}) \\\\\n\\mu_{ij} & = \\beta_0 + \\beta_1 \\text{time}_{ij} + u_{0i} + u_{1i} \\text{time}_{ij} \\\\\n\\log(\\sigma_{i\\color{#A65141}j}) & = \\eta_0 + \\color{#A65141}{\\eta_1 \\text{time}_{ij}} + u_{2i} + \\color{#A65141}{u_{3i} \\text{time}_{ij}} \\\\\n\\begin{bmatrix} u_{0i} \\\\ u_{1i} \\\\ u_{2i} \\\\ \\color{#A65141}{u_{3i}} \\end{bmatrix} & \\sim \\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathbf S \\mathbf R \\mathbf S \\end{pmatrix} \\\\\n\\mathbf S & = \\begin{bmatrix} \\sigma_0 & 0 & 0 & 0 \\\\ 0 & \\sigma_1 & 0 & 0 \\\\ 0 & 0 & \\sigma_2 & 0 \\\\ 0 & 0 & 0 & \\sigma_3 \\end{bmatrix} \\\\\n\\mathbf R & = \\begin{bmatrix} 1 & \\rho_{12} & \\rho_{13} & \\rho_{14} \\\\ \\rho_{21} & 1 & \\rho_{23} & \\rho_{24} \\\\ \\rho_{31} & \\rho_{32} & 1 & \\rho_{34} \\\\ \\rho_{41} & \\rho_{42} & \\rho_{43} & 1 \\end{bmatrix} \\\\\n\\beta_0   & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_1, \\eta_0, \\text{and } \\eta_1 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma_0,\\dots, \\sigma_3 & \\sim \\operatorname{Exponential}(1) \\\\\n\\mathbf R & \\sim \\operatorname{LKJ}(2).\n\\end{align*}\\]\nNote how in the very first line we are now speaking in terms of \\(\\sigma_{ij}\\). Variation in the criterion \\(\\text{NA}_{ij}\\) not accounted for by the mean structure can now vary across participants, \\(i\\), and time, \\(j\\). This results in four \\(u_{xi}\\) terms, a \\(4 \\times 4\\) \\(\\mathbf S\\) matrix and a \\(4 \\times 4\\) \\(\\mathbf R\\) matrix. Here’s how you might fit the model with brms::brm(). Warning: it’ll probably take a couple hours.\n\nb14.14 &lt;- brm(\n  data = dat,\n  family = gaussian,\n  bf(N_A.std ~ 1 + day01 + (1 + day01 |i| record_id),\n     sigma ~ 1 + day01 + (1 + day01 |i| record_id)),\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 1), class = b),\n            prior(exponential(1), class = sd),\n            \n            prior(normal(0, 1), class = Intercept, dpar = sigma),\n            prior(normal(0, 1), class = b, dpar = sigma),\n            prior(exponential(1), class = sd, dpar = sigma),\n            \n            prior(lkj(2), class = cor)),\n  iter = 3000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  file = \"fits/b14.14\")\n\nAt this point, print() is starting to return a lot of output.\n\nprint(b14.14)\n\n Family: gaussian \n  Links: mu = identity; sigma = log \nFormula: N_A.std ~ 1 + day01 + (1 + day01 | i | record_id) \n         sigma ~ 1 + day01 + (1 + day01 | i | record_id)\n   Data: dat (Number of observations: 13033) \n  Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;\n         total post-warmup draws = 8000\n\nMultilevel Hyperparameters:\n~record_id (Number of levels: 193) \n                                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                        0.76      0.04     0.68     0.84 1.00     1333     2618\nsd(day01)                            0.60      0.04     0.52     0.69 1.00     2719     4834\nsd(sigma_Intercept)                  0.70      0.04     0.63     0.78 1.00     2062     4164\nsd(sigma_day01)                      0.36      0.04     0.29     0.44 1.00     5250     5865\ncor(Intercept,day01)                -0.30      0.08    -0.46    -0.14 1.00     1506     3527\ncor(Intercept,sigma_Intercept)       0.64      0.05     0.54     0.73 1.00     2000     3336\ncor(day01,sigma_Intercept)          -0.20      0.08    -0.35    -0.04 1.00     1778     3586\ncor(Intercept,sigma_day01)          -0.16      0.11    -0.36     0.05 1.00     5267     6138\ncor(day01,sigma_day01)               0.61      0.09     0.42     0.77 1.00     4446     5372\ncor(sigma_Intercept,sigma_day01)    -0.15      0.10    -0.34     0.05 1.00     4772     5813\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           0.03      0.06    -0.09     0.13 1.00      573     1085\nsigma_Intercept    -0.75      0.05    -0.85    -0.65 1.00     1154     2338\nday01              -0.15      0.05    -0.26    -0.05 1.00     1404     2793\nsigma_day01        -0.11      0.04    -0.18    -0.03 1.00     4379     5267\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur new line for ‘sigma_day01’ suggests there is a general trend for less variation in negative affect ratings over time. However, the ‘sd(sigma_day01)’ line in the ‘Group-Level Effects’ section indicates even this varies a bit across participants. At this point, a lot of the action is now in the estimates for the \\(\\mathbf R\\) matrix. Here that is in a coefficient plot.\n\nposterior_summary(b14.14) |&gt; \n  data.frame() |&gt; \n  rownames_to_column(\"par\") |&gt; \n  filter(str_detect(par, \"cor_\")) |&gt; \n  mutate(rho = str_c(\"(rho[\", c(21, 31, 32, 41, 42, 43), \"])\")) |&gt; \n  mutate(par = str_c(\"'\", par, \"'~\", rho)) |&gt; \n  \n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = par)) +\n  geom_vline(xintercept = c(-0.5, 0, 0.5), alpha = 1/4, color = \"#FCF9F0\", \n             linetype = c(2, 1, 2), linewidth = c(1/4, 1/2, 1/4)) +\n  geom_pointrange(color = \"#B1934A\") +\n  scale_y_discrete(labels = ggplot2:::parse_safe) +\n  xlim(-1, 1) +\n  labs(x = \"marginal posterior\",\n       y = NULL) +\n  theme(axis.text.y = element_text(hjust = 0, size = 9),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nNote how we attached the statistical terms from the lower triangle of the \\(\\mathbf R\\) matrix to the names from the brms output. Coefficient plots like this are somewhat helpful with MELSM parameter summaries, like this. But they leave something to be desired and they won’t scale well. One alternative is to present the posterior means in a correlation matrix plot. Our first step to prepare for the plot is to extract and wrangle the posterior summaries.\n\nlevels &lt;- c(\"beta[0]\", \"beta[1]\", \"eta[0]\", \"eta[1]\")\n\nrho &lt;- posterior_summary(b14.14) |&gt; \n  data.frame() |&gt; \n  rownames_to_column(\"param\") |&gt; \n  filter(str_detect(param, \"cor_\")) |&gt; \n  mutate(param = str_remove(param, \"cor_record_id__\")) |&gt; \n  separate(param, into = c(\"left\", \"right\"), sep = \"__\") |&gt; \n  mutate(\n    left = case_when(\n      left == \"Intercept\"       ~ \"beta[0]\",\n      left == \"day01\"           ~ \"beta[1]\",\n      left == \"sigma_Intercept\" ~ \"eta[0]\"),\n    right = case_when(\n      right == \"day01\"           ~ \"beta[1]\",\n      right == \"sigma_Intercept\" ~ \"eta[0]\",\n      right == \"sigma_day01\"     ~ \"eta[1]\"\n    )\n  ) |&gt; \n  mutate(label = formatC(Estimate, digits = 2, format = \"f\") |&gt; str_replace(\"0.\", \".\")) |&gt;\n  mutate(left  = factor(left, levels = levels),\n         right = factor(right, levels = levels)) |&gt;\n  mutate(right = fct_rev(right))\n\nrho\n\n     left   right   Estimate  Est.Error       Q2.5       Q97.5 label\n1 beta[0] beta[1] -0.3046514 0.08256235 -0.4604108 -0.13857319  -.30\n2 beta[0]  eta[0]  0.6415259 0.04699774  0.5447721  0.72880139   .64\n3 beta[1]  eta[0] -0.1990983 0.07869625 -0.3502391 -0.04456139  -.20\n4 beta[0]  eta[1] -0.1613560 0.10549884 -0.3639229  0.04753939  -.16\n5 beta[1]  eta[1]  0.6093562 0.08862745  0.4223299  0.76793831   .61\n6  eta[0]  eta[1] -0.1501403 0.09681035 -0.3351708  0.04597338  -.15\n\n\nNote how instead of naming the correlations in terms of \\(\\rho_{xx}\\), we are now referring to them as the correlations of the deviations among the population parameters, \\(\\beta_0\\) through \\(\\eta_1\\). I’m hoping this will make sense in the plot. Here it is.\n\nrho |&gt; \n  ggplot(aes(x = left, y = right)) +\n  geom_tile(aes(fill = Estimate)) +\n  geom_text(aes(label = label),\n            family = \"Courier\", size = 3) +\n  scale_x_discrete(NULL, drop = F, labels = ggplot2:::parse_safe, position = \"top\") +\n  scale_y_discrete(NULL, drop = F, labels = ggplot2:::parse_safe) +\n  scale_fill_gradient2(expression(rho),\n                       low = \"#59708b\", mid = \"#FCF9F0\", high = \"#A65141\", midpoint = 0, \n                       labels = c(-1, \"\", 0, \"\", 1), limits = c(-1, 1)) +\n  ggtitle(expression(\"The lower triangle for \"*bold(R)),\n          subtitle = \"Note, each cell is summarized by\\nits posterior mean.\") +\n  theme(axis.text = element_text(size = 12),\n        axis.ticks = element_blank(),\n        legend.text = element_text(hjust = 1))\n\n\n\n\n\n\n\n\nInterestingly, the strongest two associations involve variation around our \\(\\eta\\) parameters. The posterior mean for \\(\\rho_{31}\\) indicates participants with higher baseline levels of \\(\\text{NA}_{ij}\\) tend to vary more in their responses, particularly in the beginning. The posterior mean for \\(\\rho_{42}\\) indicates participants who show greater increases in their \\(\\text{NA}_{ij}\\) responses over time also tend to show greater relative increases in variation in those responses. You can get a little bit of a sense for this by returning once again to our participants 30 and 115.\n\nnd &lt;- dat |&gt; \n  filter(record_id %in% c(30, 115)) |&gt; \n  select(record_id, N_A.std, day01)\n\nbind_cols(\n  fitted(b14.14, newdata = nd) |&gt; \n    data.frame(),\n  predict(b14.14, newdata = nd) |&gt; \n    data.frame() |&gt; \n    select(Q2.5:Q97.5) |&gt; \n    set_names(\"p_lower\", \"p_upper\")) |&gt; \n  bind_cols(nd) |&gt; \n  \n  ggplot(aes(x = day01)) +\n  geom_smooth(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              alpha = 1/2, color = \"#8B9DAF\", fill = \"#8B9DAF\", linewidth = 1/2) +\n  geom_ribbon(aes(ymin = p_lower, ymax = p_upper),\n              alpha = 1/2, fill = \"#8B9DAF\") +\n  geom_point(aes(y = N_A.std),\n             color = \"#8B9DAF\") +\n  scale_x_continuous(breaks = c(0, 0.5, 1)) +\n  ylab(\"negative affect (standardized)\") +\n  facet_wrap(~ record_id)\n\n\n\n\n\n\n\n\nWith respect to \\(\\rho_{31}\\), participant 115 showed both a higher intercept and higher level of variability toward the beginning of the study than participant 30 did. The meaning of \\(\\rho_{42}\\) is less clear, with these two. But at least you can get a sense of why you might want to include a \\(\\eta_1\\) parameter to allow response variability to change over time, and why you might want to allow that parameter to vary across participants. Whereas response variability increased quite a bit for participant 115 over time, it stayed about the same for participant 30, perhaps even decreasing a bit.\n\n\n14.6.4 Time to go multivariate\nFor our final stage in this progression, we will fit what Donald R. Williams et al. (2020) called a M-MELSM, a multivariate mixed-effects location scale model. Recall these data have measures of both negative and positive affect. The standardized values for PA are waiting for us in the P_A.std column. Within the brms framework, this is a combination of sensibilities from Bürkner’s vignettes on distributional models (Bürkner, 2022b) and multivariate models (Bürkner, 2022c). We might express the statistical model as\n\\[\\begin{align*}\n\\color{#A65141}{\\begin{bmatrix} \\text{NA}_{ij} \\\\ \\text{PA}_{ij} \\end{bmatrix}} & \\color{#A65141}\\sim \\color{#A65141}{\\operatorname{MVNormal}\\begin{pmatrix} \\begin{bmatrix}\n\\mu_{ij}^\\text{NA} \\\\ \\mu_{ij}^\\text{PA} \\end{bmatrix}, \\mathbf \\Sigma \\end{pmatrix}} \\\\\n\n\\mu_{ij}^{\\color{#A65141}{\\text{NA}}} & = \\beta_0^\\text{NA} + \\beta_1^\\text{NA} \\text{time}_{ij} + u_{0i}^\\text{NA} + u_{1i}^\\text{NA} \\\\\n\\mu_{ij}^{\\color{#A65141}{\\text{PA}}} & = \\beta_0^\\text{PA} + \\beta_1^\\text{PA} \\text{time}_{ij} + u_{0i}^\\text{PA} + u_{1i}^\\text{PA} \\\\\n\\log \\left ( \\sigma_{ij}^{\\color{#A65141}{\\text{NA}}} \\right ) & = \\eta_0^\\text{NA} + \\eta_1^\\text{NA} \\text{time}_{ij} + u_{2i}^\\text{NA} + u_{3i}^\\text{NA} \\\\\n\\log \\left ( \\sigma_{ij}^{\\color{#A65141}{\\text{PA}}} \\right ) & = \\eta_0^\\text{PA} + \\eta_1^\\text{PA} \\text{time}_{ij} + u_{2i}^\\text{PA} + u_{3i}^\\text{PA}  \\\\\n\n\\begin{bmatrix} u_{0i}^\\text{NA}, u_{1i}^\\text{NA}, u_{2i}^\\text{NA}, u_{3i}^\\text{NA}, u_{0i}^\\text{PA}, u_{1i}^\\text{PA}, u_{2i}^\\text{PA}, u_{3i}^\\text{PA} \\end{bmatrix}' & \\sim \\operatorname{MVNormal}(\\mathbf 0, \\mathbf S \\mathbf R \\mathbf S) \\\\\n\n\\mathbf S & = \\begin{bmatrix} \\sigma_0^\\text{NA} & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & \\sigma_1^\\text{NA} & 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & \\sigma_2^\\text{NA} & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & \\sigma_3^\\text{NA} & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & \\sigma_0^\\text{PA} & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & \\sigma_1^\\text{PA} & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_2^\\text{PA} & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_3^\\text{PA}  \\end{bmatrix} \\\\\n\n\\mathbf R & = \\begin{bmatrix}\n1 & \\rho_{12} & \\rho_{13} & \\rho_{14} & \\rho_{15} & \\rho_{16} & \\rho_{17} & \\rho_{18} \\\\\n\\rho_{21} & 1 & \\rho_{23} & \\rho_{24} & \\rho_{25} & \\rho_{26} & \\rho_{27} & \\rho_{28} \\\\\n\\rho_{31} & \\rho_{32} & 1 & \\rho_{34} & \\rho_{35} & \\rho_{36} & \\rho_{37} & \\rho_{38} \\\\\n\\rho_{41} & \\rho_{42} & \\rho_{43} & 1 & \\rho_{45} & \\rho_{46} & \\rho_{47} & \\rho_{48} \\\\\n\\rho_{51} & \\rho_{52} & \\rho_{53} & \\rho_{54} & 1 & \\rho_{56} & \\rho_{57} & \\rho_{58} \\\\\n\\rho_{61} & \\rho_{62} & \\rho_{63} & \\rho_{64} & \\rho_{65} & 1 & \\rho_{67} & \\rho_{68} \\\\\n\\rho_{71} & \\rho_{72} & \\rho_{73} & \\rho_{74} & \\rho_{75} & \\rho_{76} & 1 & \\rho_{78} \\\\\n\\rho_{81} & \\rho_{82} & \\rho_{83} & \\rho_{84} & \\rho_{85} & \\rho_{86} & \\rho_{87} & 1\n\\end{bmatrix} \\\\\n\n\\beta_0^\\text{NA} \\text{ and } \\beta_0^\\text{PA} & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_1^\\text{NA} \\text{ and } \\beta_1^\\text{PA} & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\eta_0^\\text{NA},\\dots, \\eta_1^\\text{PA} & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma_0^\\text{NA},\\dots, \\sigma_1^\\text{PA} & \\sim \\operatorname{Exponential}(1) \\\\\n\\mathbf R & \\sim \\operatorname{LKJ}(2),\n\\end{align*}\\]\nwhere the \\(\\text{NA}\\) and \\(\\text{PA}\\) superscripts indicate which variable is connected with which parameter. This is a straight multivariate generalization from the previous model, b14.14. Now we have eight parameters varying across participants, resulting in an \\(8 \\times 8\\) \\(\\mathbf S\\) matrix and an \\(8 \\times 8\\) \\(\\mathbf R\\) matrix. Here’s the brm() code.\n\nb14.15 &lt;- brm(\n  data = dat,\n  family = gaussian,\n  bf(mvbind(N_A.std, P_A.std) ~ 1 + day01 + (1 + day01 |i| record_id),\n     sigma ~ 1 + day01 + (1 + day01 |i| record_id)) + set_rescor(rescor = FALSE),\n  prior = c(prior(normal(0, 0.2), class = Intercept, resp = NAstd),\n            prior(normal(0, 1), class = b, resp = NAstd),\n            prior(exponential(1), class = sd, resp = NAstd),\n            \n            prior(normal(0, 1), class = Intercept, dpar = sigma, resp = NAstd),\n            prior(normal(0, 1), class = b, dpar = sigma, resp = NAstd),\n            prior(exponential(1), class = sd, dpar = sigma, resp = NAstd),\n            \n            prior(normal(0, 0.2), class = Intercept, resp = PAstd),\n            prior(normal(0, 1), class = b, resp = PAstd),\n            prior(exponential(1), class = sd, resp = PAstd),\n            \n            prior(normal(0, 1), class = Intercept, dpar = sigma, resp = PAstd),\n            prior(normal(0, 1), class = b, dpar = sigma, resp = PAstd),\n            prior(exponential(1), class = sd, dpar = sigma, resp = PAstd),\n            \n            prior(lkj(2), class = cor)),\n  iter = 3000, warmup = 1000, chains = 4, cores = 4,\n  seed = 14,\n  file = \"fits/b14.15\")\n\nNote how we used the resp argument to indicate which priors went with which criterion variables. For the sake of space, I’ll skip showing the print() output. By all means, check that summary out if you fit this model on your own. Though there may be some substantive insights to glean from looking at the population-level parameters and the hierarchical \\(\\sigma\\)’s, I’d argue the main action is in the \\(\\mathbf R\\) matrix. This time we’ll jump straight to showcasing their posterior means in a correlation matrix plot.\nFirst we wrangle.\n\nlevels &lt;- c(\"beta[0]^'NA'\", \"beta[1]^'NA'\", \"eta[0]^'NA'\", \"eta[1]^'NA'\",\n            \"beta[0]^'PA'\", \"beta[1]^'PA'\", \"eta[0]^'PA'\", \"eta[1]^'PA'\")\n\n# Two different options for ordering the parameters\n# levels &lt;- c(\"beta[0]^'NA'\", \"beta[1]^'NA'\", \"beta[0]^'PA'\", \"beta[1]^'PA'\", \"eta[0]^'NA'\", \"eta[1]^'NA'\", \"eta[0]^'PA'\", \"eta[1]^'PA'\")\n# levels &lt;- c(\"beta[0]^'NA'\", \"beta[0]^'PA'\", \"beta[1]^'NA'\", \"beta[1]^'PA'\",\"eta[0]^'NA'\", \"eta[0]^'PA'\", \"eta[1]^'NA'\", \"eta[1]^'PA'\")\n\nrho &lt;- posterior_summary(b14.15) |&gt; \n  data.frame() |&gt; \n  rownames_to_column(\"param\") |&gt; \n  filter(str_detect(param, \"cor_\")) |&gt; \n  mutate(param = str_remove(param, \"cor_record_id__\")) |&gt; \n  separate(param, into = c(\"left\", \"right\"), sep = \"__\") |&gt; \n  mutate(\n    left = case_when(\n      left == \"NAstd_Intercept\"       ~ \"beta[0]^'NA'\",\n      left == \"NAstd_day01\"           ~ \"beta[1]^'NA'\",\n      left == \"sigma_NAstd_Intercept\" ~ \"eta[0]^'NA'\",\n      left == \"sigma_NAstd_day01\"     ~ \"eta[1]^'NA'\",\n      left == \"PAstd_Intercept\"       ~ \"beta[0]^'PA'\",\n      left == \"PAstd_day01\"           ~ \"beta[1]^'PA'\",\n      left == \"sigma_PAstd_Intercept\" ~ \"eta[0]^'PA'\",\n      left == \"sigma_PAstd_day01\"     ~ \"eta[1]^'PA'\"\n      ),\n    right = case_when(\n      right == \"NAstd_Intercept\"       ~ \"beta[0]^'NA'\",\n      right == \"NAstd_day01\"           ~ \"beta[1]^'NA'\",\n      right == \"sigma_NAstd_Intercept\" ~ \"eta[0]^'NA'\",\n      right == \"sigma_NAstd_day01\"     ~ \"eta[1]^'NA'\",\n      right == \"PAstd_Intercept\"       ~ \"beta[0]^'PA'\",\n      right == \"PAstd_day01\"           ~ \"beta[1]^'PA'\",\n      right == \"sigma_PAstd_Intercept\" ~ \"eta[0]^'PA'\",\n      right == \"sigma_PAstd_day01\"     ~ \"eta[1]^'PA'\"\n    )\n  ) |&gt; \n  mutate(label = formatC(Estimate, digits = 2, format = \"f\") |&gt; str_replace(\"0.\", \".\")) |&gt; \n  mutate(left  = factor(left, levels = levels),\n         right = factor(right, levels = levels)) |&gt; \n  mutate(right = fct_rev(right))\n\nrho |&gt; head()\n\n          left        right   Estimate  Est.Error       Q2.5       Q97.5 label\n1 beta[0]^'NA' beta[1]^'NA' -0.2966930 0.08151278 -0.4492407 -0.13220155  -.30\n2 beta[0]^'NA'  eta[0]^'NA'  0.6329190 0.04584605  0.5367935  0.71611940   .63\n3 beta[1]^'NA'  eta[0]^'NA' -0.1869214 0.07811868 -0.3345458 -0.02844261  -.19\n4 beta[0]^'NA'  eta[1]^'NA' -0.1494339 0.10282028 -0.3467451  0.05961868  -.15\n5 beta[1]^'NA'  eta[1]^'NA'  0.5752405 0.09059977  0.3848376  0.73694797   .58\n6  eta[0]^'NA'  eta[1]^'NA' -0.1433994 0.09469969 -0.3213243  0.04859973  -.14\n\n\nNow we plot!\n\nrho |&gt; \n  full_join(rename(rho, right = left, left = right),\n            by = c(\"left\", \"right\", \"Estimate\", \"Est.Error\", \"Q2.5\", \"Q97.5\", \"label\")) |&gt;\n  \n  ggplot(aes(x = left, y = right)) +\n  geom_tile(aes(fill = Estimate)) +\n  geom_hline(yintercept = 4.5, color = \"#100F14\") +\n  geom_vline(xintercept = 4.5, color = \"#100F14\") +\n  geom_text(aes(label = label),\n            family = \"Courier\", size = 3) +\n  scale_x_discrete(NULL, expand = c(0, 0), labels = ggplot2:::parse_safe, position = \"top\") +\n  scale_y_discrete(NULL, expand = c(0, 0), labels = ggplot2:::parse_safe) +\n  scale_fill_gradient2(expression(rho),\n                       low = \"#59708b\", mid = \"#FCF9F0\", high = \"#A65141\", midpoint = 0,\n                       labels = c(-1, \"\", 0, \"\", 1), limits = c(-1, 1)) +\n  theme(axis.text = element_text(size = 12),\n        axis.ticks = element_blank(),\n        legend.text = element_text(hjust = 1))\n\n\n\n\n\n\n\n\nThe full_join() business just before the ggplot2 code is how we got the full \\(8 \\times 8\\) matrix. If you’re curious, see what happens if you run the code without that part.\nTo help orient you to the plot, I’ve divided it into four quadrants. The upper left and lower right quadrants are the correlations among the varying parameters for the N_A.std and P_A.std ratings, respectively. The other two quadrants are the correlations for those parameters between N_A.std and P_A.std. As a reminder, this matrix, as with any other correlation matrix, is symmetrical across the diagonal.\nTo my eye, a few things pop out. First, the correlations within N_A.std are generally higher than those within P_A.std. Second, the correlations among the parameters between N_A.std and P_A.std are generally higher than those within them. Finally, all three of the largest correlations have to do with variation in the \\(\\eta\\) parameters. Two of them are basically the same as those we focused on for b14.14. The new one, \\(\\rho_{73}\\), indicates that participants’ baseline ratings for N_A.std tended to vary in a similar way as their baseline ratings for P_A.std.\n\n14.6.4.1 Plot with uncertainty\nAs fond as I am with that last correlation plot, it has a glaring defect: there is no expression of uncertainty. Sometimes we express uncertainty with percentile-based intervals. Other times we do so with marginal densities. But the correlation plots only describe the marginal posteriors for all the \\(\\rho\\) parameters with their means–no uncertainty. If you have one or a small few correlations to plot, coefficient or density plots might do. However, they don’t scale well. If you don’t believe me, just try. I’m not sure there are any good solutions to this, but it can be helpful to at least try grappling with the issue.\nFortunately for us, Matthew Kay (creator of the tidybayes package) has already tried his hand at a few approaches. For all the deets, check out the multivariate-regression.md file in his uncertainty-examples GitHub repo. One of his more imaginative approaches is to use what he calls dithering. Imagine breaking each of the cells in our correlation plot, above, into a \\(50 \\times 50 = 2{,}500\\)-cell grid. Now assign each of the cells within that grid one of the values from the HMC draws of that correlation’s posterior distribution. Then color code each of those cells by that value in the same basic way we color coded our previous correlation plots. Simultaneously do that for all of the correlations within the \\(\\mathbf R\\) matrix and plot them in a faceted plot. That’s the essence of the dithering approach. This is all probably hard to make sense of in words. Hopefully it will all come together with a little code and the resulting plot. Hold on to your hat.\n\nlevels &lt;- c(\"beta[0]^'NA'\", \"beta[1]^'NA'\", \"eta[0]^'NA'\", \"eta[1]^'NA'\",\n            \"beta[0]^'PA'\", \"beta[1]^'PA'\", \"eta[0]^'PA'\", \"eta[1]^'PA'\")\n\nrho &lt;- as_draws_df(b14.15) |&gt; \n  select(starts_with(\"cor_\")) |&gt; \n  slice_sample(n = 50 * 50) |&gt; \n  bind_cols(crossing(x = 1:50, y = 1:50)) |&gt; \n  pivot_longer(cols = -c(x:y)) |&gt; \n  mutate(name = str_remove(name, \"cor_record_id__\")) |&gt; \n  separate(name, into = c(\"col\", \"row\"), sep = \"__\") |&gt; \n  mutate(\n    col = case_when(\n      col == \"NAstd_Intercept\"       ~ \"beta[0]^'NA'\",\n      col == \"NAstd_day01\"           ~ \"beta[1]^'NA'\",\n      col == \"sigma_NAstd_Intercept\" ~ \"eta[0]^'NA'\",\n      col == \"sigma_NAstd_day01\"     ~ \"eta[1]^'NA'\",\n      col == \"PAstd_Intercept\"       ~ \"beta[0]^'PA'\",\n      col == \"PAstd_day01\"           ~ \"beta[1]^'PA'\",\n      col == \"sigma_PAstd_Intercept\" ~ \"eta[0]^'PA'\",\n      col == \"sigma_PAstd_day01\"     ~ \"eta[1]^'PA'\"\n    ),\n    row = case_when(\n      row == \"NAstd_Intercept\"       ~ \"beta[0]^'NA'\",\n      row == \"NAstd_day01\"           ~ \"beta[1]^'NA'\",\n      row == \"sigma_NAstd_Intercept\" ~ \"eta[0]^'NA'\",\n      row == \"sigma_NAstd_day01\"     ~ \"eta[1]^'NA'\",\n      row == \"PAstd_Intercept\"       ~ \"beta[0]^'PA'\",\n      row == \"PAstd_day01\"           ~ \"beta[1]^'PA'\",\n      row == \"sigma_PAstd_Intercept\" ~ \"eta[0]^'PA'\",\n      row == \"sigma_PAstd_day01\"     ~ \"eta[1]^'PA'\"\n    )\n  ) |&gt; \n  mutate(col = factor(col, levels = levels),\n         row = factor(row, levels = levels))\n\nrho |&gt; \n  full_join(rename(rho, col = row, row = col),\n            by = c(\"x\", \"y\", \"col\", \"row\", \"value\")) |&gt;\n  \n  ggplot(aes(x = x, y = y, fill = value)) +\n  geom_raster() +\n  scale_x_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = c(0, 0)) +\n  scale_fill_gradient2(expression(rho),\n                       low = \"#59708b\", mid = \"#FCF9F0\", high = \"#A65141\", midpoint = 0,\n                       labels = c(-1, \"\", 0, \"\", 1), limits = c(-1, 1)) +\n  facet_grid(row ~ col, labeller = label_parsed, switch = \"y\") +\n  theme(strip.text = element_text(size = 12))\n\n\n\n\n\n\n\n\nFrom Kay’s GitHub repo, we read: “This is akin to something like an icon array. You should still be able to see the average color (thanks to the human visual system’s ensembling processing), but also get a sense of the uncertainty by how ‘dithered’ a square looks.” Hopefully this will give you a little inspiration to find new and better ways to express the posterior uncertainty in your Bayesian correlation plots. If you come up with any great solutions, let the rest of us know!\n\n\n\n14.6.5 Growth model/MELSM wrap-up\nThis bonus section introduced a lot of material. To learn more about the conventional multilevel growth model and its extensions, check out\n\nSinger and Willett’s (2003) text, Applied longitudinal data analysis: Modeling change and event occurrence;\nMy brms/tidyverse translation of that text, Applied Longitudinal Data Analysis in brms and the tidyverse ; or\nHoffman’s (2015) text, Longitudinal analysis: Modeling within-person fluctuation and change.\n\nTo learn more about the MELSM approach and its extensions, check out\n\nHedeker et al. (2008), An application of a mixed-effects location scale model for analysis of ecological momentary assessment (EMA) data;\nHedeker et al. (2012), Modeling between- and within-subject variance in ecological momentary assessment (EMA) data using mixed-effects location scale models;\nWilliams’ and colleagues’ (2020) paper, Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity;\nWilliams’ and colleagues’ (2021) paper, Beneath the surface: Unearthing within-person variability and mean relations with Bayesian mixed models;\nWilliams’ and colleagues’ (2019) paper, A Bayesian nonlinear mixed-effects location scale model for learning;\nWilliams’ tutorial blog post, A defining feature of cognitive interference tasks: Heterogeneous within-person variance.\n\nFrom a brms standpoint, it might also be helpful to brush up on\n\nBürkner’s (2022b) vignette, Estimating distributional models with brms and\nBürkner’s (2022c) vignette, Estimating multivariate models with brms.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Adventures in Covariance</span>"
    ]
  },
  {
    "objectID": "14.html#session-info",
    "href": "14.html#session-info",
    "title": "14  Adventures in Covariance",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ape_5.8-1             ggtree_4.1.1.002      ggrepel_0.9.6         rethinking_2.42       dagitty_0.3-4        \n [6] ggdag_0.2.13          bayesplot_1.15.0.9000 patchwork_1.3.2       brms_2.23.0           Rcpp_1.1.0           \n[11] tidybayes_3.0.7       posterior_1.6.1.9000  cmdstanr_0.9.0        dutchmasters_0.1.0    lubridate_1.9.4      \n[16] forcats_1.0.1         stringr_1.6.0         dplyr_1.1.4           purrr_1.2.1           readr_2.1.5          \n[21] tidyr_1.3.2           tibble_3.3.1          ggplot2_4.0.1         tidyverse_2.0.0      \n\nloaded via a namespace (and not attached):\n  [1] RColorBrewer_1.1-3      tensorA_0.36.2.1        rstudioapi_0.17.1       jsonlite_2.0.0          shape_1.4.6.1          \n  [6] magrittr_2.0.4          TH.data_1.1-4           estimability_1.5.1      farver_2.1.2            rmarkdown_2.30         \n [11] fs_1.6.6                vctrs_0.7.0             memoise_2.0.1           htmltools_0.5.9         distributional_0.5.0   \n [16] curl_7.0.0              gridGraphics_0.5-1      StanHeaders_2.36.0.9000 htmlwidgets_1.6.4       plyr_1.8.9             \n [21] sandwich_3.1-1          emmeans_1.11.2-8        zoo_1.8-14              cachem_1.1.0            igraph_2.2.0           \n [26] lifecycle_1.0.5         pkgconfig_2.0.3         Matrix_1.7-3            R6_2.6.1                fastmap_1.2.0          \n [31] digest_0.6.39           aplot_0.2.9             ps_1.9.1                invgamma_1.2            labeling_0.4.3         \n [36] timechange_0.3.0        polyclip_1.10-7         abind_1.4-8             compiler_4.5.1          fontquiver_0.2.1       \n [41] withr_3.0.2             S7_0.2.1                backports_1.5.0         inline_0.3.21           viridis_0.6.5          \n [46] QuickJSR_1.8.1          hexbin_1.28.5           pkgbuild_1.4.8          ggforce_0.5.0           MASS_7.3-65            \n [51] rappdirs_0.3.3          loo_2.9.0.9000          tools_4.5.1             glue_1.8.0              nlme_3.1-168           \n [56] grid_4.5.1              checkmate_2.3.3         reshape2_1.4.5          generics_0.1.4          gtable_0.3.6           \n [61] tzdb_0.5.0              hms_1.1.4               tidygraph_1.3.1         utf8_1.2.6              pillar_1.11.1          \n [66] ggdist_3.3.3            yulab.utils_0.2.3       splines_4.5.1           tweenr_2.0.3            treeio_1.34.0          \n [71] lattice_0.22-7          survival_3.8-3          tidyselect_1.2.1        fontLiberation_0.1.0    knitr_1.51             \n [76] fontBitstreamVera_0.1.1 arrayhelpers_1.1-0      gridExtra_2.3           V8_8.0.1                stats4_4.5.1           \n [81] xfun_0.55               graphlayouts_1.2.2      bridgesampling_1.2-1    matrixStats_1.5.0       rstan_2.36.0.9000      \n [86] stringi_1.8.7           lazyeval_0.2.2          ggfun_0.2.0             yaml_2.3.12             boot_1.3-31            \n [91] evaluate_1.0.5          codetools_0.2-20        ggraph_2.2.2            gdtools_0.4.4           emo_0.0.0.9000         \n [96] ggplotify_0.1.3         cli_3.6.5               RcppParallel_5.1.11-1   systemfonts_1.3.1       xtable_1.8-4           \n[101] processx_3.8.6          coda_0.19-4.1           svUnit_1.0.8            rstantools_2.5.0.9000   assertthat_0.2.1       \n[106] Brobdingnag_1.2-9       viridisLite_0.4.2       mvtnorm_1.3-3           tidytree_0.4.7          ggiraph_0.9.2          \n[111] scales_1.4.0            nleqslv_3.3.5           crayon_1.5.3            rlang_1.1.7             multcomp_1.4-29",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Adventures in Covariance</span>"
    ]
  },
  {
    "objectID": "14.html#comments",
    "href": "14.html#comments",
    "title": "14  Adventures in Covariance",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nAngrist, J. D., & Keueger, A. B. (1991). Does compulsory school attendance affect schooling and earnings? The Quarterly Journal of Economics, 106(4), 979–1014. https://doi.org/10.2307/2937954\n\n\nBetancourt, M. (2017). Robust Gaussian processes in Stan. https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html\n\n\nBürkner, P.-C. (2022a). brms reference manual, Version 2.18.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2022b). Estimating distributional models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html\n\n\nBürkner, P.-C. (2022c). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html\n\n\nBürkner, P.-C. (2022d). Estimating phylogenetic multilevel models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_phylogenetics.html\n\n\nGelman, A., & Imbens, G. (2019). Why high-order polynomials should not be used in regression discontinuity designs. Journal of Business & Economic Statistics, 37(3), 447–456. https://doi.org/10.1080/07350015.2017.1366909\n\n\nHedeker, D., Mermelstein, R. J., & Demirtas, H. (2008). An application of a mixed-effects location scale model for analysis of ecological momentary assessment (EMA) data. Biometrics, 64(2), 627–634. https://doi.org/10.1111/j.1541-0420.2007.00924.x\n\n\nHedeker, D., Mermelstein, R. J., & Demirtas, H. (2012). Modeling between- and within-Subject variance in ecological momentary assessment (EMA) data using mixed-effects location scale models. Statistics in Medicine, 31(27). https://doi.org/10.1002/sim.5338\n\n\nHoffman, L. (2015). Longitudinal analysis: Modeling within-Person fluctuation and change (1 edition). Routledge. https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025\n\n\nKahle, D., & Stamey, J. (2017). invgamma: The inverse gamma distribution [Manual]. https://CRAN.R-project.org/package=invgamma\n\n\nKay, M. (2020). Marginal distribution of a single correlation from an LKJ distribution. https://mjskay.github.io/ggdist/reference/lkjcorr_marginal.html\n\n\nKoster, J. M., & Leckie, G. (2014). Food sharing networks in lowland Nicaragua: An application of the social relations model to count data. Social Networks, 38, 100–110. https://doi.org/10.1016/j.socnet.2014.02.002\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKurz, A. S. (2026). Applied longitudinal data analysis in brms and the tidyverse (version 0.0.4). https://solomon.quarto.pub/alda/\n\n\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nParadis, Emmanuel, Blomberg, S., Bolker, B., Brown, J., Claramunt, S., Claude, J., Cuong, H. S., Desper, R., Didier, G., Durand, B., Dutheil, J., Ewing, R., Gascuel, O., Guillerme, T., Heibl, C., Ives, A., Jones, B., Krah, F., Lawson, D., … de Vienne, D. (2022). ape: Analyses of phylogenetics and evolution [Manual]. https://CRAN.R-project.org/package=ape\n\n\nParadis, E., & Schliep, K. (2019). ape 5.0: An environment for modern phylogenetics and evolutionary analyses in R. Bioinformatics (Oxford, England), 35, 526–528. https://doi.org/10.1093/bioinformatics/bty633\n\n\nPeng, R. D., Kross, S., & Anderson, B. (2017). Mastering software development in {R}. https://github.com/rdpeng/RProgDA\n\n\nRast, P., Hofer, S. M., & Sparks, C. (2012). Modeling individual differences in within-person variation of negative and positive affect in a mixed effects location scale model using BUGS/JAGS. Multivariate Behavioral Research, 47(2), 177–200. https://doi.org/10.1080/00273171.2012.658328\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal data analysis: Modeling change and event occurrence. Oxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nStreet, S. E., Navarrete, A. F., Reader, S. M., & Laland, K. N. (2017). Coevolution of cultural intelligence, extended life history, sociality, and brain size in primates. Proceedings of the National Academy of Sciences, 114(30), 7908–7914. https://doi.org/10.1073/pnas.1620734114\n\n\nThoen, E. (2022). dutchmasters [Manual]. https://github.com/EdwinTh/dutchmasters\n\n\nVermeer, J. (1665). Girl with a pearl earring.\n\n\nWatson, D., Clark, L. A., & Tellegen, A. (1988). Development and validation of brief measures of positive and negative affect: The PANAS scales. Journal of Personality and Social Psychology, 54(6), 1063–1070. https://doi.org/10.1037/0022-3514.54.6.1063\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2-book.org/\n\n\nWilliams, Donald R., Martin, S. R., Liu, S., & Rast, P. (2020). Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity. European Journal of Psychological Assessment, 36(6), 981–997. https://doi.org/10.1027/1015-5759/a000624\n\n\nWilliams, Donald R., Martin, S. R., Liu, S., & Rast, P. (2021). Bayesian multivariate mixed-effects location scale modeling of longitudinal relations among affective traits, states, and physical activity. European Journal of Psychological Assessment. https://doi.org/10.1027/1015-5759/a000624\n\n\nWilliams, Donald R., Martin, S. R., & Rast, P. (2022). Putting the individual into reliability: Bayesian testing of homogeneous within-Person variance in hierarchical models. Behavior Research Methods, 54(3), 1272–1290. https://doi.org/10.3758/s13428-021-01646-x\n\n\nWilliams, Donald R., Mulder, J., Rouder, J. N., & Rast, P. (2021). Beneath the surface: Unearthing within-Person variability and mean relations with Bayesian mixed models. Psychological Methods, 26(1), 74. https://doi.org/10.1037/met0000270\n\n\nWilliams, Donald R., Zimprich, D. R., & Rast, P. (2019). A Bayesian nonlinear mixed-effects location scale model for learning. Behavior Research Methods, 51(5), 1968–1986. https://doi.org/10.3758/s13428-019-01255-9\n\n\nYu, G. (2020a). Using ggtree to visualize data on tree-like structures. Current Protocols in Bioinformatics, 69(1), e96. https://doi.org/10.1002/cpbi.96\n\n\nYu, G. (2020b). Data integration, manipulation and visualization of phylogenetic trees. https://yulab-smu.github.io/treedata-book/\n\n\nYu, G., Lam, T. T.-Y., Zhu, H., & Guan, Y. (2018). Two methods for mapping and visualizing associated data on phylogeny using ggtree. Molecular Biology and Evolution, 35(12), 3041–3043. https://doi.org/10.1093/molbev/msy194\n\n\nYu, G., Smith, D. K., Zhu, H., Guan, Y., & Lam, T. T.-Y. (2017). ggtree: An R package for visualization and annotation of phylogenetic trees with their covariates and other associated data. Methods in Ecology and Evolution, 8(1), 28–36. https://doi.org/10.1111/2041-210X.12628",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Adventures in Covariance</span>"
    ]
  },
  {
    "objectID": "15.html",
    "href": "15.html",
    "title": "15  Missing Data and Other Opportunities",
    "section": "",
    "text": "15.1 Measurement error\nFor the opening example, we’re playing with the conditional probability\n\\[\n\\Pr(\\text{burnt down} \\mid \\text{burnt up}) = \\frac{\\Pr(\\text{burnt up, burnt down})}{\\Pr(\\text{burnt up})}.\n\\]\nGiven McElreath’s setup, it works out that\n\\[\n\\Pr(\\text{burnt down} \\mid \\text{burnt up}) = \\frac{1/3}{1/2} = \\frac{2}{3}.\n\\]\nWe might express the math toward the bottom of page 489 in tibble form like this.\nI understood McElreath’s simulation (R code 15.1) better after breaking it apart. The first part of sim_pancake() takes one random draw from the integers 1, 2, and 3. It just so happens that if we set set.seed(1), the code returns a 1.\nSo here’s what it looks like if we use seeds 2:11.\nEach of those value_returned values stands for one of the three pancakes: 1 = BB, 2 = BU, and 3 = UU. In the next line, McElreath made slick use of a matrix to specify that. Here’s what the matrix looks like.\nSee how the three columns are identified as [,1], [,2], and [,3]? If, say, we wanted to subset the values in the second column, we’d execute\nwhich returns a numeric vector.\nThat 1 0 corresponds to the pancake with one burnt (i.e., 1) and one unburnt (i.e., 0) side. So when McElreath then executed sample(sides), he randomly sampled from one of those two values. In the case of pancake == 2, he randomly sampled one the pancake with one burnt and one unburnt side. Had he sampled from pancake == 1, he would have sampled from the pancake with both sides burnt.\nGoing forward, let’s amend McElreath’s sim_pancake() function so it will take a seed argument, which will allow us to make the output reproducible.\nLet’s take this baby for a whirl.\nTake a look at what we’ve done.\nNow we use pivot_wider() and summarise() to get the value we’ve been working for.\nThe results are within rounding error of the ideal 2/3.\nLet’s grab those WaffleDivorce data from back in Section 5.1.\ndata(WaffleDivorce, package = \"rethinking\")\nd &lt;- WaffleDivorce\nrm(WaffleDivorce)\nIn anticipation of R code 15.3 and 15.5, wrangle the data a little.\nd &lt;- d |&gt; \n  mutate(D_obs = (Divorce - mean(Divorce)) / sd(Divorce),\n         D_sd  = Divorce.SE / sd(Divorce),\n         M     = (Marriage - mean(Marriage)) / sd(Marriage),\n         A     = (MedianAgeMarriage - mean(MedianAgeMarriage)) / sd(MedianAgeMarriage),\n         M_obs = M,\n         M_sd  = Marriage.SE / sd(Marriage))\nIn previous editions of this book, we relied on the dark theme functions from the ggdark package (Grantham, 2019) for the plot themes. Though ggdark was formerly available on CRAN, it was removed on 2025-11-07 due to some unresolved issues (see here). There’s currently a promising looking pull request on GitHub that may fix the issue, but until the issue is resolved we’ll make substitutes by hand.\ntheme_set(\n  theme_bw(ink = \"white\", paper = \"black\") +\n    theme(legend.position = \"none\",\n          panel.grid = element_blank())\n)\n\ndark_theme_void &lt;- function(...) {\n  theme_void(ink = \"white\", paper = \"black\", ...)\n}\n\n# To reset the ggplot2 theme to its default parameters, execute:\n# ggplot2::theme_set(theme_gray())\nFor the rest of our color palette, we’ll use colors from the viridis package (Garnier, 2021), which provides a variety of colorblind-safe color palettes (see Rudis et al., 2018).\nlibrary(viridis)\nThe viridis_pal() function gives a list of colors within a given palette. The colors in each palette fall on a spectrum. Within viridis_pal(), the option argument allows one to select a given spectrum, “C”, in our case. The final parentheses, (), allows one to determine how many discrete colors one would like to break the spectrum up by. We’ll choose 7.\nviridis_pal(option = \"C\")(7)\n\n[1] \"#0D0887FF\" \"#5D01A6FF\" \"#9C179EFF\" \"#CC4678FF\" \"#ED7953FF\" \"#FDB32FFF\" \"#F0F921FF\"\nWith a little data wrangling, we can put the colors of our palette in a tibble and display them in a plot.\ntibble(factor       = \"a\",\n       number       = factor(1:7),\n       color_number = str_c(1:7, \". \", viridis_pal(option = \"C\")(7))) |&gt; \n  \n  ggplot(aes(x = factor, y = number)) +\n  geom_tile(aes(fill = number)) +\n  geom_text(aes(label = color_number, color = number %in% c(\"5\", \"6\", \"7\"))) +\n  scale_x_discrete(NULL, breaks = NULL, expand = c(0, 0)) +\n  scale_y_discrete(NULL, breaks = NULL, expand = c(0, 0)) +\n  scale_color_manual(values = c(\"black\", \"white\")) +\n  scale_fill_viridis(option = \"C\", discrete = T, direction = -1) +\n  ggtitle(\"Behold: viridis C!\")\nNow, let’s make use of our custom theme and reproduce/reimagine Figure 15.1.a.\ncolor &lt;- viridis_pal(option = \"C\")(7)[7]\n\np1 &lt;- d |&gt;\n  ggplot(aes(x = MedianAgeMarriage, \n             y = Divorce,\n             ymin = Divorce - Divorce.SE, \n             ymax = Divorce + Divorce.SE)) +\n  geom_pointrange(shape = 20, alpha = 2/3, color = color) +\n  labs(x = \"Median age marriage\" , \n       y = \"Divorce rate\")\nNotice how viridis_pal(option = \"C\")(7)[7] called the seventh color in the color scheme, \"#F0F921FF\". For Figure 15.1.b, we’ll select the sixth color in the palette by coding viridis_pal(option = \"C\")(7)[6]. We’ll then combine the two subplots with patchwork.\ncolor &lt;- viridis_pal(option = \"C\")(7)[6]\n\np2 &lt;- d |&gt;\n  ggplot(aes(x = log(Population), \n             y = Divorce,\n             ymin = Divorce - Divorce.SE, \n             ymax = Divorce + Divorce.SE)) +\n  geom_pointrange(shape = 20, alpha = 2/3, color = color) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"log population\")\n\nlibrary(patchwork)\np1 | p2\nJust like in the text, our plot shows states with larger populations tend to have smaller measurement error. The relation between measurement error and MedianAgeMarriage is less apparent.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Missing Data and Other Opportunities</span>"
    ]
  },
  {
    "objectID": "15.html#measurement-error",
    "href": "15.html#measurement-error",
    "title": "15  Missing Data and Other Opportunities",
    "section": "",
    "text": "15.1.0.1 Rethinking: Generative thinking, Bayesian inference\n\nBayesian models are generative, meaning they can be used to simulate observations just as well as they can be used to estimate parameters. One benefit of this fact is that a statistical model can be developed by thinking hard about how the data might have arisen. This includes sampling and measurement, as well as the nature of the process we are studying. Then let Bayesian updating discover the implications. (p. 491, emphasis in the original)\n\n\n\n15.1.1 Error on the outcome\nNow make a DAG of our data with ggdag.\n\nlibrary(ggdag)\n\ndag_coords &lt;- tibble(\n  name = c(\"A\", \"M\", \"D\", \"Dobs\", \"eD\"),\n  x    = c(1, 2, 2, 3, 4),\n  y    = c(2, 3, 1, 1, 1))\n\ndagify(M    ~ A,\n       D    ~ A + M,\n       Dobs ~ D + eD,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"D\", \"eD\"), \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"A\", \"D\", \"M\", expression(italic(e)[D]), expression(D[obs]))) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) +\n  dark_theme_void()\n\n\n\n\n\n\n\n\nNote our use of the dark_theme_void() function. But more to the substance of the matter,\n\nthere’s a lot going on here. But we can proceed one step at a time. The left triangle of this DAG is the same system that we worked with back in Section 5.1.1. Age at marriage (\\(A\\)) influences divorce (\\(D\\)) both directly and indirectly, passing through marriage rate (\\(M\\)). Then we have the observation model. The true divorce rate \\(D\\) cannot be observed, so it is circled as an unobserved node. However we do get to observe \\(D_\\text{obs}\\), which is a function of both the true rate \\(D\\) and some unobserved error \\(e_\\text{D}\\). (p. 492)\n\nTo get a better sense of what we’re about to do, imagine for a moment that each state’s divorce rate is normally distributed with a mean of Divorce and standard deviation Divorce.SE. Those distributions would be like this.\n\nd |&gt; \n  mutate(Divorce_distribution = str_c(\"Divorce ~ Normal(\", Divorce, \", \", Divorce.SE, \")\")) |&gt; \n  select(Loc, Divorce_distribution) |&gt; \n  head()\n\n  Loc         Divorce_distribution\n1  AL Divorce ~ Normal(12.7, 0.79)\n2  AK Divorce ~ Normal(12.5, 2.05)\n3  AZ Divorce ~ Normal(10.8, 0.74)\n4  AR Divorce ~ Normal(13.5, 1.22)\n5  CA    Divorce ~ Normal(8, 0.24)\n6  CO Divorce ~ Normal(11.6, 0.94)\n\n\n\nHere’s how to define the error distribution for each divorce rate. For each observed value \\(D_{\\text{OBS},i}\\), there will be one parameter, \\(D_{\\text{TRUE},i}\\), defined by:\n\\[D_{\\text{OBS},i} \\sim \\operatorname{Normal}(D_{\\text{TRUE},i}, D_{\\text{SE},i})\\]\nAll this does is define the measurement \\(D_{\\text{OBS},i}\\) as having the specified Gaussian distribution centered on the unknown parameter \\(D_{\\text{TRUE},i}\\). So the above defines a probability for each State \\(i\\)’s observed divorce rate, given a known measurement error. (p. 493)\n\nOur model will follow the form\n\\[\\begin{align*}\n\\color{#5D01A6FF}{\\text{Divorce}_{\\text{OBS}, i}} & \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(\\text{Divorce}_{\\text{TRUE}, i}, \\text{Divorce}_{\\text{SE}, i})} \\\\\n\\color{#5D01A6FF}{\\text{Divorce}_{\\text{TRUE}, i}} & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu     & = \\alpha + \\beta_1 \\text A_i + \\beta_2 \\text M_i \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nFire up brms.\n\nlibrary(brms)\n\nWith brms, we accommodate measurement error in the criterion using the mi() syntax, following the general form &lt;response&gt; | mi(&lt;se_response&gt;). This follows a missing data logic, resulting in Bayesian missing data imputation for the criterion values. The mi() syntax is based on the missing data capabilities for brms, which we will cover in greater detail in the second half of this chapter.\n\n# Put the data into a `list()`\ndlist &lt;- list(\n  D_obs = d$D_obs,\n  D_sd  = d$D_sd,\n  M     = d$M,\n  A     = d$A)\n\nb15.1 &lt;- brm(\n  data = dlist, \n  family = gaussian,\n  D_obs | mi(D_sd) ~ 1 + A + M,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 15,\n  # Note this line\n  save_pars = save_pars(latent = TRUE),\n  file = \"fits/b15.01\")\n\nCheck the model summary.\n\nprint(b15.1)\n\n Family: gaussian \n  Links: mu = identity \nFormula: D_obs | mi(D_sd) ~ 1 + A + M \n   Data: dlist (Number of observations: 50) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.05      0.10    -0.24     0.14 1.00     4597     3090\nA            -0.62      0.16    -0.92    -0.30 1.00     3829     3423\nM             0.05      0.17    -0.27     0.38 1.00     3671     3297\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.58      0.11     0.39     0.81 1.00     1682     1720\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nTo return the summaries for the D_true[i] parameters, you might execute posterior_summary(b15.1) or b15.1$fit.\n\nposterior_summary(b15.1) |&gt; \n  round(digits = 2) |&gt; \n  data.frame()\n\n            Estimate Est.Error   Q2.5  Q97.5\nb_Intercept    -0.05      0.10  -0.24   0.14\nb_A            -0.62      0.16  -0.92  -0.30\nb_M             0.05      0.17  -0.27   0.38\nsigma           0.58      0.11   0.39   0.81\nIntercept      -0.05      0.10  -0.24   0.14\nYl[1]           1.16      0.36   0.46   1.88\nYl[2]           0.69      0.56  -0.41   1.79\nYl[3]           0.43      0.34  -0.23   1.08\nYl[4]           1.42      0.46   0.52   2.34\nYl[5]          -0.90      0.13  -1.15  -0.65\nYl[6]           0.66      0.40  -0.10   1.45\nYl[7]          -1.37      0.36  -2.05  -0.67\nYl[8]          -0.34      0.47  -1.26   0.58\nYl[9]          -1.90      0.59  -3.05  -0.72\nYl[10]         -0.62      0.17  -0.96  -0.28\nYl[11]          0.76      0.28   0.21   1.31\nYl[12]         -0.55      0.49  -1.53   0.41\nYl[13]          0.18      0.51  -0.85   1.15\nYl[14]         -0.87      0.23  -1.32  -0.43\nYl[15]          0.56      0.31  -0.03   1.16\nYl[16]          0.28      0.40  -0.54   1.08\nYl[17]          0.50      0.43  -0.34   1.32\nYl[18]          1.25      0.35   0.59   1.94\nYl[19]          0.43      0.39  -0.30   1.21\nYl[20]          0.40      0.54  -0.60   1.49\nYl[21]         -0.56      0.32  -1.18   0.06\nYl[22]         -1.10      0.26  -1.61  -0.59\nYl[23]         -0.27      0.26  -0.78   0.25\nYl[24]         -1.00      0.31  -1.62  -0.41\nYl[25]          0.43      0.40  -0.33   1.23\nYl[26]         -0.03      0.31  -0.67   0.59\nYl[27]         -0.01      0.50  -0.99   1.00\nYl[28]         -0.15      0.39  -0.92   0.61\nYl[29]         -0.26      0.50  -1.20   0.75\nYl[30]         -1.80      0.24  -2.26  -1.33\nYl[31]          0.17      0.41  -0.64   1.00\nYl[32]         -1.66      0.16  -1.98  -1.34\nYl[33]          0.12      0.24  -0.36   0.61\nYl[34]         -0.05      0.51  -1.11   0.92\nYl[35]         -0.12      0.23  -0.57   0.32\nYl[36]          1.28      0.41   0.49   2.11\nYl[37]          0.22      0.35  -0.46   0.91\nYl[38]         -1.03      0.22  -1.45  -0.59\nYl[39]         -0.93      0.54  -1.96   0.16\nYl[40]         -0.68      0.32  -1.31  -0.05\nYl[41]          0.24      0.55  -0.82   1.33\nYl[42]          0.75      0.33   0.10   1.39\nYl[43]          0.20      0.18  -0.17   0.56\nYl[44]          0.80      0.42  -0.05   1.60\nYl[45]         -0.41      0.53  -1.42   0.68\nYl[46]         -0.39      0.26  -0.90   0.09\nYl[47]          0.13      0.30  -0.46   0.73\nYl[48]          0.56      0.45  -0.32   1.44\nYl[49]         -0.64      0.28  -1.19  -0.08\nYl[50]          0.85      0.58  -0.33   1.98\nlprior         -1.37      0.45  -2.45  -0.71\nlp__          -78.16      6.57 -91.47 -65.54\n\n\nOur rows Yl[1] through Yl[50] correspond to what rethinking named D_true[1] through D_true[50]. Here’s the code for our Figure 15.2.a.\n\nlibrary(ggrepel)\n\nstates &lt;- c(\"AL\", \"AR\", \"ME\", \"NH\", \"RI\", \"DC\", \"VT\", \"AK\", \"SD\", \"UT\", \"ID\", \"ND\", \"WY\")\n\nd_est &lt;- posterior_summary(b15.1) |&gt; \n  data.frame() |&gt; \n  rownames_to_column(\"term\") |&gt; \n  mutate(D_est = Estimate) |&gt; \n  select(term, D_est) |&gt; \n  filter(str_detect(term, \"Yl\")) |&gt; \n  bind_cols(d)\n\ncolor &lt;- viridis_pal(option = \"C\")(7)[5]\n\np1 &lt;- d_est |&gt;\n  ggplot(aes(x = D_sd, y = D_est - D_obs)) +\n  geom_hline(yintercept = 0, color = \"white\", linetype = 2) +\n  geom_point(alpha = 2/3, color = color) +\n  geom_text_repel(data = d_est |&gt; filter(Loc %in% states),  \n                  aes(label = Loc), \n                  color = \"white\", size = 3, seed = 15) \n\nWe’ll use a little as_draws_df() + expand_grid() magic to help with our version of Figure 15.2.b.\n\nlibrary(tidybayes)\n\nstates &lt;- c(\"AR\", \"ME\", \"RI\", \"ID\", \"WY\", \"ND\", \"MN\")\n\ncolor &lt;- viridis_pal(option = \"C\")(7)[4]\n\np2 &lt;- as_draws_df(b15.1) |&gt; \n  expand_grid(A = seq(from = -3.5, to = 3.5, length.out = 50)) |&gt; \n  mutate(fitted = b_Intercept + b_A * A) |&gt; \n  \n  ggplot(aes(x = A)) +\n  stat_lineribbon(aes(y = fitted),\n                  .width = 0.95, size = 1/3, color = \"grey50\", fill = \"grey20\") +\n  geom_segment(data = d_est,\n               aes(xend = A, y = D_obs, yend = D_est),\n               linewidth = 1/5) +\n  geom_point(data = d_est,\n             aes(y = D_obs),\n             color = color) +\n  geom_point(data = d_est,\n             aes(y = D_est),\n             shape = 1, stroke = 1/3) +\n  geom_text_repel(data = d |&gt; filter(Loc %in% states),  \n                  aes(y = D_obs, label = Loc), \n                  color = \"white\", size = 3, seed = 15) +\n  labs(x = \"median age marriage (std)\",\n       y = \"divorce rate (std)\") +\n  coord_cartesian(xlim = range(d$A), \n                  ylim = range(d$D_obs))\n\nNow combine the two ggplots and plot.\n\np1 | p2\n\n\n\n\n\n\n\n\nIf you look closely, our plot on the left is flipped relative to the one in the text. I’m pretty sure my code is correct, which leaves me to believe McElreath accidentally flipped the ordering in his code and made his \\(y\\)-axis ‘D_obs - D_est.’ Happily, our plot on the right matches up nicely with the one in the text.\n\n\n15.1.2 Error on both outcome and predictor\nNow we update the DAG to account for measurement error in the predictor.\n\ndag_coords &lt;- tibble(\n  name = c(\"A\", \"M\", \"Mobs\", \"eM\", \"D\", \"Dobs\", \"eD\"),\n  x    = c(1, 2, 3, 4, 2, 3, 4),\n  y    = c(2, 3, 3, 3, 1, 1, 1))\n\ndagify(M    ~ A,\n       D    ~ A + M,\n       Mobs ~ M + eM,\n       Dobs ~ D + eD,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"A\", \"Mobs\", \"Dobs\"), \"b\", \"a\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"A\", \"D\", \"M\", \n                                     expression(italic(e)[D]), expression(italic(e)[M]), \n                                     expression(D[obs]), expression(M[obs]))) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) +\n  dark_theme_void()\n\n\n\n\n\n\n\n\nWe will express this DAG in an augmented statistical model following the form\n\\[\\begin{align*}\n\\text{Divorce}_{\\text{OBS}, i}  & \\sim \\operatorname{Normal}(\\text{Divorce}_{\\text{TRUE}, i}, \\text{Divorce}_{\\text{SE}, i}) \\\\\n\\text{Divorce}_{\\text{TRUE}, i} & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + \\beta_1 \\text A_i + \\beta_2 \\color{#5D01A6FF}{\\text{Marriage}_{\\text{TRUE}, i}} \\\\\n\\color{#5D01A6FF}{\\text{Marriage}_{\\text{OBS}, i}} & \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(\\text{Marriage}_{\\text{TRUE}, i}, \\text{Marriage}_{\\text{SE}, i})} \\\\\n\\color{#5D01A6FF}{\\text{Marriage}_{\\text{TRUE}, i}} & \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 1)} \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0, 0.2) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nThe current version brms allows users to specify error on predictors with an me() statement in the form of me(predictor, sd_predictor) where sd_predictor is a vector in the data denoting the size of the measurement error, presumed to be in a standard-deviation metric.\n\n# Put the data into a `list()`\ndlist &lt;- list(\n  D_obs = d$D_obs,\n  D_sd  = d$D_sd,\n  M_obs = d$M_obs,\n  M_sd  = d$M_sd,\n  A     = d$A)\n\nb15.2 &lt;- brm(\n  data = dlist, \n  family = gaussian,\n  D_obs | mi(D_sd) ~ 1 + A + me(M_obs, M_sd),\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(normal(0, 1), class = meanme),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 15,\n  # Note this line\n  save_pars = save_pars(latent = TRUE),\n  file = \"fits/b15.02\")\n\nWe’ll use posterior_summary(), again, to get a sense of those depth=2 summaries.\n\nposterior_summary(b15.2) |&gt;\n round(digits = 2)\n\nDue to space concerns, I’m not going to show the results, here. You can do that on your own. Basically, now in addition to the posterior summaries for the Yl[i] parameters (what McElreath called \\(D_{\\text{TRUE}, i}\\)), we now get posterior summaries for Xme_meM_obs[i] (what McElreath called \\(M_{\\text{TRUE}, i}\\)). Note that you’ll need to specify save_pars = save_pars(latent = TRUE) in the brm() function in order to save the posterior samples of error-adjusted variables obtained by using the me() argument. Without doing so, functions like predict() may give you trouble. Here’s our version of Figure 15.3.\n\ncolor_y &lt;- viridis_pal(option = \"C\")(7)[7]\ncolor_p &lt;- viridis_pal(option = \"C\")(7)[2]\n\n# Wrangle\nfull_join(\n  # D\n  tibble(Loc   = d |&gt; pull(Loc),\n         D_obs = d |&gt; pull(D_obs),\n         D_est = posterior_summary(b15.2) |&gt; \n           data.frame() |&gt; \n           rownames_to_column(\"term\") |&gt; \n           filter(str_detect(term, \"Yl\")) |&gt; \n           pull(Estimate)) |&gt; \n    pivot_longer(-Loc, values_to = \"d\") |&gt; \n    mutate(name = if_else(name == \"D_obs\", \"observed\", \"posterior\")),\n  # M\n  tibble(Loc   = d |&gt; pull(Loc),\n         M_obs = d |&gt; pull(M_obs),\n         M_est = posterior_summary(b15.2) |&gt; \n           data.frame() |&gt; \n           rownames_to_column(\"term\") |&gt; \n           filter(str_detect(term, \"Xme_\")) |&gt; \n           pull(Estimate)) |&gt; \n    pivot_longer(-Loc, values_to = \"m\") |&gt; \n    mutate(name = if_else(name == \"M_obs\", \"observed\", \"posterior\")),\n  by = c(\"Loc\", \"name\"))  |&gt; \n  \n  # Plot\n  ggplot(aes(x = m, y = d)) +\n  geom_line(aes(group = Loc),\n            linewidth = 1/4) +\n  geom_point(aes(color = name)) +\n  scale_color_manual(values = c(color_p, color_y)) +\n  labs(x = \"Marriage rate (std)\", \n       y = \"Divorce rate (std)\",\n       subtitle = \"Shrinkage of both divorce rate and marriage rate\")\n\n\n\n\n\n\n\n\nThe yellow points are model-implied; the purple ones are of the original data. It turns out our brms model regularized just a little more aggressively than McElreath’s rethinking model.\nAnyway,\n\nThe big take home point for this section is that when you have a distribution of values, don’t reduce it down to a single value to use in a regression. Instead, use the entire distribution. Anytime we use an average value, discarding the uncertainty around that average, we risk overconfidence and spurious inference. This doesn’t only apply to measurement error, but also to cases in which data are averaged before analysis. (p. 497)\n\n\n\n15.1.3 Measurement terrors\nMcElreath invited us to consider a few more DAGs. The first is an instance where both sources of measurement error have a common cause, \\(P\\).\n\ndag_coords &lt;- tibble(\n  name = c(\"A\", \"M\", \"Mobs\", \"eM\", \"D\", \"Dobs\", \"eD\", \"P\"),\n  x    = c(1, 2, 3, 4, 2, 3, 4, 5),\n  y    = c(2, 3, 3, 3, 1, 1, 1, 2))\n\ndagify(M    ~ A,\n       D    ~ A + M,\n       Mobs ~ M + eM,\n       Dobs ~ D + eD,\n       eM   ~ P,\n       eD   ~ P,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"A\", \"Mobs\", \"Dobs\", \"P\"), \"b\", \"a\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"A\", \"D\", \"M\", \"P\", \n                                     expression(italic(e)[D]), expression(italic(e)[M]), \n                                     expression(D[obs]), expression(M[obs]))) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) +\n  dark_theme_void()\n\n\n\n\n\n\n\n\nThe second instance is when the true marriage rate \\(M\\) has a causal effect on the measurement error for Divorce, \\(e_\\text{D}\\).\n\ndag_coords &lt;- tibble(\n  name = c(\"A\", \"M\", \"Mobs\", \"eM\", \"D\", \"Dobs\", \"eD\"),\n  x    = c(1, 2, 3, 4, 2, 3, 4),\n  y    = c(2, 3, 3, 3, 1, 1, 1))\n\ndagify(M    ~ A,\n       D    ~ A + M,\n       Mobs ~ M + eM,\n       Dobs ~ D + eD,\n       eD   ~ M,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"A\", \"Mobs\", \"Dobs\"), \"b\", \"a\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"A\", \"D\", \"M\", \n                                     expression(italic(e)[D]), expression(italic(e)[M]), \n                                     expression(D[obs]), expression(M[obs]))) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) +\n  dark_theme_void()\n\n\n\n\n\n\n\n\nThe final example is when we have negligible measurement error for \\(M\\) and \\(D\\), but known nonignorable measurement error for the causal variable \\(A\\).\n\ndag_coords &lt;- tibble(\n  name = c(\"eA\", \"Aobs\", \"A\", \"M\", \"D\"),\n  x    = c(1, 2, 3, 4, 4),\n  y    = c(2, 2, 2, 3, 1))\n\ndagify(Aobs ~ A + eA,\n       M    ~ A,\n       D    ~ A,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"A\", \"eA\"), \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"A\", expression(italic(e)[A]), expression(A[obs]), \"D\", \"M\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) +\n  dark_theme_void()\n\n\n\n\n\n\n\n\nOn page 498, we read:\n\nIn this circumstance, it can happen that a naive regression of \\(D\\) on \\(A_\\text{obs}\\) and \\(M\\) will strongly suggest that \\(M\\) influences \\(D\\). The reason is that \\(M\\) contains information about the true \\(A\\). And \\(M\\) is measured more precisely than \\(A\\) is. It’s like a proxy \\(A\\). Here’s a small simulation you can toy with that will produce such a frustration:\n\n\nn &lt;- 500\n\nset.seed(15)\n\ndat &lt;- tibble(A = rnorm(n, mean = 0, sd = 1)) |&gt; \n  mutate(M     = rnorm(n, mean = -A, sd = 1),\n         D     = rnorm(n, mean =  A, sd = 1),\n         A_obs = rnorm(n, mean =  A, sd = 1))\n\nTo get a sense of the havoc ignoring measurement error can cause, we’ll fit to models. These aren’t in the text, but, you know, let’s live a little. The first model will include A, the true predictor for D. The second model will include A_obs instead, the version of A with measurement error added in.\n\n# The model with `A` containing no measurement error\nb15.2b &lt;- brm(\n  data = dat, \n  family = gaussian,\n  D ~ 1 + A + M,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 15,\n  # Note this line\n  save_pars = save_pars(latent = TRUE),\n  file = \"fits/b15.02b\")\n\n# The model where `A` has measurement error, but we ignore it\nb15.2c &lt;- brm(\n  data = dat, \n  family = gaussian,\n  D ~ 1 + A_obs + M,\n  prior = c(prior(normal(0, 0.2), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 15,\n  # Note this line\n  save_pars = save_pars(latent = TRUE),\n  file = \"fits/b15.02c\")\n\nCheck the summaries.\n\nprint(b15.2b)\n\n Family: gaussian \n  Links: mu = identity \nFormula: D ~ 1 + A + M \n   Data: dat (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.01      0.04    -0.08     0.09 1.00     3258     2465\nA             0.89      0.06     0.77     1.01 1.00     2968     2746\nM            -0.04      0.04    -0.13     0.04 1.00     3099     2638\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.01      0.03     0.94     1.07 1.00     3873     2770\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nprint(b15.2c)\n\n Family: gaussian \n  Links: mu = identity \nFormula: D ~ 1 + A_obs + M \n   Data: dat (Number of observations: 500) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.05      0.05    -0.05     0.15 1.00     3912     3018\nA_obs         0.29      0.04     0.21     0.37 1.00     3593     2946\nM            -0.35      0.04    -0.43    -0.28 1.00     3458     2860\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.14      0.04     1.07     1.22 1.00     3780     2812\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nModel b15.2b, where A contains no measurement error, comes close to reproducing the data-generating parameters. The second model, b15.2c, which used A infused with measurement error (i.e., A_obs), is a disaster. A coefficient plot might help the comparison.\n\n# For annotation\ntext &lt;- tibble(\n  fit      = \"b15.2b\",\n  term     = \"beta[0]\",\n  Estimate = fixef(b15.2b, probs = 0.99)[\"Intercept\", 3],\n  label    = \"In this plot, we like the yellow posteriors.\")\n\n# Wrangle\nbind_rows(posterior_summary(b15.2b) |&gt; data.frame() |&gt; rownames_to_column(\"term\"),\n          posterior_summary(b15.2c) |&gt; data.frame() |&gt; rownames_to_column(\"term\")) |&gt; \n  filter(term != \"Intercept\") |&gt;\n  filter(term != \"lp__\") |&gt; \n  filter(term != \"lprior\") |&gt;\n  mutate(term = rep(c(str_c(\"beta[\", 0:2, \"]\"), \"sigma\"), times = 2),\n         fit  = rep(c(\"b15.2b\", \"b15.2c\"), each = n() / 2)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = Estimate, y = fit)) +\n  geom_vline(xintercept = 0, linetype = 3, alpha = 1/2) +\n  geom_pointrange(aes(xmin = Q2.5, xmax = Q97.5, color = fit)) +\n  geom_text(data = text,\n            aes(label = label),\n            hjust = 0, color = color_y) +\n  scale_color_manual(values = c(color_y, \"white\")) +\n  labs(x = \"marginal posterior\",\n       y = NULL) +\n  facet_wrap(~ term, labeller = label_parsed, ncol = 1) +\n  theme(axis.ticks.y = element_blank(),\n        strip.background = element_rect(color = \"transparent\", fill = \"transparent\"))",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Missing Data and Other Opportunities</span>"
    ]
  },
  {
    "objectID": "15.html#missing-data",
    "href": "15.html#missing-data",
    "title": "15  Missing Data and Other Opportunities",
    "section": "15.2 Missing data",
    "text": "15.2 Missing data\n\nWith measurement error, the insight is to realize that any uncertain piece of data can be replaced by a distribution that reflects uncertainty. But sometimes data are simply missing–no measurement is available at all. At first, this seems like a lost cause. What can be done when there is no measurement at all, not even one with error?…\n\n\nSo what can we do instead? We can think causally about missingness, and we can use the model to impute missing values. A generative model tells you whether the process that produced the missing values will also prevent the identification of causal effects. (p. 499, emphasis in the original)\n\nStarting with version 2.2.0, brms supports Bayesian missing data imputation using adaptations of the multivariate syntax (Bürkner, 2022a). Bürkner’s (2022b) vignette, Handle missing values with brms, can provide a nice overview.\n\n15.2.0.1 Rethinking: Missing data are meaningful data\n\nThe fact that a variable has an unobserved value is still an observation. It is data, just with a very special value. The meaning of this value depends upon the context. Consider for example a questionnaire on personal income. If some people refuse to fill in their income, this may be associated with low (or high) income. Therefore a model that tries to predict the missing values can be enlightening. (p. 499)\n\n\n\n15.2.1 DAG ate my homework\nWe’ll start this section off with our version of Figure 15.4. It’s going to take a bit of effort on our part to make a nice representation those four DAGs. Here we make panels a, b, and d.\n\n# Panel a\ndag_coords &lt;- tibble(\n  name = c(\"S\", \"H\", \"Hs\", \"D\"),\n  x    = c(1, 2, 2, 1),\n  y    = c(2, 2, 1, 1))\n\np1 &lt;- dagify(\n  H  ~ S,\n  Hs ~ H + D,\n  coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name == \"H\", \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(label = c(\"D\", \"H\", \"H*\", \"S\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\")\n\n# Panel b\np2 &lt;- dagify(\n  H  ~ S,\n  Hs ~ H + D,\n  D  ~ S,\n  coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name == \"H\", \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(label = c(\"D\", \"H\", \"H*\", \"S\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\")\n\n# Panel d\np4 &lt;- dagify(\n  H  ~ S,\n  Hs ~ H + D,\n  D  ~ H,\n  coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name == \"H\", \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(label = c(\"D\", \"H\", \"H*\", \"S\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\")\n\nMake panel c.\n\ndag_coords &lt;- tibble(\n  name = c(\"S\", \"H\", \"Hs\", \"D\", \"X\"),\n  x    = c(1, 2, 2, 1, 1.5),\n  y    = c(2, 2, 1, 1, 1.5))\n\np3 &lt;- dagify(\n  H  ~ S + X,\n  Hs ~ H + D,\n  D  ~ X,\n  coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"H\", \"X\"), \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(label = c(\"D\", \"H\", \"H*\", \"S\", \"X\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\")\n\nNow combine, adjust a little, and plot.\n\n(p1 + p2 + p3 + p4) +\n  plot_annotation(tag_levels = \"a\", tag_prefix = \"(\", tag_suffix = \")\") &\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) &\n  dark_theme_void() +\n  theme(panel.background = element_rect(fill = \"grey8\"),\n        plot.margin = margin(0.15, 0.15, 0.15, 0.15, \"in\"))\n\n\n\n\n\n\n\n\nOn page 500, we read:\n\nConsider a sample of students, all of whom own dogs. The students produce homework (\\(H\\)). This homework varies in quality, influenced by how much each student studies (\\(S\\)). We could simulate 100 students, their attributes, and their homework like this:\n\n\nn &lt;- 100\n\nset.seed(15)\n\nd &lt;- tibble(s = rnorm(n, mean = 0, sd = 1)) |&gt; \n  mutate(h   = rbinom(n, size = 10, plogis(s)),\n         d_a = rbinom(n, size = 1, prob = 0.5),\n         d_b = ifelse(s &gt; 0, 1, 0)) |&gt;\n  mutate(hm_a = ifelse(d_a == 1, NA, h),\n         hm_b = ifelse(d_b == 1, NA, h))\n\nd\n\n# A tibble: 100 × 6\n         s     h   d_a   d_b  hm_a  hm_b\n     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1  0.259      6     0     1     6    NA\n 2  1.83       8     0     1     8    NA\n 3 -0.340      4     0     0     4     4\n 4  0.897      6     1     1    NA    NA\n 5  0.488      8     1     1    NA    NA\n 6 -1.26       3     1     0    NA     3\n 7  0.0228     3     1     1    NA    NA\n 8  1.09       6     1     1    NA    NA\n 9 -0.132      6     0     0     6     6\n10 -1.08       3     1     0    NA     3\n# ℹ 90 more rows\n\n\nIn that code block, we simulated the data corresponding to McElreath’s R code 15.8 through 15.10. We have two d and hm variables. d_a and hm_a correspond to McElreath’s R code 15.9 and the DAG in panel a. d_b and hm_b correspond to McElreath’s R code 15.10 and the DAG in panel b.\nThis wasn’t in the text, but here we’ll plot h, hm_a, and hm_b to get a sense of how the first two missing data examples compare to the original data.\n\nd |&gt; \n  pivot_longer(starts_with(\"h\")) |&gt; \n  mutate(name = factor(\n    name, \n    levels = c(\"h\", \"hm_a\", \"hm_b\"),\n    labels = c(\"true distribution\", \"missing completely at random\", \"missing conditional on s\"))) |&gt; \n  drop_na(value) |&gt; \n  \n  ggplot(aes(x = s, y = value, color = name == \"true distribution\")) + \n  geom_point(alpha = 2/3) +\n  scale_y_continuous(breaks = 1:10) +\n  scale_color_manual(values = viridis_pal(option = \"C\")(7)[6:7]) +\n  facet_wrap(~ name)\n\n\n\n\n\n\n\n\nThe left panel is the ideal situation letting us learn what we want to know, what is the effect of studying on the grade you’ll get on your homework (\\(S \\rightarrow H\\)). Once we enter in a missing data process (i.e., dogs \\(D\\) eating homework), we end up with \\(H^*\\), the homework left over after the dogs. Thus the homework outcomes we collect are a combination of the full set of homework and the hungry dogs. The middle panel depicts the scenario where the dogs eat the homework completely at random, \\(H \\rightarrow H^* \\leftarrow D\\). In the right panel, we consider a scenario where the dogs only and always eat the homework on the occasions the students studied more than average, \\(H \\rightarrow H^* \\leftarrow D \\leftarrow S\\).\nThe situation in the third DAG is more complicated. Now homework is conditional on both studying and how noisy it is in a students home, \\(X\\). Also, our new variable \\(X\\) isn’t measured and whether the dogs eat the homework is also conditional on that unmeasured \\(X\\). Here’s the new data simulation.\n\nn &lt;- 1000\n\nset.seed(501)\n\nd &lt;- tibble(x = rnorm(n, mean = 0, sd = 1),\n            s = rnorm(n, mean = 0, sd = 1)) |&gt; \n  mutate(h = rbinom(n, size = 10, plogis(2 + s - 2 * x)),\n         d = ifelse(x &gt; 1, 1, 0)) |&gt;\n  mutate(hm = ifelse(d == 1, NA, h))\n\nd\n\n# A tibble: 1,000 × 5\n         x      s     h     d    hm\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1  0.577   1.15     10     0    10\n 2  0.617  -0.786     7     0     7\n 3  0.452   0.958     9     0     9\n 4  0.226   0.754     8     0     8\n 5 -0.845   0.689    10     0    10\n 6 -1.43    0.176    10     0    10\n 7 -1.65    0.280    10     0    10\n 8  0.0356 -0.397     8     0     8\n 9  0.184   0.261     7     0     7\n10  1.22   -1.01      2     1    NA\n# ℹ 990 more rows\n\n\nThose data look like this.\n\nd |&gt; \n  pivot_longer(starts_with(\"h\")) |&gt; \n  mutate(name = factor(\n    name, \n    levels = c(\"h\", \"hm\"),\n    labels = c(\"true distribution\", \"missing conditional on x\"))) |&gt; \n  drop_na(value) |&gt; \n  \n  ggplot(aes(x = s, y = value, color = name == \"true distribution\")) + \n  geom_point(alpha = 1/4) +\n  scale_y_continuous(breaks = 1:10) +\n  scale_color_manual(values = viridis_pal(option = \"C\")(7)[6:7]) +\n  facet_wrap(~ name)\n\n\n\n\n\n\n\n\nFit the model using the data with no missingness.\n\nb15.3 &lt;- brm(\n  data = d,\n  family = binomial,\n  h | trials(10) ~ 1 + s,\n  prior = c(prior(normal(0, 1), class = Intercept),\n            prior(normal(0, 0.5), class = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 15,\n  file = \"fits/b15.03\")\n\nCheck the results.\n\nprint(b15.3)\n\n Family: binomial \n  Links: mu = logit \nFormula: h | trials(10) ~ 1 + s \n   Data: d (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.11      0.02     1.06     1.16 1.00     2906     2559\ns             0.69      0.03     0.64     0.74 1.00     2714     2530\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nSince this is not the data-generating model, we shouldn’t be all that surprised the coefficient for s is off (it should be 1). Because this is an example of where we didn’t collect data on \\(X\\), we can think of our incorrect results as a case of omitted variable bias. Here’s what happens when we run the model on hm, the homework variable after the hungry dogs got to it.\n\nb15.4 &lt;- brm(\n  data = d |&gt; filter(d == 0),\n  family = binomial,\n  h | trials(10) ~ 1 + s,\n  prior = c(prior(normal(0, 1), class = Intercept),\n            prior(normal(0, 0.5), class = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 15,\n  file = \"fits/b15.04\")\n\nCheck the results.\n\nprint(b15.4)\n\n Family: binomial \n  Links: mu = logit \nFormula: h | trials(10) ~ 1 + s \n   Data: filter(d, d == 0) (Number of observations: 820) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     1.79      0.03     1.73     1.87 1.00     2061     2136\ns             0.83      0.03     0.76     0.89 1.00     1977     2358\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nInterestingly, both the intercept and the coefficient for s are now less biased. Because both \\(H\\) and \\(D\\) are conditional on \\(X\\), omitting cases based on \\(X\\) resulted in a model that conditional on \\(X\\), even though \\(X\\) wasn’t directly in the statistical model. This won’t always be the case. Consider what happens when we have a different missing data mechanism.\n\nd &lt;- d |&gt; \n  mutate(d = ifelse(abs(x) &lt; 1, 1, 0)) |&gt;\n  mutate(hm = ifelse(d == 1, NA, h))\n\nd\n\n# A tibble: 1,000 × 5\n         x      s     h     d    hm\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n 1  0.577   1.15     10     1    NA\n 2  0.617  -0.786     7     1    NA\n 3  0.452   0.958     9     1    NA\n 4  0.226   0.754     8     1    NA\n 5 -0.845   0.689    10     1    NA\n 6 -1.43    0.176    10     0    10\n 7 -1.65    0.280    10     0    10\n 8  0.0356 -0.397     8     1    NA\n 9  0.184   0.261     7     1    NA\n10  1.22   -1.01      2     0     2\n# ℹ 990 more rows\n\n\nHere’s what then updated data look like.\n\nd |&gt; \n  pivot_longer(starts_with(\"h\")) |&gt; \n  mutate(name = factor(\n    name, \n    levels = c(\"h\", \"hm\"),\n    labels = c(\"true distribution\", \"missing conditional on x\"))) |&gt; \n  drop_na(value) |&gt; \n  \n  ggplot(aes(x = s, y = value, color = name == \"true distribution\")) + \n  geom_point(alpha = 1/4) +\n  scale_y_continuous(breaks = 1:10) +\n  scale_color_manual(values = viridis_pal(option = \"C\")(7)[6:7]) +\n  facet_wrap(~ name)\n\n\n\n\n\n\n\n\nMcElreath didn’t fit this model in the text, but he encouraged us to do so on our own (p. 503). Here it is.\n\nb15.4b &lt;- brm(\n  data = d |&gt; filter(d == 0),\n  family = binomial,\n  h | trials(10) ~ 1 + s,\n  prior = c(prior(normal(0, 1), class = Intercept),\n            prior(normal(0, 0.5), class = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 15,\n  file = \"fits/b15.04b\")\n\n\nprint(b15.4b)\n\n Family: binomial \n  Links: mu = logit \nFormula: h | trials(10) ~ 1 + s \n   Data: filter(d, d == 0) (Number of observations: 307) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.34      0.04     0.27     0.42 1.00     3677     2416\ns             0.49      0.04     0.41     0.57 1.00     3499     2659\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nYep, “now missingness makes things worse” (p. 503).\n\n15.2.1.1 Rethinking: Naming completely at random\nMcElreath briefly mentioned the terms missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). I share his sentiments; these terms are awful. However, they’re peppered throughout the missing data literature and I recommend you familiarize yourself with them. In his endnote #227, McElreath pointed readers to the authoritative work of Donald B. Rubin (1976) and Little and Rubin (2019, though he referenced the second edition, whereas I’m referencing the third). Baraldi & Enders (2010) is a nice primer, too. Also, the great Donald Rubin has several lectures available online. Here’s a link to a talk on causal inference, which includes bits of insights into missing data analysis and lots of historical tidbits, too.\n\n\n\n15.2.2 Imputing primates\nWe return to the milk data.\n\ndata(milk, package = \"rethinking\")\nd &lt;- milk\nrm(milk)\n\n# Transform\nd &lt;- d |&gt;\n  mutate(neocortex.prop = neocortex.perc / 100,\n         logmass        = log(mass)) |&gt; \n  mutate(k = (kcal.per.g - mean(kcal.per.g)) / sd(kcal.per.g),\n         b = (neocortex.prop - mean(neocortex.prop, na.rm = T)) / sd(neocortex.prop, na.rm = T),\n         m = (logmass - mean(logmass)) / sd(logmass))\n\nNote how we set na.rm = T within the mean() and sd() functions when computing b. See what happens if you leave that part out. As hinted at above and explicated in the text, we’re missing 12 values for neocortex.prop.\n\nd |&gt; \n  count(is.na(neocortex.prop))\n\n  is.na(neocortex.prop)  n\n1                 FALSE 17\n2                  TRUE 12\n\n\nWe dropped those values when we fit the models back in Section 5.2. To get a sense of whether this was a bad idea, let’s consider the model with a DAG. Ignoring the missing data, we have this.\n\ndag_coords &lt;- tibble(\n  name = c(\"M\", \"U\", \"K\", \"B\"),\n  x    = c(1, 2, 2, 3),\n  y    = c(2, 2, 1, 2))\n\ndagify(M ~ U,\n       B ~ U,\n       K ~ M + B,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name == \"U\", \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text() +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) +\n  dark_theme_void()\n\n\n\n\n\n\n\n\n“\\(M\\) is body mass, \\(B\\) is neocortex percent, \\(K\\) is milk energy, and \\(U\\) is some unobserved variable that renders \\(M\\) and \\(B\\) positively correlated” (p. 504). Because we have missingness in \\(B\\), our data in hand are actually \\(B^*\\). McElreath considered three processes that may have generated these missing data. Here are the DAGs.\n\ndag_coords &lt;- tibble(\n  name = c(\"M\", \"U\", \"K\", \"B\", \"RB\", \"Bs\"),\n  x    = c(1, 2, 2, 3, 2, 3),\n  y    = c(2, 2, 1, 2, 3, 3))\n\n# Left\np1 &lt;- dagify(\n  M  ~ U,\n  B  ~ U,\n  K  ~ M + B,\n  Bs ~ RB + B,\n  coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"U\", \"B\"), \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"B\", \"M\", expression(R[B]), \"U\", expression(B^'*'), \"K\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\")\n\n# Middle\np2 &lt;- dagify(\n  M  ~ U,\n  B  ~ U,\n  K  ~ M + B,\n  Bs ~ RB + B,\n  RB ~ M,\n  coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"U\", \"B\"), \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"B\", \"M\", expression(R[B]), \"U\", expression(B^'*'), \"K\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\")\n\n# Right\np3 &lt;- dagify(\n  M  ~ U,\n  B  ~ U,\n  K  ~ M + B,\n  Bs ~ RB + B,\n  RB ~ B,\n  coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"U\", \"B\"), \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"B\", \"M\", expression(R[B]), \"U\", expression(B^'*'), \"K\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\")\n\n# Combine, entitle, and display\n(p1 + p2 + p3) &\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) &\n  dark_theme_void() &\n  theme(panel.background = element_rect(fill = \"black\"),\n        plot.background = element_rect(fill = \"grey8\", color = \"grey8\"),\n        plot.margin = margin(0.1, 0.1, 0.1, 0.1, \"in\"))\n\n\n\n\n\n\n\n\nIn each of the DAGs, the new variable \\(R_B\\) simply indicates whether a given species has missingness in \\(B^*\\), much like our dog variable \\(D\\) indicated the missing data in the DAGs from the earlier DAGs. The big difference between then and now is that whereas we had a sense of what was causing the missing data in the earlier examples (i.e., those hungry \\(D\\) dogs), now we only have a generic missing data mechanism, \\(R_B\\). In the middle of page 505, McElreath asked we consider one more missing data mechanism, this time with a new unmeasured causal variable \\(V\\).\n\ndag_coords &lt;- tibble(\n  name = c(\"M\", \"U\", \"K\", \"B\", \"Bs\", \"RB\", \"V\"),\n  x    = c(1, 2, 2, 3, 4, 4, 3.4),\n  y    = c(2, 2, 1, 2, 2, 1, 1.45))\n\ndagify(M  ~ U,\n       B  ~ U + V,\n       K  ~ M + B,\n       Bs ~ RB + B,\n       RB ~ V,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name %in% c(\"U\", \"B\", \"V\"), \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"B\", \"M\", expression(R[B]), \"U\", \"V\", expression(B^'*'), \"K\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) +\n  dark_theme_void()\n\n\n\n\n\n\n\n\nHowever, our statistical model will follow the form\n\\[\\begin{align*}\nK_i     & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i   & = \\alpha + \\beta_1 \\color{#5D01A6FF}{B_i} + \\beta_2 \\log M_i \\\\\n\\color{#5D01A6FF}{B_i} & \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(\\nu, \\sigma_B)} \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1) \\\\\n\\color{#5D01A6FF}\\nu & \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\\n\\color{#5D01A6FF}{\\sigma_B} & \\color{#5D01A6FF}\\sim \\color{#5D01A6FF}{\\operatorname{Exponential}(1)},\n\\end{align*}\\]\nwhere we simply presume the missing values in \\(B_i\\), which was \\(B^*\\) in our DAGs, are unrelated to any of the other variables in the model. But those missing values in \\(B_i\\) values do get their own prior distribution, \\(\\operatorname{Normal}(\\nu, \\sigma_B)\\). If you look closely, you’ll discover the prior McElreath reported for \\(\\nu\\) \\([\\operatorname{Normal}(0.5, 1)]\\) does not match up with his rethinking::ulam() code in his R code block 15.17, \\(\\operatorname{Normal}(0, 0.5)\\). Here we use the latter.\nWhen writing a multivariate model in brms, I find it easier to save the model code by itself and then insert it into the brm() function. Otherwise, things start to feel cluttered.\n\n# Here's the primary `k` model\nb_model &lt;- bf(k ~ 1 + mi(b) + m) + \n  # Here's the model for the missing `b` data \n  bf(b | mi() ~ 1) + \n  # Here we set the residual correlations for the two models to zero\n  set_rescor(FALSE)\n\nNote the mi(b) syntax in the k model. This indicates that the predictor, b, has missing values that are themselves being modeled. To get a sense of how to specify the priors for such a model in brms, use the get_prior() function.\n\nget_prior(data = d,\n          family = gaussian,\n          b_model)\n\n                   prior     class coef group resp dpar nlpar lb ub tag       source\n    student_t(3, 0, 2.5) Intercept               b                           default\n    student_t(3, 0, 2.5)     sigma               b             0             default\n                  (flat)         b               k                           default\n                  (flat)         b    m          k                      (vectorized)\n                  (flat)         b  mib          k                      (vectorized)\n student_t(3, -0.3, 2.5) Intercept               k                           default\n    student_t(3, 0, 2.5)     sigma               k             0             default\n\n\nWith the one-step Bayesian imputation procedure in brms, you might need to use the resp argument when specifying non-default priors. Now fit the model.\n\nb15.5 &lt;- brm(\n  data = d, \n  family = gaussian,\n  b_model,  # Here we insert the model\n  prior = c(prior(normal(0, 0.5), class = Intercept, resp = k),\n            prior(normal(0, 0.5), class = Intercept, resp = b),\n            prior(normal(0, 0.5), class = b,         resp = k),\n            prior(exponential(1), class = sigma,     resp = k),\n            prior(exponential(1), class = sigma,     resp = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 15,\n  file = \"fits/b15.05\")\n\nWith a model like this, print() only gives up part of the picture.\n\nprint(b15.5)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity\n         mu = identity \nFormula: k ~ 1 + mi(b) + m \n         b | mi() ~ 1 \n   Data: d (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nk_Intercept     0.03      0.16    -0.30     0.35 1.00     4087     3220\nb_Intercept    -0.04      0.21    -0.46     0.39 1.00     3378     2750\nk_m            -0.54      0.20    -0.93    -0.13 1.00     2271     2695\nk_mib           0.49      0.24     0.01     0.93 1.00     1472     2646\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_k     0.84      0.14     0.61     1.16 1.00     2070     3008\nsigma_b     1.02      0.18     0.73     1.41 1.00     2398     2615\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote that for the parameters summarized in the ‘Population-Level Effects:’ section, the criterion is indexed in the prefix. The parameters in the ‘Family Specific Parameters:’, however, have the criteria indexed in the suffix. I don’t know why. Anyway, we can get a summary of the imputed values with posterior_summary().\n\nposterior_summary(b15.5) |&gt;\n  round(digits = 2)\n\n              Estimate Est.Error   Q2.5  Q97.5\nb_k_Intercept     0.03      0.16  -0.30   0.35\nb_b_Intercept    -0.04      0.21  -0.46   0.39\nb_k_m            -0.54      0.20  -0.93  -0.13\nbsp_k_mib         0.49      0.24   0.01   0.93\nsigma_k           0.84      0.14   0.61   1.16\nsigma_b           1.02      0.18   0.73   1.41\nIntercept_k       0.03      0.16  -0.30   0.35\nIntercept_b      -0.04      0.21  -0.46   0.39\nYmi_b[2]         -0.58      0.95  -2.41   1.32\nYmi_b[3]         -0.70      0.95  -2.55   1.22\nYmi_b[4]         -0.72      0.97  -2.62   1.24\nYmi_b[5]         -0.30      0.89  -2.05   1.50\nYmi_b[9]          0.47      0.94  -1.41   2.35\nYmi_b[14]        -0.15      0.91  -1.93   1.76\nYmi_b[15]         0.18      0.87  -1.56   1.88\nYmi_b[17]         0.27      0.89  -1.54   2.02\nYmi_b[19]         0.52      0.92  -1.32   2.36\nYmi_b[21]        -0.44      0.90  -2.25   1.36\nYmi_b[23]        -0.29      0.91  -2.08   1.53\nYmi_b[26]         0.14      0.94  -1.75   1.96\nlprior           -4.17      0.81  -6.09  -2.97\nlp__            -81.38      3.93 -89.77 -74.82\n\n\nThe imputed b values are indexed by occasion number from the original data. This is in contrast with McElreath’s precis() output, which simply serially indexes the missing values as B_impute[1], B_impute[2], and so on.\nBefore we move on to the next model, let’s plot to get a sense of what we’ve done.\n\nas_draws_df(b15.5) |&gt; \n  select(starts_with(\"Ymi_b\")) |&gt; \n  set_names(filter(d, is.na(b)) |&gt; pull(species)) |&gt; \n  pivot_longer(everything(),\n               names_to = \"species\") |&gt; \n  \n  ggplot(aes(x = value, \n             y = reorder(species, value))) +\n  stat_slab(fill = viridis_pal(option = \"C\")(7)[4], \n            alpha = 3/4, height = 1.5, slab_color = \"black\", slab_size = 1/4) +\n  labs(x = \"imputed values for b\",\n       y = NULL) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\nHere’s the model that drops the cases with NAs on b.\n\nb15.6 &lt;- brm(\n  data = d, \n  family = gaussian,\n  k ~ 1 + b + m,\n  prior = c(prior(normal(0, 0.5), class = Intercept),\n            prior(normal(0, 0.5), class = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 15,\n  file = \"fits/b15.06\")\n\nIf you run this on your computer, you’ll notice the following message at the top: “Rows containing NAs were excluded from the model.” This time print() gives us the same basic summary information as posterior_summary().\n\nprint(b15.6)\n\n Family: gaussian \n  Links: mu = identity \nFormula: k ~ 1 + b + m \n   Data: d (Number of observations: 17) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.10      0.19    -0.28     0.49 1.00     3109     2307\nb             0.60      0.28    -0.01     1.12 1.00     1962     2224\nm            -0.63      0.25    -1.10    -0.10 1.00     1995     2316\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.88      0.19     0.60     1.32 1.00     2280     2664\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can’t use McElreath’s plot(coeftab()) trick with our brms output, but we can still get by.\n\n# Wrangle\nbind_rows(fixef(b15.5) |&gt; data.frame() |&gt; rownames_to_column(\"term\"),\n          fixef(b15.6) |&gt; data.frame() |&gt; rownames_to_column(\"term\")) |&gt; \n  slice(c(4:3, 6:7)) |&gt; \n  mutate(term = str_c(\"beta[\", c(1:2, 1:2), \"]\"),\n         fit  = rep(c(\"b15.5\", \"b15.6\"), each = n() / 2)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = Estimate, y = fit)) +\n  geom_vline(xintercept = 0, alpha = 1/2, linetype = 3) +\n  geom_pointrange(aes(xmin = Q2.5, xmax = Q97.5)) +\n  labs(x = \"marginal posterior\",\n       y = NULL) +\n  facet_wrap(~ term, labeller = label_parsed, ncol = 1) +\n  theme(axis.ticks.y = element_blank(),\n        strip.background = element_rect(color = \"transparent\", fill = \"transparent\"))\n\n\n\n\n\n\n\n\nThe model using Bayesian imputation (b15.5) used more information, resulting in narrower marginal posteriors for \\(\\beta_1\\) and \\(\\beta_2\\). Because it wasted perfectly good information, the conventional b15.6 model was less certain.\nIn order to make our version of Figure 15.5, we’ll want to add the summary values for the imputed b data from b15.1 to the primary data file d.\n\nd &lt;- d |&gt; \n  mutate(row = 1:n()) |&gt; \n  left_join(\n    posterior_summary(b15.5) |&gt; \n      data.frame() |&gt; \n      rownames_to_column(\"term\") |&gt; \n      filter(str_detect(term, \"Ymi\")) |&gt; \n      mutate(row = str_extract(term, \"(\\\\d)+\") |&gt; as.integer()),\n    by = \"row\") \n\nd |&gt; \n  select(species, k:Q97.5) |&gt; \n  glimpse()\n\nRows: 29\nColumns: 10\n$ species   &lt;fct&gt; Eulemur fulvus, E macaco, E mongoz, E rubriventer, Lemur catta, Alouatta seniculus, A palliata, Cebus apella, …\n$ k         &lt;dbl&gt; -0.9400408, -0.8161263, -1.1259125, -1.0019980, -0.2585112, -1.0639553, -0.5063402, 1.5382486, 1.6621631, 1.72…\n$ b         &lt;dbl&gt; -2.080196025, NA, NA, NA, NA, -0.508641289, -0.508641289, 0.010742472, NA, 0.213469683, -1.461961806, -0.98613…\n$ m         &lt;dbl&gt; -0.4558357, -0.4150024, -0.3071581, -0.5650254, -0.3874772, 0.1274408, 0.1407505, -0.3071581, -1.0508443, -1.0…\n$ row       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29\n$ term      &lt;chr&gt; NA, \"Ymi_b[2]\", \"Ymi_b[3]\", \"Ymi_b[4]\", \"Ymi_b[5]\", NA, NA, NA, \"Ymi_b[9]\", NA, NA, NA, NA, \"Ymi_b[14]\", \"Ymi_…\n$ Estimate  &lt;dbl&gt; NA, -0.5830310, -0.7044893, -0.7152075, -0.2971713, NA, NA, NA, 0.4743642, NA, NA, NA, NA, -0.1549032, 0.17922…\n$ Est.Error &lt;dbl&gt; NA, 0.9462630, 0.9468543, 0.9720820, 0.8900150, NA, NA, NA, 0.9445337, NA, NA, NA, NA, 0.9106710, 0.8715274, N…\n$ Q2.5      &lt;dbl&gt; NA, -2.413219, -2.548091, -2.623875, -2.048046, NA, NA, NA, -1.413304, NA, NA, NA, NA, -1.927931, -1.557906, N…\n$ Q97.5     &lt;dbl&gt; NA, 1.322098, 1.216156, 1.240403, 1.500836, NA, NA, NA, 2.351685, NA, NA, NA, NA, 1.764372, 1.882271, NA, 2.01…\n\n\nNow make Figure 15.5.\n\ncolor &lt;- viridis_pal(option = \"D\")(7)[4]\n\n# Left\np1 &lt;- d |&gt; \n  ggplot(aes(y = k)) +\n  geom_point(aes(x = b),\n             color = color) +\n  geom_pointrange(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5),\n                  linewidth = 1/4, shape = 1, size = 1/2, stroke = 1/4) +\n  labs(x = \"neocortex percent (std)\",\n       y = \"kcal milk (std)\") +\n  coord_cartesian(xlim = range(d$b, na.rm = T))\n\n# Right\np2 &lt;- d |&gt; \n  ggplot(aes(x = m)) +\n  geom_point(aes(y = b),\n             color = color) +\n  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n                  linewidth = 1/4, shape = 1, size = 1/2, stroke = 1/4) +\n  labs(x = \"log body mass (std)\",\n       y = \"neocortex percent (std)\") +\n  coord_cartesian(ylim = range(d$b, na.rm = T))\n\n# Combine\np1 + p2\n\n\n\n\n\n\n\n\n“We can improve this model by changing the imputation model to estimate the relationship between the two predictors” (p. 509). In the text, McElreath accomplished this with the model\n\\[\\begin{align*}\nK_i   & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + \\beta_1 \\color{#5D01A6FF}{B_i} + \\beta_2 \\log M_i \\\\\n\\color{#5D01A6FF}{\\begin{bmatrix} M_i \\\\ B_i \\end{bmatrix}} & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{MVNormal}\n\\begin{pmatrix}\n\\begin{bmatrix} \\mu_M \\\\\\mu_B \\end{bmatrix},\n\\mathbf \\Sigma \\end{pmatrix}} \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1) \\\\\n\\color{#5D01A6FF}{\\mu_M} & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\\n\\color{#5D01A6FF}{\\mu_B} & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\\n\\color{#5D01A6FF}{\\mathbf \\Sigma} & \\color{#5D01A6FF} = \\color{#5D01A6FF}{\\operatorname{\\mathbf S \\mathbf R \\mathbf S}} \\\\\n\\color{#5D01A6FF}{\\mathbf S} & \\color{#5D01A6FF} = \\color{#5D01A6FF}{\\begin{bmatrix} \\sigma_M & 0 \\\\ 0 & \\sigma_B \\end{bmatrix}} \\\\\n\\color{#5D01A6FF}{\\mathbf R} & \\color{#5D01A6FF} = \\color{#5D01A6FF}{\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix}} \\\\\n\\color{#5D01A6FF}{\\sigma_M}  & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Exponential}(1)} \\\\\n\\color{#5D01A6FF}{\\sigma_B}  & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Exponential}(1)} \\\\\n\\color{#5D01A6FF} \\rho       & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{LKJ}(2)},\n\\end{align*}\\]\nwhich expresses the relationship between the two predictors with a residual correlation matrix, \\(\\mathbf \\Sigma\\). Importantly, though \\(\\mathbf \\Sigma\\) involves the variables \\(B_i\\) and \\(M_i\\), it does not directly involve the criterion, \\(K_i\\). As it turns out, the current version of brms cannot handle a model of this form. When you fit multivariate models with residual correlations, you have to set them for either all variables or none of them. For a little more on this topic, you can skim through the Brms and heterogeneous residual covariance - equivalent of “at.level” function thread on the Stan Forums. Bürkner’s response to the initial question indicated this kind of model will be available in brms version 3.0+, which I suspect will entail a substantial reworking of the multivariate syntax. Until then, we can fit the alternative model\n\\[\\begin{align*}\nK_i   & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + \\beta_1 \\color{#5D01A6FF}{B_i} + \\beta_2 \\log M_i \\\\\n\\color{#5D01A6FF}{B_i}   & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(\\nu_i, \\sigma_B)} \\\\\n\\color{#5D01A6FF}{\\nu_i} & \\color{#5D01A6FF} = \\color{#5D01A6FF}{\\gamma + \\delta_1 \\log M_i} \\\\\n\\alpha  & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\beta_2 & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1) \\\\\n\\color{#5D01A6FF}\\gamma     & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\\n\\color{#5D01A6FF}{\\delta_1} & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Normal}(0, 0.5)} \\\\\n\\color{#5D01A6FF}{\\sigma_B} & \\color{#5D01A6FF} \\sim \\color{#5D01A6FF}{\\operatorname{Exponential}(1)},\n\\end{align*}\\]\nwhich captures the relation among the two predictors as a regression of \\(M_i\\) predicting \\(B_i\\). Here’s how to fit the model with brms.\n\nb_model &lt;- mvbf(\n  bf(k ~ 1 + mi(b) + m), \n  bf(b | mi() ~ 1 + m), \n  rescor = FALSE)\n\nb15.7 &lt;- brm(\n  data = d, \n  family = gaussian,\n  b_model,\n  prior = c(prior(normal(0, 0.5), class = Intercept, resp = k),\n            prior(normal(0, 0.5), class = Intercept, resp = b),\n            prior(normal(0, 0.5), class = b, resp = k),\n            prior(normal(0, 0.5), class = b, resp = b),\n            prior(exponential(1), class = sigma,     resp = k),\n            prior(exponential(1), class = sigma,     resp = b)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 15,\n  file = \"fits/b15.07\")\n\nLet’s see what we did.\n\nprint(b15.7)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity\n         mu = identity \nFormula: k ~ 1 + mi(b) + m \n         b | mi() ~ 1 + m \n   Data: d (Number of observations: 29) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nk_Intercept     0.03      0.16    -0.29     0.33 1.00     4065     3157\nb_Intercept    -0.05      0.15    -0.36     0.25 1.00     3325     3131\nk_m            -0.65      0.22    -1.06    -0.20 1.00     1645     2332\nb_m             0.59      0.15     0.28     0.88 1.00     4121     2837\nk_mib           0.59      0.26     0.08     1.08 1.00     1443     2456\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_k     0.82      0.14     0.60     1.12 1.00     1857     2573\nsigma_b     0.71      0.13     0.51     1.00 1.00     2015     2618\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe have two intercepts, k_Intercept (\\(\\alpha\\)) and b_Intercept (\\(\\gamma\\)). Both are near zero because both k and b are standardized. Our k_mib (\\(\\beta_1\\)) and k_m (\\(\\beta_2\\)) parameters correspond with McElreath’s bB and bM parameters, respectively. Our summaries for them are very similar to his. Notice our summary for b_m (\\(\\delta_1\\)). McElreath doesn’t have a parameter exactly like that, but his close analogue is Rho_BM[1,2] (also Rho_BM[2,1], which is really the same thing). Also, notice how our summary for b_m is almost the same as the summary for McElreath’s Rho_BM[1,2]. This is because a univariable regression coefficient between two standardized variables is in the same metric as a correlation and McElreath’s Rho_BM[1,2] is just that–a correlation. If you fit McElreath’s model with rethinking and execute precis(m15.7, depth = 3), you’ll see that our sigma_k (\\(\\sigma\\)) summary corresponds nicely with his summary for sigma. However, our sigma_b (\\(\\sigma_B\\)) is not the same as his Sigma_BM[2]. Why? Because whereas our sigma_b is a residual standard deviation after accounting for the effect of m on b, McElreath’s Sigma_BM[2] is just an estimate of the standard deviation of b. Also, notice that whereas McElreath’s output has a Sigma_BM[1] parameter, we have no direct analogue. Why? Because although m is a predictor variable for both k and b, we did not give it its own likelihood. McElreath, in contrast, entered m into the bivariate likelihood with b.\nOur workflow for Figure 15.6 is largely the same as for Figure 15.5. The biggest difference is we need to remove the columns term through upper from our earlier model before we can replace them with those from the current model. After that, it’s basically cut and paste.\n\nd &lt;- d |&gt; \n  select(-(term:Q97.5)) |&gt; \n  left_join(\n    posterior_summary(b15.7) |&gt; \n      data.frame() |&gt; \n      rownames_to_column(\"term\") |&gt; \n      filter(str_detect(term, \"Ymi\")) |&gt; \n      mutate(row = str_extract(term, \"(\\\\d)+\") |&gt; as.integer()),\n    by = \"row\")\n\nd |&gt; \n  select(species, k:Q97.5) |&gt; \n  glimpse()\n\nRows: 29\nColumns: 10\n$ species   &lt;fct&gt; Eulemur fulvus, E macaco, E mongoz, E rubriventer, Lemur catta, Alouatta seniculus, A palliata, Cebus apella, …\n$ k         &lt;dbl&gt; -0.9400408, -0.8161263, -1.1259125, -1.0019980, -0.2585112, -1.0639553, -0.5063402, 1.5382486, 1.6621631, 1.72…\n$ b         &lt;dbl&gt; -2.080196025, NA, NA, NA, NA, -0.508641289, -0.508641289, 0.010742472, NA, 0.213469683, -1.461961806, -0.98613…\n$ m         &lt;dbl&gt; -0.4558357, -0.4150024, -0.3071581, -0.5650254, -0.3874772, 0.1274408, 0.1407505, -0.3071581, -1.0508443, -1.0…\n$ row       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29\n$ term      &lt;chr&gt; NA, \"Ymi_b[2]\", \"Ymi_b[3]\", \"Ymi_b[4]\", \"Ymi_b[5]\", NA, NA, NA, \"Ymi_b[9]\", NA, NA, NA, NA, \"Ymi_b[14]\", \"Ymi_…\n$ Estimate  &lt;dbl&gt; NA, -0.61795816, -0.66220796, -0.78002708, -0.41078196, NA, NA, NA, -0.23812970, NA, NA, NA, NA, -0.70877651, …\n$ Est.Error &lt;dbl&gt; NA, 0.6577113, 0.6791776, 0.6814547, 0.6265196, NA, NA, NA, 0.6863201, NA, NA, NA, NA, 0.6575743, 0.6374764, N…\n$ Q2.5      &lt;dbl&gt; NA, -1.8497430, -1.9531706, -2.1477297, -1.6506746, NA, NA, NA, -1.5979021, NA, NA, NA, NA, -1.9648952, -1.243…\n$ Q97.5     &lt;dbl&gt; NA, 0.7293671, 0.6920071, 0.5968854, 0.8499724, NA, NA, NA, 1.0857268, NA, NA, NA, NA, 0.6237932, 1.2977006, N…\n\n\nNow make Figure 15.6.\n\ncolor &lt;- viridis_pal(option = \"D\")(7)[3]\n\np1 &lt;- d |&gt; \n  ggplot(aes(y = k)) +\n  geom_point(aes(x = b),\n             color = color) +\n  geom_pointrange(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5),\n                  linewidth = 1/4, shape = 1, size = 1/2, stroke = 1/4) +\n  labs(x = \"neocortex percent (std)\",\n       y = \"kcal milk (std)\") +\n  coord_cartesian(xlim = range(d$b, na.rm = T))\n\np2 &lt;- d |&gt; \n  ggplot(aes(x = m)) +\n  geom_point(aes(y = b),\n             color = color) +\n  geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n                  linewidth = 1/4, shape = 1, size = 1/2, stroke = 1/4) +\n  labs(x = \"log body mass (std)\",\n       y = \"neocortex percent (std)\") +\n  coord_cartesian(ylim = range(d$b, na.rm = T))\n\np1 + p2\n\n\n\n\n\n\n\n\nThe results further show that our fully standardized regression coefficient (\\(\\delta_1\\)) had the same effect on the Bayesian imputation as McElreath’s residual correlation. Our \\(\\delta_1\\) coefficient is basically just a correlation in disguise.\n\n15.2.2.1 Rethinking: Multiple imputations\n\nMissing data imputation has a messy history. There are many forms of imputation… A common non-Bayesian procedure is multiple imputation. Multiple imputation was developed in the context of survey non-response, and it actually has a Bayesian justification. But it was invented when Bayesian imputation on the desktop was impractical, so it tries to approximate the full Bayesian solution to a “missing at random” missingness model. If you aren’t comfortable dropping incomplete cases, then you shouldn’t be comfortable using multiple imputation either. The procedure performs multiple draws from an approximate posterior distribution of the missing values, performs separate analyses with these draws, and then combines the analyses in a way that approximates full Bayesian imputation. Multiple imputation is more limited than full Bayesian imputation, so now we just use the real thing. (p. 511)\n\nWe won’t be walking through an example in this ebook, but you should know that brms is capable of multiple imputation, too. You can find an example of multiple imputation in Bürkner’s (2022b) vignette, Handle missing values with brms. To learn about the origins of this approach, check out the authoritative work by Rubin (1987, 1996) and Little and Rubin (2019).\n\n\n\n15.2.3 Where is your god now?\n“Sometimes there are no statistical solutions to scientific problems. But even then, careful statistical thinking can be useful because it will tell us that there is no statistical solution” (p. 512).\nLet’s load the Moralizing_gods data from Whitehouse et al. (2019).1\n1 As it turns out, the article connected to the Moralizing_gods data, Whitehouse et al. (2019), has been retracted. You can find the details at Critique topples Nature paper on belief in gods, on Retraction Watch. It appears the retraction is, in part, a reaction to this preprint which, interestingly enough, questions the authors’ missing data methods. The preprint has since been published as Beheim et al. (2021).\ndata(Moralizing_gods, package = \"rethinking\")\nd &lt;- Moralizing_gods\nrm(Moralizing_gods)\n\nTake a look at the new data.\n\nglimpse(d)\n\nRows: 864\nColumns: 5\n$ polity          &lt;fct&gt; Big Island Hawaii, Big Island Hawaii, Big Island Hawaii, Big Island Hawaii, Big Island Hawaii, Big Islan…\n$ year            &lt;int&gt; 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, -600, -500, -400, -300, -200, -100, 0, 100, 200, 3…\n$ population      &lt;dbl&gt; 3.729643, 3.729643, 3.598340, 4.026240, 4.311767, 4.205113, 4.373960, 5.157593, 4.997439, 1.547543, 1.54…\n$ moralizing_gods &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ writing         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,…\n\n\nThe bulk of the values for moralizing_gods are missing and very few of the remaining values are 0’s.\n\nd |&gt; \n  count(moralizing_gods) |&gt; \n  mutate(`%` = 100 * n / sum(n))\n\n  moralizing_gods   n         %\n1               0  17  1.967593\n2               1 319 36.921296\n3              NA 528 61.111111\n\n\nTo get a sense of how these values are distributed, here’s our version of Figure 15.7.\n\nd |&gt; \n  mutate(mg = factor(ifelse(is.na(moralizing_gods), 2, 1 - moralizing_gods),\n                     levels = 0:2,\n                     labels = c(\"present\", \"absent\", \"unknown\"))) |&gt; \n  \n  ggplot(aes(x = year, y = population, color = mg)) +\n  geom_point(alpha = 2/3) +\n  scale_color_manual(\"Moralizing gods\", \n                     values = viridis_pal(option = \"D\")(7)[c(5, 7, 3)]) +\n  labs(x = \"Time (year)\",\n       y = \"Population size (log)\",\n       subtitle = '\"This is a highly non-random missingness pattern\" (p. 514).') +\n  theme(legend.position = c(0.125, 0.67))\n\n\n\n\n\n\n\n\nHere are the counts broken down by gods and literacy status.\n\nd |&gt; \n  mutate(gods     = moralizing_gods,\n         literacy = writing) |&gt; \n  count(gods, literacy) |&gt; \n  mutate(`%` = 100 * n / sum(n))\n\n  gods literacy   n          %\n1    0        0  16  1.8518519\n2    0        1   1  0.1157407\n3    1        0   9  1.0416667\n4    1        1 310 35.8796296\n5   NA        0 442 51.1574074\n6   NA        1  86  9.9537037\n\n\nThe bulk of the missing moralizing_gods values are from non-literate polities and the figure above shows that smaller polities also tend to have missing values. We can try to make sense of all this with McElreath’s DAG.\n\ndag_coords &lt;- tibble(\n  name = c(\"P\", \"W\", \"G\", \"RG\", \"Gs\"),\n  x    = c(1, 2.33, 4, 5.67, 7),\n  y    = c(2, 1, 2, 1, 2)) \n\ndagify(P  ~ G,\n       W  ~ P,\n       RG ~ W,\n       Gs ~ G + RG,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name == \"G\", \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"G\", \"P\", expression(R[G]), \"W\", expression(G^'*'))) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) +\n  dark_theme_void()\n\n\n\n\n\n\n\n\n\nHere \\(P\\) is rate of population growth (not the same as the population size variable in the data), \\(G\\) is the presence of belief in moralizing gods (which is unobserved), \\(G^*\\) is the observed variable with missing values, \\(W\\) is writing, and \\(R_G\\) is the missing values indicator. This is an optimistic scenario, because it assumes there are no unobserved confounds among \\(P\\), \\(G\\), and \\(W\\). These are purely observational data, recall. But the goal is to use this example to think through the impact of missing data. If we can’t recover from missing data with the DAG above, adding confounds isn’t going to help. (p. 515)\n\nConsider the case of Hawaii.\n\nd |&gt; \n  filter(polity == \"Big Island Hawaii\") |&gt; \n  select(year, writing, moralizing_gods)\n\n  year writing moralizing_gods\n1 1000       0              NA\n2 1100       0              NA\n3 1200       0              NA\n4 1300       0              NA\n5 1400       0              NA\n6 1500       0              NA\n7 1600       0              NA\n8 1700       0              NA\n9 1800       0               1\n\n\n\nWhat happened in 1778? Captain James Cook and his crew finally made contact….\nAfter Captain Cook, Hawaii is correctly coded with 1 for belief in moralizing gods. It is also a fact that Hawaii never developed its own writing system. So there is no direct evidence of when moralizing gods appeared in Hawaii. Any imputation model needs to decide how to fill in those NA values. With so much missing data, any imputation model would necessarily make very strong assumptions. (pp. 515–516)\n\n\n15.2.3.1 Rethinking: Present details about missing data\n\nClear documentation of missing data and its treatment is necessary. This is best done with a causal model that makes transparent what is being assumed about the source of missing values and simultaneously justifies how they are handled. But the minimum is to report the counts of missing values in each variable and what was done with them. (p. 516, emphasis added)\n\nI strongly agree with McElreath on this point. When doing peer-reviews, I regularly inquire about missing data and how, if present, they were handled.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Missing Data and Other Opportunities</span>"
    ]
  },
  {
    "objectID": "15.html#sec-Categorical-errors-and-discrete-absences",
    "href": "15.html#sec-Categorical-errors-and-discrete-absences",
    "title": "15  Missing Data and Other Opportunities",
    "section": "15.3 Categorical errors and discrete absences",
    "text": "15.3 Categorical errors and discrete absences\n\nDiscrete unobserved variables require discrete parameters. There are two issues with discrete parameters. First, a discrete variable will not produce a smooth surface for Hamiltonian Monte Carlo to glide around on. HMC just doesn’t do discrete variables. Second, other estimation approaches also have problems with discrete parameter spaces, because discrete jumps are difficult to calibrate. Chains tend to get stuck for long periods.\nBut that doesn’t mean we are stuck. In almost every case, we don’t need to sample discrete parameters at all. Instead we can use a special technique, known to experts as a “weighted average,” to remove discrete parameters from the model. After sampling the other parameters, we can then use their samples to compute the posterior distribution of any dis- crete parameter that we removed. So no information is given up. (pp. 516–517)\n\n\n15.3.1 Discrete cats\nHere is McElreath’s next DAG.\n\ndag_coords &lt;- tibble(\n  name = c(\"Rc\", \"Cs\", \"C\", \"N\"),\n  x    = 1:4,\n  y    = 1)\n\ndagify(Cs ~ Rc + C,\n       N  ~ C,\n       coords = dag_coords) |&gt;\n  tidy_dagitty() |&gt; \n  mutate(color = ifelse(name == \"C\", \"a\", \"b\")) |&gt; \n  \n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(aes(color = color),\n                 size = 7, show.legend = F) +\n  geom_dag_text(parse = T, label = c(\"C\",expression(R[C]), expression(C^\"*\"), \"N\")) +\n  geom_dag_edges(edge_colour = \"#FCF9F0\") +\n  scale_color_manual(values = c(viridis_pal(option = \"C\")(7)[2], \"black\")) +\n  dark_theme_void()\n\n\n\n\n\n\n\n\n“The presence/absence of a cat \\(C\\) influences the number of sung notes \\(N\\). Because of missing values \\(R_C\\) however, we only observe \\(C^*\\)” (p. 517). McElreath’s proposed generative model was\n\\[\\begin{align*}\nN_i & \\sim \\operatorname{Poisson}(\\lambda_i) \\\\\n\\log \\lambda_i & = \\alpha + \\beta C_i \\\\\nC_i & \\sim \\operatorname{Bernoulli}(k) \\\\\nR_{C, i} & \\sim \\operatorname{Bernoulli}(r).\n\\end{align*}\\]\nWe can simulate data along those lines with a few lines of code.\n\nset.seed(9)\n\nn_houses &lt;- 1000L \nalpha &lt;- 5\nbeta &lt;- (-3)\nk &lt;- 0.5\nr &lt;- 0.2\n\ndat &lt;- tibble(cat = rbinom(n_houses, size = 1, prob = k)) |&gt; \n  mutate(notes = rpois(n_houses, lambda = alpha + beta * cat),\n         r_c   = rbinom(n_houses, size = 1, prob = r)) |&gt; \n  mutate(cat_obs = if_else(r_c == 1, (-9L), cat))\n\nglimpse(dat)\n\nRows: 1,000\nColumns: 4\n$ cat     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1…\n$ notes   &lt;int&gt; 3, 5, 11, 6, 9, 6, 6, 6, 1, 3, 4, 6, 3, 7, 6, 3, 6, 3, 4, 4, 2, 0, 4, 7, 3, 1, 7, 8, 2, 2, 5, 1, 6, 4, 7, 4, 3, …\n$ r_c     &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ cat_obs &lt;int&gt; 0, 0, 0, 0, 0, 0, -9, 0, 1, 1, 0, -9, 1, -9, 0, 1, 0, 1, 0, 0, 1, -9, -9, -9, -9, -9, 0, -9, 1, 1, 0, 1, 0, 1, 0…\n\n\nNote how we constructed cat_obs by replacing several values of cat with -9.\n\nThere is nothing special about this value. The model will skip them. But it is usually good to use some invalid value, so that if you make a mistake in coding, an error will result. In this case, since cat has a Bernoulli distribution, if the model ever asks for the probability of observing -9, there should be an error, because -9 is impossible. (p. 518)\n\n😢 Sadly, this is as far as I’m going in this section. I still haven’t made sense of McElreath’s weighted average approach to missing data and I’m not sure whether it’s even possible with brms. If you have insights in to how one might accommodate categorical missing data with brms, share them with the rest of us on GitHub.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Missing Data and Other Opportunities</span>"
    ]
  },
  {
    "objectID": "15.html#summary",
    "href": "15.html#summary",
    "title": "15  Missing Data and Other Opportunities",
    "section": "15.4 Summary",
    "text": "15.4 Summary\n\nThis chapter has been a quick introduction to the design and implementation of measurement error and missing data models. Measurement error and missing data have causes. Incorporating those causes into the generative model helps us decide how error and missingness impact inference as well as how to design a statistical procedure. (p. 521)\n\nIf modern missing data methods are new to you, you might also check out van Burren’s great online (2018) text, Flexible imputation of missing data. I’m also a fan of Enders’s (2022) Applied missing data analysis, for which you can download a free sample chapter by clicking here.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Missing Data and Other Opportunities</span>"
    ]
  },
  {
    "objectID": "15.html#bonus-bayesian-meta-analysis-with-odds-ratios",
    "href": "15.html#bonus-bayesian-meta-analysis-with-odds-ratios",
    "title": "15  Missing Data and Other Opportunities",
    "section": "15.5 Bonus: Bayesian meta-analysis with odds ratios",
    "text": "15.5 Bonus: Bayesian meta-analysis with odds ratios\nIf your mind isn’t fully blown by those measurement-error and missing-data models, let’s keep building. As it turns out, meta-analyses are often just special kinds of multilevel measurement-error models. Thus, you can use brms::brm() to fit Bayesian meta-analyses, too.\nBefore we proceed, I should acknowledge that this section is heavily influenced by Matti Vourre’s great blog post, Bayesian meta-analysis with R, Stan, and brms. Since neither editions of McElreath’s text directly address meta-analyses, we’ll also have to borrow a bit from Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin’s (2013) Bayesian data analysis, Third edition.\n\n15.5.1 How do meta-analyses fit into the picture?\nLet Gelman and colleagues introduce the topic:\n\nDiscussions of meta-analysis are sometimes imprecise about the estimands of interest in the analysis, especially when the primary focus is on testing the null hypothesis of no effect in any of the studies to be combined. Our focus is on estimating meaningful parameters, and for this objective there appear to be three possibilities, accepting the overarching assumption that the studies are comparable in some broad sense. The first possibility is that we view the studies as identical replications of each other, in the sense we regard the individuals in all the studies as independent samples from a common population, with the same outcome measures and so on. A second possibility is that the studies are so different that the results of any one study provide no information about the results of any of the others. A third, more general, possibility is that we regard the studies as exchangeable but not necessarily either identical or completely unrelated; in other words we allow differences from study to study, but such that the differences are not expected a priori to have predictable effects favoring one study over another…. this third possibility represents a continuum between the two extremes, and it is this exchangeable model (with unknown hyperparameters characterizing the population distribution) that forms the basis of our Bayesian analysis….\nThe first potential estimand of a meta-analysis, or a hierarchically structured problem in general, is the mean of the distribution of effect sizes, since this represents the overall ‘average’ effect across all studies that could be regarded as exchangeable with the observed studies. Other possible estimands are the effect size in any of the observed studies and the effect size in another, comparable (exchangeable) unobserved study. (pp. 125–126, emphasis in the original)\n\nThe basic version of a Bayesian meta-analysis follows the form\n\\[y_j \\sim \\operatorname{Normal}(\\theta_j, \\sigma_j),\\]\nwhere \\(y_j\\) = the point estimate for the effect size of a single study, \\(j\\), which is presumed to have been a draw from a Normal distribution centered on \\(\\theta_j\\). The data in meta-analyses are typically statistical summaries from individual studies. The one clear lesson from this chapter is that those estimates themselves come with error and those errors should be fully expressed in the meta-analytic model. The standard error from study \\(j\\) is specified \\(\\sigma_j\\), which is also a stand-in for the standard deviation of the Normal distribution from which the point estimate was drawn. Do note, we’re not estimating \\(\\sigma_j\\), here. Those values we take directly from the original studies.\nBuilding on the model, we further presume that study \\(j\\) is itself just one draw from a population of related studies, each of which have their own effect sizes. As such, we presume \\(\\theta_j\\) itself has a distribution following the form\n\\[\\theta_j \\sim \\operatorname{Normal}(\\mu, \\tau),\\]\nwhere \\(\\mu\\) is the meta-analytic effect (i.e., the population mean) and \\(\\tau\\) is the variation around that mean, what you might also think of as \\(\\sigma_\\tau\\).\n\n\n15.5.2 We need some data\nOur data in this section come from the second large-scale replication project by the Many Labs team (Klein et al., 2018). Of the 28 studies replicated in the study, we will focus on the replication of the trolley experiment from Hauser et al. (2007). Here’s how the study was described by Klein and colleagues:\n\nAccording to the principle of double effect, an act that harms other people is more morally permissible if the act is a foreseen side effect rather than the means to the greater good. Hauser et al. (2007) compared participants’ reactions to two scenarios to test whether their judgments followed this principle. In the foreseen-side-effect scenario, a person on an out-of-control train changed the train’s trajectory so that the train killed one person instead of five. In the greater-good scenario, a person pushed a fat man in front of a train, killing him, to save five people. Whereas \\(89\\%\\) of participants judged the action in the foreseen-side-effect scenario as permissible \\((95 \\% \\; \\text{CI} = [87\\%, 91\\%]),\\) only \\(11\\%\\) of participants in the greater-good scenario judged it as permissible \\((95 \\% \\; \\text{CI} = [9\\%, 13\\%])\\). The difference between the percentages was significant\\(,\\) \\(\\chi^2(1, N = 2,646) = 1,615.96,\\) \\(p &lt; .001,\\) \\(w = .78,\\) \\(d = 2.50,\\) \\(95 \\% \\; \\text{CI} = [2.22, 2.86]\\). Thus, the results provided evidence for the principle of double effect. (p. 459, emphasis in the original)\n\nYou can find supporting materials for the replication project on the Open Science Framework at https://osf.io/8cd4r/. The relevant subset of the data for the replication of Hauser et al. come from the Trolley Dilemma 1 (Hauser et al., 2007) folder within the OSFdata.zip (https://osf.io/ag2pd/). I’ve downloaded the file and saved it on GitHub.\nHere we load the data.\n\nfile_url &lt;- \"https://raw.githubusercontent.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/master/data/Hauser_1_study_by_order_all_CLEAN_CASE.csv\"\n\nd &lt;- readr::read_csv(file_url)\n\nd &lt;- d |&gt; \n  mutate(y   = ifelse(variable == \"Yes\", 1, 0),\n         loc = factor(Location,\n                      levels = distinct(d, Location) |&gt; pull(Location),\n                      labels = 1:59))\n\nglimpse(d)\n\nRows: 6,842\nColumns: 29\n$ uID              &lt;dbl&gt; 65, 68, 102, 126, 145, 263, 267, 298, 309, 318, 350, 356, 376, 431, 438, 447, 483, 485, 520, 531, 565, …\n$ variable         &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n$ factor           &lt;chr&gt; \"SideEffect\", \"SideEffect\", \"SideEffect\", \"SideEffect\", \"SideEffect\", \"SideEffect\", \"SideEffect\", \"Side…\n$ .id              &lt;chr&gt; \"ML2_Slate1_Brazil__Portuguese_execution_illegal_r.csv\", \"ML2_Slate1_Brazil__Portuguese_execution_illeg…\n$ source           &lt;chr&gt; \"brasilia\", \"brasilia\", \"brasilia\", \"wilfredlaur\", \"wilfredlaur\", \"ubc\", \"ubc\", \"toronto\", \"toronto\", \"…\n$ haus1.1          &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2…\n$ haus1.1t_1       &lt;dbl&gt; 39.054, 36.792, 56.493, 21.908, 25.635, 50.633, 58.661, 50.137, 51.717, 28.122, 41.889, 53.995, 28.380,…\n$ haus2.1          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ haus2.1t_1       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ Source.Global    &lt;chr&gt; \"brasilia\", \"brasilia\", \"brasilia\", \"wilfredlaur\", \"wilfredlaur\", \"ubc\", \"ubc\", \"toronto\", \"toronto\", \"…\n$ Source.Primary   &lt;chr&gt; \"brasilia\", \"brasilia\", \"brasilia\", \"wilfredlaur\", \"wilfredlaur\", \"ubc\", \"ubc\", \"toronto\", \"toronto\", \"…\n$ Source.Secondary &lt;chr&gt; \"brasilia\", \"brasilia\", \"brasilia\", \"wilfredlaur\", \"wilfredlaur\", \"ubc\", \"ubc\", \"toronto\", \"toronto\", \"…\n$ Country          &lt;chr&gt; \"Brazil\", \"Brazil\", \"Brazil\", \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Canada\", \"Ca…\n$ Location         &lt;chr&gt; \"Social and Work Psychology Department, University of Brasilia, DF, Brazil\", \"Social and Work Psycholog…\n$ Language         &lt;chr&gt; \"Portuguese\", \"Portuguese\", \"Portuguese\", \"English\", \"English\", \"English\", \"English\", \"English\", \"Engli…\n$ Weird            &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0…\n$ Execution        &lt;chr&gt; \"illegal\", \"illegal\", \"illegal\", \"illegal\", \"illegal\", \"illegal\", \"illegal\", \"illegal\", \"illegal\", \"ill…\n$ SubjectPool      &lt;chr&gt; \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"…\n$ Setting          &lt;chr&gt; \"In a classroom\", \"In a classroom\", \"In a classroom\", \"In a lab\", \"In a lab\", \"In a lab\", \"In a lab\", \"…\n$ Tablet           &lt;chr&gt; \"Computers\", \"Computers\", \"Computers\", \"Computers\", \"Computers\", \"Computers\", \"Computers\", \"Computers\",…\n$ Pencil           &lt;chr&gt; \"No, the whole study was on the computer (except maybe consent/debriefing)\", \"No, the whole study was o…\n$ StudyOrderN      &lt;chr&gt; \"Hauser|Ross.Slate1|Rottenstrich|Graham|Kay|Inbar|Anderson|VanLange|Huang|Bauer|Critcher|Alter|Miyamoto…\n$ IDiffOrderN      &lt;chr&gt; \"ID: Global self-esteem SISE|ID: Mood|ID: Subjective wellbeing|ID: Disgust scale (slate 1 only -- for I…\n$ study.order      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ analysis.type    &lt;chr&gt; \"Order\", \"Order\", \"Order\", \"Order\", \"Order\", \"Order\", \"Order\", \"Order\", \"Order\", \"Order\", \"Order\", \"Ord…\n$ subset           &lt;chr&gt; \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\", \"all\"…\n$ case.include     &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n$ y                &lt;dbl&gt; 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0…\n$ loc              &lt;fct&gt; 1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 3, 4, 4, 3, 5, 5, 5, 6, 6, 6…\n\n\nThe total sample size is \\(N = 6{,}842\\).\n\nd |&gt; \n  distinct(uID) |&gt; \n  count()\n\n# A tibble: 1 × 1\n      n\n  &lt;int&gt;\n1  6842\n\n\nAll cases are to be included.\n\nd |&gt; \n  count(case.include)\n\n# A tibble: 1 × 2\n  case.include     n\n  &lt;lgl&gt;        &lt;int&gt;\n1 TRUE          6842\n\n\nThe data were collected in 59 locations with sample sizes ranging from 34 to 325.\n\nd |&gt; \n  count(Location) |&gt; \n  \n  ggplot(aes(x = n)) +\n  stat_dots() +\n  scale_x_continuous(\"sample size, by location\", limits = c(0, NA)) +\n  scale_y_continuous(NULL, breaks = NULL)\n\n\n\n\n\n\n\n\n\n\n15.5.3 Our effect size will be an odds ratio\nHere’s how Klein and colleagues summarized their primary results:\n\nIn the aggregate replication sample \\((N = 6,842\\) after removing participants who responded in less than \\(4\\) s\\(),\\) \\(71\\%\\) of participants judged the action in the foreseen-side-effect scenario as permissible, but only \\(17\\%\\) of participants in the greater-good scenario judged it as permissible. The difference between the percentages was significant, \\(p = 2.2 \\text e^{-16},\\) \\(\\text{OR} = 11.54,\\) \\(d = 1.35,\\) \\(95\\% \\; \\text{CI} = [1.28, 1.41]\\). The replication results were consistent with the double-effect hypothesis, and the effect was about half the magnitude of the original \\((d = 1.35,\\) \\(95\\% \\; \\text{CI} = [1.28, 1.41],\\) vs. original \\(d = 2.50)\\). (p. 459)\n\nHere is the breakdown of the outcome and primary experimental condition, which will confirm the two empirical percentages mentioned, above.\n\nd |&gt; \n  count(variable, factor) |&gt; \n  group_by(factor) |&gt; \n  mutate(percent = 100 * n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   factor [2]\n  variable factor          n percent\n  &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;\n1 No       GreaterGood  2781    82.8\n2 No       SideEffect   1026    29.4\n3 Yes      GreaterGood   577    17.2\n4 Yes      SideEffect   2458    70.6\n\n\nThough the authors presented their overall effect size with a \\(p\\)-value, an odds-ratio (OR), and a Cohen’s \\(d\\) (i.e., a kind of standardized mean difference), we will focus on the OR. The primary data are binomial counts, which are well-handled with logistic regression. When you perform a logistic regression where a control condition is compared with some experimental condition, the difference between those conditions may be expressed as an OR. To get a sense of what that is, we’ll first practice fitting a logistic regression model with the frequentist glm() function. Here are the results based on the subset of data from the first location.\n\nglm0 &lt;- glm(\n  data = d |&gt; filter(loc == 1),\n  y ~ factor, family = binomial(logit))\n\nsummary(glm0)\n\n\nCall:\nglm(formula = y ~ factor, family = binomial(logit), data = filter(d, \n    loc == 1))\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.5404     0.3673  -4.194 2.74e-05 ***\nfactorSideEffect   2.3232     0.4754   4.887 1.02e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 139.47  on 101  degrees of freedom\nResidual deviance: 110.98  on 100  degrees of freedom\nAIC: 114.98\n\nNumber of Fisher Scoring iterations: 4\n\n\nJust like with brms, the base-R glm() function returns the results of a logistic regression model in the log-odds metric. The intercept is the log-odds probability of selecting yes in the study for participants in the GreaterGood condition. The ‘factorSideEffect’ parameter is the difference in log-odds probability for participants in the SideEffect condition. Here’s what happens when you exponentiate that coefficient.\n\ncoef(glm0)[2] |&gt; exp()\n\nfactorSideEffect \n        10.20833 \n\n\nThat, my friends, is an odds ratio (OR). Odds ratios are simply exponentiated logistic regression coefficients. The implication of this particular OR is that those in the SideEffect condition have about 10 times the odds of selecting yes compared to those in the GreaterGood condition. In the case of this subset of the data, that’s 18% yeses versus 69%, which seems like a large difference, to me.\n\nd |&gt; \n  filter(loc == 1) |&gt; \n  count(variable, factor) |&gt; \n  group_by(factor) |&gt; \n  mutate(percent = 100 * n / sum(n)) |&gt; \n  filter(variable == \"Yes\")\n\n# A tibble: 2 × 4\n# Groups:   factor [2]\n  variable factor          n percent\n  &lt;chr&gt;    &lt;chr&gt;       &lt;int&gt;   &lt;dbl&gt;\n1 Yes      GreaterGood     9    17.6\n2 Yes      SideEffect     35    68.6\n\n\n\n\n15.5.4 Log-odds, odds ratios, and modeling effect sizes\nThough it’s common for researchers to express their effect sizes as odds ratios, we don’t want to work directly with odds ratios in a meta-analysis. Why? Well, think back on why we model binomial data with the logit link. The logit link transforms a bounded \\([0, 1]\\) parameter space into an unbounded parameter space ranging from negative to positive infinity. For us Bayesians, it also provides a context in which our \\(\\beta\\) parameters are approximately Gaussian. However, when we exponentiate those approximately Gaussian log-odds coefficients, the resulting odds ratios aren’t so Gaussian any more. This is why, even if our ultimate goal is to express a meta-analytic effect as an OR, we want to work with effect sizes in the log-odds metric. It allows us to use the Bayesian meta-analytic framework outlined by Gelman and colleagues, above,\n\\[\\begin{align*}\ny_j      & \\sim \\operatorname{Normal}(\\theta_j, \\sigma_j) \\\\\n\\theta_j & \\sim \\operatorname{Normal}(\\mu, \\tau),\n\\end{align*}\\]\nwhere \\(y_j\\) is the point estimate in the \\(j\\)th study still in the log-odds scale. After fitting the model, we can then exponentiate the meta-analytic parameter \\(\\mu\\) into the OR metric.\n\n\n15.5.5 Compute the study-specific effect sizes\nOur d data from the Klein et al replication study includes the un-aggregated data from all of the study locations combined. Before we compute our meta-analysis, we’ll need to compute the study-specific effect sizes and standard errors. Here we do so within a nested tibble.\n\nlibrary(broom)\n\nglms &lt;- d |&gt; \n  select(loc, y, factor) |&gt; \n  nest(data = c(y, factor)) |&gt; \n  mutate(glm = map(data, ~update(glm0, data = .))) |&gt; \n  mutate(coef = map(glm, tidy)) |&gt; \n  select(-data, -glm) |&gt; \n  unnest(coef) |&gt; \n  filter(term == \"factorSideEffect\")\n\n# What did we do?\nglms |&gt; \n  mutate_if(is.double, round, digits = 3)\n\n# A tibble: 59 × 6\n   loc   term             estimate std.error statistic p.value\n   &lt;fct&gt; &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 1     factorSideEffect     2.32     0.475      4.89       0\n 2 2     factorSideEffect     3.64     0.644      5.64       0\n 3 3     factorSideEffect     2.37     0.399      5.96       0\n 4 4     factorSideEffect     2.24     0.263      8.54       0\n 5 5     factorSideEffect     2.02     0.505      4.00       0\n 6 6     factorSideEffect     2.49     0.571      4.36       0\n 7 7     factorSideEffect     2.53     0.658      3.84       0\n 8 8     factorSideEffect     1.78     0.459      3.87       0\n 9 9     factorSideEffect     1.81     0.378      4.79       0\n10 10    factorSideEffect     2.37     0.495      4.79       0\n# ℹ 49 more rows\n\n\nIn the estimate column we have all the \\(y_j\\) values and std.error contains the corresponding \\(\\sigma_j\\) values. Here they are in a plot.\n\ncolor &lt;- viridis_pal(option = \"C\")(7)[5]\n\nglms |&gt; \n  ggplot(aes(x = std.error, y = estimate)) +\n  geom_point(color = color) +\n  labs(x = expression(sigma[italic(j)]~(\"log-odds\")),\n       y = expression(italic(y[j])~(\"log-odds\")))\n\n\n\n\n\n\n\n\n\n\n15.5.6 Fit the Bayesian meta-analysis\nNow are data are ready, we can express our first Bayesian meta-analysis with the formula\n\\[\\begin{align*}\n\\text{estimate}_j & \\sim \\operatorname{Normal}(\\theta_j, \\; \\text{std.error}_j) \\\\\n\\theta_j   & \\sim \\operatorname{Normal}(\\mu, \\tau) \\\\\n\\mu        & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\tau       & \\sim \\operatorname{Exponential}(1),\n\\end{align*}\\]\nwhere the last two lines spell out our priors. As we learned in Section 11.1, the \\(\\operatorname{Normal}(0, 1.5)\\) prior in the log-odds space is just about flat on the probability space. If you wanted to be more conservative, consider something like \\(\\operatorname{Normal}(0, 1)\\). Here’s how to fit the model with brms.\n\nb15.10 &lt;- brm(\n  data = glms, \n  family = gaussian,\n  estimate | se(std.error) ~ 1 + (1 | loc),\n  prior = c(prior(normal(0, 1.5), class = Intercept),\n            prior(exponential(1), class = sd)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 15,\n  file = \"fits/b15.10\")\n\nse() is one of the brms helper functions designed to provide additional information about the criterion variable. Here it informs brm() that each estimate value has an associated measurement error defined in the std.error column. Unlike the mi() function, which we used earlier in the chapter to accommodate measurement error and the Bayesian imputation of missing data, the se() function is specially designed to handle meta-analyses. se() contains a sigma argument which is set to FALSE by default. This will return a model with no estimate for sigma, which is what we want. The uncertainty around the estimate-value for each study \\(j\\) has already been encoded in the data as std.error.\nLet’s look at the model results.\n\nprint(b15.10)\n\n Family: gaussian \n  Links: mu = identity \nFormula: estimate | se(std.error) ~ 1 + (1 | loc) \n   Data: glms (Number of observations: 59) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~loc (Number of levels: 59) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.43      0.09     0.26     0.61 1.00     1574     2287\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     2.55      0.09     2.37     2.73 1.00     3511     3050\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.00      0.00     0.00     0.00   NA       NA       NA\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur estimate for heterogeneity across studies, \\(\\tau\\), is about 0.4, suggesting modest differences across the studies. The meta-analytic effect, \\(\\mu\\), is about 2.5. Both, recall, are in the log-odds metric. Here we exponentiate \\(\\mu\\) to get our odds ratio.\n\nfixef(b15.10)[, -2] |&gt; exp()\n\nEstimate     Q2.5    Q97.5 \n12.75481 10.74502 15.27348 \n\n\nIf you look back up to the results reported by Klein and colleagues, you’ll see this is rather close to their OR estimate of 11.54.\n\n\n15.5.7 Fit the Bayesian muiltilevel alternative\nWe said earlier that meta-analysis is just a special case of the multilevel model, applied to summary data. We typically perform meta-analyses on data summaries because historically it has not been the norm among researchers to make their data publicly available. So effect size summaries were the best we typically had for aggregating study results. However, times are changing (e.g., here, here). In this case, Klein and colleagues engaged in open-science practices and reported all their data. Thus we can just directly fit the model\n\\[\\begin{align*}\n\\text{y}_{ij} & \\sim \\operatorname{Binomial}(n = 1, p_{ij}) \\\\\n\\operatorname{logit}(p_{ij}) & \\sim \\alpha + \\beta \\text{factor}_{ij} + u_{\\alpha j} + u_{\\beta j} \\text{factor}_{ij} \\\\\n\n\\begin{bmatrix} u_{\\alpha j} \\\\ u_{\\beta j} \\end{bmatrix} & \\sim \\operatorname{MVNormal} \\begin{pmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\mathbf{SRS} \\end{pmatrix} \\\\\n\n\\mathbf S & = \\begin{bmatrix} \\sigma_\\alpha & 0 \\\\ 0 & \\sigma_\\beta \\end{bmatrix} \\\\\n\\mathbf R & = \\begin{bmatrix} 0 & \\rho_{\\alpha \\beta} \\\\ \\rho_{\\beta \\alpha} & 0 \\end{bmatrix} \\\\\n\n\\alpha & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\beta  & \\sim \\operatorname{Normal}(0, 1.5) \\\\\n\\sigma_\\alpha & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_\\beta  & \\sim \\operatorname{Exponential}(1) \\\\\n\\mathbf R & \\sim \\operatorname{LKJ}(2),\n\\end{align*}\\]\nwhere the criterion variable, \\(y\\), is nested in \\(i\\) participants within \\(j\\) locations. The \\(\\beta\\) parameter is analogous to the meta-analytic effect (\\(\\mu\\)) and \\(\\sigma_\\beta\\) is analogous to the expression of heterogeneity in the meta-analytic effect (\\(\\tau\\)). Here is how to fit the model with brms.\n\nb15.11 &lt;- brm(\n  data = d, \n  family = binomial,\n  y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc),\n  prior = c(prior(normal(0, 1.5), class = b),\n            prior(exponential(1), class = sd),\n            prior(lkj(2), class = cor)),\n  iter = 2000, warmup = 1000, cores = 4, chains = 4,\n  seed = 15,\n  file = \"fits/b15.11\")\n\nThe results for the focal parameters are very similar to those from b15.10.\n\nprint(b15.11)\n\n Family: binomial \n  Links: mu = logit \nFormula: y | trials(1) ~ 0 + Intercept + factor + (1 + factor | loc) \n   Data: d (Number of observations: 6842) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nMultilevel Hyperparameters:\n~loc (Number of levels: 59) \n                                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)                       0.42      0.07     0.31     0.57 1.00     1555     2292\nsd(factorSideEffect)                0.49      0.09     0.33     0.67 1.00      959     1845\ncor(Intercept,factorSideEffect)    -0.32      0.19    -0.63     0.10 1.00     1343     2318\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           -1.67      0.08    -1.82    -1.52 1.00     2018     2127\nfactorSideEffect     2.57      0.09     2.39     2.75 1.00     1864     1915\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere’s the multilevel version of the effect size as an odds ratio.\n\nfixef(b15.11)[2, -2] |&gt; exp()\n\nEstimate     Q2.5    Q97.5 \n13.03125 10.88212 15.71980 \n\n\nHere we compare the study specific effect sizes, \\(\\theta_j\\), by our two modeling approaches.\n\ncolor &lt;- viridis_pal(option = \"C\")(7)[3]\n\n# How many levels are there?\nn_loc &lt;- distinct(d, loc) |&gt; count() |&gt; pull(n)\n\n# Rank by meta-analysis\nranks &lt;- tibble(Estimate = coef(b15.10)$loc[, 1, \"Intercept\"],\n                index    = 1:n_loc) |&gt; \n  arrange(Estimate) |&gt; \n  mutate(rank = 1:n_loc)\n\n# Combine parameter summaries\nrbind(coef(b15.10)$loc[, , \"Intercept\"],\n      coef(b15.11)$loc[, , \"factorSideEffect\"]) |&gt; \n  data.frame() |&gt; \n  mutate(index = rep(1:n_loc, times = 2),\n         type  = rep(c(\"meta-analysis\", \"multilevel model\"), each = n_loc)) |&gt; \n  # Add the ranks\n  left_join(select(ranks, -Estimate), \n            by = \"index\") |&gt; \n  \n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +\n  geom_pointrange(color = color, size = 1/4) +\n  scale_x_continuous(expression(log-odds~effect~size~(theta[italic(j)])), limits = c(0, 4.5)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ type)\n\n\n\n\n\n\n\n\nThe results are very similar. You might be curious how to show these results in a more conventional looking forest plot where the names of the groups (typically studies) for the \\(\\theta_j\\) values are listed on the left, the point estimate and 95% interval summaries are listed on the right, and the summary for the population level effect, \\(\\mu\\), is listed beneath all all the \\(\\theta_j\\)’s. That’ll require some prep work. First we’ll need to reformat the location names. I’ll save the results in an object called labs.\n\nlabs &lt;- d |&gt; \n  mutate(lab = case_when(\n    Location == \"Social and Work Psychology Department, University of Brasilia, DF, Brazil\" ~ \"University of Brasilia\",\n    Location == \"Wilfrid Laurier University, Waterloo, Ontario, Canada\" ~ \"Wilfrid Laurier University\",\n    Location == \"University of British Columbia, Vancouver, Canada\" ~ \"University of British Columbia\",\n    Location == \"University of Toronto, Scarborough\" ~ \"University of Toronto\",\n    Location == \"Division of Social Science, The Hong Kong University of Science and Technology, Hong Kong, China\" ~ \"Hong Kong University of Science and Technology\",\n    Location == \"Chinese Academy of Science, Beijing, China\" ~ \"Chinese Academy of Science\",\n    Location == \"Shanghai International Studies University, SISU Intercultural Institute, Shanghai, China\" ~ \"Shanghai International Studies University\",\n    Location == \"Guangdong Literature & Art Vocational College, Guangzhou, China\" ~ \"Guangdong Literature & Art Vocational College\",\n    Location == \"The University of J. E. Purkyně, Ústí nad Labem, Czech Republic\" ~ \"The University of J. E. Purkyně\",\n    Location == \"University of Leuven, Belgium\" ~ \"University of Leuven\",\n    Location == \"Department of Experimental and Applied Psychology, VU Amsterdam, 1081BT, Amsterdam, The Netherlands\" ~ \"VU Amsterdam\",\n    Location == \"Department of Social Psychology, Tilburg University, P.O. Box 90153, Tilburg, 5000 LE, Netherlands\" ~ \"Department of Social Psychology, Tilburg University\",\n    Location == \"Eindhoven University of Technology, Eindhoven, Netherlands\" ~ \"Eindhoven University of Technology\",\n    Location == \"Department of Communication and Information Sciences, P.O. Box 90153, Tilburg, 5000 LE, Netherlands\" ~ \"Department of Communication and Information Sciences, Tilburg University\",\n    Location == \"University of Navarra, Spain\" ~ \"University of Navarra\",\n    Location == \"University of Lausanne, Switzerland\" ~ \"University of Lausanne\",\n    Location == \"Université de Poitiers, France\" ~ \"Université de Poitiers\",\n    Location == \"Eotvos Lorand University, in Budapest, Hungary\" ~ \"Eotvos Lorand University\",\n    Location == \"MTurk India Workers\" ~ \"MTurk India Workers\",\n    Location == \"University of Winchester, Winchester, Hampshire, England\" ~ \"University of Winchester\",\n    Location == \"Doshisha University, Kyoto, Japan\" ~ \"Doshisha University\",\n    Location == \"Victoria University of Wellington, New Zealand\" ~ \"Victoria University of Wellington\",\n    Location == \"University of Social Sciences and Humanities, Wroclaw, Poland\" ~ \"University of Social Sciences and Humanities\",\n    Location == \"Department of Psychology, SWPS University of Social Sciences and Humanities Campus Sopot, Sopot, Poland\" ~ \"SWPS University of Social Sciences and Humanities Campus Sopot\",\n    Location == \"badania.net\" ~ \"badania.net\",\n    Location == \"Universidade do Porto, Portugal\" ~ \"Universidade do Porto\",\n    Location == \"University of Belgrade, Belgrade, Serbia\" ~ \"University of Belgrade\",\n    Location == \"University of Johannesburg, Johanneburg, South Africa\" ~ \"University of Johannesburg\",\n    Location == \"Santiago, Chile\" ~ \"Santiago, Chile\",\n    Location == \"Universidad de Costa Rica, Costa Rica\" ~ \"Universidad de Costa Rica\",\n    Location == \"National Autonomous University of Mexico in Mexico City\" ~ \"National Autonomous University of Mexico\",\n    Location == \"University of the Republic, Montevideo, Uruguay\" ~ \"University of the Republic\",\n    Location == \"Lund University, Lund, Sweden\" ~ \"Lund University\",\n    Location == \"Academia Sinica, Taiwan National Taiwan Normal University, Taiwan\" ~ \"Taiwan National Taiwan Normal University\",\n    Location == \"Bilgi University, Istanbul, Turkey\" ~ \"Bilgi University\",\n    Location == \"Koç University, Istanbul, Turkey\" ~ \"Koç University\",\n    Location == \"American University of Sharjah, United Arab Emirates\" ~ \"American University of Sharjah\",\n    Location == \"University of Hawaii, Honolulu, HI\" ~ \"University of Hawaii\",\n    Location == \"Social Science and Policy Studies Department, Worcester Polytechnic Institute, Worcester, MA 01609\" ~ \"Worcester Polytechnic Institute\",\n    Location == \"Department of Psychology, Washington and Lee University, Lexington, VA 24450\" ~ \"Washington and Lee University\",\n    Location == \"Department of Psychology, San Diego State University, San Diego, CA 92182\" ~ \"San Diego State University\",\n    Location == \"Tufts\" ~ \"Tufts\",\n    Location == \"University of Florida, Florida\" ~ \"University of Florida\",\n    Location == \"University of Illinois at Urbana-Champaign, Champaign, IL\" ~ \"University of Illinois at Urbana-Champaign\",\n    Location == \"Pacific Lutheran University, Tacoma, WA\" ~ \"Pacific Lutheran University\",\n    Location == \"University of Virginia, VA\" ~ \"University of Virginia\",\n    Location == \"Marian University, Indianapolis, IN\" ~ \"Marian University\",\n    Location == \"Department of Psychology, Ithaca College, Ithaca, NY 14850\" ~ \"Ithaca College\",\n    Location == \"University of Michigan\" ~ \"University of Michigan\",\n    Location == \"Department of Psychology, Pennsylvania State University Abington, Abington, PA 19001\" ~ \"Pennsylvania State University Abington\",\n    Location == \"Department of Psychology, Texas A&M University, College Station, TX 77843\" ~ \"Texas A&M University\",\n    Location == \"William Paterson University, Wayne, NJ\" ~ \"William Paterson University\",\n    Location == \"Department of Cognitive Science, Occidental College, Los Angeles, CA\" ~ \"Occidental College\",\n    Location == \"The Pennsylvania State University\" ~ \"The Pennsylvania State University\",\n    Location == \"MTurk US Workers\" ~ \"MTurk US Workers\",\n    Location == \"University of Graz AND the Universty of Vienna\" ~ \"University of Graz and the Universty of Vienna\",\n    Location == \"University of Potsdam, Germany\" ~ \"University of Potsdam\",\n    Location == \"Open University of Hong Kong\" ~ \"Open University of Hong Kong\",\n    Location == \"Concepción, Chile\" ~ \"Concepción\"\n  )) |&gt; \n  distinct(loc, lab)\n\n# What is this?\nlabs |&gt; \n  glimpse()\n\nRows: 59\nColumns: 2\n$ loc &lt;fct&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 3…\n$ lab &lt;chr&gt; \"University of Brasilia\", \"Wilfrid Laurier University\", \"University of British Columbia\", \"University of Toronto\", \"…\n\n\nNow we’ll do some tricky wrangling with the output from coef() and fixef() to arrange the odds ratio summaries for the population average and the location-specific results.\n\n# This will help us format the labels on the secondary y-axis\nmy_format &lt;- function(number) {\n  formatC(number, digits = 2, format = \"f\")\n}\n\n# Grab the theta_j summaries\ngroups &lt;- coef(b15.11)$loc[, , \"factorSideEffect\"] |&gt; \n  data.frame() |&gt; \n  mutate(loc = distinct(d, loc) |&gt; pull()) |&gt; \n  arrange(Estimate)\n\n# Grab the mu summary\naverage &lt;- fixef(b15.11) |&gt; \n  data.frame() |&gt; \n  slice(2) |&gt; \n  mutate(loc = \"Average\")\n\n# Combine and wrangle\npost &lt;- bind_rows(groups, average) |&gt; \n  mutate(rank     = c(1:59, 0),\n         Estimate = exp(Estimate),\n         Q2.5     = exp(Q2.5),\n         Q97.5    = exp(Q97.5)) |&gt; \n  left_join(labs, by = \"loc\") |&gt; \n  arrange(rank) |&gt; \n  mutate(label   = ifelse(is.na(lab), \"POPULATION AVERAGE\", lab),\n         summary = str_c(my_format(Estimate), \" [\", my_format(Q2.5), \", \", my_format(Q97.5), \"]\"))\n\n# What have we done?\npost |&gt; \n  glimpse()\n\nRows: 60\nColumns: 9\n$ Estimate  &lt;dbl&gt; 13.031252, 5.993160, 7.120835, 7.909707, 7.911516, 7.974898, 8.176425, 8.497452, 8.612409, 8.624163, 9.468991,…\n$ Est.Error &lt;dbl&gt; 0.09276243, 0.23584341, 0.35278999, 0.33377328, 0.36358500, 0.23316401, 0.33742529, 0.31857643, 0.34481692, 0.…\n$ Q2.5      &lt;dbl&gt; 10.882119, 3.855201, 3.484971, 4.061792, 3.810479, 5.119782, 4.151619, 4.551310, 4.327285, 4.655110, 4.920004,…\n$ Q97.5     &lt;dbl&gt; 15.719796, 9.591853, 13.626337, 15.052954, 15.904785, 12.460087, 15.742125, 15.735170, 16.899349, 15.363993, 1…\n$ loc       &lt;chr&gt; \"Average\", \"19\", \"38\", \"8\", \"32\", \"55\", \"5\", \"34\", \"22\", \"9\", \"6\", \"58\", \"24\", \"7\", \"50\", \"15\", \"4\", \"11\", \"23…\n$ rank      &lt;dbl&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, …\n$ lab       &lt;chr&gt; NA, \"MTurk India Workers\", \"University of Hawaii\", \"Guangdong Literature & Art Vocational College\", \"Universit…\n$ label     &lt;chr&gt; \"POPULATION AVERAGE\", \"MTurk India Workers\", \"University of Hawaii\", \"Guangdong Literature & Art Vocational Co…\n$ summary   &lt;chr&gt; \"13.03 [10.88, 15.72]\", \"5.99 [3.86, 9.59]\", \"7.12 [3.48, 13.63]\", \"7.91 [4.06, 15.05]\", \"7.91 [3.81, 15.90]\",…\n\n\nHere’s our custom forest plot.\n\npost |&gt; \n  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = rank)) +\n  geom_interval(aes(color = label == \"POPULATION AVERAGE\"),\n                size = 1/2) +\n  geom_point(aes(size = 1 - Est.Error, color = label == \"POPULATION AVERAGE\"),\n             shape = 15) +\n  scale_x_continuous(\"odds ratio\", breaks = 1:6 * 10, expand = expansion(mult = c(0.005, 0.005))) +\n  scale_y_continuous(NULL, breaks = 0:59, expand = c(0, 0), limits = c(-1, 60),\n                     labels = pull(post, label),\n                     sec.axis = dup_axis(labels = pull(post, summary))) +\n  scale_color_viridis_d(option = \"C\", begin = 0.33, end = 0.67) +\n  scale_size_continuous(range = c(1, 3.5)) +\n  theme(text = element_text(family = \"Times\"),\n        axis.text.y = element_text(color = \"white\", hjust = 0, size = 7),\n        axis.text.y.right = element_text(hjust = 1, size = 7),\n        axis.ticks.y = element_blank(),\n        panel.background = element_rect(fill = \"grey8\"),\n        panel.border = element_rect(color = \"transparent\"))\n\n\n\n\n\n\n\n\nYou may have noticed this plot is based on the results of our multilevel model, b15.11. We could have done the same basic thing with the results from the more conventional meta-analysis model, b15.10, too.\nI’m not aware this it typical in random effect meta-analyses, but it might be useful to further clarify the meaning of the two primary parameters, \\(\\mu\\) and \\(\\tau\\). Like with the forest plot, above, we could examine these with either b15.10 or b15.11. For kicks, we’ll use b15.10 (the conventional Bayesian meta-analysis). In the output from as_draws_df(b15.10), \\(\\mu\\) and \\(\\tau\\) are in the columns named b_Intercept and sd_loc__Intercept, respectively.\n\npost &lt;- as_draws_df(b15.10)\n\npost |&gt; \n  select(b_Intercept:sd_loc__Intercept) |&gt; \n  head()\n\n# A tibble: 6 × 2\n  b_Intercept sd_loc__Intercept\n        &lt;dbl&gt;             &lt;dbl&gt;\n1        2.58             0.448\n2        2.48             0.293\n3        2.59             0.246\n4        2.60             0.322\n5        2.57             0.452\n6        2.42             0.491\n\n\nIf you scroll back above, you’ll see our random effect meta-analysis explicitly presumed our empirical effect-size estimates \\(y_j\\) are approximations of the true effect sizes \\(\\theta_j\\), which are themselves normally distributed in the population of possible effect sizes from similar studies: \\(\\theta_j \\sim \\operatorname{Normal}(\\mu, \\tau)\\). Why not use our posterior draws to simulate draws from \\(\\operatorname{Normal}(\\mu, \\tau)\\) to get a sense of what this distribution might look like? Recall that the parameters are in the log-odds metric. We’ll present the distribution in that metric and as odds ratios.\n\ncolor &lt;- viridis_pal(option = \"C\")(7)[6]\nset.seed(15)\n\npost |&gt; \n  transmute(lo = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept),\n            or = rnorm(n(), mean = b_Intercept, sd = sd_loc__Intercept) |&gt; exp()) |&gt; \n  slice(1:1e3) |&gt; \n  pivot_longer(lo:or, values_to = \"effect size\") |&gt; \n  mutate(name = factor(name, labels = c(\"log-odds\", \"odds ratio\"))) |&gt; \n  \n  ggplot(aes(x = `effect size`, y = 0)) +\n  geom_dots(color = color, fill = color) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(Normal(mu*', '*tau))) +\n  facet_wrap(~ name, scales = \"free\") +\n  theme(text = element_text(family = \"Times\"),\n        strip.background = element_rect(color = \"transparent\"))\n\n\n\n\n\n\n\n\nBoth panels show 1,000 draws, each of which is depicted by a single dot. If we were to run this experiment 1,000 times and compute the effect size separately for each one, this is what we’d expect those distributions of effect sizes to look like. Seems like there’s a lot of variation in there, eh? The next time you observe your fellow scientists debating over whether a study replicated or not, keep these distributions in mind. Once you start thinking about distributions, replication becomes a tricky notion.\n\n\n15.5.8 Relative to the muiltilevel model, meta-analyses have limitations\nAlthough the Bayesian meta-analysis and multilevel models produced very similar results for the effect size, the two are not equivalent. Because the meta-analysis was based on summary information from the effect sizes along, it does not offer all of the insights available from the multilevel model. Other than the random effects, the only population parameters offered by the meta-analysis are the effect size, \\(\\mu\\), and the standard deviation around that effect size, \\(\\tau\\). These parameters are equivalent to our multilevel model parameters \\(\\beta\\) and \\(\\sigma_\\beta\\), respectively. In addition, the multilevel model added group-level parameter for the average performance in the control condition, \\(\\alpha\\), the population-level standard deviation around that mean, \\(\\sigma_\\alpha\\), and the correlation between the group-specific \\(\\alpha_j\\)’s and \\(\\beta_j\\)’s, which we called \\(\\rho_{\\alpha \\beta}\\).\nTo build a sense of why we might to want these parameters, too, we’ll extract the posterior draws for the multilevel model b15.11.\n\npost &lt;- as_draws_df(b15.11)\n\nHere we’ll make a scatter plot the posterior means of the random intercepts, \\(\\alpha_j\\)’s, and the random slopes, \\(\\beta_j\\)’s.\n\ncolor &lt;- viridis_pal(option = \"C\")(7)[7]\n\nc &lt;- full_join(\n  # Intercepts\n  post |&gt; \n    select(`r_loc[1,Intercept]`:`r_loc[59,Intercept]`) |&gt; \n    set_names(1:59) |&gt; \n    mutate(iter = 1:n()) |&gt; \n    pivot_longer(-iter, values_to = \"intercepts\"),\n  # Slopes\n  post |&gt; \n    select(`r_loc[1,factorSideEffect]`:`r_loc[59,factorSideEffect]`) |&gt; \n    set_names(1:59) |&gt; \n    mutate(iter = 1:n()) |&gt; \n    pivot_longer(-iter, values_to = \"slopes\"),\n  by = c(\"iter\", \"name\")\n)\n\n# Summarize by the means\nc |&gt; \n  group_by(name) |&gt; \n  summarise(intercepts = mean(intercepts),\n            slopes = mean(slopes)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = intercepts, y = slopes)) +\n  geom_point(color = color) +\n  labs(x = expression(alpha[italic(j)]),\n       y = expression(beta[italic(j)]))\n\n\n\n\n\n\n\n\nThis provides some information, which we wouldn’t get from a meta-analysis. However, the plot is a little deceiving. First, it doesn’t provide an expression of the uncertainty on the effects. Second, it doesn’t give full justice to the correlation between the two effects, \\(\\rho_{\\alpha \\beta}\\). If you look back up at the print() output from Section 15.5.7, you’ll see that correlation was about -0.32. We can take some cues from Section 14.4 and express the bivariate posteriors as ellipses, rather than simple mean-level points.\n\nc |&gt; \n  ggplot(aes(x = intercepts, y = slopes)) +\n  stat_ellipse(aes(group = name),\n               geom = \"polygon\", \n               alpha = 1/2, fill = color, \n               level = 0.05, linewidth = 0) +\n  stat_ellipse(level = 0.95, color = color) +\n  labs(x = expression(alpha[italic(j)]),\n       y = expression(beta[italic(j)]))\n\n\n\n\n\n\n\n\nThe small semitransparent ellipses depict the inner-most 5% of each of the \\(j\\)-level bivariate posteriors. The larger ellipse is the 95% boundary, collapsing across all levels of \\(j\\). Both kinds of summaries help depict the negative correlation between the \\(\\alpha_j\\)’s and \\(\\beta_j\\)’s. That negative correlation means suggests the studies which had larger effect sizes for the control condition tended to have smaller effect sizes for the experimental condition. There’s no way you would know that from the meta-analysis approach.\nAnother way to explore the relation between the model intercepts and slopes is to plot their implications on the probability scale. The way we parameterized the model, the greater-good condition is expressed the \\(\\alpha_j\\) and the side-effect condition is expressed as \\(\\alpha_j + \\beta_j\\). If we were working directly with the as_draws_df() output, we’d have to do fancy conversions with the plogis() function. An easier way is to just use fitted().\n\nbreaks &lt;- c(0, plogis(c(fixef(b15.11)[1, 1], fixef(b15.11)[1, 1] + fixef(b15.11)[2, 1])), 1)\n\nnd &lt;- d |&gt; distinct(loc, factor)\n\nfitted(b15.11, newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd) |&gt; \n  mutate(factor = if_else(factor == \"SideEffect\", \"Side effect\", \"Greater good\")) |&gt; \n  \n  ggplot(aes(x = factor, y = Estimate, group = loc)) +\n  geom_line(alpha = 1/2, color = color) +\n  scale_y_continuous(\"probability\", breaks = breaks,\n                     labels = round(breaks, digits = 2), limits = 0:1) +\n  xlab(NULL) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\nEach of the 59 levels of our grouping variable loc is depicted in one of those 59 lines. Sadly, this approach doesn’t do a good job expressing the uncertainty in those lines. Each one is based on the posterior mean. Regardless, this plot offers more insights that were not available from the meta-analysis.\n\n\n15.5.9 Parting thoughts\nThere are other things you might do with these data. For example, you might inspect how much the effect size varies between those from WEIRD and non-WEIRD countries. You might also model the data as clustered by Language rather than by Location. But I think we’ve gone far enough to get you started.\nIf you’d like to learn more about these methods, do check out Vourre’s Bayesian meta-analysis with R, Stan, and brms. You might also read Williams, Rast, and Bürkner’s (2018) manuscript, Bayesian meta-analysis with weakly informative prior distributions. For an alternative workflow, consider the baggr package (Wiecek & Meager, 2022), which is designed to fit hierarchical Bayesian meta-analyses with Stan under the hood. Also, I’m not aware that brms currently offers a convenient way to fit a meta-analysis model that takes selection bias into account. However, the thread in the Stan forums called Bayesian random-effects meta-analysis with selection models (publication bias) provides the Stan code for such a model.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Missing Data and Other Opportunities</span>"
    ]
  },
  {
    "objectID": "15.html#session-info",
    "href": "15.html#session-info",
    "title": "15  Missing Data and Other Opportunities",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] broom_1.0.10      tidybayes_3.0.7   ggrepel_0.9.6     brms_2.23.0       Rcpp_1.1.0        ggdag_0.2.13      patchwork_1.3.2  \n [8] viridis_0.6.5     viridisLite_0.4.2 lubridate_1.9.4   forcats_1.0.1     stringr_1.6.0     dplyr_1.1.4       purrr_1.2.1      \n[15] readr_2.1.5       tidyr_1.3.2       tibble_3.3.1      ggplot2_4.0.1     tidyverse_2.0.0  \n\nloaded via a namespace (and not attached):\n [1] gridExtra_2.3           inline_0.3.21           sandwich_3.1-1          rlang_1.1.7             magrittr_2.0.4         \n [6] multcomp_1.4-29         matrixStats_1.5.0       compiler_4.5.1          loo_2.9.0.9000          vctrs_0.7.0            \n[11] reshape2_1.4.5          quadprog_1.5-8          crayon_1.5.3            pkgconfig_2.0.3         arrayhelpers_1.1-0     \n[16] fastmap_1.2.0           backports_1.5.0         labeling_0.4.3          ggraph_2.2.2            utf8_1.2.6             \n[21] rmarkdown_2.30          tzdb_0.5.0              bit_4.6.0               xfun_0.55               cachem_1.1.0           \n[26] jsonlite_2.0.0          tweenr_2.0.3            parallel_4.5.1          R6_2.6.1                stringi_1.8.7          \n[31] RColorBrewer_1.1-3      StanHeaders_2.36.0.9000 boot_1.3-31             estimability_1.5.1      assertthat_0.2.1       \n[36] rstan_2.36.0.9000       knitr_1.51              zoo_1.8-14              bayesplot_1.15.0.9000   Matrix_1.7-3           \n[41] splines_4.5.1           igraph_2.2.0            timechange_0.3.0        tidyselect_1.2.1        rstudioapi_0.17.1      \n[46] abind_1.4-8             yaml_2.3.12             codetools_0.2-20        curl_7.0.0              dagitty_0.3-4          \n[51] pkgbuild_1.4.8          lattice_0.22-7          plyr_1.8.9              withr_3.0.2             bridgesampling_1.2-1   \n[56] S7_0.2.1                posterior_1.6.1.9000    coda_0.19-4.1           evaluate_1.0.5          survival_3.8-3         \n[61] RcppParallel_5.1.11-1   polyclip_1.10-7         ggdist_3.3.3            pillar_1.11.1           tensorA_0.36.2.1       \n[66] checkmate_2.3.3         stats4_4.5.1            distributional_0.5.0    generics_0.1.4          vroom_1.6.6            \n[71] hms_1.1.4               rstantools_2.5.0.9000   scales_1.4.0            xtable_1.8-4            emo_0.0.0.9000         \n[76] glue_1.8.0              emmeans_1.11.2-8        tools_4.5.1             mvtnorm_1.3-3           graphlayouts_1.2.2     \n[81] tidygraph_1.3.1         grid_4.5.1              QuickJSR_1.8.1          nlme_3.1-168            ggforce_0.5.0          \n[86] cli_3.6.5               svUnit_1.0.8            Brobdingnag_1.2-9       V8_8.0.1                gtable_0.3.6           \n[91] digest_0.6.39           TH.data_1.1-4           htmlwidgets_1.6.4       farver_2.1.2            memoise_2.0.1          \n[96] htmltools_0.5.9         lifecycle_1.0.5         bit64_4.6.0-1           MASS_7.3-65",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Missing Data and Other Opportunities</span>"
    ]
  },
  {
    "objectID": "15.html#comments",
    "href": "15.html#comments",
    "title": "15  Missing Data and Other Opportunities",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBaraldi, A. N., & Enders, C. K. (2010). An introduction to modern missing data analyses. Journal of School Psychology, 48(1), 5–37. https://doi.org/10.1016/j.jsp.2009.10.001\n\n\nBeheim, B., Atkinson, Q. D., Bulbulia, J., Gervais, W., Gray, R. D., Henrich, J., Lang, M., Monroe, M. W., Muthukrishna, M., Norenzayan, A., et al. (2021). Treatment of missing data determined conclusions regarding moralizing gods. Nature, 595(7866), E29–E34. https://doi.org/10.1038/s41586-019-1043-4\n\n\nBürkner, P.-C. (2022a). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html\n\n\nBürkner, P.-C. (2022b). Handle missing values with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html\n\n\nEnders, C. K. (2022). Applied missing data analysis (Second Edition). Guilford Press. http://www.appliedmissingdata.com/\n\n\nGarnier, S. (2021). viridis: Default color maps from ’matplotlib’ [Manual]. https://CRAN.R-project.org/package=viridis\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/\n\n\nGrantham, N. (2019). ggdark: Dark mode for ’ggplot2’ themes [Manual]. https://CRAN.R-project.org/package=ggdark\n\n\nHauser, M., Cushman, F., Young, L., Jin, R. K.-X., & Mikhail, J. (2007). A dissociation between moral judgments and justifications. Mind & Language, 22(1), 1–21. https://doi.org/10.1111/j.1468-0017.2006.00297.x\n\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A. (2018). Many Labs 2: Investigating variation in replicability across samples and settings. Advances in Methods and Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\n\nLittle, R. J. A., & Rubin, D. B. (2019). Statistical analysis with missing data. John Wiley & Sons. https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nRubin, Donald B. (1976). Inference and missing data. Biometrika, 63(3), 581–592. https://doi.org/10.1093/biomet/63.3.581\n\n\nRubin, Donald B. (1987). Multiple imputation for nonresponse in surveys. John Wiley & Sons Inc. https://doi.org/10.1002/9780470316696\n\n\nRubin, Donald B. (1996). Multiple imputation after 18+ years. Journal of the American Statistical Association, 91(434), 473–489. https://doi.org/10.1080/01621459.1996.10476908\n\n\nRudis, B., Ross, N., & Garnier, S. (2018). The viridis color palettes. https://cran.r-project.org/package=viridis/vignettes/intro-to-viridis.html\n\n\nvan Buuren, S. (2018). Flexible imputation of missing data (Second Edition). CRC Press. https://stefvanbuuren.name/fimd/\n\n\nWhitehouse, H., François, P., Savage, P. E., Currie, T. E., Feeney, K. C., Cioni, E., Purcell, R., Ross, R. M., Larson, J., Baines, J., ter Haar, B., Covey, A., & Turchin, P. (2019). Complex societies precede moralizing gods throughout world history. Nature, 568(7751), 226–229. https://doi.org/10.1038/s41586-019-1043-4\n\n\nWiecek, W., & Meager, R. (2022). baggr: Bayesian aggregate treatment effects [Manual]. https://CRAN.R-project.org/package=baggr\n\n\nWilliams, D. R., Rast, P., & Bürkner, P.-C. (2018). Bayesian meta-analysis with weakly informative prior distributions. https://doi.org/10.31234/osf.io/7tbrm",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Missing Data and Other Opportunities</span>"
    ]
  },
  {
    "objectID": "16.html",
    "href": "16.html",
    "title": "16  Generalized Linear Madness",
    "section": "",
    "text": "16.0.0.1 Rethinking: Bespoken for\nMcElreath then reported he was going to work with Stan code, via rstan::stan(), in this chapter because of the unique demands of some of the models. Our approach will be mixed. We can fit at least a few of the models with brms, particularly with help from the non-linear syntax. However, some of the models to come are either beyond the current scope of brms or are at least beyond my current skill set. In those cases, we’ll follow McElreath’s approach and fit the models with stan().",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear Madness</span>"
    ]
  },
  {
    "objectID": "16.html#geometric-people",
    "href": "16.html#geometric-people",
    "title": "16  Generalized Linear Madness",
    "section": "16.1 Geometric people",
    "text": "16.1 Geometric people\n\nBack in Chapter 4, you met linear regression in the context of building a predictive model of height using weight. You even saw how to measure non-linear associations between the two variables. But nothing in that example was scientifically satisfying. The height-weight model was just a statistical device. It contains no biological information and tells us nothing about how the association between height and weight arises. Consider for example that weight obviously does not cause height, at least not in humans. If anything, the causal relationship is the reverse.\nSo now let’s try to do better. Why? Because when the model is scientifically inspired, rather than just statistically required, disagreements between model and data are informative of real causal relationships.\nSuppose for example that a person is shaped like a cylinder. Of course a person isn’t exactly shaped like a cylinder. There are arms and a head. But let’s see how far this cylinder model gets us. The weight of the cylinder is a consequence of the volume of the cylinder. And the volume of the cylinder is a consequence of growth in the height and width of the cylinder. So if we can relate the height to the volume, then we’d have a model to predict weight from height. (p. 526, emphasis in the original)\n\n\n16.1.1 The scientific model\nIf we let \\(V\\) stand for volume, \\(r\\) stand for a radius, and \\(h\\) stand for height, we can solve for volume by\n\\[V = \\pi r^2 h.\\]\nIf we further presume a person’s radius is unknown, but some proportion (\\(p\\)) of height (\\(ph\\)), we can rewrite the formula as\n\\[\\begin{align*}\nV & = \\pi (ph)^2 h \\\\\n  & = \\pi p^2 h^3.\n\\end{align*}\\]\nThough we’re not interested in volume per se, we might presume weight is some proportion of volume. Thus we could include a final parameter \\(k\\) to stand for the conversion form weight to volume, leaving us with the formula\n\\[W = kV = k \\pi p^2 h^3,\\]\nwhere \\(W\\) denotes weight.\n\n\n16.1.2 The statistical model\nFor one last time, together, let’s load the Howell1 data.\n\nlibrary(tidyverse)\ndata(Howell1, package = \"rethinking\")\nd &lt;- Howell1\nrm(Howell1)\n\n# Scale observed variables\nd &lt;- d |&gt; \n  mutate(w = weight / mean(weight),\n         h = height / mean(height))\n\nMcElreath’s proposed statistical model follows the form\n\\[\\begin{align*}\n\\text{w}_i  & \\sim \\operatorname{Log-Normal}(\\mu_i, \\sigma) \\\\\n\\exp(\\mu_i) & = k \\pi p^2 \\text{h}_i^3 \\\\\nk      & \\sim \\operatorname{Exponential}(0.5) \\\\\np      & \\sim \\operatorname{Beta}(2, 18) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1), && \\text{where} \\\\\n\\text w_i & = \\text{weight}_i \\big / \\overline{\\text{weight}}, && \\text{and} \\\\\n\\text h_i & = \\text{height}_i \\big / \\overline{\\text{height}}.\n\\end{align*}\\]\nThe Log-Normal likelihood ensures the predictions for \\(\\text{weight}_i\\) will always be non-negative. Because our parameter \\(p\\) is the ratio of radius to height, \\(p = r / h\\), it must be positive. Since people are typically taller than their width, it should also be less than one, and probably substantially less than that. Our next step will be taking a look at our priors.\nFor the plots in this chapter, we’ll give a nod the minimalistic plots in the authoritative text by Gelman et al. (2013), Bayesian data analysis: Third edition. Just to be a little kick, we’ll set the font to family = \"Times\". Most of the adjustments will come from ggthemes::theme_base().\n\nlibrary(ggthemes)\n\ntheme_set(\n  theme_base(base_size = 12) +\n    theme(text = element_text(family = \"Times\"),\n          axis.text = element_text(family = \"Times\"),\n          axis.ticks = element_line(linewidth = 0.25),\n          axis.ticks.length = unit(0.1, \"cm\"),\n          panel.background = element_rect(fill = \"white\", linewidth = 0.1),\n          plot.background = element_blank(),\n          strip.background = element_blank()\n          )\n  )\n\nNow we have our theme, let’s get a sense of our priors.\n\nlibrary(tidybayes)\nlibrary(brms)\n\nc(prior(beta(2, 18), nlpar = p, coef = italic(p)),\n  prior(exponential(0.5), nlpar = p, coef = italic(k)),\n  prior(exponential(1), class = sigma, coef = sigma)) |&gt; \n  \n  parse_dist(prior) |&gt;\n  \n  ggplot(aes(y = 0, dist = .dist, args = .args)) +\n  stat_dist_halfeye(.width = 0.5, \n                    n = 2e3, normalize = \"xy\",\n                    p_limits = c(0, 0.9995), size = 1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(theta)) +\n  facet_wrap(~ coef, scales = \"free_x\", labeller = label_parsed)\n\n\n\n\n\n\n\n\nHere the points are the posterior medians and the horizontal lines the quantile-based 50% intervals. Turns out that \\(\\operatorname{Beta}(2, 18)\\) prior for \\(p\\) pushes the bulk of the prior mass down near zero. The beta distribution also forces the parameter space for \\(p\\) to range between 0 and 1. If we denote the two parameters of the beta distribution as \\(\\alpha\\) and \\(\\beta\\), we can compute the mean for any beta distribution as \\(\\alpha / (\\alpha + \\beta)\\). Thus the mean for our \\(\\operatorname{Beta}(2, 18)\\) prior is \\(2 / (2 + 18) = 2 / 20 = 0.1\\).\nBecause we computed our weight and height variables, w and h, by dividing the original variables by their respective means, each now has a mean of 1.\n\nd |&gt; \n  pivot_longer(w:h) |&gt; \n  group_by(name) |&gt; \n  summarise(mean = mean(value))\n\n# A tibble: 2 × 2\n  name   mean\n  &lt;chr&gt; &lt;dbl&gt;\n1 h         1\n2 w         1\n\n\nHere’s their bivariate distribution in a scatter plot.\n\nd |&gt; \n  ggplot(aes(x = h, y = w)) +\n  geom_vline(xintercept = 1, color = \"grey50\", linetype = 2, linewidth = 1/4) +\n  geom_hline(yintercept = 1, color = \"grey50\", linetype = 2, linewidth = 1/4) +\n  geom_point(size = 1/4)\n\n\n\n\n\n\n\n\nWith this scaling, here is the formula for an individual with average weight and height:\n\\[\\begin{align*}\n1 & = k \\pi p^2 1^3 \\\\\n  & = k \\pi p^2.\n\\end{align*}\\]\nIf you assume \\(p &lt; .5\\), \\(k\\) must be greater than 1. \\(k\\) also has to be positive. To get a sense of this, we can further work the algebra:\n\\[\\begin{align*}\n1 & = k \\pi p^2 \\\\\n1/k  & = \\pi p^2 \\\\\nk  & = 1 / \\pi p^2.\n\\end{align*}\\]\nTo get a better sense of that relation, we might plot.\n\ntibble(p = seq(from = 0.001, to = 0.499, by = 0.001)) |&gt; \n  mutate(k = 1 / (pi * p^2)) |&gt; \n  \n  ggplot(aes(x = p, y = k)) +\n  geom_line() +\n  labs(x = expression(italic(p)),\n       y = expression(italic(k))) +\n  coord_cartesian(ylim = c(0, 500))\n\n\n\n\n\n\n\n\nMcElreath’s quick and dirty solution was to set \\(k \\sim \\operatorname{Exponential}(0.5)\\), which has a prior predictive mean of 2.\nBy setting up his model formula as exp(mu) = ..., McElreath effectively used the log link. It turns out that brms only supports the identity and inverse links for family = lognormal. However, we can sneak in the log link by nesting the right-hand side of the formula within log().\n\nb16.1 &lt;- brm(\n  data = d,\n  family = lognormal,\n  bf(w ~ log(3.141593 * k * p^2 * h^3),\n     k + p ~ 1,\n     nl = TRUE),\n  prior = c(prior(beta(2, 18), nlpar = p, lb = 0, ub = 1),\n            prior(exponential(0.5), nlpar = k, lb = 0),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 16,\n  file = \"fits/b16.01\")\n\nCheck the parameter summary.\n\nprint(b16.1)\n\n Family: lognormal \n  Links: mu = identity \nFormula: w ~ log(3.141593 * k * p^2 * h^3) \n         k ~ 1\n         p ~ 1\n   Data: d (Number of observations: 544) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nk_Intercept     5.71      2.78     2.07    13.08 1.00     1015     1069\np_Intercept     0.25      0.06     0.15     0.38 1.00     1015     1106\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.21      0.01     0.19     0.22 1.00     1529     1373\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nMcElreath didn’t show the parameter summary for his m16.1 in the text. If you fit the model with both rethinking and brms, you’ll see our b16.1 matches up quite well. To make our version of Figure 16.2, we’ll use a GGally::ggpairs() workflow. First we’ll save our customizes settings for the three subplot types.\n\nmy_lower &lt;- function(data, mapping, ...) {\n  \n  # Get the x and y data to use the other code\n  x &lt;- eval_data_col(data, mapping$x)\n  y &lt;- eval_data_col(data, mapping$y)\n  \n  # Compute the correlations\n  corr &lt;- cor(x, y, method = \"p\", use = \"pairwise\")\n  abs_corr &lt;- abs(corr)\n  \n  # Plot the cor value\n  ggally_text(\n    label = formatC(corr, digits = 2, format = \"f\") |&gt; str_replace(\"0.\", \".\"),\n    mapping = aes(), \n    color = \"black\",\n    family = \"Times\",\n    size = 3.5) +\n    scale_x_continuous(NULL, breaks = NULL) +\n    scale_y_continuous(NULL, breaks = NULL)\n}\n\nmy_diag &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_histogram(bins = 20, color = \"white\", fill = \"grey67\", linewidth = 1/4) +\n    scale_x_continuous(NULL, breaks = NULL) +\n    scale_y_continuous(NULL, breaks = NULL)\n}\n\nmy_upper &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_point(alpha = 1/4, size = 1/4) +\n    scale_x_continuous(NULL, breaks = NULL) +\n    scale_y_continuous(NULL, breaks = NULL)\n}\n\nNow we make our version of Figure 16.2.a.\n\nlibrary(GGally)\n\nas_draws_df(b16.1) |&gt; \n  select(b_k_Intercept:sigma) |&gt; \n  set_names(c(\"italic(k)\", \"italic(p)\", \"sigma\")) |&gt; \n  ggpairs(upper = list(continuous = my_upper),\n          diag = list(continuous = my_diag),\n          lower = list(continuous = my_lower),\n          labeller = label_parsed) +\n  theme(strip.text = element_text(size = 8),\n        strip.text.y = element_text(angle = 0))\n\n\n\n\n\n\n\n\nWe see the lack of identifiability of \\(k\\) and \\(p\\) resulted in a strong inverse relation between them. Now here’s how we might make Figure 16.2.b.\n\nnd &lt;- tibble(h = seq(from = 0, to = 1.5, length.out = 50))\n\np &lt;- predict(b16.1, newdata = nd) |&gt; \n  data.frame() |&gt; \n  bind_cols(nd)\n\nd |&gt; \n  ggplot(aes(x = h)) +\n  geom_smooth(data = p,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              color = \"black\", fill = \"grey67\", linewidth = 1/4) +\n  geom_point(aes(y = w),\n             size = 1/3) +\n  coord_cartesian(xlim = c(0, max(d$h)),\n                  ylim = c(0, max(d$w))) +\n  labs(x = \"height (scaled)\",\n       y = \"weight (scaled)\")\n\n\n\n\n\n\n\n\nOverall the model did okay, but the poor fit for the cases with lower values of height and weight suggests we might be missing important differences between children and adults.\n\n\n16.1.3 GLM in disguise\nRecall that because brms does not support the log link for the Log-Normal likelihood, we recast our b16.1 likelihood as\n\\[\\begin{align*}\n\\text{w}_i & \\sim \\operatorname{Log-Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i      & = \\log(k \\pi p^2 \\text{h}_i^3).\n\\end{align*}\\]\nBecause multiplication becomes addition on the log scale, we can also express this as\n\\[\\begin{align*}\n\\text{w}_i & \\sim \\operatorname{Log-Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i      & = \\log(k) + \\log(\\pi) + 2 \\log(p) + 3 \\log(\\text{h}_i),\n\\end{align*}\\]\nwhich means our fancy non-linear model is just linear regression on the log scale. McElreath pointed this out\n\nto highlight one of the reasons that generalized linear models are so powerful. Lots of natural relationships are GLM relationships, on a specific scale of measurement. At the same time, the GLM approach wants to simply estimate parameters which may be informed by a proper theory, as in this case. (p. 531)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear Madness</span>"
    ]
  },
  {
    "objectID": "16.html#hidden-minds-and-observed-behavior",
    "href": "16.html#hidden-minds-and-observed-behavior",
    "title": "16  Generalized Linear Madness",
    "section": "16.2 Hidden minds and observed behavior",
    "text": "16.2 Hidden minds and observed behavior\nLoad the Boxes data (van Leeuwen et al., 2018).\n\ndata(Boxes, package = \"rethinking\")\nd &lt;- Boxes\nrm(Boxes)\n\nrethinking::precis(d)\n\n                    mean        sd 5.5% 94.5%      histogram\ny              2.1208267 0.7279860    1     3     ▃▁▁▁▇▁▁▁▁▅\ngender         1.5055644 0.5003669    1     2     ▇▁▁▁▁▁▁▁▁▇\nage            8.0302067 2.4979055    5    13     ▇▃▅▃▃▃▂▂▂▁\nmajority_first 0.4848967 0.5001696    0     1     ▇▁▁▁▁▁▁▁▁▇\nculture        3.7519873 1.9603189    1     8 ▃▂▁▇▁▂▁▂▁▂▁▁▁▁\n\n\nThe data are from 629 children.\n\nd |&gt; count()\n\n    n\n1 629\n\n\nTheir ages ranged from 4 to 14 years and, as indicated by the histogram above, the bulk were on the younger end.\n\nd |&gt; \n  summarise(min = min(age),\n            max = max(age))\n\n  min max\n1   4  14\n\n\nHere’s a depiction of our criterion variable y.\n\n# Wrangle\nd |&gt; \n  mutate(color = factor(y,\n                        levels = 1:3,\n                        labels = c(\"unchosen\", \"majority\", \"minority\"))) |&gt;\n  mutate(y = str_c(y, \" (\", color, \")\")) |&gt; \n  count(y) |&gt; \n  mutate(percent = (100 * n / sum(n))) |&gt; \n  mutate(label = str_c(round(percent, digits = 1), \"%\")) |&gt; \n  \n  # Plot\n  ggplot(aes(y = y)) +\n  geom_col(aes(x = n)) +\n  geom_text(aes(x = n + 4, label = label),\n            family = \"Times\", hjust = 0) +\n  scale_x_continuous(expression(italic(n)), expand = expansion(mult = c(0, 0.1))) +\n  labs(y = NULL,\n       title = \"The criterion variable y indexes three kinds of color choices\",\n       subtitle = \"Children tended to prefer the 'majority' color.\") +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\n16.2.1 The scientific model\n\nThe key, as always, is to think generatively. Consider for example a group of children in which half of them choose at random and the other half follow the majority. If we simulate choices for these children, we can figure out how often we might see the “2” choice, the one that indicates the majority color. (p. 532)\n\nHere’s an alternative way to set up McEreath’s R code 16.6.\n\nset.seed(16)\n\nn &lt;- 30  # Number of children\n\ntibble(name  = rep(c(\"y1\", \"y2\"), each = n / 2),\n       value = c(sample(1:3, size = n / 2, replace = T),\n                 rep(2, n / 2))) |&gt; \n  mutate(y = sample(value)) |&gt; \n  summarise(`number of 2s` = sum(y == 2) / n)\n\n# A tibble: 1 × 1\n  `number of 2s`\n           &lt;dbl&gt;\n1          0.633\n\n\n\nWe’ll consider 5 different strategies children might use.\n\nFollow the Majority: Copy the majority demonstrated color.\nFollow the Minority: Copy the minority demonstrated color.\nMaverick: Choose the color that no demonstrator chose.\nRandom: Choose a color at random, ignoring the demonstrators.\nFollow First: Copy the color that was demonstrated first. This was either the majority color (when majority_first equals 1) or the minority color (when 0). (p. 533)\n\n\n\n\n16.2.2 The statistical model\nOur statistical will follow the form\n\\[\\begin{align*}\ny_i & \\sim \\operatorname{Categorical}(\\theta) \\\\\n\\theta_j & = \\sum_{s = 1}^5 p_s \\Pr(j \\mid s) \\;\\;\\; \\text{for} \\; j = 1, \\dots, 3 \\\\\np & \\sim \\operatorname{Dirichlet}(4, 4, 4, 4, 4),\n\\end{align*}\\]\nwhere \\(s\\) indexes one of our five latent strategies and \\(\\Pr(j \\mid s)\\) is the probability of one of the three color choices given a child is using the \\(s\\)th strategy. The \\(\\sum_{s = 1}^5 p_s\\) portion conveys these five probabilities are treated as a simplex, which means they will sum to one. We give these probabilities a Dirichlet prior, which we learned about back in Section 12.4. Here’s what that prior looks like.\n\nset.seed(16)\n\nrdirichlet(n = 1e5, alpha = rep(4, times = 5)) |&gt; \n  data.frame() |&gt; \n  set_names(1:5) |&gt; \n  pivot_longer(everything()) |&gt; \n  mutate(name  = name |&gt; as.double(),\n         alpha = str_c(\"alpha[\", name, \"]\")) |&gt; \n  \n  ggplot(aes(x = value, group = name)) + \n  geom_histogram(fill = \"grey67\", binwidth = 0.02, boundary = 0) +\n  scale_x_continuous(expression(italic(p[s])), breaks = c(0, 0.2, 0.5, 1), \n                     labels = c(0, 0.2, 0.5, 1), limits = c(0, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(subtitle = expression(\"Dirichlet\"*(4*\", \"*4*\", \"*4*\", \"*4*\", \"*4))) +\n  facet_wrap(~ alpha, labeller = label_parsed, nrow = 1)\n\n\n\n\n\n\n\n\n\n\n16.2.3 Coding the statistical model\nLet’s take a look at McElreath’s Stan code.\n\nlibrary(rethinking)\ndata(Boxes_model) \ncat(Boxes_model)\n\n\ndata{\n    int N;\n    int y[N];\n    int majority_first[N];\n}\nparameters{\n    simplex[5] p;\n}\nmodel{\n    vector[5] phi;\n    \n    // prior\n    p ~ dirichlet( rep_vector(4,5) );\n    \n    // probability of data\n    for ( i in 1:N ) {\n        if ( y[i]==2 ) phi[1]=1; else phi[1]=0; // majority\n        if ( y[i]==3 ) phi[2]=1; else phi[2]=0; // minority\n        if ( y[i]==1 ) phi[3]=1; else phi[3]=0; // maverick\n        phi[4]=1.0/3.0;                         // random\n        if ( majority_first[i]==1 )             // follow first\n            if ( y[i]==2 ) phi[5]=1; else phi[5]=0;\n        else\n            if ( y[i]==3 ) phi[5]=1; else phi[5]=0;\n        \n        // compute log( p_s * Pr(y_i|s )\n        for ( j in 1:5 ) phi[j] = log(p[j]) + log(phi[j]);\n        // compute average log-probability of y_i\n        target += log_sum_exp( phi );\n    }\n}\n\n\nI’m not aware that one can fit this model directly with brms. My guess is that if it’s possible, it would require a custom likelihood (see Bürkner, 2022). If you can reproduce McElreath’s Stan code with this or some other approach in brms, please share your code on GitHub. In the mean time, we’re going to follow along with the text and fit the model with rstan::stan().\nHowever, we do need to discuss McElreath’s Boxes_model code. The recommended syntax for Stan has evolved over time, and McElreath’s Boxes_model uses some older conventions that are no longer supported. If you try fitting it now (01-2026), you’ll probably get a warning message. Based on the discussion in this GitHub issue, I recommend you use this code instead. For clarity, I’ll name it boxes_model.1\n1 I’m borrowing heavily from Michael S. Truong’s Boxed_model code, here.\nboxes_model &lt;- \"\ndata{\n  int N;\n  array[N] int y;\n  array[N] int majority_first;\n}\nparameters{\n  simplex[5] p;\n}\nmodel{\n  vector[5] phi;\n  \n  // Prior\n  p ~ dirichlet(rep_vector(4, 5));\n    \n  // Probability of data\n  for (i in 1:N) {\n    if (y[i] == 2) phi[1] = 1; else phi[1] = 0;  // Majority\n    if (y[i] == 3) phi[2] = 1; else phi[2] = 0;  // Minority\n    if (y[i] == 1) phi[3] = 1; else phi[3] = 0;  // Maverick\n    phi[4] = 1.0 / 3.0;                          // Random\n    if (majority_first[i] == 1)                  // Follow first\n      if (y[i] == 2) phi[5] = 1; else phi[5] = 0;\n    else\n      if (y[i] == 3) phi[5] = 1; else phi[5] = 0;\n        \n    // Compute log(p_s * Pr(y_i | s)\n    for (j in 1:5) phi[j] = log(p[j]) + log(phi[j]);\n    // Compute average log-probability of y_i\n    target += log_sum_exp(phi);\n    }\n}\n\"\n\nHere we fit the model with stan().\n\n# Prep data \ndat_list &lt;- list(\n  N = nrow(d),\n  y = d$y,\n  majority_first = d$majority_first)\n\n# Run the sampler\nm16.2 &lt;- stan(\n  # model_code = Boxes_model,\n  model_code = boxes_model,\n  data = dat_list, \n  chains = 3, cores = 3,\n  seed = 16)\n\nHere’s what it looks like if you print() a fit object from the stan() function.\n\nprint(m16.2)\n\n variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n     lp__ -667.15 -666.85 1.45 1.30 -670.07 -665.41 1.01      426      734\n     p[1]    0.26    0.26 0.04 0.04    0.20    0.32 1.00      405      545\n     p[2]    0.14    0.14 0.03 0.03    0.09    0.19 1.00      444      687\n     p[3]    0.15    0.15 0.03 0.03    0.10    0.20 1.00      475      511\n     p[4]    0.19    0.19 0.08 0.08    0.07    0.33 1.00      327      385\n     p[5]    0.26    0.26 0.03 0.03    0.21    0.31 1.00     1281     1093\n\n\nHere’s the summary with rethinking::precis().\n\nprecis(m16.2, depth = 2)\n\n          mean         sd       5.5%     94.5%     rhat  ess_bulk\np[1] 0.2574518 0.03521257 0.20245706 0.3155251 1.000934  405.4738\np[2] 0.1402624 0.03207212 0.08797653 0.1899260 1.002103  444.1565\np[3] 0.1492242 0.02960295 0.10034039 0.1950138 1.002817  475.5274\np[4] 0.1933581 0.07649605 0.07656383 0.3269195 1.003647  327.3736\np[5] 0.2597035 0.03184324 0.20826551 0.3096719 1.001545 1281.2734\n\n\nHere we show marginal posterior for \\(p_s\\).\n\nlabel &lt;- c(\"Majority~(italic(s)[1])\", \n           \"Minority~(italic(s)[2])\",\n           \"Maverick~(italic(s)[3])\", \n           \"Random~(italic(s)[4])\", \n           \"Follow~First~(italic(s)[5])\")\n\nprecis(m16.2, depth = 2) |&gt; \n  data.frame() |&gt; \n  mutate(name = factor(label, levels = label)) |&gt; \n  mutate(name = fct_rev(name)) |&gt; \n  \n  ggplot(aes(x = mean, xmin = X5.5., xmax = X94.5., y = name)) +\n  geom_vline(xintercept = 0.2, linetype = 2, linewidth = 1/4) +\n  geom_pointrange(linewidth = 1/4, size = 1/4) +\n  scale_x_continuous(expression(italic(p[s])), \n                     breaks = 0:5 / 5, limits = c(0, 1)) +\n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  theme(axis.text.y = element_text(hjust = 0))\n\n\n\n\n\n\n\n\nAs an alternative, this might be a good time to revisit the tidybayes::stat_ccdfinterval() approach (see Section 12.3.3), which will depict those posteriors with a bar plot where the ends of the bars depict our uncertainty in terms of cumulative density curves.\n\nextract.samples(m16.2) |&gt; \n  data.frame() |&gt; \n  set_names(label) |&gt; \n  pivot_longer(everything()) |&gt; \n  mutate(name = factor(name, levels = label)) |&gt; \n  mutate(name = fct_rev(name)) |&gt; \n  \n  ggplot(aes(x = value, y = name)) +\n  geom_vline(xintercept = 0.2, linetype = 2, linewidth = 1/4) +\n  stat_ccdfinterval(.width = 0.95, alpha = 0.8, size = 1/4) +\n  scale_x_continuous(expression(italic(p[s])), breaks = 0:5 / 5,\n                     expand = c(0, 0), limits = c(0, 1)) +\n  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +\n  theme(axis.text.y = element_text(hjust = 0),\n        axis.ticks.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n16.2.4 State space models\nIn this section of the text, McElreath mentioned state space models and hidden Markov models. Based on this thread in the Stan forums and this issue in the brms GitHub repo, it looks like state space models are not supported in brms at this time. As for hidden Markov models, I’m not sure whether they are supported by brms. The best I could find was this thread on the Stan forums. If you know more about either of these topics, please share your knowledge on this book’s GitHub repo.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear Madness</span>"
    ]
  },
  {
    "objectID": "16.html#ordinary-differential-nut-cracking",
    "href": "16.html#ordinary-differential-nut-cracking",
    "title": "16  Generalized Linear Madness",
    "section": "16.3 Ordinary differential nut cracking",
    "text": "16.3 Ordinary differential nut cracking\nLoad the Panda_nuts data (Boesch et al., 2019).\n\ndata(Panda_nuts, package = \"rethinking\")\nd &lt;- Panda_nuts\nrm(Panda_nuts)\n\nAnticipating McElreath’s R code 16.11, we’ll wrangle a little.\n\nd &lt;- d |&gt; \n  mutate(n     = nuts_opened,\n         age_s = age / max(age))\n\nglimpse(d)\n\nRows: 84\nColumns: 9\n$ chimpanzee  &lt;int&gt; 11, 11, 18, 18, 18, 11, 11, 17, 7, 1, 22, 9, 9, 9, 9, 9, 9, 9, 9, 9, 15, 7, 9, 1, 7, 13, 13, 7, 13, 7, 3, 3,…\n$ age         &lt;int&gt; 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, …\n$ sex         &lt;fct&gt; m, m, f, f, f, m, m, f, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, m, f, f, f, m, m, m, …\n$ hammer      &lt;fct&gt; G, G, wood, G, L, Q, Q, wood, G, L, wood, G, G, G, G, G, G, G, G, G, G, G, G, L, G, wood, G, G, G, wood, G, …\n$ nuts_opened &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 58, 4, 21, 9, 2, 30, 19, 13, 6, 11, 1, 2, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 8, …\n$ seconds     &lt;dbl&gt; 61.0, 37.0, 20.0, 14.0, 13.0, 24.0, 30.5, 135.0, 24.0, 13.0, 34.0, 66.5, 5.0, 24.0, 20.0, 6.0, 42.0, 43.0, 2…\n$ help        &lt;fct&gt; N, N, N, y, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, y, N, N, N, N, N, y, y, N, N, N, N, N, N, N, …\n$ n           &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 58, 4, 21, 9, 2, 30, 19, 13, 6, 11, 1, 2, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 8, …\n$ age_s       &lt;dbl&gt; 0.1875, 0.1875, 0.2500, 0.2500, 0.2500, 0.2500, 0.2500, 0.3125, 0.3125, 0.3125, 0.3125, 0.3750, 0.3750, 0.37…\n\n\nOur criterion is n, the number of Panda nuts opened by a chimpanzee on a given occasion. The two focal predictor variables are age and seconds. Here they are depicted in a pairs plot.\n\nd |&gt; \n  select(n, age, seconds) |&gt; \n  ggpairs(upper = list(continuous = my_upper),\n          diag = list(continuous = my_diag),\n          lower = list(continuous = my_lower)) +\n  theme(strip.text.y = element_text(angle = 0, hjust = 0))\n\n\n\n\n\n\n\n\n\n16.3.1 Scientific model\nAs a starting point, McElreath proposed we the strength of a chimpanzee would relate to the number of nuts they might open. We don’t have a measure of strength, but we do have age, which is a proxy for how close a chimp might be to their maximum body size, and we presume body size would be proportional to strength. If we let \\(t\\) index time, \\(M_\\text{max}\\) be the maximum body size (mass), \\(M_t\\) be the current body size, and \\(k\\) stand for the rate of skill gain the comes with age, we can write\n\\[M_t = M_\\text{max} [1 - \\exp(-kt) ]\\]\nto solve for mass at a given age (von Bertalanffy, 1934). But again, we actually care about strength, not mass. Letting \\(S_t\\) be strength at time \\(t\\), we can express a proportional relation between the two as \\(S_t = \\beta M_t\\). Now if we let \\(\\lambda\\) stand in for the number of nuts opening, \\(\\alpha\\) express the relation of strength to nut opening, we can write\n\\[\\lambda = \\alpha S_t^\\theta = \\alpha \\big ( \\beta M_\\text{max} [1 - \\exp(-kt) ]  \\big ) ^\\theta,\\]\n“where \\(\\theta\\) is some exponent greater than 1” (p. 538). If we rescale \\(M_\\text{max} = 1\\), we can simplify the equation to\n\\[\\lambda = \\alpha \\beta^\\theta [1 - \\exp(-kt) ]^\\theta.\\]\nAs “the product \\(\\alpha \\beta^\\theta\\) in the front just rescales strength to nuts-opened-per-second” (p. 538), we can collapse it to a single parameter, \\(\\phi\\), which leaves us with\n\\[\\lambda = \\phi [1 - \\exp(-kt) ]^\\theta.\\]\nThis is our scientific model.\n\n\n16.3.2 Statistical model\nNow if we let \\(n_i\\) be the number of nuts opened, we can write our statistical model as\n\\[\\begin{align*}\nn_i & \\sim \\operatorname{Poisson}(\\lambda_i) \\\\\n\\lambda_i & = \\text{seconds}_i \\, \\phi [1 - \\exp(-k \\,\\text{age}_i) ]^\\theta,\n\\end{align*}\\]\nwhere we have replaced our time index, \\(t\\), with the variable age. By including the variable seconds in the equation, we have scaled the results to be nuts per second. McElreath proposed the priors:\n\\[\\begin{align*}\n\\phi   & \\sim \\operatorname{Log-Normal}(\\log 1, 0.10) \\\\\nk      & \\sim \\operatorname{Log-Normal}(\\log 2, 0.25) \\\\\n\\theta & \\sim \\operatorname{Log-Normal}(\\log 5, 0.25),\n\\end{align*}\\]\nall of which were Log-Normal to ensure the parameters were positive and continuous. To get a sense of what these priors implied, he simulated. Here’s our version of his simulations, which make up Figure 16.4.\n\nn &lt;- 1e4\n\n# Define the x-axis breaks\nat &lt;- 0:6 / 4\n\n# How many prior draws would you like?\nn_draws &lt;- 50\n\n# Simulate\nset.seed(16)\n\nprior &lt;- tibble(\n  index = 1:n,\n  phi   = rlnorm(n, meanlog = log(1), sdlog = 0.1),\n  k     = rlnorm(n, meanlog = log(2), sdlog = 0.25),\n  theta = rlnorm(n, meanlog = log(5), sdlog = 0.25)) |&gt; \n  slice_sample(n = n_draws) |&gt; \n  expand_grid(age = seq(from = 0, to = 1.5, length.out = 1e2)) |&gt; \n  mutate(bm = 1 - exp(-k * age),\n         ns = phi * (1 - exp(-k * age))^theta)\n\n# Left panel\np1 &lt;- prior |&gt; \n  ggplot(aes(x = age, y = bm, group = index)) +\n  geom_line(alpha = 1/2, linewidth = 1/4) +\n  scale_x_continuous(breaks = at, labels = round(at * max(d$age))) +\n  ylab(\"body mass\")\n\n# Right panel\np2 &lt;- prior |&gt; \n  ggplot(aes(x = age, y = ns, group = index)) +\n  geom_line(alpha = 1/2, linewidth = 1/4) +\n  scale_x_continuous(breaks = at, labels = round(at * max(d$age))) +\n  ylab(\"nuts per second\")\n\n# Combine, entitle, and display\nlibrary(patchwork)\n\np1 + p2 + \n  plot_annotation(title = \"Prior predictive simulation for the nut opening model\",\n                  subtitle = \"Each panel shows the results from 50 prior draws.\")\n\n\n\n\n\n\n\n\nMcElreath suggested we inspect the distributions of these priors. Here they are in a series of histograms.\n\nset.seed(16)\n\ntibble(phi         = rlnorm(n, meanlog = log(1), sdlog = 0.1),\n       `italic(k)` = rlnorm(n, meanlog = log(2), sdlog = 0.25),\n       theta       = rlnorm(n, meanlog = log(5), sdlog = 0.25)) |&gt; \n  pivot_longer(everything()) |&gt; \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 40, boundary = 0, fill = \"grey67\") +\n  scale_x_continuous(\"marginal prior\", limits = c(0, NA)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  facet_wrap(~ name, scales = \"free\", labeller = label_parsed)\n\n\n\n\n\n\n\n\nHappily, we can fit this model using the non-linear brms syntax.\n\nb16.4 &lt;- brm(\n  data = d,\n  family = poisson(link = identity),\n  bf(n ~ seconds * phi * (1 - exp(-k * age_s))^theta,\n     phi + k + theta ~ 1,\n     nl = TRUE),\n  prior = c(prior(lognormal(log(1), 0.1), nlpar = phi, lb = 0),\n            prior(lognormal(log(2), 0.25), nlpar = k, lb = 0),\n            prior(lognormal(log(5), 0.25), nlpar = theta, lb = 0)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 16,\n  file = \"fits/b16.04\")\n\nCheck the parameter summary.\n\nprint(b16.4)\n\n Family: poisson \n  Links: mu = identity \nFormula: n ~ seconds * phi * (1 - exp(-k * age_s))^theta \n         phi ~ 1\n         k ~ 1\n         theta ~ 1\n   Data: d (Number of observations: 84) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi_Intercept       0.87      0.04     0.79     0.95 1.00     1403     1778\nk_Intercept         5.96      0.55     4.86     7.04 1.00     1077     1321\ntheta_Intercept     9.77      1.99     6.40    14.19 1.00     1143     1392\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNo we might get a sense of what this posterior means by plotting nuts per second as a function of age in our version of Figure 16.5.\n\nset.seed(16)\nas_draws_df(b16.4) |&gt; \n  slice_sample(n = n_draws) |&gt; \n  expand_grid(age = seq(from = 0, to = 1.5, length.out = 1e2)) |&gt; \n  mutate(ns = b_phi_Intercept * (1 - exp(-b_k_Intercept * age))^b_theta_Intercept) |&gt; \n  \n  ggplot() +\n  geom_line(aes(x = age, y = ns, group = .draw),\n            alpha = 1/2, linewidth = 1/4) +\n  geom_jitter(data = d,\n              aes(x = age_s, y = n / seconds, size = seconds),\n              color = \"grey50\", shape = 1, width = 0.01) +\n  scale_x_continuous(breaks = at, labels = round(at * max(d$age))) +\n  scale_size_continuous(breaks = c(1, 50, 100), limits = c(1, NA)) +\n  labs(y = \"nuts per second\",\n       title = \"Posterior predictive distribution for the\\nnut opening model\") +\n  theme(legend.background = element_blank(),\n        legend.position = c(0.9, 0.25))\n\n\n\n\n\n\n\n\nLooks like things flatten out around age == 16. Yet since the data drop off at that age, we probably shouldn’t get overconfident.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear Madness</span>"
    ]
  },
  {
    "objectID": "16.html#sec-Population-dynamics",
    "href": "16.html#sec-Population-dynamics",
    "title": "16  Generalized Linear Madness",
    "section": "16.4 Population dynamics",
    "text": "16.4 Population dynamics\nLoad the Lynx_Hare population dynamics data (Hewitt, 1921).\n\ndata(Lynx_Hare, package = \"rethinking\")\n\nglimpse(Lynx_Hare)\n\nRows: 21\nColumns: 3\n$ Year &lt;int&gt; 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1…\n$ Lynx &lt;dbl&gt; 4.0, 6.1, 9.8, 35.2, 59.4, 41.7, 19.0, 13.0, 8.3, 9.1, 7.4, 8.0, 12.3, 19.5, 45.7, 51.1, 29.7, 15.8, 9.7, 10.1, 8.6\n$ Hare &lt;dbl&gt; 30.0, 47.2, 70.2, 77.4, 36.3, 20.6, 18.1, 21.4, 22.0, 25.4, 27.1, 40.3, 57.0, 76.6, 52.3, 19.5, 11.2, 7.6, 14.6, 16…\n\n\nAs McElreath indicated in his endnote #238 (p. 570), this example is based on Stan case study by the great Bob Carpenter, Predator-prey population dynamics: The Lotka-Volterra model in Stan. You might bookmark that link. It’ll come up later in this section.\nFigure 6.6 will give us a sense of how the lynx and hare populations ebbed and flowed.\n\n# For annotation\ntext &lt;- tibble(\n  name  = c(\"Hare\", \"Lynx\"),\n  label = c(\"Lepus\", \"Lynx\"),\n  Year  = c(1913.5, 1915.5),\n  value = c(78, 52))\n\n# Wrangle\nLynx_Hare |&gt; \n  pivot_longer(-Year) |&gt; \n  \n  # Plot!\n  ggplot(aes(x = Year, y = value)) +\n  geom_line(aes(color = name),\n            linewidth = 1/4) +\n  geom_point(aes(fill = name),\n             color = \"white\", shape = 21, size = 3) +\n  geom_text(data = text,\n            aes(label = label, color = name),\n            family = \"Times\", hjust = 0) +\n  scale_y_continuous(\"thousands of pelts\", breaks = 0:4 * 20, limits = c(0, 90)) +\n  scale_fill_grey(start = 0, end = 0.5) +\n  scale_color_grey(start = 0, end = 0.5) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nNote, however, that these are numbers of pelts, not of actual animals. This will become important when we start modeling.\nA typical way to model evenly-spaced time series data like this would be with an autoregressive model with the basic structure\n\\[\\operatorname{E}(y_t) = \\alpha + \\beta_1 y_{t-1},\\]\nwhere \\(t\\) indexes time and \\(t - 1\\) is the time point immediately before \\(t\\). Models following this form are called first-order autoregressive models, AR(1), meaning that the current time point is only influenced by the previous time point, but none of the earlier ones. You can build on this format by adding other predictors. A natural way would be to use a predictor from \\(t - 1\\) to predict \\(y_t\\), following the form\n\\[\\operatorname{E}(y_t) = \\alpha + \\beta_1 y_{t-1} + \\beta_2 x_{t-1}.\\]\nBut that’s still a first-order model. A second-order model, AR(2), would include a term for \\(y_{t - 2}\\), such as\n\\[\\operatorname{E}(y_t) = \\alpha + \\beta_1 y_{t-1} + \\beta_2 x_{t-1} + \\beta_3 y_{t-2}.\\]\nMcElreath isn’t a huge fan of these models, particularly from the scientific modeling perspective he developed in this chapter. But brms can fit them and we’ll practice a little in a bonus section, later on. In the mean time, we’ll follow along and learn about ordinary differential equations (ODEs).\n\n16.4.1 The scientific model\nWe’ll start off simple and focus first on hares. If we let \\(H_t\\) be the number of hares at time \\(t\\), we can express the rate of change in the hare population as\n\\[\\frac{\\mathrm{d} H}{\\mathrm{d} t} = H_t \\times (\\text{birth rate}) - H_t \\times (\\text{death rate}).\\]\nIf we presume both birth rates and death rates (mortality rates) are constants, we might denote them \\(b_H\\) and \\(m_H\\), respectively, and re-express the formula as\n\\[\\frac{\\mathrm{d} H}{\\mathrm{d} t} = H_t b_H - H_t m_H = H_t (b_H - m_H).\\]\nBuilding, if we let \\(L_t\\) stand for the number of lynx present at time \\(t\\), we can allow the mortality rate depend on that variable with the expanded formula\n\\[\\frac{\\mathrm{d} H}{\\mathrm{d} t} = H_t (b_H - L_t m_H).\\]\nWe can expand this even further to model how the number of hares at a given time influence the birth rate for lynx (\\(b_L\\)) to help us model the rate of change in the lynx population as\n\\[\\frac{\\mathrm{d} L}{\\mathrm{d} t} = L_t (H_t b_L - m_L),\\]\nwhere the lynx mortality rate (\\(m_l\\)) is now constant. This is called the Lotka-Volterra model (Lotka, 1925; Volterra, 1926). You may have noticed how the above equations shifted our focus from what were were originally interested in, \\(\\operatorname{E}(H_t)\\), to a rate of change, \\(\\mathrm{d} H / \\mathrm{d} t\\). Happily, our equation for \\(\\mathrm{d} H / \\mathrm{d} t\\), “tells us how to update \\(H\\) after each tiny unit of passing time \\(\\mathrm d t\\)” (p. 544). You update by\n\\[H_{t +\\mathrm d t} = H_t + \\mathrm d t \\frac{\\mathrm d H}{\\mathrm d t} = H_t + \\mathrm d t H_t (b_H - L_t m_H).\\] Here we’ll use a custom function called sim_lynx_hare() to simulate how this can work. Our version of the function is very similar to the one McElreath displayed in his R code 16.14, but we changed it so it returns a tibble which includes a time index, t.\n\nsim_lynx_hare &lt;- function(n_steps, init, theta, dt = 0.002) { \n  L &lt;- rep(NA, n_steps)\n  H &lt;- rep(NA, n_steps)\n  \n  # Set initial values\n  L[1] &lt;- init[1]\n  H[1] &lt;- init[2]\n  \n  for (i in 2:n_steps) {\n    H[i] &lt;- H[i - 1] + dt * H[i - 1] * (theta[1] - theta[2] * L[i - 1])\n    L[i] &lt;- L[i - 1] + dt * L[i - 1] * (theta[3] * H[i - 1] - theta[4])\n  }\n  \n  # Return a tibble\n  tibble(t = 1:n_steps,\n         H = H,\n         L = L)\n}\n\nNow we simulate.\n\n# Set the four theta values\ntheta &lt;- c(0.5, 0.05, 0.025, 0.5)\n\n# Simulate\nz &lt;- sim_lynx_hare(n_steps = 1e4, \n                   init = c(filter(Lynx_Hare, Year == 1900) |&gt; pull(\"Lynx\"), \n                            filter(Lynx_Hare, Year == 1900) |&gt; pull(\"Hare\")), \n                   theta = theta)\n\n# What did we do?\nglimpse(z)\n\nRows: 10,000\nColumns: 3\n$ t &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32,…\n$ H &lt;dbl&gt; 30.00000, 30.01800, 30.03600, 30.05401, 30.07203, 30.09005, 30.10807, 30.12610, 30.14413, 30.16217, 30.18021, 30.19826…\n$ L &lt;dbl&gt; 4.000000, 4.002000, 4.004005, 4.006014, 4.008028, 4.010046, 4.012069, 4.014097, 4.016129, 4.018166, 4.020208, 4.022254…\n\n\nEach row is a stand-in index for time. Here we’ll explicitly add a time column and them plot the results in our version of Figure 16.7.\n\nz |&gt; \n  pivot_longer(-t) |&gt; \n  \n  ggplot(aes(x = t, y = value, color = name)) +\n  geom_line(linewidth = 1/4) +\n  scale_x_continuous(expression(time~(italic(t))), breaks = NULL) +\n  scale_y_continuous(\"number (thousands)\", breaks = 0:4 * 10, limits = c(0, 45)) +\n  scale_color_grey(start = 0, end = 0.5) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n“This model produces cycles, similar to what we see in the data. The model behaves this way, because lynx eat hares. Once the hares are eaten, the lynx begin to die off. Then the cycle repeats (p. 545).”\n\n\n16.4.2 The statistical model\nIf we continue to let \\(H_t\\) and \\(L_t\\) be the number of hares and lynx at time \\(t\\), we might also want to rehearse the distinction between those numbers and our observations by letting \\(h_t\\) and \\(l_t\\) stand for the observed numbers of hares and lynx. These observed numbers, recall, are from counts of pelts. We want a statistical model that can connect \\(h_t\\) to \\(H_t\\) and connect \\(l_t\\) to \\(L_t\\). Part of that model would include the probability a hare was trapped on a given year, \\(p_h\\), and a similar probability for a lynx getting trapped, \\(p_l\\). To make things worse, further imagine the number of pelts for each, in a given year, was rounded to the nearest \\(100\\) and divided by \\(1{,}000\\). Those are our values.\nWe practice simulating all this in Figure 16.8. Here we propose a population of \\(H_t = 10^4\\) hares and an average trapping rate of about \\(10\\%\\), as expressed by \\(p_t \\sim \\operatorname{Beta}(2, 18)\\). As described above, we then divide the number of observed pelts by \\(1{,}000\\) and round the results, yielding \\(h_t\\).\n\nn  &lt;- 1e4\nHt &lt;- 1e4\n\nset.seed(16)\n\n# Simulate\ntibble(pt = rbeta(n, shape1 = 2, shape2 = 18)) |&gt; \n  mutate(ht = rbinom(n, size = Ht, prob = pt)) |&gt; \n  mutate(ht = round(ht / 1000, digits = 2)) |&gt; \n  \n  # Plot\n  ggplot(aes(x = ht)) +\n  geom_histogram(binwidth = 0.1, color = \"white\", \n                 fill = \"grey67\", linewidth = 1/4) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(thousand~of~pelts~(italic(h[t]))))\n\n\n\n\n\n\n\n\nOn page 546, McElreath encouraged us to try the simulation with different values of \\(H_t\\) and \\(p_t\\). Here we’ll do so with a \\(3 \\times 3\\) grid of \\(H_t = \\{5{,}000, 10{,}000, 15{,}000\\}\\) and \\(p_t \\sim \\{ \\operatorname{Beta}(2, 18), \\operatorname{Beta}(10, 10), \\operatorname{Beta}(18, 2) \\}\\).\n\ntibble(shape1 = c(2, 10, 18),\n       shape2 = c(18, 10, 2)) |&gt; \n  expand_grid(Ht = c(5e3, 1e4, 15e3)) |&gt; \n  # Simulate\n  mutate(pt = map2(.x = shape1, .y = shape2, .f = \\(x, y) rbeta(n, shape1 = x, shape2 = y))) |&gt; \n  mutate(ht = map2(.x = Ht, .y = pt, .f = \\(x, y) rbinom(n, size = x, prob = y)))\n\n# A tibble: 9 × 5\n  shape1 shape2    Ht pt             ht            \n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;         &lt;list&gt;        \n1      2     18  5000 &lt;dbl [10,000]&gt; &lt;int [10,000]&gt;\n2      2     18 10000 &lt;dbl [10,000]&gt; &lt;int [10,000]&gt;\n3      2     18 15000 &lt;dbl [10,000]&gt; &lt;int [10,000]&gt;\n4     10     10  5000 &lt;dbl [10,000]&gt; &lt;int [10,000]&gt;\n5     10     10 10000 &lt;dbl [10,000]&gt; &lt;int [10,000]&gt;\n6     10     10 15000 &lt;dbl [10,000]&gt; &lt;int [10,000]&gt;\n7     18      2  5000 &lt;dbl [10,000]&gt; &lt;int [10,000]&gt;\n8     18      2 10000 &lt;dbl [10,000]&gt; &lt;int [10,000]&gt;\n9     18      2 15000 &lt;dbl [10,000]&gt; &lt;int [10,000]&gt;\n\n\n\nset.seed(16)\n\n# Define the 3 X 3 grid\ntibble(shape1 = c(2, 10, 18),\n       shape2 = c(18, 10, 2)) |&gt; \n  expand_grid(Ht = c(5e3, 1e4, 15e3)) |&gt; \n  # Simulate\n  mutate(pt = map2(.x = shape1, .y = shape2, .f = \\(x, y) rbeta(n, shape1 = x, shape2 = y))) |&gt; \n  mutate(ht = map2(.x = Ht, .y = pt, .f = \\(x, y) rbinom(n, size = x, prob = y))) |&gt; \n  unnest(c(pt, ht)) |&gt; \n  # Wrangle\n  mutate(ht    = round(ht / 1000, digits = 2),\n         beta  = str_c(\"italic(p[t])%~%'Beta '(\", shape1, \", \", shape2, \")\"),\n         Htlab = str_c(\"italic(H[t])==\", Ht)) |&gt;\n  mutate(beta  = factor(beta,\n                        levels = c(\"italic(p[t])%~%'Beta '(2, 18)\", \"italic(p[t])%~%'Beta '(10, 10)\", \"italic(p[t])%~%'Beta '(18, 2)\")),\n         Htlab = factor(Htlab,\n                        levels = c(\"italic(H[t])==15000\", \"italic(H[t])==10000\", \"italic(H[t])==5000\"))) |&gt; \n  \n  # Plot!\n  ggplot(aes(x = ht)) +\n  geom_histogram(aes(fill = beta == \"italic(p[t])%~%'Beta '(2, 18)\" & Htlab == \"italic(H[t])==10000\"),\n                 binwidth = 0.25, boundary = 0, linewidth = 1/10) +\n  geom_vline(aes(xintercept = Ht / 1000), \n             linetype = 2, linewidth = 1/4) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  scale_fill_grey(start = 0.67, end = 0, breaks = NULL) +\n  xlab(expression(thousand~of~pelts~(italic(h[t])))) +\n  facet_grid(Htlab ~ beta, labeller = label_parsed, scales = \"free_y\")\n\n\n\n\n\n\n\n\nThe vertical dashed lines mark off the maximum values in each panel. The histogram in black is of the simulation parameters based on our version of Figure 16.8, above.\nMcElreath’s proposed model is\n\\[\\begin{align*}\nh_t & \\sim \\operatorname{Log-Normal} \\big (\\log(p_H H_t), \\sigma_H \\big) \\\\\nl_t & \\sim \\operatorname{Log-Normal} \\big (\\log(p_L L_t), \\sigma_L \\big) \\\\\nH_1 & \\sim \\operatorname{Log-Normal}(\\log 10, 1) \\\\\nL_1 & \\sim \\operatorname{Log-Normal}(\\log 10, 1) \\\\\nH_{T &gt;1} & = H_1 + \\int_1^T H_t (b_H - m_H L_t) \\mathrm{d} t \\\\\nL_{T &gt;1} & = L_1 + \\int_1^T L_t (b_L H_T - m_L) \\mathrm{d} t \\\\\n\\sigma_H & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_L & \\sim \\operatorname{Exponential}(1) \\\\\np_H & \\sim \\operatorname{Beta}(\\alpha_H, \\beta_H) \\\\\np_L & \\sim \\operatorname{Beta}(\\alpha_L, \\beta_L) \\\\\nb_H & \\sim \\operatorname{Half-Normal}(1, 0.5) \\\\\nb_L & \\sim \\operatorname{Half-Normal}(0.5, 0.5) \\\\\nm_H & \\sim \\operatorname{Half-Normal}(0.5, 0.5) \\\\\nm_L & \\sim \\operatorname{Half-Normal}(1, 0.5).\n\\end{align*}\\]\nIt’s not immediately clear from the text, but if you look closely at the output from cat(Lynx_Hare_model) (see below), you’ll see \\(\\alpha_H = \\alpha_L = 40\\) and \\(\\beta_H = \\beta_L = 200\\). If you’re curious, here’s a plot of what the \\(\\operatorname{Beta}(40, 200)\\) prior looks like.\n\nset.seed(16)\n\ntibble(p = rbeta(n = 1e6, shape1 = 40, shape2 = 200)) |&gt;\n  ggplot(aes(x = p)) +\n  geom_histogram(binwidth = 0.005, boundary = 0, color = \"white\", \n                 fill = \"grey67\", linewidth = 1/6) +\n  scale_x_continuous(expression(prior~predictive~distribution~of~italic(p[H])~and~italic(p[L])), \n                     breaks = 0:5 / 5, expand = c(0, 0), limits = c(0, 1)) +\n  scale_y_continuous(NULL, breaks = NULL, expand = expansion(mult = c(0, 0.05))) +\n  labs(subtitle = expression(\"1,000,000 draws from Beta\"*(40*\", \"*200)))\n\n\n\n\n\n\n\n\nThe \\(\\operatorname{Beta}(40, 200)\\) prior suggests an average trapping rate near 16%.\n⚠️ The content to follow is going to diverge from the text, a bit. As you can see from the equation, above, McElreath’s statistical model is a beast. We can fit this model with brms, but the workflow is more complicated than usual. To make this material more approachable, I am going to divide the remainder of this section into two subsections. In the first subsection, we’ll fit a simplified version of McElreath’s m16.5, which does not contain the measurement-error portion. In the second subsection, we’ll tack on the measurement-error portion and fit the full model. ⚠️\n\n16.4.2.1 The simple Lotka-Volterra model\nBefore we get into it, I should acknowledge that this brms approach to fitting ODE’s is a direct result of the generous contributions from Markus Gesmann. It was one of his older blog posts, PK/PD reserving models, that led me to believe one could fit an ODE model with brms. When I reached out to Gesmann on GitHub (see Issue #18), he went so far as to write a new blog post on exactly this model: Fitting multivariate ODE models with brms. The workflow to follow is something of a blend of the methods in his blog post, McElreath’s model in the text, and the original post by Carpenter that started this all.\nAs far as the statistical model goes, we might express the revision of McElreath’s model omitting the measurement-error portion as\n\\[\\begin{align*}\nh_t & \\sim \\operatorname{Log-Normal} \\big (\\log(H_t), \\sigma_H \\big) \\\\\nl_t & \\sim \\operatorname{Log-Normal} \\big (\\log(L_t), \\sigma_L \\big) \\\\\nH_1 & \\sim \\operatorname{Log-Normal}(\\log 10, 1) \\\\\nL_1 & \\sim \\operatorname{Log-Normal}(\\log 10, 1) \\\\\nH_{T &gt;1} & = H_1 + \\int_1^T H_t (b_H - m_H L_t) \\mathrm{d} t \\\\\nL_{T &gt;1} & = L_1 + \\int_1^T L_t (b_L H_T - m_L) \\mathrm{d} t \\\\\nb_H & \\sim \\operatorname{Half-Normal}(1, 0.5) \\\\\nb_L & \\sim \\operatorname{Half-Normal}(0.5, 0.5) \\\\\nm_H & \\sim \\operatorname{Half-Normal}(0.5, 0.5) \\\\\nm_L & \\sim \\operatorname{Half-Normal}(1, 0.5) \\\\\n\\sigma_H & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_L & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nWith the exception of the priors for the \\(\\sigma\\) parameters, this is basically the same model Carpenter fit with his original Stan code. Carpenter expressed his model using a different style of notation, but the parts are all there.\nAs for our brms, the first issue we need to address is that, at the time of this writing, brms is only set up to fit a univariate ODE model. As Gesmann pointed out, the way around this is to convert the Lynx_Hare data into the long format where the pelt values from the Lynx and Hare columns are all listed in a pelts columns and the two animal populations are differentiated in a population column. We’ll call this long version of the data Lynx_Hare_long.\n\nLynx_Hare_long &lt;- Lynx_Hare |&gt; \n  pivot_longer(-Year,\n               names_to = \"population\", \n               values_to = \"pelts\") |&gt; \n  mutate(delta = if_else(population == \"Lynx\", 1, 0),\n         t     = Year - min(Year) + 1) |&gt; \n  arrange(delta, Year)\n\n# What did we do?\nhead(Lynx_Hare_long)\n\n# A tibble: 6 × 5\n   Year population pelts delta     t\n  &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1900 Hare        30       0     1\n2  1901 Hare        47.2     0     2\n3  1902 Hare        70.2     0     3\n4  1903 Hare        77.4     0     4\n5  1904 Hare        36.3     0     5\n6  1905 Hare        20.6     0     6\n\n\nYou’ll note how we converted the information in the population column into a dummy variable, delta, which is coded 0 = hares, 1 = lynxes. It’s that dummy variable that will allow us to adjust our model formula so we express a bivariate model as if it were univariate. You’ll see. Also notice how we added a t index for time. This is because the Stan code to follow will expect us to index time in that way.\nThe next step is to write a script that will tell brms how to tell Stan how to fit a Lotka-Volterra model. In his blog, Gesmann called this LotkaVolterra. Our script to follow is a very minor adjustment of his.2\n2 You might also note this script is an update from earlier versions of the ebook. Stan has updated some of its recommended syntax, and it has also discarded the old integrate_ode_rk45() function for ode_rk45().\nLotkaVolterra &lt;- \"\n  // Sepcify dynamical system (ODEs)\n  vector ode_LV(\n    real t,          // Time\n    vector y,        // The system rate\n    vector theta) {  // The parameters (i.e., the birth and mortality rates)\n  \n  // The outcome\n  vector[2] dydt;\n  \n  // Differential equations\n  dydt[1] = (theta[1] - theta[2] * y[2]) * y[1];  // Hare process\n  dydt[2] = (theta[3] * y[1] - theta[4]) * y[2];  // Lynx process\n  \n  return dydt;\n}\n\n  // Integrate ODEs and prepare output\n  real LV(\n    real t, \n    real Hare0, real Lynx0, \n    real brHare, real mrHare, \n    real brLynx, real mrLynx,\n    real delta) {\n  vector[2] y0;     // Initial values\n  vector[4] theta;  // Parameters\n  array[1] vector[2] y;  // ODE solution\n  \n  // Set initial values\n  y0[1] = Hare0; y0[2] = Lynx0;\n  \n  // Set parameters\n  theta[1] = brHare; theta[2] = mrHare;\n  theta[3] = brLynx; theta[4] = mrLynx;\n  \n  // Solve ODEs\n  y = ode_rk45(ode_LV, y0, 0, rep_array(t, 1), theta); \n  \n  // Return relevant population values\n  return (y[1, 1] * (1 - delta) + y[1, 2] * delta);\n}\n\"\n\nIf you study this, you’ll see echos of Carpenter’s original Stan code and connections to McElreath’s Stan code (execute cat(Lynx_Hare_model) from his R code 16.17 block), too. But take special notice of the last two lines, above. Those lines use the delta dummy to differentiate the model results for the hare and lynx populations, respectively.\nNext we define our formula input. To keep from overwhelming the brm() code, we’ll save it, here, as an independent object called lv_formula.\n\nlv_formula &lt;- bf(\n  pelts ~ log(eta),\n  # Use our LV() function from above\n  nlf(eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta)),\n  # Initial population state\n  H1 ~ 1, L1 ~ 1,\n  # Hare parameters\n  bh ~ 1, mh ~ 1,\n  # Lynx parameters\n  bl ~ 1, ml ~ 1,\n  # Population-based measurement errors\n  sigma ~ 0 + population,\n  nl = TRUE)\n\nNote our use of the LV() function in the nlf() line. That’s a function defined in the LotkaVolterra script, above, which will allow us to connect the variables and parameters in our formula code to the underlying statistical model. Next we define our priors and save them as an independent object called lv_priors.\n\nlv_priors &lt;- c(\n  prior(lognormal(log(10), 1), nlpar = H1, lb = 0),\n  prior(lognormal(log(10), 1), nlpar = L1, lb = 0),\n  prior(normal(1, 0.5),     nlpar = bh, lb = 0),\n  prior(normal(0.05, 0.05), nlpar = bl, lb = 0),\n  prior(normal(0.05, 0.05), nlpar = mh, lb = 0),\n  prior(normal(1, 0.5),     nlpar = ml, lb = 0),\n  prior(exponential(1), dpar = sigma, lb = 0)\n)\n\nNow whether you can fit this model in brms may depend on which version you’re using. For details, see the footnote.3 In short, just update to the current version. Within our brm() code, notice our stanvars settings. Also, I find these models benefit from setting init = 0. Happily, this model fit in just about four minutes on my 2019 MacBook Pro.\n3 We first fit this model with brms version 2.14.4. At that time, the only way to get it to run successfully was by using backend = \"cmdstan\". I’m not really interested in tackling what that setting means, but rest assured that exciting things are happening for brms and Stan. If you’d like to learn more, check out the (2022) vignette by Webber and Bürkner, Running brms models with within-chain parallelization. But anyways, the current version of brms (2.23.0) no longer requires that backend setting and the complications it entails. The default works fine.\nb16.5a &lt;- brm(\n  data = Lynx_Hare_long, \n  family = brmsfamily(\"lognormal\", link_sigma = \"identity\"),\n  formula = lv_formula, \n  prior = lv_priors,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  init = 0,\n  stanvars = stanvar(scode = LotkaVolterra, block = \"functions\"),\n  file = \"fits/b16.05a\")\n\nOn page 548, McElreath recommend we check the chains. Here we’ll pretty them up with help from bayesplot.\n\nlibrary(bayesplot)\n\ncolor_scheme_set(\"gray\")\n\ncol_names &lt;- c(\"italic(H)[1]\", \"italic(L)[1]\", \n               str_c(\"italic(\", c(\"b[H]\", \"m[H]\", \"b[L]\", \"m[L]\"), \")\"), \n               \"sigma[italic(H)]\", \"sigma[italic(L)]\", \"Chain\")\n\nas_draws_df(b16.5a) |&gt; \n  select(b_H1_Intercept:b_sigma_populationLynx, .chain) |&gt;\n  set_names(col_names) |&gt; \n  \n  mcmc_trace(facet_args = list(labeller = label_parsed), \n             linewidth = 0.15) +\n  scale_x_continuous(breaks = NULL) +\n  theme(legend.key.size = unit(0.15, 'in'),\n        legend.position = c(0.97, 0.13))\n\n\n\n\n\n\n\n\nThey look like a dream. Now inspect the parameter summary.\n\nprint(b16.5a)\n\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: pelts ~ log(eta) \n         eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta)\n         H1 ~ 1\n         L1 ~ 1\n         bh ~ 1\n         mh ~ 1\n         bl ~ 1\n         ml ~ 1\n         sigma ~ 0 + population\n   Data: Lynx_Hare_long (Number of observations: 42) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nH1_Intercept            23.42      2.09    19.55    27.88 1.00     2495     2454\nL1_Intercept             6.69      0.70     5.39     8.16 1.00     2007     2211\nbh_Intercept             0.55      0.06     0.43     0.68 1.00     1150     1623\nmh_Intercept             0.03      0.00     0.02     0.04 1.00     1306     2068\nbl_Intercept             0.02      0.00     0.02     0.03 1.00     1324     1654\nml_Intercept             0.80      0.09     0.63     0.99 1.00     1191     1557\nsigma_populationHare     0.25      0.04     0.18     0.35 1.00     2672     2105\nsigma_populationLynx     0.25      0.04     0.18     0.35 1.00     2748     2691\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAs Gesmann covered in his blog, we need to use the brms::expose_functions() function to expose Stan functions to R before we use some of our favorite post-processing functions.\n\nexpose_functions(b16.5a, vectorize = TRUE)\n\nNow we’re ready to plot our results like McElreath did in Figure 16.9a. Our first step will be to use predict().\n\np &lt;- predict(\n  b16.5a, \n  summary = F, \n  # How many posterior predictive draws would you like?\n  ndraws = 21)\n\nstr(p)\n\n num [1:21, 1:42] 35.1 41 30.9 22.8 20.5 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\nWe’re ready to plot!\n\n# Ror annotation\ntext &lt;- tibble(\n  population = c(\"Hare\", \"Lynx\"),\n  label      = c(\"Lepus\", \"Lynx\"),\n  Year       = c(1914, 1916.5),\n  value      = c(92, 54))\n\n# Wrangle\np &lt;- p |&gt; \n  data.frame() |&gt; \n  set_names(1:42) |&gt; \n  mutate(iter = 1:n()) |&gt; \n  pivot_longer(-iter, names_to = \"row\") |&gt; \n  mutate(row = as.double(row)) |&gt; \n  left_join(Lynx_Hare_long |&gt; mutate(row  = 1:n()),\n            by = \"row\")\n\n# Plot\np |&gt; \n  ggplot(aes(x = Year, y = value)) +\n  geom_line(aes(group = interaction(iter, population), color = population),\n            alpha = 1/2, linewidth = 1/3) +\n  geom_point(data = p |&gt; filter(iter == 1),\n             aes(x = Year, fill = population),\n             color = \"white\", shape = 21, size = 3, stroke = 1/5) +\n  geom_text(data = text,\n            aes(label = label, color = population),\n            family = \"Times\", hjust = 0) +\n  scale_y_continuous(\"thousands of pelts\", breaks = 0:6 * 20) +\n  scale_color_grey(start = 0, end = 0.5, breaks = NULL) +\n  scale_fill_grey(start = 0, end = 0.5, breaks = NULL) +\n  coord_cartesian(ylim = c(0, 120))\n\n\n\n\n\n\n\n\nSince this version of the model didn’t include a measurement-error process, we don’t have a clear way to make an analogue of Figure 16.9b. We’ll contend with that in the next section.\n\n\n16.4.2.2 Add a measurement-error process to the Lotka-Volterra model\nNow we have a sense of what the current Lotka-Volterra workflow looks like for brms, we’re ready to complicate our model a bit. Happily, we won’t need to update our LotkaVolterra code. That’s good as it is. But we will need to make a couple minor adjustments to our model formula object, which we now call lv_formula_error. Make special note of the first bf() line and the last line before we set nl = TRUE. That’s where all the measurement-error action is at.\n\nlv_formula_error &lt;- bf(\n  # This is new\n  pelts ~ log(eta * p),\n  nlf(eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta)),\n  H1 ~ 1, L1 ~ 1,\n  bh ~ 1, mh ~ 1,\n  bl ~ 1, ml ~ 1,\n  sigma ~ 0 + population,\n  # This is new, too\n  p ~ 0 + population,\n  nl = TRUE)\n\nUpdate the priors and save them as lv_priors_error.\n\nlv_priors_error &lt;- c(\n  prior(lognormal(log(10), 1), nlpar = H1, lb = 0),\n  prior(lognormal(log(10), 1), nlpar = L1, lb = 0),\n  prior(normal(1, 0.5),     nlpar = bh, lb = 0),\n  prior(normal(0.05, 0.05), nlpar = bl, lb = 0),\n  prior(normal(0.05, 0.05), nlpar = mh, lb = 0),\n  prior(normal(1, 0.5),     nlpar = ml, lb = 0),\n  prior(exponential(1), dpar = sigma, lb = 0),\n  # Here's our new prior setting\n  prior(beta(40, 200), nlpar = p, lb = 0, ub = 1)\n)\n\nThat wasn’t all that bad, was it? Okay, fit the full brms analogue to McElreath’s m16.5. If you followed along closely, all should go well.\n\nb16.5b &lt;- brm(\n  data = Lynx_Hare_long, \n  family = brmsfamily(\"lognormal\", link_sigma = \"identity\"),\n  formula = lv_formula_error, \n  prior = lv_priors_error, \n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  init = 0,\n  stanvars = stanvar(scode = LotkaVolterra, block = \"functions\"),\n  file = \"fits/b16.05b\")\n\nOnce again, check the quality of the chains.\n\ncol_names &lt;- c(\"italic(H)[1]\", \"italic(L)[1]\", \n               str_c(\"italic(\", c(\"b[H]\", \"m[H]\", \"b[L]\", \"m[L]\"), \")\"), \n               \"italic(p[H])\", \"italic(p[L])\", \"\n               sigma[italic(H)]\", \"sigma[italic(L)]\", \"Chain\")\n\nas_draws_df(b16.5b) |&gt; \n  select(b_H1_Intercept:b_sigma_populationLynx, .chain) |&gt; \n  set_names(col_names) |&gt; \n  \n  mcmc_trace(facet_args = list(labeller = label_parsed), \n             linewidth = 0.15) +\n  scale_x_continuous(breaks = NULL) +\n  theme(legend.key.size = unit(0.15, 'in'),\n        legend.position = c(0.55, 0.13))\n\n\n\n\n\n\n\n\nThey look great! Now inspect the model parameter summary.\n\nprint(b16.5b)\n\n Family: lognormal \n  Links: mu = identity; sigma = identity \nFormula: pelts ~ log(eta * p) \n         eta ~ LV(t, H1, L1, bh, mh, bl, ml, delta)\n         H1 ~ 1\n         L1 ~ 1\n         bh ~ 1\n         mh ~ 1\n         bl ~ 1\n         ml ~ 1\n         sigma ~ 0 + population\n         p ~ 0 + population\n   Data: Lynx_Hare_long (Number of observations: 42) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nH1_Intercept           132.02     22.39    93.68   182.38 1.00     1860     2078\nL1_Intercept            38.50      6.82    27.23    53.53 1.00     1999     2362\nbh_Intercept             0.55      0.06     0.43     0.68 1.00     2123     2058\nmh_Intercept             0.00      0.00     0.00     0.01 1.00     1941     1924\nbl_Intercept             0.00      0.00     0.00     0.01 1.00     1773     2160\nml_Intercept             0.80      0.09     0.64     0.99 1.00     2128     2062\np_populationHare         0.18      0.02     0.13     0.23 1.00     1857     1976\np_populationLynx         0.18      0.02     0.13     0.23 1.00     1771     2088\nsigma_populationHare     0.25      0.04     0.18     0.35 1.00     4008     2536\nsigma_populationLynx     0.25      0.04     0.18     0.36 1.00     3573     2830\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf you fit McElreath’s m16.5, you’ll see our parameter summaries are very similar to his. Okay, we’re now ready to make the real analogue of McElreath’s Figure 16.9. First we’ll make and save the plot for the top panel.\n\n# Get the posterior predictive draws\np &lt;- predict(\n  b16.5b, \n  summary = F, \n  # How many posterior predictive draws would you like?\n  ndraws = 21) |&gt;\n  # Wrangle\n  data.frame() |&gt; \n  set_names(1:42) |&gt; \n  mutate(iter = 1:n()) |&gt; \n  pivot_longer(-iter, names_to = \"row\") |&gt; \n  mutate(row = as.double(row)) |&gt; \n  left_join(Lynx_Hare_long |&gt; mutate(row = 1:n()),\n            by = \"row\")\n\n# Plot!\np1 &lt;- p |&gt; \n  ggplot(aes(x = Year, y = value)) +\n  geom_line(aes(group = interaction(iter, population), color = population),\n            alpha = 1/2, linewidth = 1/3) +\n  geom_point(data = p |&gt; filter(iter == 1),\n             aes(x = Year, fill = population),\n             color = \"white\", shape = 21, size = 3, stroke = 1/5) +\n  geom_text(data = text,\n            aes(label = label, color = population),\n            family = \"Times\", hjust = 0) +\n  scale_y_continuous(\"thousands of pelts\", breaks = 0:6 * 20) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_color_grey(start = 0, end = 0.5, breaks = NULL) +\n  scale_fill_grey(start = 0, end = 0.5, breaks = NULL) +\n  coord_cartesian(ylim = c(0, 120))\n\nOur workflow for the second panel will differ a bit from above and a lot from McElreath’s rethinking-based workflow. In essence, we won’t get the same kind of output McElreath got when he executed post &lt;- extract.samples(m16.5). Our post &lt;- as_draws_df(b16.5b) call only get’s us part of the way there. So we’ll have to be tricky and supplement those results with a little fitted() magic.\n\npost &lt;- as_draws_df(b16.5b)\n\nf &lt;- fitted(b16.5b, summary = F)\n\nNow we’re ready to make our version of the bottom panel of Figure 16.9. The trick is to divide our fitted() based results by the appropriate posterior draws from our \\(p\\) parameters. This is a way of hand computing the post$pop values McElreath showed off in his R code 16.20 block.\n\np2 &lt;- cbind(f[, 1:21]  / post$b_p_populationHare,\n            f[, 22:42] / post$b_p_populationLynx) |&gt; \n  data.frame() |&gt; \n  set_names(1:42) |&gt; \n  mutate(iter = 1:n()) |&gt; \n  pivot_longer(-iter, names_to = \"row\") |&gt; \n  mutate(row = as.double(row)) |&gt; \n  left_join(Lynx_Hare_long |&gt; mutate(row = 1:n()),\n            by = \"row\")  |&gt; \n  filter(iter &lt; 22) |&gt; \n\n  # Plot!\n  ggplot(aes(x = Year, y = value)) +\n  geom_line(aes(group = interaction(iter, population), color = population),\n            alpha = 1/2, linewidth = 1/3) +\n  scale_y_continuous(\"thousands of animals\", breaks = 0:5 * 100) +\n  scale_color_grey(start = 0, end = 0.5, breaks = NULL) +\n  scale_fill_grey(start = 0, end = 0.5, breaks = NULL) +\n  coord_cartesian(ylim = c(0, 500))\n\nNow combine the two ggplot2 with patchwork to make the full Figure 16.9 in all its glory.\n\np1 / p2\n\n\n\n\n\n\n\n\nBoom!\n\n\n\n16.4.3 Lynx lessons Bonus: Practice with the autoregressive model\nBack in Section 16.4, we briefly discussed how autoregressive models are a typical way to explore processes like those in lynx-hare data. In this bonus section, we’ll practice fitting a few of these. To start off, we’ll restrict ourselves to focusing on just one of the criteria, Hare. Our basic autoregressive model will follow the form\n\\[\\begin{align*}\n\\text{Hare}_t & \\sim \\operatorname{Normal}(\\mu_t, \\sigma) \\\\\n\\mu_t & = \\alpha + \\beta_1 \\text{Hare}_{t - 1} \\\\\n\\alpha  & \\sim \\; ? \\\\\n\\beta_1 & \\sim \\; ? \\\\\n\\sigma  & \\sim \\operatorname{Exponential}(1),\n\\end{align*}\\]\nwere \\(\\beta_1\\) is the first-order autoregressive coefficient and the question marks in the third and fourth lines indicate we’re not wedding ourselves to specific priors, at the moment. Also, note the \\(t\\) subscripts, which denote which time period the observation is drawn from, which in these data is \\(\\text{Year} = 1900, 1901, \\dots, 1920\\). Conceptually, \\(t\\) is now and \\(t - 1\\) the time point just before now. So if we were particularly interested in \\(\\operatorname E (\\text{Hare}_{t = 1920})\\), \\(\\text{Hare}_{t - 1}\\) would be the same as \\(\\text{Hare}_{t = 1919}\\).\nWith brms, you can fit a model like this using the ar() function. By default, ar() presumes the criterion variable (Hare, in this case) is ordered chronologically. If you’re unsure or just want to be on the safe side, you can enter your time variable in the time argument. Also, though the ar() function presumes a first-order autoregressive structure by default, it is capable of fitting models with higher-order autoregressive structures. You can manually specify this with the p argument. Here’s how to fit our simple AR(1) model with explicit ar() syntax.\n\nb16.6 &lt;- brm(\n  data = Lynx_Hare,\n  family = gaussian,\n  Hare ~ 1 + ar(time = Year, p = 1),\n  prior(exponential(0.04669846), class = sigma),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 16,\n  file = \"fits/b16.06\")\n\nYou may have noticed we just went with the default brms priors for \\(\\alpha\\) and \\(\\beta_1\\). We got the value for the exponential prior for \\(\\sigma\\) by executing the following.\n\n1 / sd(Lynx_Hare$Hare)\n\n[1] 0.04669846\n\n\nHere’s the model summary.\n\nprint(b16.6)\n\n Family: gaussian \n  Links: mu = identity \nFormula: Hare ~ 1 + ar(time = Year, p = 1) \n   Data: Lynx_Hare (Number of observations: 21) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nCorrelation Structures:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nar[1]     0.74      0.17     0.42     1.06 1.00     2300     2426\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    29.96      8.97    11.45    47.92 1.00     2661     2258\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    16.34      2.68    12.13    22.59 1.00     3153     2578\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nOur autoregressive \\(\\beta_1\\) parameter is summarized in the ‘Correlation Structures,’ in which it’s called ‘ar[1].’ Another more old-school way to fit a autoregressive model is by manually computing a lagged version of your criterion variable. In R, you can do this with the lag() function.\n\nLynx_Hare &lt;- Lynx_Hare |&gt; \n  mutate(Hare_1 = lag(Hare))\n\nhead(Lynx_Hare)\n\n  Year Lynx Hare Hare_1\n1 1900  4.0 30.0     NA\n2 1901  6.1 47.2   30.0\n3 1902  9.8 70.2   47.2\n4 1903 35.2 77.4   70.2\n5 1904 59.4 36.3   77.4\n6 1905 41.7 20.6   36.3\n\n\nLook closely at the relation between the values in the Hare and Hare_1 columns. They are set up such that \\(\\text{Hare}_{\\text{Year} = 1901} = \\text{Hare\\_1}_{\\text{Year} = 1900}\\), \\(\\text{Hare}_{\\text{Year} = 1902} = \\text{Hare\\_1}_{\\text{Year} = 1901}\\), and so on. Unfortunately, this approach does produce a single missing value in the first time point for the lagged variable, Hare_1. Here’s how you might use such a variable to manually fit an autoregressive model with brms::brm().\n\nb16.7 &lt;- brm(\n  data = Lynx_Hare,\n  family = gaussian,\n  Hare ~ 1 + Hare_1,\n  prior = c(prior(normal(0, 1), class = b),\n            prior(exponential(0.04669846), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 16,\n  file = \"fits/b16.07\")\n\n\nprint(b16.7)\n\n Family: gaussian \n  Links: mu = identity \nFormula: Hare ~ 1 + Hare_1 \n   Data: Lynx_Hare (Number of observations: 20) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    10.46      7.22    -3.94    24.97 1.00     3344     2637\nHare_1        0.67      0.18     0.31     1.04 1.00     3315     2515\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    17.05      3.00    12.34    24.05 1.00     2957     2696\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nDid you notice how the fourth line in the output read ‘Number of observations: 20?’ That’s because we had that one missing value for Hare_1. One quick and dirty hack might be to use the missing data syntax we learned from Chapter 15. Here’s how that might look.\n\nb16.8 &lt;- brm(\n  data = Lynx_Hare,\n  family = gaussian,\n  bf(Hare ~ 1 + mi(Hare_1)) +\n    bf(Hare_1 | mi() ~ 1) +\n    set_rescor(FALSE),\n  prior = c(prior(normal(0, 1), class = b, resp = Hare),\n            prior(exponential(0.04669846), class = sigma, resp = Hare),\n            prior(exponential(0.04669846), class = sigma, resp = Hare1)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 16,\n  file = \"fits/b16.08\")\n\nCheck the model summary.\n\nprint(b16.8)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity\n         mu = identity \nFormula: Hare ~ 1 + mi(Hare_1) \n         Hare_1 | mi() ~ 1 \n   Data: Lynx_Hare (Number of observations: 21) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nHare_Intercept     13.84      6.74     0.56    27.28 1.00     2064     2214\nHare1_Intercept    30.42      6.36    15.75    41.30 1.00     2341     1319\nHare_miHare_1       0.61      0.17     0.26     0.95 1.00     1950     2094\n\nFurther Distributional Parameters:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_Hare     16.87      2.98    12.27    23.56 1.00     3164     2735\nsigma_Hare1    23.00      4.25    16.63    33.12 1.00     2361     1706\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we have a model based on all 21 observations, again. I’m still not in love with this fix, because it presumes that value in Hare_1 was missing at random, with no accounting for the autoregressive structure. This is why, when you can, it’s probably better to use the ar() syntax.\nAnyway, here’s how we might use fitted() to get a sense of \\(\\operatorname{E}(\\text{Hare}_t)\\) from our first autoregressive model, b16.6.\n\nset.seed(16)\n\nfitted(b16.6,\n       summary = F,\n       ndraws = 21) |&gt; \n  data.frame() |&gt; \n  set_names(1900:1920) |&gt; \n  mutate(iter = 1:n()) |&gt; \n  pivot_longer(-iter) |&gt; \n  mutate(Year = as.integer(name)) |&gt; \n  \n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = value, group = iter),\n            alpha = 1/2, linewidth = 1/3) +\n  geom_point(data = Lynx_Hare,\n             aes(y = Hare),\n             color = \"white\", fill = \"black\",\n             shape = 21, size = 3, stroke = 1/4) +\n  annotate(geom = \"text\",\n           x = 1913.5, y = 85,\n           label = \"Lepus\", family = \"Times\") +\n  scale_y_continuous(\"thousands of hare pelts\", breaks = 0:6 * 20, limits = c(0, 120))\n\n\n\n\n\n\n\n\nThe model did a pretty good job capturing the non-linear trends in the data. But notice how the fitted lines appear to be one step off from the data. This is actually expected behavior for a simple AR(1) model. For insights on why, check out this thread on Stack Exchange.\nSo far we’ve been fitting the autoregressive models with the Gaussian likelihood, which is a typical approach. If you look at McElreath’s practice problem 16H3, you’ll see he proposed a bivariate autoregressive model using the Log-Normal likelihood. His approach used hand-made lagged predictors and ignored the missing value problem by dropping the first case. That model followed the form\n\\[\\begin{align*}\nL_t & \\sim \\operatorname{Log-Normal}(\\log \\mu_{L, t}, \\sigma_L) \\\\\nH_t & \\sim \\operatorname{Log-Normal}(\\log \\mu_{H, t}, \\sigma_H) \\\\\n\\mu_{L, t} & = \\alpha_L + \\beta_{L1} L_{t - 1} + \\beta_{L2} H_{t - 1} \\\\\n\\mu_{H, t} & = \\alpha_H + \\beta_{H1} H_{t - 1} + \\beta_{H2} L_{t - 1},\n\\end{align*}\\]\nwhere \\(\\beta_{L1}\\) and \\(\\beta_{H1}\\) are the autoregressive parameters and \\(\\beta_{L2}\\) and \\(\\beta_{H2}\\) are what are sometimes called the cross-lag parameters. McElreath left the priors up to us. I propose something like this:\n\\[\\begin{align*}\n\\alpha_L & \\sim \\operatorname{Normal}(\\log 10, 1) \\\\\n\\alpha_H & \\sim \\operatorname{Normal}(\\log 10, 1) \\\\\n\\beta_{L1}, \\dots, \\beta_{H2} & \\sim \\operatorname{Normal}(0, 0.5) \\\\\n\\sigma_L & \\sim \\operatorname{Exponential}(1) \\\\\n\\sigma_H & \\sim \\operatorname{Exponential}(1).\n\\end{align*}\\]\nBefore we fit the model, we’ll need to make a lagged version of Lynx.\n\nLynx_Hare &lt;- Lynx_Hare |&gt; \n  mutate(Lynx_1 = lag(Lynx))\n\nBecause the predictor variables are not centered at zero, we’ll want to use the 0 + Intercept... syntax. Now fit the bivariate autoregressive model.\n\nb16.9 &lt;- brm(\n  data = Lynx_Hare,\n  family = lognormal,\n  bf(Hare ~ 0 + Intercept + Hare_1 + Lynx_1) +\n    bf(Lynx ~ 0 + Intercept + Lynx_1 + Hare_1) +\n    set_rescor(FALSE),\n  prior = c(prior(normal(log(10), 1), class = b, resp = Hare, coef = Intercept),\n            prior(normal(log(10), 1), class = b, resp = Lynx, coef = Intercept),\n            prior(normal(0, 0.5), class = b, resp = Hare),\n            prior(normal(0, 0.5), class = b, resp = Lynx),\n            prior(exponential(1), class = sigma, resp = Hare),\n            prior(exponential(1), class = sigma, resp = Lynx)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4,\n  seed = 16,\n  file = \"fits/b16.09\")\n\nCheck the summary.\n\nprint(b16.9)\n\n Family: MV(lognormal, lognormal) \n  Links: mu = identity\n         mu = identity \nFormula: Hare ~ 0 + Intercept + Hare_1 + Lynx_1 \n         Lynx ~ 0 + Intercept + Lynx_1 + Hare_1 \n   Data: Lynx_Hare (Number of observations: 20) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nHare_Intercept     3.02      0.17     2.69     3.33 1.00     2479     2328\nHare_Hare_1        0.02      0.00     0.02     0.03 1.00     3039     2430\nHare_Lynx_1       -0.02      0.00    -0.03    -0.01 1.00     3514     2615\nLynx_Intercept     1.44      0.12     1.20     1.69 1.00     2314     2319\nLynx_Lynx_1        0.03      0.00     0.02     0.04 1.00     3851     2792\nLynx_Hare_1        0.02      0.00     0.02     0.03 1.00     3051     2677\n\nFurther Distributional Parameters:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_Hare     0.33      0.06     0.23     0.47 1.00     2482     2287\nsigma_Lynx     0.23      0.04     0.17     0.33 1.00     2520     2627\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHere we’ll use fitted() to make a variant of the posterior predictions from the top portion of Figure 16.9.\n\nrbind(fitted(b16.9, resp = \"Hare\"),\n      fitted(b16.9, resp = \"Lynx\")) |&gt; \n  data.frame() |&gt; \n  mutate(name = rep(c(\"Hare\", \"Lynx\"), each = n() / 2),\n         year = rep(1901:1920, times = 2)) |&gt; \n  \n  ggplot(aes(x = year)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, group = name, fill = name),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate, group = name, color = name)) +\n  geom_point(data = Lynx_Hare |&gt; pivot_longer(Lynx:Hare),\n             aes(x = Year, y = value, color = name),\n             size = 2) +\n  scale_x_continuous(limits = c(1900, 1920)) +\n  scale_y_continuous(\"thousands of pelts\", breaks = 0:6 * 20) +\n  scale_color_grey(start = 0, end = 0.5, breaks = NULL) +\n  scale_fill_grey(start = 0, end = 0.5, breaks = NULL)\n\n\n\n\n\n\n\n\nIn the next practice problem (16H4), McElreath suggested we “adapt the autoregressive model to use a two-step lag variable” (p. 551, emphasis added). Using the verbiage from above, we might also refer to that as second-order autoregressive model, AR(2). That would be a straight generalization of the approach we just took. I’ll leave the exercise to the interested reader.\nThe kinds autoregressive models we fit in this section are special cases of what are called autoregressive moving average (ARMA) models. If you’re in the social sciences, Hamaker and Brose have a nice (2009) chapter explaining AR, ARMA, and other related models, which you can download from ReserachGate here. ARMA models are available in brms with help from the arma() function.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear Madness</span>"
    ]
  },
  {
    "objectID": "16.html#session-info",
    "href": "16.html#session-info",
    "title": "16  Generalized Linear Madness",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] parallel  stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] bayesplot_1.15.0.9000 patchwork_1.3.2       rethinking_2.42       posterior_1.6.1.9000  cmdstanr_0.9.0       \n [6] GGally_2.4.0          brms_2.23.0           Rcpp_1.1.0            tidybayes_3.0.7       ggthemes_5.1.0       \n[11] lubridate_1.9.4       forcats_1.0.1         stringr_1.6.0         dplyr_1.1.4           purrr_1.2.1          \n[16] readr_2.1.5           tidyr_1.3.2           tibble_3.3.1          ggplot2_4.0.1         tidyverse_2.0.0      \n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.8            tidyselect_1.2.1        farver_2.1.2            loo_2.9.0.9000          S7_0.2.1               \n [6] fastmap_1.2.0           TH.data_1.1-4           tensorA_0.36.2.1        digest_0.6.39           timechange_0.3.0       \n[11] estimability_1.5.1      lifecycle_1.0.5         StanHeaders_2.36.0.9000 processx_3.8.6          survival_3.8-3         \n[16] magrittr_2.0.4          compiler_4.5.1          rlang_1.1.7             tools_4.5.1             utf8_1.2.6             \n[21] yaml_2.3.12             knitr_1.51              emo_0.0.0.9000          labeling_0.4.3          bridgesampling_1.2-1   \n[26] htmlwidgets_1.6.4       curl_7.0.0              pkgbuild_1.4.8          plyr_1.8.9              RColorBrewer_1.1-3     \n[31] BH_1.90.0-1             abind_1.4-8             multcomp_1.4-29         withr_3.0.2             grid_4.5.1             \n[36] stats4_4.5.1            xtable_1.8-4            inline_0.3.21           emmeans_1.11.2-8        scales_1.4.0           \n[41] MASS_7.3-65             cli_3.6.5               mvtnorm_1.3-3           crayon_1.5.3            rmarkdown_2.30         \n[46] generics_0.1.4          RcppParallel_5.1.11-1   rstudioapi_0.17.1       reshape2_1.4.5          tzdb_0.5.0             \n[51] rstan_2.36.0.9000       splines_4.5.1           assertthat_0.2.1        matrixStats_1.5.0       vctrs_0.7.0            \n[56] V8_8.0.1                Matrix_1.7-3            sandwich_3.1-1          jsonlite_2.0.0          callr_3.7.6            \n[61] hms_1.1.4               arrayhelpers_1.1-0      ggdist_3.3.3            glue_1.8.0              ps_1.9.1               \n[66] ggstats_0.11.0          codetools_0.2-20        distributional_0.5.0    shape_1.4.6.1           stringi_1.8.7          \n[71] gtable_0.3.6            QuickJSR_1.8.1          pillar_1.11.1           htmltools_0.5.9         Brobdingnag_1.2-9      \n[76] RcppEigen_0.3.4.0.2     R6_2.6.1                evaluate_1.0.5          lattice_0.22-7          backports_1.5.0        \n[81] rstantools_2.5.0.9000   gridExtra_2.3           coda_0.19-4.1           nlme_3.1-168            checkmate_2.3.3        \n[86] xfun_0.55               zoo_1.8-14              pkgconfig_2.0.3",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear Madness</span>"
    ]
  },
  {
    "objectID": "16.html#comments",
    "href": "16.html#comments",
    "title": "16  Generalized Linear Madness",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nBoesch, C., Bombjaková, D., Meier, A., & Mundry, R. (2019). Learning curves and teaching when acquiring nut-cracking in humans and chimpanzees. Scientific Reports, 9(1), 1515. https://doi.org/10.1038/s41598-018-38392-8\n\n\nBürkner, P.-C. (2022). Define custom response distributions with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian data analysis (Third Edition). CRC press. https://stat.columbia.edu/~gelman/book/\n\n\nHamaker, E. L., & Dolan, C. V. (2009). Idiographic data analysis: Quantitative methods—from simple to advanced. In J. Valsiner, P. C. M. Molenaar, M. C. D. P. Lyra, & N. Chaudhary (Eds.), Dynamic process methodology in the social and developmental sciences (pp. 191–216). Springer. https://doi.org/10.1007/978-0-387-95922-1_9\n\n\nHewitt, C. G. (1921). The conservation of the wild life of Canada. Charles Scribner’s Sons.\n\n\nLotka, A. J. (1925). Principles of physical biology. Waverly.\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nvan Leeuwen, E. J. C., Cohen, E., Collier-Baker, E., Rapold, C. J., Schäfer, M., Schütte, S., & Haun, D. B. M. (2018). The development of human social learning across seven societies. Nature Communications, 9(1), 2076. https://doi.org/10.1038/s41467-018-04468-2\n\n\nVolterra, V. (1926). Fluctuations in the abundance of a species considered mathematically. Nature, 118(2972), 558–560. https://doi.org/10.1038/118558a0\n\n\nvon Bertalanffy, L. (1934). Untersuchungen Über die Gesetzlichkeit des Wachstums. Wilhelm Roux’ Archiv für Entwicklungsmechanik der Organismen, 131(4), 613–652. https://doi.org/10.1007/BF00650112\n\n\nWeber, S., & Bürkner, P.-C. (2022). Running brms models with within-chain parallelization. https://CRAN.R-project.org/package=brms/vignettes/brms_threading.html",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Generalized Linear Madness</span>"
    ]
  },
  {
    "objectID": "17.html",
    "href": "17.html",
    "title": "17  Horoscopes Insights",
    "section": "",
    "text": "17.1 Use R Notebooks (or Quarto files)\nIn this final chapter, there are no models for us to fit and no figures for use to reimagine. McElreath took the opportunity to comment more broadly on the scientific process. He made a handful of great points, some of which I’ll quote in a bit. But for the bulk of this chapter, I’d like to take the opportunity to pass on a few of my own insights about workflow. I hope they’re of use.\nOMG\nI first started using R in the winter of 2015/2016. Right from the start, I learned how to code from within the RStudio environment. But within RStudio I was using simple scripts. No longer. I now use R Notebooks for just about everything, including my scientific projects and my academic webpage and blog. Nathan Stephens wrote a nice blog post, Why I love R Notebooks. I agree. This has fundamentally changed my workflow as a scientist. I only wish I’d learned about this before starting my dissertation project. So it goes…\nDo yourself a favor, adopt R Notebooks into your workflow. Do it today. If you prefer to learn with videos, here’s a nice intro by Kristine Yu and another one by JJ Allaire. Try it out for like one afternoon and you’ll be hooked.’\nUpdate. I still use R Notebooks, which are still totally awesome. But I’ve been slowly switching over to Quarto files over the past couple of years. For my basic scientific projects, they’re almost the same as R Notebooks. But Quarto files make is much nicer when you want to make a website or publish an ebook. To learn more about Quarto, go to https://quarto.org/. If you’re using R Studio, they’re already available from the Quarto Document... option when you navigate to the New File dropdown.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#save-your-model-fits",
    "href": "17.html#save-your-model-fits",
    "title": "17  Horoscopes Insights",
    "section": "17.2 Save your model fits",
    "text": "17.2 Save your model fits\nIt’s embarrassing how long it took for this to dawn on me.\nUnlike classical statistics, Bayesian models using MCMC take a while to compute. Most of the simple models in McElreath’s text take 30 seconds up to a couple minutes. If your data are small, well-behaved and of a simple structure, you might have a lot of wait times in that range in your future.\nIt hasn’t been that way, for me.\nMost of my data have a complicated multilevel structure and often aren’t very well behaved. It’s normal for my models to take an hour or several to fit. Once you start measuring your model fit times in hours, you do not want to fit these things more than once. So, it’s not enough to document my code in a nice R Notebook file. I need to save my brm() fit objects in external files.\nConsider this model. It’s taken from Bürkner’s (2022b) vignette, Estimating multivariate models with brms. It took about one minute for my personal laptop to fit.\n\nlibrary(brms)\ndata(\"BTdata\", package = \"MCMCglmm\")\n\n\nb17.1 &lt;- brm(\n  data = BTdata,\n  family = gaussian,\n  bf(mvbind(tarsus, back) ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam)) +\n    set_rescor(TRUE), \n  chains = 2, cores = 2,\n  seed = 17)\n\nONe minute isn’t terribly long to wait, but still. I’d prefer to never have to wait a minute for that model, again. Sure, if I save my code in a document like this, I will always be able to fit the model in the future. But I can work smarter. Here I’ll save my b17.1 object outside of R with the save() function.\n\nsave(b17.1, file = \"fits/b17.01.rda\")\n\nHopefully y’all are savvy Bayesian R users and find this insultingly remedial. But if it’s new to you like it was me, you can learn more about .rda files here.\nNow b17.1 is saved outside of R, I can safely remove it and then reload it.\n\nrm(b17.1)\nload(\"fits/b17.01.rda\")\n\nThe file took a fraction of a second to reload. Once reloaded, I can perform typical operations, like examine summaries of the model parameters or refreshing my memory on what data I used.\n\nprint(b17.1)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity\n         mu = identity \nFormula: tarsus ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) \n         back ~ sex + hatchdate + (1 | p | fosternest) + (1 | q | dam) \n   Data: BTdata (Number of observations: 828) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nMultilevel Hyperparameters:\n~dam (Number of levels: 106) \n                                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(tarsus_Intercept)                     0.48      0.05     0.39     0.59 1.00      935     1195\nsd(back_Intercept)                       0.26      0.07     0.11     0.39 1.00      311      560\ncor(tarsus_Intercept,back_Intercept)    -0.50      0.22    -0.91    -0.04 1.00      506      517\n\n~fosternest (Number of levels: 104) \n                                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(tarsus_Intercept)                     0.27      0.06     0.17     0.38 1.00      696     1075\nsd(back_Intercept)                       0.35      0.06     0.23     0.47 1.00      412      746\ncor(tarsus_Intercept,back_Intercept)     0.71      0.20     0.25     0.98 1.00      323      583\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntarsus_Intercept    -0.41      0.07    -0.55    -0.27 1.00     1230     1167\nback_Intercept      -0.01      0.07    -0.15     0.12 1.00     2251     1293\ntarsus_sexMale       0.77      0.06     0.65     0.88 1.00     3348     1300\ntarsus_sexUNK        0.23      0.13    -0.02     0.48 1.00     2901     1317\ntarsus_hatchdate    -0.04      0.06    -0.15     0.07 1.00     1231     1216\nback_sexMale         0.01      0.07    -0.13     0.14 1.00     3048     1334\nback_sexUNK          0.15      0.15    -0.15     0.45 1.00     3014     1416\nback_hatchdate      -0.09      0.05    -0.19     0.02 1.00     2377     1552\n\nFurther Distributional Parameters:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_tarsus     0.76      0.02     0.72     0.80 1.00     2012     1554\nsigma_back       0.90      0.02     0.85     0.95 1.00     2460     1502\n\nResidual Correlations: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nrescor(tarsus,back)    -0.05      0.04    -0.13     0.02 1.00     2723     1604\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nhead(b17.1$data)\n\n       tarsus  sex  hatchdate fosternest     dam       back\n1 -1.89229718  Fem -0.6874021      F2102 R187557  1.1464212\n2  1.13610981 Male -0.6874021      F1902 R187559 -0.7596521\n3  0.98468946 Male -0.4279814       A602 R187568  0.1449373\n4  0.37900806 Male -1.4656641      A1302 R187518  0.2555847\n5 -0.07525299  Fem -1.4656641      A2602 R187528 -0.3006992\n6 -1.13519543  Fem  0.3502805      C2302 R187945  1.5577219\n\n\nThe other option, which we’ve been using extensively throughout the earlier chapters, is to use file argument within the brms::brm() function. You can read about the origins of the argument in issue #472 on the brms GitHub repo. To make use of the file argument, specify a character string. brm() will then save your fitted model object in an external .rds file via the saveRDS() function. Let’s give it a whirl, this time with an interaction.\n\nb17.2 &lt;- brm(\n  data = BTdata,\n  family = gaussian,\n  bf(mvbind(tarsus, back) ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam)) +\n    set_rescor(TRUE), \n  chains = 2, cores = 2,\n  seed = 17,\n  file = \"fits/b17.02\")\n\nNow b17.2 is saved in my fits folder outside of R, I can safely remove it and then reload it.\n\nrm(b17.2)\n\nWe might load b17.2 with the readRDS() function.\n\nb17.2 &lt;- readRDS(\"fits/b17.02.rds\")\n\nNow we can work with b17.2 as desired.\n\nfixef(b17.2)\n\n                             Estimate  Est.Error        Q2.5       Q97.5\ntarsus_Intercept         -0.407187226 0.07072158 -0.54804293 -0.27190775\nback_Intercept           -0.012886241 0.06527942 -0.14159551  0.11165077\ntarsus_sexMale            0.767087723 0.05853218  0.65740660  0.88108744\ntarsus_sexUNK             0.192399658 0.14596787 -0.09388989  0.47641641\ntarsus_hatchdate         -0.049360650 0.06579074 -0.18054760  0.08010410\ntarsus_sexMale:hatchdate  0.012755570 0.05751723 -0.09693957  0.12953029\ntarsus_sexUNK:hatchdate   0.063041413 0.12169950 -0.17988536  0.29943080\nback_sexMale              0.006151895 0.06566112 -0.12204744  0.13189381\nback_sexUNK               0.149214462 0.16998410 -0.18497893  0.46862455\nback_hatchdate           -0.048512751 0.06220318 -0.17132565  0.07668044\nback_sexMale:hatchdate   -0.080900526 0.06716127 -0.20912414  0.05298468\nback_sexUNK:hatchdate    -0.040353487 0.14079000 -0.32264015  0.23253958\n\n\nThe file method has another handy feature. Let’s remove b17.2 one more time to see.\n\nrm(b17.2)\n\nIf you’ve fit a brm() model once and saved the results with file, executing the same brm() code will not re-fit the model. Rather, it will just load and return the model from the .rds file.\n\nb17.2 &lt;- brm(\n  data = BTdata,\n  family = gaussian,\n  bf(mvbind(tarsus, back) ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam)) +\n    set_rescor(TRUE), \n  chains = 2, cores = 2,\n  seed = 15,\n  file = \"fits/b17.02\")\n\nIt takes just a fraction of a second. Once again, we’re ready to work with b17.2.\n\nb17.2$formula\n\ntarsus ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam) \nback ~ sex * hatchdate + (1 | p | fosternest) + (1 | q | dam) \n\n\nAnd if you’d like to remind yourself what the name of that external file was or what folder you saved it in, you can extract it from the brm() fit object.\n\nb17.2$file\n\n[1] \"fits/b17.02.rds\"\n\n\nAlso, see Gavin Simpson’s blog post, A better way of saving and loading objects in R, for a discussion on the distinction between .rda and .rds files.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#build-your-models-slowly",
    "href": "17.html#build-your-models-slowly",
    "title": "17  Horoscopes Insights",
    "section": "17.3 Build your models slowly",
    "text": "17.3 Build your models slowly\nThe model from Bürkner’s vignette, b17.1, was no joke. If you wanted to be verbose about it, it was a multilevel, multivariate, multivariable model. It had a cross-classified multilevel structure, two predictors (for each criterion), and two criteria. Not only is that a lot to keep track of, there’s a whole lot of places for things to go wrong.\nEven if that was the final model I was interested in as a scientist, I still wouldn’t start with it. I’d build up incrementally, just to make sure nothing looked fishy. One place to start would be a simple intercepts-only model.\n\nb17.0 &lt;- brm(\n  data = BTdata, \n  family = gaussian,\n  bf(mvbind(tarsus, back) ~ 1) + set_rescor(TRUE), \n  chains = 2, cores = 2,\n  file = \"fits/b17.00\")\n\n\nplot(b17.0, widths = c(2, 3))\n\n\n\n\n\n\n\nprint(b17.0)\n\n Family: MV(gaussian, gaussian) \n  Links: mu = identity\n         mu = identity \nFormula: tarsus ~ 1 \n         back ~ 1 \n   Data: BTdata (Number of observations: 828) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\ntarsus_Intercept     0.00      0.04    -0.07     0.07 1.00     2693     1530\nback_Intercept       0.00      0.03    -0.07     0.07 1.01     2612     1636\n\nFurther Distributional Parameters:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma_tarsus     1.00      0.02     0.95     1.05 1.00     2440     1450\nsigma_back       1.00      0.03     0.96     1.05 1.00     2681     1350\n\nResidual Correlations: \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nrescor(tarsus,back)    -0.03      0.04    -0.10     0.04 1.01     2188     1411\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nIf the chains look good and the summary statistics are about what I’d expect, I’m on good footing to keep building up to the model I really care about. The results from this model, for example, suggest that both criteria were standardized (i.e., intercepts at 0 and \\(\\sigma\\)’s at 1). If that wasn’t what I intended, I’d rather catch it here than spend a whole minute fitting the more complicated b17.1 model, the parameters for which are sufficiently complicated that I may have had trouble telling what scale the data were on.\nNote, this is not the same as \\(p\\)-hacking (Simmons et al., 2011) or wandering aimlessly down the garden of forking paths (Gelman & Loken, 2013). We are not chasing the flashiest model to put in a paper. Rather, this is just good pragmatic data science. If you start off with a theoretically-justified but complicated model and run into computation problems or produce odd-looking estimates, it won’t be clear where things went awry. When you build up, step by step, it’s easier to catch data cleaning failures, coding goofs and the like.\nSo, when I’m working on a project, I fit one or a few simplified models before fitting my complicated model of theoretical interest. This is especially the case when I’m working with model types that are new to me or that I haven’t worked with in a while. I document each step in my R Notebook files and I save the fit objects for each in external files. I have caught surprises this way. Hopefully this will help you catch your mistakes, too.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#look-at-your-data",
    "href": "17.html#look-at-your-data",
    "title": "17  Horoscopes Insights",
    "section": "17.4 Look at your data",
    "text": "17.4 Look at your data\nRelatedly, and perhaps even a precursor, you should always plot your data before fitting a model. There were plenty examples of this in the text, but it’s worth it making explicit. Simple summary statistics are great, but they’re not enough. For an entertaining exposition, check out Matejka and Fitzmaurice’s (2017) Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing. Though it might make for a great cocktail party story, I’d hate to pollute the scientific literature with a linear model based on a set of dinosaur-shaped data.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#consider-using-the-0-intercept-syntax",
    "href": "17.html#consider-using-the-0-intercept-syntax",
    "title": "17  Horoscopes Insights",
    "section": "17.5 Consider using the 0 + Intercept syntax",
    "text": "17.5 Consider using the 0 + Intercept syntax\nWe covered this a little in the last couple chapters (e.g., Section 15.5.7, Section 16.4.3), but it’s easy to miss. If your real-world model has predictors (i.e., isn’t an intercept-only model), it’s important to keep track of how you have centered those predictors. When you specify a prior for a brms Intercept (i.e., an intercept resulting from the y ~ x or y ~ 1 + x style of syntax), that prior is applied under the presumption all the predictors are mean centered. In the Population-level (‘fixed’) effects subsection of the set_prior section of the brms reference manual (Bürkner, 2022a), we read:\n\nNote that technically, this prior is set on an intercept that results when internally centering all population-level predictors around zero to improve sampling efficiency. On this centered intercept, specifying a prior is actually much easier and intuitive than on the original intercept, since the former represents the expected response value when all predictors are at their means. To treat the intercept as an ordinary population-level effect and avoid the centering parameterization, use 0 + Intercept on the right-hand side of the model formula.\n\nWe get a little more information from the Parameterization of the population-level intercept subsection of the brmsformula section:\n\nThis behavior can be avoided by using the reserved (and internally generated) variable Intercept. Instead of y ~ x, you may write y ~ 0 + Intercept + x. This way, priors can be defined on the real intercept, directly. In addition, the intercept is just treated as an ordinary population-level effect and thus priors defined on b will also apply to it. Note that this parameterization may be less efficient than the default parameterization discussed above.\n\nWe didn’t bother using the 0 + Intercept syntax for most of our models because McElreath chose to emphasize mean-centered and standardized predictors in the second edition of his text. But this will not always be the case. Sometimes you might have a good reason not to center your predictors. In those cases, the 0 + Intercept syntax can make a difference. Regardless, do set your Intercept priors with care.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#annotate-your-workflow",
    "href": "17.html#annotate-your-workflow",
    "title": "17  Horoscopes Insights",
    "section": "17.6 Annotate your workflow",
    "text": "17.6 Annotate your workflow\nIn a typical model-fitting file, I’ll load my data, perhaps transform the data a bit, fit several models, and examine the output of each with trace plots, model summaries, information criteria, and the like. In my early days, I just figured each of these steps were self-explanatory.\nNope.\n“In every project you have at least one other collaborator; future-you. You don’t want future-you to curse past-you.”\nMy experience was that even a couple weeks between taking a break from a project and restarting it was enough time to make my earlier files confusing. And they were my files. I now start each R document1 with an introductory paragraph or two explaining exactly what the purpose of the file is. I separate my major sections by headers and subheaders. My working files are peppered with bullets, sentences, and full on paragraphs between code blocks.\n1 Historically I’ve used R Notebook files for my projects, but in recent years I’ve moved to Quarto. This ebook, for example, was originally built with Notebook files, but is not based on Quarto. Both are great and, IMO, vastly suprior to simple R script files.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#annotate-your-code",
    "href": "17.html#annotate-your-code",
    "title": "17  Horoscopes Insights",
    "section": "17.7 Annotate your code",
    "text": "17.7 Annotate your code\nThis idea is implicit in McElreath’s text, but it’s easy to miss the message. I know I did, at first. I find this is especially important for data wrangling. I’m a tidyverse guy and, for me, the big-money verbs like mutate(), pivot_longer(), select(), filter(), group_by(), and summarise() take care of the bulk of my data wrangling. But every once and a while I need to do something less common, like with str_extract() or case_when(). When I end up using a new or less familiar function, I typically annotate right in the code and even sometimes leave a hyperlink to some R-bloggers post or stackoverflow question that explained how to use it.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#break-up-your-workflow",
    "href": "17.html#break-up-your-workflow",
    "title": "17  Horoscopes Insights",
    "section": "17.8 Break up your workflow",
    "text": "17.8 Break up your workflow\nI’ve also learned to break up my projects into multiple R Notebook files. If you have a small project for which you just want a quick and dirty plot, fine, do it all in one file. My typical scientific projects have:\n\na primary data cleaning file;\na file with basic descriptive statistics and the like;\nat least one primary analysis file;\npossible secondary and tertiary analysis files;\na file or two for my major figures; and\na file explaining and depicting my priors, often accompanied by my posteriors, for comparison.\n\nPutting all that information in one R Notebook file would be overwhelming. Your workflow might well look different, but hopefully you get the idea. You don’t want working files with thousands of lines of code.\nTo get a sense of what this can look like in practice, you might follow this link, https://osf.io/vekpf/, to the OSF project site for one of my conference presentations (Kurz et al., 2019) from the Before Times. In the wiki, I explained the contents of 10 files supporting the analyses we presented at the conference. Each of those 10 .html files has its origin in its own .Rmd file.\nMainly to keep Jenny Bryan from setting my computer on fire, I’m also in the habit of organizing interconnected project files with help from RStudio Projects. You can learn more about these from Chapter 8 in R4DS (Grolemund & Wickham, 2017). They might seem trivial, at first, but I’ve come to value the simple ways in which RStudio Projects help streamline my workflow.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#code-in-public",
    "href": "17.html#code-in-public",
    "title": "17  Horoscopes Insights",
    "section": "17.9 Code in public",
    "text": "17.9 Code in public\nIf you would like to improve the code you write for data-wrangling, modeling, and/or visualizing, code in public. Yes, it can be intimidating. Yes, you will probably make mistakes. If you’re lucky, others will point them out and you will revise and learn and grow. 🌱\nYou can do this on any number of mediums, such as\n\nGitHub (e.g., here),\npersonal blogs (e.g., here),\nthe Open Science Framework (e.g., here),\nonline books (e.g., here),\nfull-blown YouTube lectures (e.g., here), or even with\nbrief Twitter GIFs (e.g., here).\n\nI’ve found that just the possibility that others might look at my code makes it more likely I’ll slow down, annotate, and try to use more consistent formatting. Hopefully it will benefit you, too.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#check-out-social-media",
    "href": "17.html#check-out-social-media",
    "title": "17  Horoscopes Insights",
    "section": "17.10 Check out social media",
    "text": "17.10 Check out social media\nIf you haven’t already, joint the science and statistics conversation the rest of us are having on social media. You can find me on Bluesky and Twitter, but I’ve seen other interesting folks hanging out on platforms like Mastodon and Substack, too.\nThere are also some great blogs out there. Of primary interest, McElreath posts once in a blue moon at https://elevanth.org/blog/. Andrew Gelman regularly blogs at https://statmodeling.stat.columbia.edu/. Many of Gelman’s posts are great, but don’t miss the action in the comments sections. Every so often, I post on brms-related content at https://solomonkurz.netlify.app/blog/.\nAlso, do check out the Stan Forums. They have a special brms tag there, under which you can find all kinds of hot brms talk.\nBut if you’re new to the world of asking for help with your code online, you might acquaint yourself with the notion of a minimally reproducible example. In short, a good minimally reproducible example helps others help you. If you fail to do this, prepare for some snark.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#extra-rethinking-related-resources",
    "href": "17.html#extra-rethinking-related-resources",
    "title": "17  Horoscopes Insights",
    "section": "17.11 Extra Rethinking-related resources",
    "text": "17.11 Extra Rethinking-related resources\nYou may have noticed I don’t offer solutions to the homework problems. Happily, David Salazar has a blog series on the homework problems highlighting a tiydverse- and rethinking-oriented workflow; find his first post here. Ania Kawiecki made a beautiful website solving the homework problems using the tiydverse and INLA (Rue et al., 2009), which you can find here. Vincent Arel-Bundock has a nice website where he worked through the primary material in the book chapters using rstan and the tidyverse, which you can find here. Also, Andrew Heiss has been coding through some of the examples in McElreath’s 2022 lecture series, which you can find here.\nIf you find other resources along these lines, please tell me about it in a GitHub issue.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#but-like-how-do-i-do-this-for-real",
    "href": "17.html#but-like-how-do-i-do-this-for-real",
    "title": "17  Horoscopes Insights",
    "section": "17.12 But, like, how do I do this for real?",
    "text": "17.12 But, like, how do I do this for real?\nMcElreath’s text and my ebook are designed to teach you how to get started doing applied Bayesian statistics. Neither does a great job teaching you how to write them up for a professional presentation, like a peer-reviewed journal article. What I will do, however, is give you some places to look. Just before releasing the 0.2.0 version of this ebook, I asked the good people on twitter to share their favorite examples of applied Bayesian statistics in the scientific literature. See here for the thread.\nAt first, folks were a little shy, but eventually the crowd came through in spades! For a full list, feel free to peruse my tweet thread. For your convenience, here’s a list of a good bunch of the works people shared:2\n2 This list is not exhaustive of the links people shared on twitter. My basic inclusion criteria were that they were (a) peer-reviewed articles (b) with a substantive focus that I could (c) easily find a PDF link for. When someone shared several of their works, I simply pulled the first one that fulfilled those criteria. Also, inclusion on this list is not a personal endorsement. I haven’t read most of them. You get what you pay for, friends.\nAllen et al. (2020, PDF link)\nAmlie-Lefond et al. (2020, PDF link)\nCasillas (2021, PDF link)\nDavis et al. (2020, PDF link)\nGirard et al. (2021, PDF link)\nHaines et al. (2018, PDF link)\nKale et al. (2020, PDF link)\nNogueira et al. (2018, PDF link)\nRoss et al. (2020, PDF link)\nSilbiger et al. (2019, PDF link)\n\nAt a quick glance, these papers cover a reasonably broad range of topics. I hope they give you a sense of how to write up your work.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#parting-wisdom",
    "href": "17.html#parting-wisdom",
    "title": "17  Horoscopes Insights",
    "section": "17.13 Parting wisdom",
    "text": "17.13 Parting wisdom\nOkay, that’s enough from me. Let’s start wrapping this project up with some McElreath.\n\nThere is an aspect of science that you do personally control: openness. Pre-plan your research together with the statistical analysis. Doing so will improve both the research design and the statistics. Document it in the form of a mock analysis that you would not be ashamed to share with a colleague. Register it publicly, perhaps in a simple repository, like Github or any other. But your webpage will do just fine, as well. Then collect the data. Then analyze the data as planned. If you must change the plan, that’s fine. But document the changes and justify them. Provide all of the data and scripts necessary to repeat your analysis. Do not provide scripts and data “on request,” but rather put them online so reviewers of your paper can access them without your interaction. There are of course cases in which full data cannot be released, due to privacy concerns. But the bulk of science is not of that sort.\nThe data and its analysis are the scientific product. The paper is just an advertisement. If you do your honest best to design, conduct, and document your research, so that others can build directly upon it, you can make a difference. (p. 555)\n\nToward that end, also check out the OSF and their YouTube channel, here. Katie Corker gets the last words: “Open science is stronger because we’re doing this together.”",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#session-info",
    "href": "17.html#session-info",
    "title": "17  Horoscopes Insights",
    "section": "Session info",
    "text": "Session info\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Ventura 13.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] brms_2.23.0 Rcpp_1.1.0 \n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6            tensorA_0.36.2.1        xfun_0.55               ggplot2_4.0.1           QuickJSR_1.8.1         \n [6] htmlwidgets_1.6.4       inline_0.3.21           lattice_0.22-7          vctrs_0.7.0             tools_4.5.1            \n[11] generics_0.1.4          stats4_4.5.1            curl_7.0.0              parallel_4.5.1          sandwich_3.1-1         \n[16] tibble_3.3.1            pkgconfig_2.0.3         Matrix_1.7-3            checkmate_2.3.3         RColorBrewer_1.1-3     \n[21] S7_0.2.1                assertthat_0.2.1        distributional_0.5.0    RcppParallel_5.1.11-1   lifecycle_1.0.5        \n[26] compiler_4.5.1          farver_2.1.2            stringr_1.6.0           Brobdingnag_1.2-9       codetools_0.2-20       \n[31] htmltools_0.5.9         bayesplot_1.15.0.9000   yaml_2.3.12             crayon_1.5.3            pillar_1.11.1          \n[36] MASS_7.3-65             StanHeaders_2.36.0.9000 bridgesampling_1.2-1    abind_1.4-8             multcomp_1.4-29        \n[41] nlme_3.1-168            posterior_1.6.1.9000    rstan_2.36.0.9000       tidyselect_1.2.1        digest_0.6.39          \n[46] mvtnorm_1.3-3           stringi_1.8.7           purrr_1.2.1             reshape2_1.4.5          dplyr_1.1.4            \n[51] labeling_0.4.3          splines_4.5.1           fastmap_1.2.0           grid_4.5.1              cli_3.6.5              \n[56] magrittr_2.0.4          emo_0.0.0.9000          loo_2.9.0.9000          survival_3.8-3          pkgbuild_1.4.8         \n[61] TH.data_1.1-4           withr_3.0.2             scales_1.4.0            backports_1.5.0         timechange_0.3.0       \n[66] lubridate_1.9.4         estimability_1.5.1      rmarkdown_2.30          emmeans_1.11.2-8        matrixStats_1.5.0      \n[71] gridExtra_2.3           zoo_1.8-14              coda_0.19-4.1           evaluate_1.0.5          knitr_1.51             \n[76] V8_8.0.1                rstantools_2.5.0.9000   rlang_1.1.7             xtable_1.8-4            glue_1.8.0             \n[81] rstudioapi_0.17.1       jsonlite_2.0.0          plyr_1.8.9              R6_2.6.1",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "17.html#comments",
    "href": "17.html#comments",
    "title": "17  Horoscopes Insights",
    "section": "Comments",
    "text": "Comments\n\n\n\n\nAllen, M. A., Flynn, M. E., Machain, C. M., & Stravers, A. (2020). Outside the wire: US military deployments and public opinion in host states. American Political Science Review, 114(2), 326–341. https://doi.org/10.1017/S0003055419000868\n\n\nAmlie-Lefond, C., Shaw, D. W., Cooper, A., Wainwright, M. S., Kirton, A., Felling, R. J., Abraham, M. G., Mackay, M. T., Dowling, M. M., Torres, M., et al. (2020). Risk of intracranial hemorrhage following intravenous tPA (Tissue-Type Plasminogen Activator) for acute stroke is low in children. Stroke; a Journal of Cerebral Circulation, 51(2), 542–548. https://doi.org/10.1161/STROKEAHA.119.027225\n\n\nBürkner, P.-C. (2022a). brms reference manual, Version 2.18.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2022b). Estimating multivariate models with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html\n\n\nCasillas, J. V. (2021). Interlingual interactions elicit performance mismatches not “compromise” categories in early bilinguals: Evidence from meta-analysis and coronal stops. Languages, 6(1), 9. https://doi.org/10.3390/languages6010009\n\n\nDavis, F. P., Nern, A., Picard, S., Reiser, M. B., Rubin, G. M., Eddy, S. R., & Henry, G. L. (2020). A genetic, genomic, and computational resource for exploring neural circuit function. Elife, 9, e50901. https://doi.org/10.7554/eLife.50901\n\n\nGelman, A., & Loken, E. (2013). The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition” or “p-Hacking” and the research hypothesis was posited ahead of time. 17. https://stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf\n\n\nGirard, J. M., Cohn, J. F., Yin, L., & Morency, L.-P. (2021). Reconsidering the duchenne smile: Formalizing and testing hypotheses about eye constriction and positive emotion. Affective Science, 1–16. https://doi.org/10.1007/s42761-020-00030-w\n\n\nGrolemund, G., & Wickham, H. (2017). R for data science. O’Reilly. https://r4ds.had.co.nz\n\n\nHaines, N., Vassileva, J., & Ahn, W.-Y. (2018). The outcome-representation learning model: A novel reinforcement learning model of the Iowa Gambling Task. Cognitive Science, 42(8), 2534–2561. https://doi.org/10.1111/cogs.12688\n\n\nKale, A., Kay, M., & Hullman, J. (2020). Visual reasoning strategies for effect size judgments and decisions. IEEE Transactions on Visualization and Computer Graphics. https://doi.org/10.1109/TVCG.2020.3030335\n\n\nKurz, A. S., DeBeer, B. B., Kimbrel, N. A., Morissette, S. B., & Meyer, E. C. (2019, October 16). Even with treatment, functional impairment and quality of life remain remarkably stable over two years in post-9/11 Iraq and Afghanistan war veterans. The 4th Annual San Antonio Combat PTSD Conference. https://osf.io/vekpf/\n\n\nMatejka, J., & Fitzmaurice, G. (2017). Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing. https://www.autodesk.com/research/publications/same-stats-different-graphs\n\n\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nNogueira, R. G., Jadhav, A. P., Haussen, D. C., Bonafe, A., Budzik, R. F., Bhuva, P., Yavagal, D. R., Ribo, M., Cognard, C., Hanel, R. A., et al. (2018). Thrombectomy 6 to 24 hours after stroke with a mismatch between deficit and infarct. New England Journal of Medicine, 378(1), 11–21. https://doi.org/10.1056/NEJMoa1706442\n\n\nRoss, C. T., Winterhalder, B., & McElreath, R. (2020). Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates. Social Psychological and Personality Science, 1948550620916071. https://doi.org/10.1177/1948550620916071\n\n\nRue, H., Martino, S., & Chopin, N. (2009). Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series b (Statistical Methodology), 71(2), 319–392. https://doi.org/10.1111/j.1467-9868.2008.00700.x\n\n\nSilbiger, N. J., Goodbody-Gringley, G., Bruno, J. F., & Putnam, H. M. (2019). Comparative thermal performance of the reef-building coral Orbicella franksi at its latitudinal range limits. Marine Biology, 166(10), 1–14. https://doi.org/10.1007/s00227-019-3573-6\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>~~Horoscopes~~ Insights</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Aden-Buie, G. (2022). ggpomological:\nPomological plot theme for ggplot2 [Manual]. https://github.com/gadenbuie/ggpomological\n\n\nAgresti, A. (2015). Foundations of linear and generalized linear\nmodels. John Wiley & Sons. https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034\n\n\nAkaike, H. (1998). Information theory and an extension of the maximum\nlikelihood principle. In Selected papers of Hirotugu\nAkaike (pp. 199–213). Springer. https://www.springer.com/gp/book/9780387983554\n\n\nAllen, M. A., Flynn, M. E., Machain, C. M., & Stravers, A. (2020).\nOutside the wire: US military deployments and public\nopinion in host states. American Political Science Review,\n114(2), 326–341. https://doi.org/10.1017/S0003055419000868\n\n\nAmlie-Lefond, C., Shaw, D. W., Cooper, A., Wainwright, M. S., Kirton,\nA., Felling, R. J., Abraham, M. G., Mackay, M. T., Dowling, M. M.,\nTorres, M., et al. (2020). Risk of intracranial hemorrhage following\nintravenous tPA (Tissue-Type\nPlasminogen Activator) for acute stroke is low in children.\nStroke; a Journal of Cerebral Circulation, 51(2),\n542–548. https://doi.org/10.1161/STROKEAHA.119.027225\n\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Scientists rise up\nagainst statistical significance. Nature, 567(7748),\n305–307. https://doi.org/10.1038/d41586-019-00857-9\n\n\nAngrist, J. D., & Keueger, A. B. (1991). Does compulsory school\nattendance affect schooling and earnings? The Quarterly Journal of\nEconomics, 106(4), 979–1014. https://doi.org/10.2307/2937954\n\n\nAono, Y. (2012). Long-term change in climate and floral phenophase.\nChikyu Kankyo (Global Environment), 17. http://atmenv.envi.osakafu-u.ac.jp/aono/kyophenotemp4/\n\n\nAono, Y., & Kazui, K. (2008). Phenological data series of cherry\ntree flowering in Kyoto, Japan, and its\napplication to reconstruction of springtime temperatures since the 9th\ncentury. International Journal of Climatology, 28(7),\n905–914. https://doi.org/10.1002/joc.1594\n\n\nAono, Y., & Saito, S. (2010). Clarifying springtime temperature\nreconstructions of the medieval period by gap-filling the cherry blossom\nphenological data series at Kyoto, Japan.\nInternational Journal of Biometeorology, 54(2),\n211–219. https://doi.org/10.1007/s00484-009-0272-x\n\n\nArnold, J. B. (2021). ggthemes:\nExtra themes, scales and geoms for ’ggplot2’. https://CRAN.R-project.org/package=ggthemes\n\n\nAtkins, D. C., Baldwin, S. A., Zheng, C., Gallop, R. J., &\nNeighbors, C. (2013). A tutorial on count regression and zero-altered\ncount models for longitudinal substance use data. Psychology of\nAddictive Behaviors : Journal of the Society of Psychologists in\nAddictive Behaviors, 27(1), 166–177. https://doi.org/10.1037/a0029508\n\n\nBaraldi, A. N., & Enders, C. K. (2010). An introduction to modern\nmissing data analyses. Journal of School Psychology,\n48(1), 5–37. https://doi.org/10.1016/j.jsp.2009.10.001\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random\neffects structure for confirmatory hypothesis testing: Keep\nit maximal. Journal of Memory and Language, 68(3),\n255–278. https://doi.org/10.1016/j.jml.2012.11.001\n\n\nBarrett, M. (2022a). ggdag:\nAnalyze and create elegant directed acyclic graphs. https://CRAN.R-project.org/package=ggdag\n\n\nBarrett, M. (2022b). An introduction to ggdag. https://CRAN.R-project.org/package=ggdag/vignettes/intro-to-ggdag.html\n\n\nBaumer, B. S., Kaplan, D. T., & Horton, N. J. (2021). Modern\ndata science with R (2nd edition). Taylor &\nFrancis Group, LLC. https://mdsr-book.github.io/mdsr2e/\n\n\nBeheim, B., Atkinson, Q. D., Bulbulia, J., Gervais, W., Gray, R. D.,\nHenrich, J., Lang, M., Monroe, M. W., Muthukrishna, M., Norenzayan, A.,\net al. (2021). Treatment of missing data determined conclusions\nregarding moralizing gods. Nature, 595(7866), E29–E34.\nhttps://doi.org/10.1038/s41586-019-1043-4\n\n\nBetancourt, M. (2017). Robust Gaussian processes in\nStan. https://betanalpha.github.io/assets/case_studies/gp_part3/part3.html\n\n\nBetancourt, M. (2018). Bayes sparse regression. https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html\n\n\nBickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975). Sex bias in\ngraduate admissions: Data from Berkeley.\nScience, 187(4175), 398–404. https://doi.org/10.1126/science.187.4175.398\n\n\nBoesch, C., Bombjaková, D., Meier, A., & Mundry, R. (2019). Learning\ncurves and teaching when acquiring nut-cracking in humans and\nchimpanzees. Scientific Reports, 9(1), 1515. https://doi.org/10.1038/s41598-018-38392-8\n\n\nBorges, JL. (1941). El jardin de senderos que se bifurcan. Buenos\nAires: Sur. Translated by\nD. A. Yates (1964). In\nLabyrinths: Selected Stories & Other\nWritings (pp. 19–29). New Directions.\n\n\nBrilleman, S., Crowther, M., Moreno-Betancur, M., Buros Novik, J., &\nWolfe, R. (2018). Joint longitudinal and time-to-event models via\nStan. https://github.com/stan-dev/stancon_talks/\n\n\nBürkner, P.-C. (2017). brms: An\nR package for Bayesian multilevel models using\nStan. Journal of Statistical Software,\n80(1), 1–28. https://doi.org/10.18637/jss.v080.i01\n\n\nBürkner, P.-C. (2018). Advanced Bayesian multilevel\nmodeling with the R package brms. The R Journal,\n10(1), 395–411. https://doi.org/10.32614/RJ-2018-017\n\n\nBürkner, P.-C. (2022a). brms reference\nmanual, Version 2.18.0. https://CRAN.R-project.org/package=brms/brms.pdf\n\n\nBürkner, P.-C. (2022b). brms:\nBayesian regression models using ’Stan’.\nhttps://CRAN.R-project.org/package=brms\n\n\nBürkner, P.-C. (2022c). Estimating distributional models with\nbrms. https://CRAN.R-project.org/package=brms/vignettes/brms_distreg.html\n\n\nBürkner, P.-C. (2022d). Define custom response distributions with\nbrms. https://CRAN.R-project.org/package=brms/vignettes/brms_customfamilies.html\n\n\nBürkner, P.-C. (2022e). Estimating monotonic effects with brms.\nhttps://CRAN.R-project.org/package=brms/vignettes/brms_monotonic.html\n\n\nBürkner, P.-C. (2022f). Estimating multivariate models with\nbrms. https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html\n\n\nBürkner, P.-C. (2022g). Estimating non-linear models with brms.\nhttps://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html\n\n\nBürkner, P.-C. (2022h). Estimating phylogenetic multilevel models\nwith brms. https://CRAN.R-project.org/package=brms/vignettes/brms_phylogenetics.html\n\n\nBürkner, P.-C. (2022i). Handle missing values with brms. https://CRAN.R-project.org/package=brms/vignettes/brms_missings.html\n\n\nBürkner, P.-C. (2022j). Parameterization of response distributions\nin brms. https://CRAN.R-project.org/package=brms/vignettes/brms_families.html\n\n\nBürkner, P.-C., & Charpentier, E. (2020). Modelling monotonic\neffects of ordinal predictors in Bayesian regression\nmodels. British Journal of Mathematical and Statistical\nPsychology. https://doi.org/10.1111/bmsp.12195\n\n\nBürkner, P.-C., Gabry, J., Kay, M., & Vehtari, A. (2022). posterior: Tools for working with\nposterior distributions. https://CRAN.R-project.org/package=posterior\n\n\nBürkner, P.-C., & Vuorre, M. (2019). Ordinal regression models in\npsychology: A tutorial. Advances in Methods and\nPractices in Psychological Science, 2(1), 77–101. https://doi.org/10.1177/2515245918823199\n\n\nCarvalho, C. M., Polson, N. G., & Scott, J. G. (2009). Handling\nsparsity via the horseshoe. Artificial Intelligence and\nStatistics, 73–80. http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf\n\n\nCasella, G., & George, E. I. (1992). Explaining the\nGibbs sampler. The American Statistician,\n46(3), 167–174. https://doi.org/10.1080/00031305.1992.10475878\n\n\nCasillas, J. V. (2021). Interlingual interactions elicit performance\nmismatches not “compromise” categories in early bilinguals:\nEvidence from meta-analysis and coronal stops.\nLanguages, 6(1), 9. https://doi.org/10.3390/languages6010009\n\n\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2013).\nApplied multiple regression/correlation analysis for the behavioral\nsciences (Third Edition). Routledge. https://doi.org/10.4324/9780203774441\n\n\nCover, T. M., & Thomas, J. A. (2006). Elements of information\ntheory (2nd Edition). John Wiley & Sons. https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959\n\n\nCumming, G. (2014). The new statistics: Why and how.\nPsychological Science, 25(1), 7–29. https://doi.org/10.1177/0956797613504966\n\n\nCushman, F., Young, L., & Hauser, M. (2006). The role of conscious\nreasoning and intuition in moral judgment: Testing three\nprinciples of harm. Psychological Science, 17(12),\n1082–1089. https://doi.org/10.1111/j.1467-9280.2006.01834.x\n\n\nDavis, F. P., Nern, A., Picard, S., Reiser, M. B., Rubin, G. M., Eddy,\nS. R., & Henry, G. L. (2020). A genetic, genomic, and computational\nresource for exploring neural circuit function. Elife,\n9, e50901. https://doi.org/10.7554/eLife.50901\n\n\nde Rooij, M., & Weeda, W. (2020). Cross-validation: A\nmethod every psychologist should know. Advances in Methods and\nPractices in Psychological Science, 3(2), 248–263. https://doi.org/10.1177/2515245919898466\n\n\nDunn, P. K., & Smyth, G. K. (2018). Generalized linear models\nwith examples in R. Springer. https://link.springer.com/book/10.1007/978-1-4419-0118-7\n\n\nEfron, B., & Morris, C. (1977). Stein’s paradox in statistics.\nScientific American, 236(5), 119–127. https://doi.org/10.1038/scientificamerican0577-119\n\n\nEnders, C. K. (2022). Applied missing data analysis (Second\nEdition). Guilford Press. http://www.appliedmissingdata.com/\n\n\nFernández i Marín, X. (2016). ggmcmc:\nAnalysis of MCMC samples and\nBayesian inference. Journal of Statistical\nSoftware, 70(9), 1–20. https://doi.org/10.18637/jss.v070.i09\n\n\nFernández i Marín, X. (2021). ggmcmc:\nTools for analyzing MCMC simulations from\nBayesian inference [Manual]. https://CRAN.R-project.org/package=ggmcmc\n\n\nFreckleton, R. P. (2002). On the misuse of residuals in ecology:\nRegression of residuals vs. Multiple regression.\nJournal of Animal Ecology, 71(3), 542–545. https://doi.org/10.1046/j.1365-2656.2002.00618.x\n\n\nGabry, J. (2022). Plotting MCMC draws using the\nbayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/plotting-mcmc-draws.html\n\n\nGabry, J., & Goodrich, B. (2022). rstanarm: Bayesian applied regression\nmodeling via stan [Manual]. https://CRAN.R-project.org/package=rstanarm\n\n\nGabry, J., & Mahr, T. (2022). bayesplot: Plotting for\nBayesian models. https://CRAN.R-project.org/package=bayesplot\n\n\nGabry, J., & Modrák, M. (2022). Visual MCMC\ndiagnostics using the bayesplot package. https://CRAN.R-project.org/package=bayesplot/vignettes/visual-mcmc-diagnostics.html\n\n\nGabry, J., Simpson, D., Vehtari, A., Betancourt, M., & Gelman, A.\n(2019). Visualization in Bayesian workflow. Journal of\nthe Royal Statistical Society: Series A (Statistics in Society),\n182(2), 389–402. https://doi.org/10.1111/rssa.12378\n\n\nGarnier, S. (2021). viridis:\nDefault color maps from ’matplotlib’ [Manual]. https://CRAN.R-project.org/package=viridis\n\n\nGelman, A. (2005). Analysis of variance–Why it is more\nimportant than ever. Annals of Statistics, 33(1),\n1–53. https://doi.org/10.1214/009053604000001048\n\n\nGelman, A. (2006). Prior distributions for variance parameters in\nhierarchical models (comment on article by Browne and\nDraper). Bayesian Analysis, 1(3),\n515–534. https://doi.org/10.1214/06-BA117A\n\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A.,\n& Rubin, D. B. (2013). Bayesian data analysis (Third\nEdition). CRC press. https://stat.columbia.edu/~gelman/book/\n\n\nGelman, A., Goodrich, B., Gabry, J., & Vehtari, A. (2019). R-squared\nfor Bayesian regression models. The American\nStatistician, 73(3), 307–309. https://doi.org/10.1080/00031305.2018.1549100\n\n\nGelman, A., & Greenland, S. (2019). Are confidence intervals better\ntermed “uncertainty intervals”? BMJ, l5381. https://doi.org/10.1136/bmj.l5381\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other\nstories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGelman, A., & Imbens, G. (2019). Why high-order polynomials should\nnot be used in regression discontinuity designs. Journal of Business\n& Economic Statistics, 37(3), 447–456. https://doi.org/10.1080/07350015.2017.1366909\n\n\nGelman, A., & Little, T. C. (1997). Postratification into many\ncategories using hierarchical logistic regression. Survey\nMethodology, 23, 127–135. https://stat.columbia.edu/~gelman/research/published/poststrat3.pdf\n\n\nGelman, A., & Loken, E. (2013). The garden of forking paths:\nWhy multiple comparisons can be a problem, even when there\nis no “fishing expedition” or\n“p-Hacking” and the research hypothesis was\nposited ahead of time. 17. https://stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf\n\n\nGelman, A., Simpson, D., & Betancourt, M. (2017). The prior can\noften only be understood in the context of the likelihood. Entropy.\nAn International and Interdisciplinary Journal of Entropy and\nInformation Studies, 19(10), 555. https://doi.org/10.3390/e19100555\n\n\nGelman, A., & Stern, H. (2006). The difference between\n“significant” and “not significant” is not\nitself statistically significant. The American Statistician,\n60(4), 328–331. https://doi.org/10.1198/000313006X152649\n\n\nGeman, S., & Geman, D. (1984). Stochastic relaxation,\nGibbs distributions, and the Bayesian\nrestoration of images. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, PAMI-6(6), 721–741. https://doi.org/10.1109/TPAMI.1984.4767596\n\n\nGirard, J. M., Cohn, J. F., Yin, L., & Morency, L.-P. (2021).\nReconsidering the duchenne smile: Formalizing and testing\nhypotheses about eye constriction and positive emotion. Affective\nScience, 1–16. https://doi.org/10.1007/s42761-020-00030-w\n\n\nGohel, D. (2022). flextable:\nFunctions for tabular reporting [Manual]. https://CRAN.R-project.org/package=flextable\n\n\nGohel, D. (2023). Using the flextable R package.\nhttps://ardata-fr.github.io/flextable-book/\n\n\nGrafen, A., & Hails, R. (2002). Modern statistics for the life\nsciences. Oxford University Press. https://global.oup.com/academic/product/modern-statistics-for-the-life-sciences-9780199252312?\n\n\nGrantham, N. (2019). ggdark:\nDark mode for ’ggplot2’ themes [Manual]. https://CRAN.R-project.org/package=ggdark\n\n\nGrolemund, G., & Wickham, H. (2017). R for data science.\nO’Reilly. https://r4ds.had.co.nz\n\n\nHaines, N., Vassileva, J., & Ahn, W.-Y. (2018). The\noutcome-representation learning model: A novel\nreinforcement learning model of the Iowa Gambling Task.\nCognitive Science, 42(8), 2534–2561. https://doi.org/10.1111/cogs.12688\n\n\nHamaker, E. L., & Dolan, C. V. (2009). Idiographic data analysis:\nQuantitative methods—from simple to advanced. In J.\nValsiner, P. C. M. Molenaar, M. C. D. P. Lyra, & N. Chaudhary\n(Eds.), Dynamic process methodology in the social and developmental\nsciences (pp. 191–216). Springer. https://doi.org/10.1007/978-0-387-95922-1_9\n\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The elements\nof statistical learning: Data mining, inference, and\nprediction. Springer Science & Business Media. https://doi.org/10.1007/978-0-387-84858-7\n\n\nHauer, E. (2004). The harm done by tests of significance. Accident\nAnalysis & Prevention, 36(3), 495–500. https://doi.org/10.1016/S0001-4575(03)00036-8\n\n\nHauser, M., Cushman, F., Young, L., Jin, R. K.-X., & Mikhail, J.\n(2007). A dissociation between moral judgments and justifications.\nMind & Language, 22(1), 1–21. https://doi.org/10.1111/j.1468-0017.2006.00297.x\n\n\nHayes, A. F. (2017). Introduction to mediation, moderation, and\nconditional process analysis: A regression-based\napproach. Guilford publications. https://www.guilford.com/books/Introduction-to-Mediation-Moderation-and-Conditional-Process-Analysis/Andrew-Hayes/9781462534654\n\n\nHealy, K. (2018). Data visualization: A practical\nintroduction. Princeton University Press. https://socviz.co/\n\n\nHedeker, D., Mermelstein, R. J., & Demirtas, H. (2008). An\napplication of a mixed-effects location scale model for analysis of\necological momentary assessment (EMA) data.\nBiometrics, 64(2), 627–634. https://doi.org/10.1111/j.1541-0420.2007.00924.x\n\n\nHedeker, D., Mermelstein, R. J., & Demirtas, H. (2012). Modeling\nbetween- and within-Subject variance in ecological\nmomentary assessment (EMA) data using mixed-effects\nlocation scale models. Statistics in Medicine, 31(27).\nhttps://doi.org/10.1002/sim.5338\n\n\nHenderson, E. (2022). ghibli:\nStudio ghibli colour palettes [Manual]. https://CRAN.R-project.org/package=ghibli\n\n\nHenry, L., & Wickham, H. (2020). purrr: Functional programming\ntools. https://CRAN.R-project.org/package=purrr\n\n\nHewitt, C. G. (1921). The conservation of the wild life of\nCanada. Charles Scribner’s Sons.\n\n\nHilbe, J. M. (2011). Negative binomial regression (Second\nEdition). https://doi.org/10.1017/CBO9780511973420\n\n\nHinde, K., & Milligan, L. A. (2011). Primate milk:\nProximate mechanisms and ultimate perspectives.\nEvolutionary Anthropology: Issues, News, and Reviews,\n20(1), 9–23. https://doi.org/10.1002/evan.20289\n\n\nHoffman, L. (2015). Longitudinal analysis: Modeling\nwithin-Person fluctuation and change (1 edition).\nRoutledge. https://www.routledge.com/Longitudinal-Analysis-Modeling-Within-Person-Fluctuation-and-Change/Hoffman/p/book/9780415876025\n\n\nHowell, N. (2001). Demography of the dobe! Kung\n(2nd Edition). Routledge. https://www.routledge.com/Demography-of-the-Dobe-Kung/Howell/p/book/9780202306490\n\n\nHowell, N. (2010). Life histories of the Dobe!\nKung: Food, fatness, and well-being over the\nlife span (Vol. 4). Univ of California Press. https://www.ucpress.edu/book/9780520262348/life-histories-of-the-dobe-kung\n\n\nJohnson, W., Carothers, A., & Deary, I. J. (2008). Sex differences\nin variability in general intelligence: A new look at the\nold question. Perspectives on Psychological Science,\n3(6), 518–531. https://doi.org/10.1111/j.1745-6924.2008.00096.x\n\n\nKahle, D., & Stamey, J. (2017). invgamma: The inverse gamma\ndistribution [Manual]. https://CRAN.R-project.org/package=invgamma\n\n\nKale, A., Kay, M., & Hullman, J. (2020). Visual reasoning strategies\nfor effect size judgments and decisions. IEEE Transactions on\nVisualization and Computer Graphics. https://doi.org/10.1109/TVCG.2020.3030335\n\n\nKay, M. (2020). Marginal distribution of a single correlation from\nan LKJ distribution. https://mjskay.github.io/ggdist/reference/lkjcorr_marginal.html\n\n\nKay, M. (2021). Extracting and visualizing tidy draws from brms\nmodels. https://mjskay.github.io/tidybayes/articles/tidy-brms.html\n\n\nKay, M. (2022). tidybayes:\nTidy data and ’geoms’ for Bayesian\nmodels. https://CRAN.R-project.org/package=tidybayes\n\n\nKennedy, L., & Gelman, A. (2021). Know your population and know your\nmodel: Using model-based regression and poststratification\nto generalize findings beyond the observed sample. Psychological\nMethods, 26(5), 547–558. https://doi.org/10.1037/met0000362\n\n\nKievit, R., Frankenhuis, W. E., Waldorp, L., & Borsboom, D. (2013).\nSimpson’s paradox in psychological science: A practical guide.\nFrontiers in Psychology, 4. https://doi.org/10.3389/fpsyg.2013.00513\n\n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B.,\nAlper, S., Aveyard, M., Axt, J. R., Babalola, M. T., Bahník, Š., Batra,\nR., Berkics, M., Bernstein, M. J., Berry, D. R., Bialobrzeska, O.,\nBinan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. A.\n(2018). Many Labs 2: Investigating variation\nin replicability across samples and settings. Advances in Methods\nand Practices in Psychological Science, 1(4), 443–490. https://doi.org/10.1177/2515245918810225\n\n\nKline, M. A., & Boyd, R. (2010). Population size predicts\ntechnological complexity in Oceania. Proceedings of the\nRoyal Society B: Biological Sciences, 277(1693),\n2559–2564. https://doi.org/10.1098/rspb.2010.0452\n\n\nKolczynska, M., Bürkner, P.-C., Kennedy, L., & Vehtari, A. (2020).\nTrust in state institutions in Europe, 1989-2019.\nSocArXiv. https://doi.org/10.31235/osf.io/3v5g7\n\n\nKoster, J. M., & Leckie, G. (2014). Food sharing networks in lowland\nNicaragua: An application of the social\nrelations model to count data. Social Networks, 38,\n100–110. https://doi.org/10.1016/j.socnet.2014.02.002\n\n\nKruschke, J. K. (2015). Doing Bayesian data analysis:\nA tutorial with R, JAGS, and\nStan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\n\n\nKullback, S., & Leibler, R. A. (1951). On information and\nsufficiency. Annals of Mathematical Statistics, 22(1),\n79–86. https://doi.org/10.1214/aoms/1177729694\n\n\nKurz, A. S. (2024). Statistical Rethinking 2 with rstan and the tidyverse (version 0.0.3). https://solomon.quarto.pub/sr2rstan/\n\n\nKurz, A. S. (2026a). Applied longitudinal data analysis in brms and\nthe tidyverse (version 0.0.4). https://solomon.quarto.pub/alda/\n\n\nKurz, A. S. (2026b). Doing Bayesian data analysis in\nbrms and the tidyverse (Version 1.3.0). https://solomon.quarto.pub/dbda2/\n\n\nKurz, A. S. (2026c). Recoding Introduction to\nMediation, Moderation, and Conditional Process Analysis\n(version 1.4.0). https://solomon.quarto.pub/immcpa2/\n\n\nKurz, A. S. (2026d). Statistical rethinking with brms, ggplot2, and the tidyverse (version 1.4.0).\nhttps://solomon.quarto.pub/sr/\n\n\nKurz, A. S., DeBeer, B. B., Kimbrel, N. A., Morissette, S. B., &\nMeyer, E. C. (2019, October 16). Even with treatment, functional\nimpairment and quality of life remain remarkably stable over two years\nin post-9/11 Iraq and Afghanistan war\nveterans. The 4th Annual San Antonio Combat PTSD\nConference. https://osf.io/vekpf/\n\n\nLinnebo, Ø. (2018). Platonism in the philosophy of mathematics. In E. N.\nZalta (Ed.), The Stanford Encyclopedia of\nPhilosophy (Spring 2018). Metaphysics Research Lab,\nStanford University. https://plato.stanford.edu/archives/spr2018/entries/platonism-mathematics/\n\n\nLittle, R. J. A., & Rubin, D. B. (2019). Statistical analysis\nwith missing data. John Wiley & Sons. https://www.wiley.com/en-us/Statistical+Analysis+with+Missing+Data%2C+3rd+Edition-p-9780470526798\n\n\nLotka, A. J. (1925). Principles of physical biology. Waverly.\n\n\nMatejka, J., & Fitzmaurice, G. (2017). Same stats, different\ngraphs: Generating datasets with varied appearance and\nidentical statistics through simulated annealing. https://www.autodesk.com/research/publications/same-stats-different-graphs\n\n\nMcCullagh, P., & Nelder, J. A. (1989). Generalized linear\nmodels (Second Edition). Chapman and Hall.\n\n\nMcElreath, R. (2015). Statistical rethinking: A\nBayesian course with examples in R and\nStan. CRC press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMcElreath, R. (2020a). rethinking\nR package. https://xcelab.net/rm/software/\n\n\nMcElreath, R. (2020b). Statistical rethinking: A\nBayesian course with examples in R and\nStan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\n\n\nMeehl, P. E. (1990). Why summaries of research on psychological theories\nare often uninterpretable. Psychological Reports,\n66(1), 195–244. https://doi.org/10.2466/pr0.1990.66.1.195\n\n\nMerkle, E. C., Fitzsimmons, E., Uanhoro, J., & Goodrich, B. (2021).\nEfficient Bayesian structural equation modeling in\nStan. Journal of Statistical Software,\n100(6), 1–22. https://doi.org/10.18637/jss.v100.i06\n\n\nMerkle, E. C., & Rosseel, Y. (2018). blavaan: Bayesian structural equation\nmodels via parameter expansion. Journal of Statistical\nSoftware, 85(4), 1–30. https://doi.org/10.18637/jss.v085.i04\n\n\nMerkle, E. C., Rosseel, Y., & Goodrich, B. (2022). blavaan: Bayesian latent variable\nanalysis. https://CRAN.R-project.org/package=blavaan\n\n\nMüller, K., & Wickham, H. (2022). tibble: Simple data frames. https://CRAN.R-project.org/package=tibble\n\n\nNavarro, D. (2019). Learning statistics with R. https://learningstatisticswithr.com\n\n\nNavarro, D. J. (2019). Between the devil and the deep blue sea:\nTensions between scientific judgement and statistical model\nselection. Computational Brain & Behavior, 2(1),\n28–34. https://doi.org/10.1007/s42113-018-0019-z\n\n\nNogueira, R. G., Jadhav, A. P., Haussen, D. C., Bonafe, A., Budzik, R.\nF., Bhuva, P., Yavagal, D. R., Ribo, M., Cognard, C., Hanel, R. A., et\nal. (2018). Thrombectomy 6 to 24 hours after stroke with a mismatch\nbetween deficit and infarct. New England Journal of Medicine,\n378(1), 11–21. https://doi.org/10.1056/NEJMoa1706442\n\n\nNowosad, J. (2019). rcartocolor:\n’CARTOColors’ palettes. https://CRAN.R-project.org/package=rcartocolor\n\n\nNunn, N., & Puga, D. (2012). Ruggedness: The blessing\nof bad geography in Africa. Review of Economics and\nStatistics, 94(1), 20–36. https://doi.org/10.1162/REST_a_00161\n\n\nPaananen, T., Bürkner, P.-C., Vehtari, A., & Gabry, J. (2020).\nAvoiding model refits in leave-one-out cross-validation with moment\nmatching. https://CRAN.R-project.org/package=loo/vignettes/loo2-moment-matching.html\n\n\nPaananen, T., Piironen, J., Bürkner, P.-C., & Vehtari, A. (2020).\nImplicitly adaptive importance sampling. http://arxiv.org/abs/1906.08850\n\n\nParadis, Emmanuel, Blomberg, S., Bolker, B., Brown, J., Claramunt, S.,\nClaude, J., Cuong, H. S., Desper, R., Didier, G., Durand, B., Dutheil,\nJ., Ewing, R., Gascuel, O., Guillerme, T., Heibl, C., Ives, A., Jones,\nB., Krah, F., Lawson, D., … de Vienne, D. (2022). ape: Analyses of phylogenetics and\nevolution [Manual]. https://CRAN.R-project.org/package=ape\n\n\nParadis, E., & Schliep, K. (2019). ape\n5.0: An environment for modern phylogenetics and\nevolutionary analyses in R. Bioinformatics (Oxford,\nEngland), 35, 526–528. https://doi.org/10.1093/bioinformatics/bty633\n\n\nPark, D. K., Gelman, A., & Bafumi, J. (2004). Bayesian multilevel\nestimation with poststratification: State-level estimates from national polls.\nPolitical Analysis, 12(4), 375–385. https://www.jstor.org/stable/25791784\n\n\nPedersen, T. L. (2022). patchwork:\nThe composer of plots. https://CRAN.R-project.org/package=patchwork\n\n\nPeng, R. D. (2022). R programming for data science. https://bookdown.org/rdpeng/rprogdatascience/\n\n\nPeng, R. D., Kross, S., & Anderson, B. (2017). Mastering\nsoftware development in {R}. https://github.com/rdpeng/RProgDA\n\n\nPivot data from wide to long — pivot_longer. (2020). https://tidyr.tidyverse.org/reference/pivot_longer.html\n\n\nPivoting. (2020). https://tidyr.tidyverse.org/articles/pivot.html\n\n\nPlummer, M. (2003). JAGS: A program for\nanalysis of Bayesian graphical models using\nGibbs sampling. Working Papers, 8. http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Plummer.pdf\n\n\nR Core Team. (2022). R: A language and environment for\nstatistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nR Library Contrast Coding Systems for categorical\nvariables. (n.d.). Retrieved October 14, 2020, from https://stats.idre.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/\n\n\nRam, K., & Wickham, H. (2018). wesanderson: A Wes Anderson palette\ngenerator [Manual]. https://CRAN.R-project.org/package=wesanderson\n\n\nRast, P., Hofer, S. M., & Sparks, C. (2012). Modeling individual\ndifferences in within-person variation of negative and positive affect\nin a mixed effects location scale model using\nBUGS/JAGS. Multivariate Behavioral\nResearch, 47(2), 177–200. https://doi.org/10.1080/00273171.2012.658328\n\n\nRevelle, W. (2022). psych:\nProcedures for psychological, psychometric, and personality\nresearch. https://CRAN.R-project.org/package=psych\n\n\nRipley, B. (2022). MASS: Support functions\nand datasets for venables and Ripley’s\nMASS. https://CRAN.R-project.org/package=MASS\n\n\nRoback, P., & Legler, J. (2021). Beyond multiple linear\nregression: Applied generalized linear models and\nmultilevel models in R. CRC Press. https://bookdown.org/roback/bookdown-BeyondMLR/\n\n\nRobert, C., & Casella, G. (2011). A short history of\nMarkov chain Monte Carlo:\nSubjective recollections from incomplete data.\nStatistical Science, 26(1), 102–115. https://arxiv.org/pdf/0808.2902.pdf\n\n\nRobinson, D., Hayes, A., & Couch, S. (2022). broom: Convert statistical objects\ninto tidy tibbles [Manual]. https://CRAN.R-project.org/package=broom\n\n\nRoss, C. T., Winterhalder, B., & McElreath, R. (2020). Racial\ndisparities in police use of deadly force against unarmed individuals\npersist after appropriately benchmarking shooting data on violent crime\nrates. Social Psychological and Personality Science,\n1948550620916071. https://doi.org/10.1177/1948550620916071\n\n\nRow-wise operations. (2026). https://dplyr.tidyverse.org/articles/rowwise.html\n\n\nRubin, Donald B. (1976). Inference and missing data.\nBiometrika, 63(3), 581–592. https://doi.org/10.1093/biomet/63.3.581\n\n\nRubin, Donald B. (1987). Multiple imputation for nonresponse in\nsurveys. John Wiley & Sons Inc. https://doi.org/10.1002/9780470316696\n\n\nRubin, Donald B. (1996). Multiple imputation after 18+ years.\nJournal of the American Statistical Association,\n91(434), 473–489. https://doi.org/10.1080/01621459.1996.10476908\n\n\nRudis, B. (2020). statebins:\nCreate united states uniform cartogram heatmaps\n[Manual]. https://CRAN.R-project.org/package=statebins\n\n\nRudis, B., Ross, N., & Garnier, S. (2018). The viridis color\npalettes. https://cran.r-project.org/package=viridis/vignettes/intro-to-viridis.html\n\n\nRue, H., Martino, S., & Chopin, N. (2009). Approximate\nBayesian inference for latent Gaussian models\nby using integrated nested Laplace approximations.\nJournal of the Royal Statistical Society: Series b (Statistical\nMethodology), 71(2), 319–392. https://doi.org/10.1111/j.1467-9868.2008.00700.x\n\n\nSchloerke, B., Crowley, J., Di Cook, Briatte, F., Marbach, M., Thoen,\nE., Elberg, A., & Larmarange, J. (2021). GGally:\nExtension to ’ggplot2’. https://CRAN.R-project.org/package=GGally\n\n\nShannon, C. E. (1948). A mathematical theory of communication. Bell\nSystem Technical Journal, 27(3), 379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x\n\n\nSilbiger, N. J., Goodbody-Gringley, G., Bruno, J. F., & Putnam, H.\nM. (2019). Comparative thermal performance of the reef-building coral\nOrbicella franksi at its latitudinal range limits.\nMarine Biology, 166(10), 1–14. https://doi.org/10.1007/s00227-019-3573-6\n\n\nSilk, J. B., Brosnan, S. F., Vonk, J., Henrich, J., Povinelli, D. J.,\nRichardson, A. S., Lambeth, S. P., Mascaro, J., & Schapiro, S. J.\n(2005). Chimpanzees are indifferent to the welfare of unrelated group\nmembers. Nature, 437(7063), 1357–1359. https://doi.org/10.1038/nature04243\n\n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011).\nFalse-positive psychology: Undisclosed flexibility in data\ncollection and analysis allows presenting anything as significant.\nPsychological Science, 22(11), 1359–1366. https://doi.org/10.1177/0956797611417632\n\n\nSinger, J. D., & Willett, J. B. (2003). Applied longitudinal\ndata analysis: Modeling change and event occurrence.\nOxford University Press, USA. https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780195152968.001.0001/acprof-9780195152968\n\n\nSlowikowski, K. (2022). ggrepel:\nAutomatically position non-overlapping text labels with\n’ggplot2’. https://CRAN.R-project.org/package=ggrepel\n\n\nSpiegelhalter, D. J., Best, N. G., Carlin, B. P., & Linde, A. V. D.\n(2002). Bayesian measures of model complexity and fit. Journal of\nthe Royal Statistical Society: Series B (Statistical Methodology),\n64(4), 583–639. https://doi.org/10.1111/1467-9868.00353\n\n\nSpiegelhalter, D., Thomas, A., Best, N., & Lunn, D. (2003).\nWinBUGS user manual. https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/manual14.pdf\n\n\nStan Development Team. (2022a). Stan functions reference,\nVersion 2.31. https://mc-stan.org/docs/functions-reference/\n\n\nStan Development Team. (2022b). Stan reference manual,\nVersion 2.31. https://mc-stan.org/docs/reference-manual/index.html\n\n\nStan Development Team. (2022c). Stan user’s guide,\nVersion 2.31. https://mc-stan.org/docs/stan-users-guide/index.html\n\n\nStan Development Team. (2023). RStan: The R\nInterface to Stan. https://CRAN.R-project.org/package=rstan/vignettes/rstan.html\n\n\nStreet, S. E., Navarrete, A. F., Reader, S. M., & Laland, K. N.\n(2017). Coevolution of cultural intelligence, extended life history,\nsociality, and brain size in primates. Proceedings of the National\nAcademy of Sciences, 114(30), 7908–7914. https://doi.org/10.1073/pnas.1620734114\n\n\nSubramanian, S. V., Kim, R., & Christakis, N. A. (2018). The\n“average” treatment effect: A construct ripe\nfor retirement. A commentary on Deaton and\nCartwright. Social Science & Medicine,\n210, 77–82. https://doi.org/10.1016/j.socscimed.2018.04.027\n\n\nTextor, J., van der Zander, B., & Ankan, A. (2021). dagitty: Graphical analysis of\nstructural causal models. https://CRAN.R-project.org/package=dagitty\n\n\nTextor, J., van der Zander, B., Gilthorpe, M. S., Liśkiewicz, M., &\nEllison, G. T. (2016). Robust causal inference using directed acyclic\ngraphs: The R package ’dagitty’. International Journal\nof Epidemiology, 45(6), 1887–1894. https://doi.org/10.1093/ije/dyw341\n\n\nThoen, E. (2022). dutchmasters\n[Manual]. https://github.com/EdwinTh/dutchmasters\n\n\nTufte, E. R. (2001). The visual display of quantitative\ninformation (Second Edition). Graphics Press. https://www.edwardtufte.com/tufte/books_vdqi\n\n\nvan Buuren, S. (2018). Flexible imputation of missing data\n(Second Edition). CRC Press. https://stefvanbuuren.name/fimd/\n\n\nvan Leeuwen, E. J. C., Cohen, E., Collier-Baker, E., Rapold, C. J.,\nSchäfer, M., Schütte, S., & Haun, D. B. M. (2018). The development\nof human social learning across seven societies. Nature\nCommunications, 9(1), 2076. https://doi.org/10.1038/s41467-018-04468-2\n\n\nVehtari, A., Gabry, J., Magnusson, M., Yao, Y., & Gelman, A. (2022).\nloo: Efficient\nleave-one-out cross-validation and WAIC for bayesian\nmodels. https://CRAN.R-project.org/package=loo/\n\n\nVehtari, A., Gelman, A., & Gabry, J. (2017). Practical\nBayesian model evaluation using leave-one-out\ncross-validation and WAIC. Statistics and\nComputing, 27(5), 1413–1432. https://doi.org/10.1007/s11222-016-9696-4\n\n\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner,\nP.-C. (2019). Rank-normalization, folding, and localization:\nAn improved for assessing convergence of\nMCMC. https://arxiv.org/abs/1903.08008?\n\n\nVenables, W. N., & Ripley, B. D. (2002). Modern applied\nstatistics with S (Fourth Edition). Springer. http://www.stats.ox.ac.uk/pub/MASS4\n\n\nVermeer, J. (1665). Girl with a pearl earring.\n\n\nVolterra, V. (1926). Fluctuations in the abundance of a species\nconsidered mathematically. Nature, 118(2972), 558–560.\nhttps://doi.org/10.1038/118558a0\n\n\nvon Bertalanffy, L. (1934). Untersuchungen Über die Gesetzlichkeit des\nWachstums. Wilhelm Roux’ Archiv für Entwicklungsmechanik der\nOrganismen, 131(4), 613–652. https://doi.org/10.1007/BF00650112\n\n\nVonesh, J. R., & Bolker, B. M. (2005). Compensatory larval responses\nshift trade-offs associated with predator-induced hatching plasticity.\nEcology, 86(6), 1580–1591. https://doi.org/10.1890/04-0535\n\n\nWalker, K. (2022). Tigris: Load census\nTIGER/Line shapefiles [Manual]. https://github.com/walkerke/tigris\n\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA\nstatement on p-values: Context, process, and purpose.\nThe American Statistician, 70(2), 129–133. https://doi.org/10.1080/00031305.2016.1154108\n\n\nWatanabe, S. (2010). Asymptotic equivalence of Bayes cross\nvalidation and widely applicable information criterion in singular\nlearning theory. Journal of Machine Learning Research,\n11(116), 3571–3594. http://jmlr.org/papers/v11/watanabe10a.html\n\n\nWatson, D., Clark, L. A., & Tellegen, A. (1988). Development and\nvalidation of brief measures of positive and negative affect: The\nPANAS scales. Journal of Personality and Social\nPsychology, 54(6), 1063–1070. https://doi.org/10.1037/0022-3514.54.6.1063\n\n\nWeber, S., & Bürkner, P.-C. (2022). Running brms models with\nwithin-chain parallelization. https://CRAN.R-project.org/package=brms/vignettes/brms_threading.html\n\n\nWhitehouse, H., François, P., Savage, P. E., Currie, T. E., Feeney, K.\nC., Cioni, E., Purcell, R., Ross, R. M., Larson, J., Baines, J., ter\nHaar, B., Covey, A., & Turchin, P. (2019). Complex societies precede\nmoralizing gods throughout world history. Nature,\n568(7751), 226–229. https://doi.org/10.1038/s41586-019-1043-4\n\n\nWickham, H. (2016). ggplot2:\nElegant graphics for data analysis. Springer-Verlag\nNew York. https://ggplot2-book.org/\n\n\nWickham, H. (2020). The tidyverse style guide. https://style.tidyverse.org/\n\n\nWickham, H. (2022). tidyverse:\nEasily install and load the ’tidyverse’. https://CRAN.R-project.org/package=tidyverse\n\n\nWickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D.,\nFrançois, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M.,\nPedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J.,\nRobinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to\nthe tidyverse. Journal of Open Source Software, 4(43),\n1686. https://doi.org/10.21105/joss.01686\n\n\nWickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K.,\nWilke, C., Woo, K., Yutani, H., & Dunnington, D. (2022). ggplot2: Create elegant data\nvisualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2\n\n\nWiecek, W., & Meager, R. (2022). baggr: Bayesian aggregate treatment\neffects [Manual]. https://CRAN.R-project.org/package=baggr\n\n\nWilke, C. O. (2019). Fundamentals of data visualization. https://clauswilke.com/dataviz/\n\n\nWilks, S. S. (1938). The large-sample distribution of the likelihood\nratio for testing composite hypotheses. The Annals of Mathematical\nStatistics, 9(1), 60–62. https://doi.org/10.1214/aoms/1177732360\n\n\nWilliams, Donald R., Martin, S. R., Liu, S., & Rast, P. (2020).\nBayesian multivariate mixed-effects location scale modeling of\nlongitudinal relations among affective traits, states, and physical\nactivity. European Journal of Psychological Assessment,\n36(6), 981–997. https://doi.org/10.1027/1015-5759/a000624\n\n\nWilliams, Donald R., Martin, S. R., Liu, S., & Rast, P. (2021).\nBayesian multivariate mixed-effects location scale modeling of\nlongitudinal relations among affective traits, states, and physical\nactivity. European Journal of Psychological Assessment. https://doi.org/10.1027/1015-5759/a000624\n\n\nWilliams, Donald R., Martin, S. R., & Rast, P. (2022). Putting the\nindividual into reliability: Bayesian testing of\nhomogeneous within-Person variance in hierarchical models.\nBehavior Research Methods, 54(3), 1272–1290. https://doi.org/10.3758/s13428-021-01646-x\n\n\nWilliams, Donald R., Mulder, J., Rouder, J. N., & Rast, P. (2021).\nBeneath the surface: Unearthing within-Person\nvariability and mean relations with Bayesian mixed models.\nPsychological Methods, 26(1), 74. https://doi.org/10.1037/met0000270\n\n\nWilliams, Donald R., Rast, P., & Bürkner, P.-C. (2018). Bayesian\nmeta-analysis with weakly informative prior distributions. https://doi.org/10.31234/osf.io/7tbrm\n\n\nWilliams, Donald R., Zimprich, D. R., & Rast, P. (2019). A\nBayesian nonlinear mixed-effects location scale model for\nlearning. Behavior Research Methods, 51(5), 1968–1986.\nhttps://doi.org/10.3758/s13428-019-01255-9\n\n\nWood, S. N. (2003). Thin-plate regression splines. Journal of the\nRoyal Statistical Society (B), 65(1), 95–114. https://doi.org/10.1111/1467-9868.00374\n\n\nWood, S. N. (2004). Stable and efficient multiple smoothing parameter\nestimation for generalized additive models. Journal of the American\nStatistical Association, 99(467), 673–686. https://doi.org/10.1198/016214504000000980\n\n\nWood, S. N. (2011). Fast stable restricted maximum likelihood and\nmarginal likelihood estimation of semiparametric generalized linear\nmodels. Journal of the Royal Statistical Society (B),\n73(1), 3–36. https://doi.org/10.1111/j.1467-9868.2010.00749.x\n\n\nWood, S. N. (2017a). Generalized additive models: An\nintroduction with R (2nd ed.). Chapman and\nHall/CRC. https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331?utm_source=crcpress.com&utm_medium=referral\n\n\nWood, S. N. (2017b). Generalized additive models: An\nintroduction with R (Second Edition). CRC Press. https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331\n\n\nWood, S. N. (2022). mgcv: Mixed\nGAM computation vehicle with automatic smoothness\nestimation. https://CRAN.R-project.org/package=mgcv\n\n\nWood, S. N., Pya, N., & Säfken, B. (2016). Smoothing parameter and\nmodel selection for general smooth models (with discussion). Journal\nof the American Statistical Association, 111, 1548–1575.\nhttps://doi.org/10.1080/01621459.2016.1180986\n\n\nXie, Y., Allaire, J. J., & Grolemund, G. (2020). R markdown:\nThe definitive guide. Chapman and\nHall/CRC. https://bookdown.org/yihui/rmarkdown/\n\n\nYao, Y., Vehtari, A., Simpson, D., & Gelman, A. (2018). Using\nstacking to average Bayesian predictive distributions (with\ndiscussion). Bayesian Analysis, 13(3), 917–1007. https://doi.org/10.1214/17-BA1091\n\n\nYarkoni, T., & Westfall, J. (2017). Choosing prediction over\nexplanation in psychology: Lessons from machine learning.\nPerspectives on Psychological Science : A Journal of the Association\nfor Psychological Science, 12(6), 1100–1122. https://doi.org/10.1177/1745691617693393\n\n\nYu, G. (2020a). Using ggtree to visualize data on tree-like structures.\nCurrent Protocols in Bioinformatics, 69(1), e96. https://doi.org/10.1002/cpbi.96\n\n\nYu, G. (2020b). Data integration, manipulation and visualization of\nphylogenetic trees. https://yulab-smu.github.io/treedata-book/\n\n\nYu, G., Lam, T. T.-Y., Zhu, H., & Guan, Y. (2018). Two methods for\nmapping and visualizing associated data on phylogeny using ggtree.\nMolecular Biology and Evolution, 35(12), 3041–3043. https://doi.org/10.1093/molbev/msy194\n\n\nYu, G., Smith, D. K., Zhu, H., Guan, Y., & Lam, T. T.-Y. (2017).\nggtree: An R package for\nvisualization and annotation of phylogenetic trees with their covariates\nand other associated data. Methods in Ecology and Evolution,\n8(1), 28–36. https://doi.org/10.1111/2041-210X.12628\n\n\nZhang, Y., & Yang, Y. (2015). Cross-validation for selecting a model\nselection procedure. Journal of Econometrics, 187(1),\n95–112. https://doi.org/10.1016/j.jeconom.2015.02.006",
    "crumbs": [
      "References"
    ]
  }
]