
@book{agrestiFoundationsLinearGeneralized2015,
  title = {Foundations of Linear and Generalized Linear Models},
  author = {Agresti, Alan},
  year = {2015},
  month = jan,
  publisher = {{John Wiley \& Sons}},
  url = {https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034},
  abstract = {A valuable overview of the most important ideas and results in statistical modeling Written by a highly-experienced author, Foundations of Linear and Generalized Linear Models is a clear and comprehensive guide to the key concepts and results of linearstatistical models. The book presents a broad, in-depth overview of the most commonly usedstatistical models by discussing the theory underlying the models, R software applications,and examples with crafted models to elucidate key ideas and promote practical modelbuilding. The book begins by illustrating the fundamentals of linear models, such as how the model-fitting projects the data onto a model vector subspace and how orthogonal decompositions of the data yield information about the effects of explanatory variables. Subsequently, the book covers the most popular generalized linear models, which include binomial and multinomial logistic regression for categorical data, and Poisson and negative binomial loglinear models for count data. Focusing on the theoretical underpinnings of these models, Foundations ofLinear and Generalized Linear Models also features:  An introduction to quasi-likelihood methods that require weaker distributional assumptions, such as generalized estimating equation methods An overview of linear mixed models and generalized linear mixed models with random effects for clustered correlated data, Bayesian modeling, and extensions to handle problematic cases such as high dimensional problems Numerous examples that use R software for all text data analyses More than 400 exercises for readers to practice and extend the theory, methods, and data analysis A supplementary website with datasets for the examples and exercises  An invaluable textbook for upper-undergraduate and graduate-level students in statistics and biostatistics courses, Foundations of Linear and Generalized Linear Models is also an excellent reference for practicing statisticians and biostatisticians, as well as anyone who is interested in learning about the most important statistical models for analyzing data.},
  googlebooks = {dgIzBgAAQBAJ},
  isbn = {978-1-118-73005-8},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  language = {en}
}

@incollection{akaike1998information,
  title = {Information Theory and an Extension of the Maximum Likelihood Principle},
  booktitle = {Selected Papers of {{Hirotugu Akaike}}},
  author = {Akaike, Hirotogu},
  year = {1998},
  pages = {199--213},
  publisher = {{Springer}},
  url = {https://www.springer.com/gp/book/9780387983554}
}

@article{amrheinScientistsRiseStatistical2019,
  title = {Scientists Rise up against Statistical Significance},
  author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
  year = {2019},
  month = mar,
  volume = {567},
  pages = {305--307},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-019-00857-9},
  url = {https://www.nature.com/articles/d41586-019-00857-9},
  urldate = {2020-05-21},
  abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
  copyright = {2020 Nature},
  file = {/Users/solomonkurz/Zotero/storage/5JQ3EHZV/Amrhein et al. - 2019 - Scientists rise up against statistical significanc.pdf;/Users/solomonkurz/Zotero/storage/IWDNAU3J/d41586-019-00857-9.html},
  journal = {Nature},
  language = {en},
  number = {7748}
}

@article{barrettAnIntroduction2020,
  title = {An Introduction to Ggdag},
  author = {Barrett, Malcolm},
  year = {2020},
  month = feb,
  url = {https://CRAN.R-project.org/package=ggdag/vignettes/intro-to-ggdag.html},
  urldate = {2020-05-31},
  language = {English}
}

@article{betancourtBayesSparse2018,
  title = {Bayes Sparse Regression},
  author = {Betancourt, Michael},
  year = {2018},
  month = mar,
  url = {https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html},
  language = {English}
}

@incollection{borgesjlJardinSenderosQue1941,
  title = {El Jardin de Senderos Que Se Bifurcan. {{Buenos Aires}}: {{Sur}}. {{Translated}} by {{D}}. {{A}}. {{Yates}} (1964)},
  booktitle = {Labyrinths: {{Selected Stories}} \& {{Other Writings}}},
  author = {{Borges, JL}},
  year = {1941},
  pages = {19--29},
  publisher = {{New Directions}},
  address = {{New York}}
}

@book{brms2020RM,
  title = {{{brms}} Reference Manual, {{Version}} 2.12.0},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2020},
  url = {https://CRAN.R-project.org/package=brms/brms.pdf}
}

@book{bugs2003UM,
  title = {{{WinBUGS}} User Manual},
  author = {Spiegelhalter, David and Thomas, Andrew and Best, Nicky and Lunn, Dave},
  year = {2003},
  month = jan,
  url = {https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/manual14.pdf}
}

@article{Bürkner2020Multivariate,
  title = {Estimating Multivariate Models with Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2020},
  month = feb,
  url = {https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html}
}

@article{Bürkner2020Non_linear,
  title = {Estimating Non-Linear Models with Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2020},
  month = feb,
  url = {https://CRAN.R-project.org/package=brms/vignettes/brms_nonlinear.html}
}

@article{burknerAdvancedBayesianMultilevel2018,
  title = {Advanced {{Bayesian}} Multilevel Modeling with the {{R}} Package Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2018},
  volume = {10},
  pages = {395--411},
  doi = {10.32614/RJ-2018-017},
  journal = {The R Journal},
  number = {1}
}

@article{burknerBrmsPackageBayesian2017,
  title = {{{brms}}: {{An R}} Package for {{Bayesian}} Multilevel Models Using {{Stan}}},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2017},
  volume = {80},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  journal = {Journal of Statistical Software},
  number = {1}
}

@article{carpenterStanProbabilisticProgramming2017,
  title = {Stan: {{A}} Probabilistic Programming Language},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  volume = {76},
  publisher = {{Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA \ldots}},
  doi = {10.18637/jss.v076.i01},
  url = {https://www.osti.gov/servlets/purl/1430202},
  journal = {Journal of statistical software},
  number = {1}
}

@inproceedings{carvalho2009handling,
  title = {Handling Sparsity via the Horseshoe},
  booktitle = {Artificial Intelligence and Statistics},
  author = {Carvalho, Carlos M and Polson, Nicholas G and Scott, James G},
  year = {2009},
  pages = {73--80},
  url = {http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf}
}

@article{casellaExplainingGibbsSampler1992,
  title = {Explaining the {{Gibbs}} Sampler},
  author = {Casella, George and George, Edward I.},
  year = {1992},
  month = aug,
  volume = {46},
  pages = {167--174},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1992.10475878},
  url = {https://ecommons.cornell.edu/bitstream/handle/1813/31670/BU-1098-MA.Revised.pdf?sequence=1},
  urldate = {2020-06-11},
  abstract = {Computer-intensive algorithms, such as the Gibbs sampler, have become increasingly popular statistical tools, both in applied and theoretical work. The properties of such algorithms, however, may sometimes not be obvious. Here we give a simple explanation of how and why the Gibbs sampler works. We analytically establish its properties in a simple case and provide insight for more complicated cases. There are also a number of examples.},
  file = {/Users/solomonkurz/Zotero/storage/7G3SEDKK/Casella and George - 1992 - Explaining the Gibbs Sampler.pdf;/Users/solomonkurz/Zotero/storage/SFZUD4XZ/00031305.1992.html},
  journal = {The American Statistician},
  keywords = {Data augmentation,Markov chains,Monte Carlo methods,Resampling techniques},
  number = {3}
}

@book{cover2006elements,
  title = {Elements of Information Theory},
  author = {Cover, Thomas M and Thomas, Joy A},
  year = {2006},
  edition = {2nd Edition},
  publisher = {{John Wiley \& Sons}},
  url = {https://www.wiley.com/en-us/Elements+of+Information+Theory\%2C+2nd+Edition-p-9780471241959},
  isbn = {978-0-471-24195-9}
}

@article{cummingNewStatisticsWhy2014,
  title = {The New Statistics: {{Why}} and How},
  shorttitle = {The {{New Statistics}}},
  author = {Cumming, Geoff},
  year = {2014},
  month = jan,
  volume = {25},
  pages = {7--29},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797613504966},
  url = {https://journals.sagepub.com/doi/pdf/10.1177/0956797613504966},
  urldate = {2020-05-21},
  abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.},
  file = {/Users/solomonkurz/Zotero/storage/UJMRBZGC/Cumming - 2014 - The New Statistics Why and How.pdf},
  journal = {Psychological Science},
  number = {1}
}

@article{derooijCrossvalidationMethodEvery2020,
  title = {Cross-Validation: {{A}} Method Every Psychologist Should Know},
  shorttitle = {Cross-{{Validation}}},
  author = {{de Rooij}, Mark and Weeda, Wouter},
  year = {2020},
  month = may,
  pages = {2515245919898466},
  publisher = {{SAGE Publications Inc}},
  issn = {2515-2459},
  doi = {10.1177/2515245919898466},
  url = {https://doi.org/10.1177/2515245919898466},
  urldate = {2020-06-03},
  abstract = {Cross-validation is a statistical procedure that every psychologist should know. Most are possibly familiar with the procedure in a global way but have not used it for the analysis of their own data. We introduce cross-validation for the purpose of model selection in a general sense, as well as an R package we have developed for this kind of analysis, and we present examples illustrating the use of this package for types of research problems that are often encountered in the social sciences. Cross-validation can be an easy-to-use alternative to null-hypothesis testing, and it has the benefit that it does not make as many assumptions.},
  file = {/Users/solomonkurz/Zotero/storage/S7SBFDUC/de Rooij and Weeda - 2020 - Cross-Validation A Method Every Psychologist Shou.pdf},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en}
}

@book{dunn2018generalized,
  title = {Generalized Linear Models with Examples in {{R}}},
  author = {Dunn, Peter K and Smyth, Gordon K},
  year = {2018},
  publisher = {{Springer}},
  url = {https://link.springer.com/book/10.1007/978-1-4419-0118-7}
}

@article{gabry2019visualization,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2019},
  volume = {182},
  pages = {389--402},
  publisher = {{Wiley Online Library}},
  doi = {10.1111/rssa.12378},
  url = {https://arxiv.org/abs/1709.01449},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  number = {2}
}

@article{gabryPlottingMCMCDraws2019,
  title = {Plotting {{MCMC}} Draws Using the Bayesplot Package},
  author = {Gabry, Jonah},
  year = {2019},
  month = nov,
  url = {https://CRAN.R-project.org/package=bayesplot/vignettes/plotting-mcmc-draws.html},
  urldate = {2020-05-26},
  language = {English}
}

@article{gabryVisualMCMCDiagnostics2020,
  title = {Visual {{MCMC}} Diagnostics Using the Bayesplot Package},
  author = {Gabry, Jonah and Modr{\'a}k, Martin},
  year = {2020},
  month = may,
  url = {https://CRAN.R-project.org/package=bayesplot/vignettes/plotting-mcmc-draws.html},
  urldate = {2020-06-11},
  language = {English}
}

@article{gelman2006difference,
  title = {The Difference between ``Significant'' and ``Not Significant'' Is Not Itself Statistically Significant},
  author = {Gelman, Andrew and Stern, Hal},
  year = {2006},
  volume = {60},
  pages = {328--331},
  publisher = {{Taylor \& Francis}},
  doi = {10.1198/000313006X152649},
  url = {https://www.tandfonline.com/doi/pdf/10.1198/000313006X152649?needAccess=true},
  journal = {The American Statistician},
  number = {4}
}

@book{gelman2013bayesian,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
  year = {2013},
  publisher = {{CRC press}},
  url = {https://stat.columbia.edu/~gelman/book/}
}

@article{gelmanAreConfidenceIntervals2019,
  title = {Are Confidence Intervals Better Termed ``Uncertainty Intervals''?},
  author = {Gelman, Andrew and Greenland, Sander},
  year = {2019},
  month = sep,
  pages = {l5381},
  issn = {0959-8138, 1756-1833},
  doi = {10.1136/bmj.l5381},
  url = {https://stat.columbia.edu/~gelman/research/published/uncertainty_intervals.pdf},
  urldate = {2020-05-21},
  file = {/Users/solomonkurz/Zotero/storage/TVDUC9Z3/Gelman and Greenland - 2019 - Are confidence intervals better termed “uncertaint.pdf},
  journal = {BMJ},
  language = {en}
}

@article{gelmanGardenForkingPaths2013,
  title = {The Garden of Forking Paths: {{Why}} Multiple Comparisons Can Be a Problem, Even When There Is No ``Fishing Expedition'' or ``p-Hacking'' and the Research Hypothesis Was Posited Ahead of Time},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2013},
  month = nov,
  pages = {17},
  url = {https://stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf},
  abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of fishing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-specified and, as a result, were contingent on data.},
  file = {/Users/solomonkurz/Zotero/storage/EA32DKC7/Gelman and Loken - The garden of forking paths Why multiple comparis.pdf},
  language = {en}
}

@article{gelmanPriorCanOften2017,
  title = {The Prior Can Often Only Be Understood in the Context of the Likelihood},
  author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
  year = {2017},
  month = oct,
  volume = {19},
  pages = {555},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/e19100555},
  url = {https://www.mdpi.com/1099-4300/19/10/555},
  urldate = {2020-06-12},
  abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys' priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/Users/solomonkurz/Zotero/storage/GITEJRKC/Gelman et al. - 2017 - The Prior Can Often Only Be Understood in the Cont.pdf;/Users/solomonkurz/Zotero/storage/FD2UD59C/555.html},
  journal = {Entropy},
  keywords = {Bayesian inference,default priors,prior distribution},
  language = {en},
  number = {10}
}

@article{gelmanRsquaredBayesianRegression2019,
  title = {R-Squared for {{Bayesian}} Regression Models},
  author = {Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Vehtari, Aki},
  year = {2019},
  month = jul,
  volume = {73},
  pages = {307--309},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1549100},
  url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1549100},
  urldate = {2020-05-16},
  journal = {The American Statistician},
  language = {en},
  number = {3}
}

@article{gemanStochasticRelaxationGibbs1984,
  title = {Stochastic Relaxation, {{Gibbs}} Distributions, and the {{Bayesian}} Restoration of Images},
  author = {Geman, Stuart and Geman, Donald},
  year = {1984},
  month = nov,
  volume = {PAMI-6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  url = {https://www.dam.brown.edu/people/documents/stochasticrelaxation.pdf},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  file = {/Users/solomonkurz/Zotero/storage/M4USX4TH/4767596.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Additive noise,Annealing,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,image restoration,Image restoration,line process,MAP estimate,Markov random field,Markov random fields,relaxation,scene modeling,spatial degradation,Stochastic processes,Temperature distribution},
  number = {6}
}

@book{grafenModernStatisticsLife2002,
  title = {Modern Statistics for the Life Sciences},
  author = {Grafen, Alan and Hails, Rosie},
  year = {2002},
  month = may,
  publisher = {{Oxford University Press}},
  address = {{Oxford, New York}},
  url = {https://global.oup.com/academic/product/modern-statistics-for-the-life-sciences-9780199252312?},
  abstract = {Model formulae represent a powerful methodology for describing, discussing, understanding, and performing the component of statistical tests known as linear statistics. It was developed for professional statisticians in the 1960s and has become increasingly available as the use of computers has grown and software has advanced. Modern Statistics for Life Scientists puts this methodology firmly within the grasp of undergraduates for the first time. The authors assume a basic knowledge of statistics--up to and including one and two sample t-tests and their non-parametric equivalents. They provide the conceptual framework needed to understand what the method does--but without mathematical proofs--and introduce the ideas in a simple and steady progression with worked examples and exercises at every stage.  This innovative text offers students a single conceptual framework for a wide range of tests-including t-tests, oneway and multiway analysis of variance, linear and polynomial regressions, and analysis of covariance-that are usually introduced separately. More importantly, it gives students a language in which they can frame questions and communicate with the computers that perform the analyses. A companion website, www.oup.com/grafenhails, provides a wealth of worked exercises in the three statistical languages; Minitab, SAS, and SPSS. Appropriate for use in statistics courses at undergraduate and graduate levels, Modern Statistics for the Life Sciences  is also a helpful resource for students in non-mathematics-based disciplines using statistics, such as geography, psychology, epidemiology, and ecology.},
  file = {/Users/solomonkurz/Zotero/storage/9CLZ5C5J/modern-statistics-for-the-life-sciences-9780199252312.html},
  isbn = {978-0-19-925231-2}
}

@book{grolemundDataScience2017,
  title = {R for Data Science},
  author = {Grolemund, Garrett and Wickham, Hadley},
  year = {2017},
  publisher = {{O'Reilly}},
  url = {https://r4ds.had.co.nz}
}

@misc{HadleyPrecisSource,
  title = {Hadley/Precis Source: {{R}}/Histospark.{{R}}},
  shorttitle = {Hadley/Precis Source},
  url = {https://rdrr.io/github/hadley/precis/src/R/histospark.R},
  urldate = {2020-05-22},
  abstract = {R/histospark.R defines the following functions:},
  file = {/Users/solomonkurz/Zotero/storage/WDBQG93J/histospark.html},
  language = {en}
}

@book{hastie2009elements,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  publisher = {{Springer Science \& Business Media}},
  doi = {10.1007/978-0-387-84858-7},
  url = {https://link.springer.com/book/10.1007\%2F978-0-387-84858-7}
}

@article{hauerHarmDoneTests2004,
  title = {The Harm Done by Tests of Significance},
  author = {Hauer, Ezra},
  year = {2004},
  month = may,
  volume = {36},
  pages = {495--500},
  issn = {00014575},
  doi = {10.1016/S0001-4575(03)00036-8},
  url = {https://statmodeling.stat.columbia.edu/wp-content/uploads/2013/03/1154-Hauer-The-harm-done-by-tests-of-significance.pdf},
  urldate = {2020-05-21},
  abstract = {Three historical episodes in which the application of null hypothesis significance testing (NHST) led to the mis-interpretation of data are described. It is argued that the pervasive use of this statistical ritual impedes the accumulation of knowledge and is unfit for use.},
  file = {/Users/solomonkurz/Zotero/storage/Y8LYGNMT/Hauer - 2004 - The harm done by tests of significance.pdf},
  journal = {Accident Analysis \& Prevention},
  language = {en},
  number = {3}
}

@book{hayes2017introduction,
  title = {Introduction to Mediation, Moderation, and Conditional Process Analysis: {{A}} Regression-Based Approach},
  author = {Hayes, Andrew F},
  year = {2017},
  publisher = {{Guilford publications}},
  url = {https://www.guilford.com/books/Introduction-to-Mediation-Moderation-and-Conditional-Process-Analysis/Andrew-Hayes/9781462534654},
  isbn = {978-1-4625-3465-4}
}

@article{hindePrimateMilkProximate2011,
  title = {Primate Milk: {{Proximate}} Mechanisms and Ultimate Perspectives},
  shorttitle = {Primate Milk},
  author = {Hinde, Katie and Milligan, Lauren A.},
  year = {2011},
  volume = {20},
  pages = {9--23},
  issn = {1520-6505},
  doi = {10.1002/evan.20289},
  url = {https://www.researchgate.net/publication/51751742_Primate_milk_Proximate_mechanisms_and_ultimate_perspectives},
  urldate = {2020-05-26},
  abstract = {To understand the evolutionary forces that have shaped primate lactation strategies, it is important to understand the proximate mechanisms of milk synthesis and their ecological and phylogenetic contexts. The lactation strategy of a species has four interrelated dimensions: the frequency and duration of nursing bouts, the period of lactation until weaning, the number and sex ratio of infants that a mother rears simultaneously, and the composition and yield of the milk that mothers synthesize. Milk synthesis, arguably the most physiologically costly component of rearing infants, remains the least studied. Energy transfer becomes energetically less efficient, transitioning from placental support to milk synthesis1, 2 just as the energy requirements for infant growth, development, and behavioral activity substantially increase. Here we review primate lactation biology and milk synthesis, integrating studies from anthropology, biology, nutrition, animal science, immunology, and biochemistry, to identify the derived and ancestral features of primate milks and enhance our understanding of primate life history.},
  copyright = {Copyright \textcopyright{} 2011 Wiley Periodicals, Inc.},
  file = {/Users/solomonkurz/Zotero/storage/7YIU3Z5X/evan.html},
  journal = {Evolutionary Anthropology: Issues, News, and Reviews},
  keywords = {infant development,lactation,life history,maternal investment,reproductive ecology},
  language = {en},
  number = {1}
}

@book{howell2001demography,
  title = {Demography of the Dobe! {{Kung}}},
  author = {Howell, Nancy},
  year = {2001},
  edition = {2nd Edition},
  publisher = {{Routledge}},
  url = {https://www.routledge.com/Demography-of-the-Dobe-Kung/Howell/p/book/9780202306490},
  isbn = {978-0-202-30649-0}
}

@book{howell2010life,
  title = {Life Histories of the {{Dobe}}! {{Kung}}: Food, Fatness, and Well-Being over the Life Span},
  author = {Howell, Nancy},
  year = {2010},
  volume = {4},
  publisher = {{Univ of California Press}},
  url = {https://www.ucpress.edu/book/9780520262348/life-histories-of-the-dobe-kung},
  isbn = {978-0-520-26234-8}
}

@misc{kayExtractingVisualizingTidy2020,
  title = {Extracting and Visualizing Tidy Draws from Brms Models},
  author = {Kay, Matthew},
  year = {2020},
  month = apr,
  url = {https://mjskay.github.io/tidybayes/articles/tidy-brms.html},
  urldate = {2020-05-17},
  abstract = {tidybayes},
  file = {/Users/solomonkurz/Zotero/storage/NT83AM3T/tidy-brms.html},
  language = {en}
}

@book{kruschkeDoingBayesianData2015,
  title = {Doing {{Bayesian}} Data Analysis: {{A}} Tutorial with {{R}}, {{JAGS}}, and {{Stan}}},
  author = {Kruschke, John K.},
  year = {2015},
  publisher = {{Academic Press}},
  url = {https://sites.google.com/site/doingbayesiandataanalysis/}
}

@article{kullbackInformationSufficiency1951,
  title = {On Information and Sufficiency},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  month = mar,
  volume = {22},
  pages = {79--86},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  url = {https://projecteuclid.org/euclid.aoms/1177729694},
  urldate = {2020-06-01},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {/Users/solomonkurz/Zotero/storage/ZRSE9FMG/Kullback and Leibler - 1951 - On Information and Sufficiency.pdf;/Users/solomonkurz/Zotero/storage/QVXJE6S9/1177729694.html},
  journal = {Annals of Mathematical Statistics},
  language = {EN},
  mrnumber = {MR39968},
  number = {1},
  zmnumber = {0042.38403}
}

@book{kurzDoingBayesianData2020,
  title = {Doing {{Bayesian}} Data Analysis in Brms and the Tidyverse},
  author = {Kurz, A. Solomon},
  year = {2020},
  month = may,
  edition = {version 0.2.0},
  url = {https://bookdown.org/content/3686/},
  urldate = {2020-05-22},
  abstract = {This project is an attempt to re-express the code in Kruschke's (2015) textbook. His models are re-fit in brms, plots are redone with ggplot2, and the general data wrangling code predominantly follows the tidyverse style.},
  file = {/Users/solomonkurz/Zotero/storage/UKHWZ73Z/3686.html}
}

@book{kurzRecodingIntroductionMediation2019,
  title = {Recoding {{Introduction}} to Mediation, Moderation, and Conditional Process Analysis},
  author = {Kurz, A. Solomon},
  year = {2019},
  month = dec,
  edition = {version 1.1.0},
  doi = {10.5281/zenodo.3589999},
  url = {https://bookdown.org/ajkurz/recoding_Hayes_2018/},
  urldate = {2020-06-10},
  abstract = {This project is an effort to connect his Hayes's conditional process analysis work with the Bayesian paradigm. Herein I refit his models with my favorite R package for Bayesian regression, B\"urkner's brms, and use the tidyverse for data manipulation and plotting.},
  file = {/Users/solomonkurz/Zotero/storage/IKVQT47J/recoding_Hayes_2018.html}
}

@book{kurzStatisticalRethinkingBrms2020,
  title = {Statistical Rethinking with Brms, Ggplot2, and the Tidyverse},
  author = {Kurz, A. Solomon},
  year = {2020},
  month = mar,
  edition = {version 1.1.0},
  doi = {10.5281/zenodo.3693202},
  url = {https://bookdown.org/content/3890/},
  urldate = {2020-05-16},
  abstract = {This project is an attempt to re-express the code in McElreath's textbook. His models are re-fit in brms, plots are redone with ggplot2, and the general data wrangling code predominantly follows the tidyverse style.},
  file = {/Users/solomonkurz/Zotero/storage/MTCXZRHZ/3890.html}
}

@book{leglerBroadeningYourStatistical2019,
  title = {Broadening Your Statistical Horizons: {{Generalized}} Linear Models and Multilevel Models},
  author = {Legler, Julie and Roback, Paul},
  year = {2019},
  url = {https://bookdown.org/roback/bookdown-bysh/}
}

@article{marinGgmcmcAnalysisMCMC2016,
  title = {{{ggmcmc}}: {{Analysis}} of {{MCMC}} Samples and {{Bayesian}} Inference},
  author = {i Mar{\'i}n, Xavier Fern{\'a}ndez},
  year = {2016},
  volume = {70},
  pages = {1--20},
  doi = {10.18637/jss.v070.i09},
  journal = {Journal of Statistical Software},
  number = {9}
}

@book{MASS2002,
  title = {Modern Applied Statistics with {{S}}},
  author = {Venables, W. N. and Ripley, B. D.},
  year = {2002},
  edition = {Fourth},
  publisher = {{Springer}},
  address = {{New York}},
  url = {http://www.stats.ox.ac.uk/pub/MASS4}
}

@book{mcelreathStatisticalRethinkingBayesian2015,
  title = {Statistical Rethinking: {{A Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  author = {McElreath, Richard},
  year = {2015},
  publisher = {{CRC press}},
  url = {https://xcelab.net/rm/statistical-rethinking/}
}

@book{mcelreathStatisticalRethinkingBayesian2020,
  title = {Statistical Rethinking: {{A Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  edition = {Second edition},
  publisher = {{CRC Press}},
  url = {https://xcelab.net/rm/statistical-rethinking/},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  isbn = {978-0-429-63914-2},
  keywords = {Mathematics / Probability \& Statistics / General},
  language = {en}
}

@article{meehlWhySummariesResearch1990,
  title = {Why Summaries of Research on Psychological Theories Are Often Uninterpretable},
  author = {Meehl, Paul E.},
  year = {1990},
  month = feb,
  volume = {66},
  pages = {195--244},
  publisher = {{SAGE Publications Inc}},
  issn = {0033-2941},
  doi = {10.2466/pr0.1990.66.1.195},
  url = {http://meehl.umn.edu/sites/meehl.dl.umn.edu/files/144whysummaries.pdf},
  urldate = {2020-05-25},
  abstract = {Null hypothesis testing of correlational predictions from weak substantive theories in soft psychology is subject to the influence of ten obfuscating factors whose effects are usually (1) sizeable, (2) opposed, (3) variable, and (4) unknown. The net epistemic effect of these ten obfuscating influences is that the usual research literature review is well-nigh uninterpretable. Major changes in graduate education, conduct of research, and editorial policy are proposed.},
  file = {/Users/solomonkurz/Zotero/storage/AYF7PQVK/Meehl - 1990 - Why Summaries of Research on Psychological Theorie.pdf},
  journal = {Psychological Reports},
  language = {en},
  number = {1}
}

@article{Merkle2018blavaan,
  title = {{{blavaan}}: {{Bayesian}} Structural Equation Models via Parameter Expansion},
  author = {Merkle, Edgar C. and Rosseel, Yves},
  year = {2018},
  volume = {85},
  pages = {1--30},
  doi = {10.18637/jss.v085.i04},
  journal = {Journal of Statistical Software},
  number = {4}
}

@article{mgcv2003,
  title = {Thin-Plate Regression Splines},
  author = {Wood, Simon N},
  year = {2003},
  volume = {65},
  pages = {95--114},
  doi = {10.1111/1467-9868.00374},
  url = {https://s3.amazonaws.com/academia.edu.documents/50461038/1467-9868.0037420161121-28486-yjewi1.pdf?response-content-disposition=inline\%3B\%20filename\%3DThin_plate_regression_splines.pdf\&X-Amz-Algorithm=AWS4-HMAC-SHA256\&X-Amz-Credential=ASIATUSBJ6BANKOPOZEO\%2F20200524\%2Fus-east-1\%2Fs3\%2Faws4_request\&X-Amz-Date=20200524T180240Z\&X-Amz-Expires=3600\&X-Amz-SignedHeaders=host\&X-Amz-Security-Token=IQoJb3JpZ2luX2VjECEaCXVzLWVhc3QtMSJIMEYCIQDOX\%2FwTEMlLub97I\%2FMAMW8be650dvGXcsUS\%2FFT7fRLTFAIhALm6fierGHEIpDpqKXIzaF7X4n0Vgz6oeQya42qaxCUbKrQDCHkQABoMMjUwMzE4ODExMjAwIgz0HouvSIxUl554eAEqkQMCsNlb\%2BQSuS55lUhyF7eBFSq665pRSMi4DyFt0hNfLRi6gKXYUyv0Cqgog7u9dhX1rpMWmmOcD9bIZCqWNHirMlqynxY7K5\%2F5\%2BCosSEzqlDE5DA2oOH4hqgemL24qvpdh\%2F7sB57yZu5O4zlmsFHotrhhSCkMzk5JR3kvuC4BRjaI055JWQR\%2F\%2Bofp\%2Fuc\%2FlD\%2FGnQ8gUHBcw847tQyUPEZvPg0\%2Frv68j3I0jWjy2kon7Jgq5GJqjpD15aIQlUcLicQ0adoxfirnV3wS14m7\%2FNr0fdddpmOpJ\%2BPDlwyqGY6PN3oo9XZYUShrRkhzaOC8aERDg0N5WbWrGIMr0nFmoM4xZqBuxob1iHyTfpmrtN14YBEbGgVRPbCjc9YxwrPe7sr\%2B\%2Ff5\%2F71aNZfNW22td0HLuUiijGW1gKyKqEE907O3oDim0QD\%2B38EmW7N0apRBqZyE4oQA440ypSXABqa7\%2FkwspRZnBQVisqdMETD4m7W9jqTYr2OONaPJMTg\%2F2951ZbnQB87aczCx6fbb7K3snnOsQmHRDDRuKr2BTrqATfPzy8Sxe79\%2BEP54Yvh\%2FVuK9bF8SxInrfmJ1nITVtm3mhhleH9\%2BmbumR0HxkiPVqEXAq2C6wI5Akj6I3zXVzBFFnNzgPA\%2BHvWcUWBH33yJ4nmhZ8dDNMruPQ0H9lmya2noeaOr0w\%2FWS3VgLpkC\%2F\%2FCAjaFQydz1WZp1onsbpi66aeLxgsIecPfqD\%2FocvtDACzZxw3JSM7G\%2BSgSpcGdRvouHivT93d8hYmjLVYTXPR2J6BqwMC\%2BNXy\%2F4jBQcvWl08pPA0FjGdoNcestAXGwNkhsWv6RvMYo7rCpR5iyTlBkk5dNRz\%2FSl7wT37hA\%3D\%3D\&X-Amz-Signature=92b8be88b62ae556465c7e485d1f99917e86dad15ccaf9b0a7def69a5c716173},
  journal = {Journal of the Royal Statistical Society (B)},
  number = {1}
}

@article{mgcv2004,
  title = {Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models},
  author = {Wood, Simon N},
  year = {2004},
  volume = {99},
  pages = {673--686},
  doi = {10.1198/016214504000000980},
  url = {https://purehost.bath.ac.uk/ws/portalfiles/portal/9228711/magic.pdf},
  journal = {Journal of the American Statistical Association},
  number = {467}
}

@article{mgcv2011,
  title = {Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models},
  author = {Wood, Simon N},
  year = {2011},
  volume = {73},
  pages = {3--36},
  doi = {10.1111/j.1467-9868.2010.00749.x},
  url = {https://people.bath.ac.uk/man54/SAMBa/ITTs/ITT2/EDF/REMLWood2009.pdf},
  journal = {Journal of the Royal Statistical Society (B)},
  number = {1}
}

@article{mgcv2016,
  title = {Smoothing Parameter and Model Selection for General Smooth Models (with Discussion)},
  author = {Wood, Simon N and Pya, Natalya and S{\"a}fken, Benjamin},
  year = {2016},
  volume = {111},
  pages = {1548--1575},
  doi = {10.1080/01621459.2016.1180986},
  url = {https://doi.org/10.1080/01621459.2016.1180986},
  journal = {Journal of the American Statistical Association}
}

@book{mgcv2017,
  title = {Generalized Additive Models: {{An}} Introduction with r},
  author = {Wood, Simon N},
  year = {2017},
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  url = {https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331?utm_source=crcpress.com\&utm_medium=referral},
  isbn = {978-1-4987-2833-1}
}

@article{navarroDevilDeepBlue2019,
  title = {Between the Devil and the Deep Blue Sea: {{Tensions}} between Scientific Judgement and Statistical Model Selection},
  shorttitle = {Between the {{Devil}} and the {{Deep Blue Sea}}},
  author = {Navarro, Danielle J.},
  year = {2019},
  month = mar,
  volume = {2},
  pages = {28--34},
  issn = {2522-0861, 2522-087X},
  doi = {10.1007/s42113-018-0019-z},
  url = {http://link.springer.com/10.1007/s42113-018-0019-z},
  urldate = {2020-05-15},
  abstract = {Discussions of model selection in the psychological literature typically frame the issues as a question of statistical inference, with the goal being to determine which model makes the best predictions about data. Within this setting, advocates of leaveone-out cross-validation and Bayes factors disagree on precisely which prediction problem model selection questions should aim to answer. In this comment, I discuss some of these issues from a scientific perspective. What goal does model selection serve when all models are known to be systematically wrong? How might ``toy problems'' tell a misleading story? How does the scientific goal of explanation align with (or differ from) traditional statistical concerns? I do not offer answers to these questions, but hope to highlight the reasons why psychological researchers cannot avoid asking them.},
  file = {/Users/solomonkurz/Zotero/storage/3D6FMZVD/Navarro - 2019 - Between the Devil and the Deep Blue Sea Tensions .pdf},
  journal = {Computational Brain \& Behavior},
  language = {en},
  number = {1}
}

@article{nunn2012ruggedness,
  title = {Ruggedness: {{The}} Blessing of Bad Geography in {{Africa}}},
  author = {Nunn, Nathan and Puga, Diego},
  year = {2012},
  volume = {94},
  pages = {20--36},
  publisher = {{MIT Press}},
  doi = {10.1162/REST_a_00161},
  url = {https://scholar.harvard.edu/files/nunn/files/ruggedness.pdf},
  journal = {Review of Economics and Statistics},
  number = {1}
}

@book{pengProgrammingDataScience2019,
  title = {R Programming for Data Science},
  author = {Peng, Roger D.},
  year = {2019},
  url = {https://bookdown.org/rdpeng/rprogdatascience/}
}

@misc{PivotDataWide2020,
  title = {Pivot Data from Wide to Long \textemdash{} Pivot\_longer},
  year = {2020},
  url = {https://tidyr.tidyverse.org/reference/pivot_longer.html},
  urldate = {2020-05-21},
  abstract = {pivot\_longer() "lengthens" data, increasing the number of rows and
decreasing the number of columns. The inverse transformation is
pivot\_wider()
Learn more in vignette("pivot").},
  file = {/Users/solomonkurz/Zotero/storage/D62EQVQQ/pivot_longer.html},
  language = {en}
}

@misc{Pivoting2020,
  title = {Pivoting},
  year = {2020},
  url = {https://tidyr.tidyverse.org/articles/pivot.html},
  urldate = {2020-05-21},
  abstract = {Learn how use the new `pivot\_longer()` and `pivot\_wider()` functions which change the representation of a dataset without changing the data it contains.},
  file = {/Users/solomonkurz/Zotero/storage/AAYYSFTP/pivot.html},
  language = {en}
}

@article{plummerJAGSProgramAnalysis2003,
  title = {{{JAGS}}: {{A}} Program for Analysis of {{Bayesian}} Graphical Models Using {{Gibbs}} Sampling},
  author = {Plummer, Martyn},
  year = {2003},
  pages = {8},
  url = {http://www.ci.tuwien.ac.at/Conferences/DSC-2003/Drafts/Plummer.pdf},
  abstract = {JAGS is a program for Bayesian Graphical modelling which aims for compatibility with Classic BUGS. The program could eventually be developed as an R package. This article explains the motivations for this program, briefly describes the architecture and then discusses some ideas for a vectorized form of the BUGS language.},
  file = {/Users/solomonkurz/Zotero/storage/UX2NECXA/Plummer - 2003 - JAGS A program for analysis of Bayesian graphical.pdf},
  journal = {Working Papers},
  language = {en}
}

@book{R-base,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  publisher = {{R Foundation for Statistical Computing}},
  address = {{Vienna, Austria}},
  url = {https://www.R-project.org/}
}

@book{R-bayesplot,
  title = {{{bayesplot}}: {{Plotting}} for {{Bayesian}} Models},
  author = {Gabry, Jonah and Mahr, Tristan},
  year = {2019},
  url = {https://CRAN.R-project.org/package=bayesplot}
}

@book{R-blavaan,
  title = {{{blavaan}}: {{Bayesian}} Latent Variable Analysis},
  author = {Merkle, Edgar C. and Rosseel, Yves and Goodrich, Ben},
  year = {2020},
  url = {https://CRAN.R-project.org/package=blavaan}
}

@book{R-brms,
  title = {{{brms}}: {{Bayesian}} Regression Models Using '{{Stan}}'},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2020},
  url = {https://CRAN.R-project.org/package=brms}
}

@book{R-dagitty,
  title = {{{dagitty}}: {{Graphical}} Analysis of Structural Causal Models},
  author = {Textor, Johannes and van {der Zander}, Benito},
  year = {2016},
  url = {https://CRAN.R-project.org/package=dagitty}
}

@book{R-GGally,
  title = {{{GGally}}: {{Extension}} to 'Ggplot2'},
  author = {Schloerke, Barret and Crowley, Jason and {Di Cook} and Briatte, Francois and Marbach, Moritz and Thoen, Edwin and Elberg, Amos and Larmarange, Joseph},
  year = {2020},
  url = {https://CRAN.R-project.org/package=GGally}
}

@book{R-ggdag,
  title = {{{ggdag}}: {{Analyze}} and Create Elegant Directed Acyclic Graphs},
  author = {Barrett, Malcolm},
  year = {2020},
  url = {https://CRAN.R-project.org/package=ggdag}
}

@book{R-ggmcmc,
  title = {{{ggmcmc}}: {{Tools}} for Analyzing {{MCMC}} Simulations from {{Bayesian}} Inference},
  author = {i Mar{\'i}n, Xavier Fern{\'a}ndez},
  year = {2020},
  url = {https://CRAN.R-project.org/package=ggmcmc}
}

@book{R-ggplot2,
  title = {{{ggplot2}}: {{Create}} Elegant Data Visualisations Using the Grammar of Graphics},
  author = {Wickham, Hadley and Chang, Winston and Henry, Lionel and Pedersen, Thomas Lin and Takahashi, Kohske and Wilke, Claus and Woo, Kara and Yutani, Hiroaki and Dunnington, Dewey},
  year = {2020},
  url = {https://CRAN.R-project.org/package=ggplot2}
}

@manual{R-ggpomological,
  title = {{{ggpomological}}: {{Pomological}} Plot Theme for Ggplot2},
  author = {{Aden-Buie}, Garrick},
  year = {2020},
  url = {https://github.com/gadenbuie/ggpomological},
  type = {Manual}
}

@book{R-ggrepel,
  title = {{{ggrepel}}: {{Automatically}} Position Non-Overlapping Text Labels with 'Ggplot2'},
  author = {Slowikowski, Kamil},
  year = {2020},
  url = {https://CRAN.R-project.org/package=ggrepel}
}

@book{R-ggthemes,
  title = {{{ggthemes}}: {{Extra}} Themes, Scales and Geoms for 'Ggplot2'},
  author = {Arnold, Jeffrey B.},
  year = {2019},
  url = {https://CRAN.R-project.org/package=ggthemes}
}

@book{R-loo,
  title = {{{loo}}: {{Efficient}} Leave-One-out Cross-Validation and {{WAIC}} for Bayesian Models},
  author = {Vehtari, Aki and Gabry, Jonah and Magnusson, Mans and Yao, Yuling and Gelman, Andrew},
  year = {2019},
  url = {https://CRAN.R-project.org/package=loo}
}

@book{R-MASS,
  title = {{{MASS}}: {{Support}} Functions and Datasets for Venables and Ripley's {{MASS}}},
  author = {Ripley, Brian},
  year = {2019},
  url = {https://CRAN.R-project.org/package=MASS}
}

@book{R-mgcv,
  title = {{{mgcv}}: {{Mixed GAM}} Computation Vehicle with Automatic Smoothness Estimation},
  author = {Wood, Simon N},
  year = {2019},
  url = {https://CRAN.R-project.org/package=mgcv}
}

@book{R-patchwork,
  title = {{{patchwork}}: {{The}} Composer of Plots},
  author = {Pedersen, Thomas Lin},
  year = {2019},
  url = {https://CRAN.R-project.org/package=patchwork}
}

@book{R-psych,
  title = {{{psych}}: {{Procedures}} for Psychological, Psychometric, and Personality Research},
  author = {Revelle, William},
  year = {2020},
  url = {https://CRAN.R-project.org/package=psych}
}

@book{R-purrr,
  title = {{{purrr}}: {{Functional}} Programming Tools},
  author = {Henry, Lionel and Wickham, Hadley},
  year = {2020},
  url = {https://CRAN.R-project.org/package=purrr}
}

@book{R-rcartocolor,
  title = {{{rcartocolor}}: '{{CARTOColors}}' Palettes},
  author = {Nowosad, Jakub},
  year = {2019},
  url = {https://CRAN.R-project.org/package=rcartocolor}
}

@book{R-rethinking,
  title = {{{rethinking}} {{R}} Package},
  author = {McElreath, Richard},
  year = {2020},
  url = {https://xcelab.net/rm/software/}
}

@manual{R-rstanarm,
  title = {{{rstanarm}}: {{Bayesian}} Applied Regression Modeling via Stan},
  author = {Gabry, Jonah and Goodrich, Ben},
  year = {2020},
  url = {https://CRAN.R-project.org/package=rstanarm},
  type = {Manual}
}

@book{R-tibble,
  title = {{{tibble}}: {{Simple}} Data Frames},
  author = {M{\"u}ller, Kirill and Wickham, Hadley},
  year = {2020},
  url = {https://CRAN.R-project.org/package=tibble}
}

@book{R-tidybayes,
  title = {{{tidybayes}}: {{Tidy}} Data and 'geoms' for {{Bayesian}} Models},
  author = {Kay, Matthew},
  year = {2020},
  url = {http://mjskay.github.io/tidybayes}
}

@book{R-tidyverse,
  title = {{{tidyverse}}: {{Easily}} Install and Load the 'Tidyverse'},
  author = {Wickham, Hadley},
  year = {2019},
  url = {https://CRAN.R-project.org/package=tidyverse}
}

@book{R-urbnmapr,
  title = {{{urbnmapr}}: {{State}} and County Maps with {{Alaska}} and {{Hawaii}}},
  author = {{Urban Institute}},
  year = {2020},
  url = {https://github.com/UrbanInstitute/urbnmapr}
}

@article{robertShortHistoryMarkov2011,
  title = {A Short History of {{Markov}} Chain {{Monte Carlo}}: {{Subjective}} Recollections from Incomplete Data},
  shorttitle = {A {{Short History}} of {{Markov Chain Monte Carlo}}},
  author = {Robert, Christian and Casella, George},
  year = {2011},
  month = feb,
  volume = {26},
  pages = {102--115},
  issn = {0883-4237},
  doi = {10.1214/10-STS351},
  url = {http://arxiv.org/abs/0808.2902},
  urldate = {2020-06-11},
  abstract = {We attempt to trace the history and development of Markov chain Monte Carlo (MCMC) from its early inception in the late 1940s through its use today. We see how the earlier stages of Monte Carlo (MC, not MCMC) research have led to the algorithms currently in use. More importantly, we see how the development of this methodology has not only changed our solutions to problems, but has changed the way we think about problems.},
  archivePrefix = {arXiv},
  eprint = {0808.2902},
  eprinttype = {arxiv},
  file = {/Users/solomonkurz/Zotero/storage/3HMI7NAQ/Robert and Casella - 2011 - A Short History of Markov Chain Monte Carlo Subje.pdf;/Users/solomonkurz/Zotero/storage/4AP273CC/0808.html},
  journal = {Statistical Science},
  keywords = {Statistics - Computation,Statistics - Methodology},
  number = {1}
}

@article{rstanarm2018,
  title = {Joint Longitudinal and Time-to-Event Models via {{Stan}}.},
  author = {Brilleman, SL and Crowther, MJ and {Moreno-Betancur}, M and Buros Novik, J and Wolfe, R},
  year = {2018},
  url = {https://github.com/stan-dev/stancon_talks/}
}

@article{shannonMathematicalTheoryCommunication1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, C. E.},
  year = {1948},
  volume = {27},
  pages = {379--423},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1948.tb01338.x},
  urldate = {2020-06-06},
  copyright = {\textcopyright{} 1948 The Bell System Technical Journal},
  file = {/Users/solomonkurz/Zotero/storage/2ZYC5TAW/Shannon - 1948 - A Mathematical Theory of Communication.pdf;/Users/solomonkurz/Zotero/storage/GJ7QEDI3/j.1538-7305.1948.tb01338.html},
  journal = {Bell System Technical Journal},
  language = {en},
  number = {3}
}

@article{simmonsFalsepositivePsychologyUndisclosed2011,
  title = {False-Positive Psychology: {{Undisclosed}} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  volume = {22},
  pages = {1359--1366},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  url = {https://journals.sagepub.com/doi/full/10.1177/0956797611417632},
  urldate = {2020-05-23},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  file = {/Users/solomonkurz/Zotero/storage/N85FNDMV/Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf},
  journal = {Psychological Science},
  language = {en},
  number = {11}
}

@article{spiegelhalterBayesianMeasuresModel2002,
  title = {Bayesian Measures of Model Complexity and Fit},
  author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and Linde, Angelika Van Der},
  year = {2002},
  volume = {64},
  pages = {583--639},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00353},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00353},
  urldate = {2020-06-12},
  abstract = {Summary. We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the `hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
  file = {/Users/solomonkurz/Zotero/storage/GGPTKT9M/Spiegelhalter et al. - 2002 - Bayesian measures of model complexity and fit.pdf;/Users/solomonkurz/Zotero/storage/SD8PUJGW/1467-9868.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Bayesian model comparison,Decision theory,Deviance information criterion,Effective number of parameters,Hierarchical models,Information theory,Leverage,Markov chain Monte Carlo methods,Model dimension},
  language = {en},
  number = {4}
}

@misc{standevelopmentteamRStanInterfaceStan2020,
  title = {{{RStan}}: The {{R}} Interface to {{Stan}}},
  author = {{Stan Development Team}},
  year = {2020},
  month = feb,
  url = {https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html},
  urldate = {2020-05-22},
  file = {/Users/solomonkurz/Zotero/storage/UNLVDTJP/rstan.html}
}

@article{vehtariPracticalBayesianModel2017,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  year = {2017},
  month = sep,
  volume = {27},
  pages = {1413--1432},
  issn = {1573-1375},
  doi = {10.1007/s11222-016-9696-4},
  url = {https://arxiv.org/pdf/1507.04544.pdf},
  urldate = {2020-06-03},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  file = {/Users/solomonkurz/Zotero/storage/I7HY567V/Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf},
  journal = {Statistics and Computing},
  language = {en},
  number = {5}
}

@article{vehtariRanknormalizationFoldingLocalization2019,
  title = {Rank-Normalization, Folding, and Localization: {{An}} Improved $\widehat{R}$ for Assessing Convergence of {{MCMC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and B{\"u}rkner, Paul-Christian},
  year = {2019},
  url = {https://arxiv.org/abs/1903.08008?},
  journal = {arXiv preprint arXiv:1903.08008}
}

@article{watanabeAsymptoticEquivalenceBayes2010,
  title = {Asymptotic Equivalence of {{Bayes}} Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory},
  author = {Watanabe, Sumio},
  year = {2010},
  volume = {11},
  pages = {3571--3594},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v11/watanabe10a.html},
  urldate = {2020-06-12},
  file = {/Users/solomonkurz/Zotero/storage/4HA4VYLM/Watanabe - 2010 - Asymptotic Equivalence of Bayes Cross Validation a.pdf;/Users/solomonkurz/Zotero/storage/MSKBHX5F/watanabe10a.html},
  journal = {Journal of Machine Learning Research},
  number = {116}
}

@book{wickhamGgplot2ElegantGraphics2016,
  title = {{{ggplot2}}: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2016},
  publisher = {{Springer-Verlag New York}},
  url = {https://ggplot2.tidyverse.org},
  isbn = {978-3-319-24277-4}
}

@article{wickhamWelcomeTidyverse2019,
  title = {Welcome to the Tidyverse},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and Fran{\c c}ois, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and M{\"u}ller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  volume = {4},
  pages = {1686},
  doi = {10.21105/joss.01686},
  journal = {Journal of Open Source Software},
  number = {43}
}

@article{wilksLargesampleDistributionLikelihood1938,
  title = {The Large-Sample Distribution of the Likelihood Ratio for Testing Composite Hypotheses},
  author = {Wilks, S. S.},
  year = {1938},
  volume = {9},
  pages = {60--62},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177732360},
  url = {https://www.jstor.org/stable/2957648},
  urldate = {2020-06-01},
  journal = {The Annals of Mathematical Statistics},
  number = {1}
}

@article{woodGeneralizedAdditiveModels,
  title = {Generalized {{Additive Models}}: An Introduction with {{R}}},
  author = {Wood, Simon N},
  pages = {397},
  file = {/Users/solomonkurz/Zotero/storage/W5HE99FH/Wood - Generalized Additive Models an introduction with .pdf},
  language = {en}
}

@book{woodGeneralizedAdditiveModels2017,
  title = {Generalized Additive Models: {{An}} Introduction with {{R}}},
  shorttitle = {Generalized {{Additive Models}}},
  author = {Wood, Simon N},
  year = {2017},
  month = may,
  edition = {Second Edition},
  publisher = {{CRC Press}},
  url = {https://www.routledge.com/Generalized-Additive-Models-An-Introduction-with-R-Second-Edition/Wood/p/book/9781498728331},
  abstract = {The first edition of this book has established itself as one of the leading references on generalized additive models (GAMs), and the only book on the topic to be introductory in nature with a wealth of practical examples and software implementation. It is self-contained, providing the necessary background in linear models, linear mixed models, and generalized linear models (GLMs), before presenting a balanced treatment of the theory and applications of GAMs and related models.   The author bases his approach on a framework of penalized regression splines, and while firmly focused on the practical aspects of GAMs, discussions include fairly full explanations of the theory underlying the methods. Use of R software helps explain the theory and illustrates the practical application of the methodology. Each chapter contains an extensive set of exercises, with solutions in an appendix or in the book's R data package gamair, to enable use as a course text or for self-study. Simon N. Wood is a professor of Statistical Science at the University of Bristol, UK, and author of the R package mgcv.},
  isbn = {978-1-4987-2834-8},
  keywords = {Mathematics / Probability \& Statistics / General},
  language = {en}
}

@article{yaoUsingStackingAverage2018,
  title = {Using Stacking to Average {{Bayesian}} Predictive Distributions (with Discussion)},
  author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and others},
  year = {2018},
  volume = {13},
  pages = {917--1007},
  publisher = {{International Society for Bayesian Analysis}},
  doi = {10.1214/17-BA1091},
  url = {https://projecteuclid.org/download/pdfview_1/euclid.ba/1516093227},
  journal = {Bayesian Analysis},
  number = {3}
}

@article{yarkoniChoosingPredictionExplanation2017,
  title = {Choosing Prediction over Explanation in Psychology: {{Lessons}} from Machine Learning},
  shorttitle = {Choosing Prediction over Explanation in Psychology},
  author = {Yarkoni, Tal and Westfall, Jacob},
  year = {2017},
  month = nov,
  volume = {12},
  pages = {1100--1122},
  issn = {1745-6916},
  doi = {10.1177/1745691617693393},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6603289/},
  urldate = {2020-06-06},
  abstract = {Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology's near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism, but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.},
  file = {/Users/solomonkurz/Zotero/storage/79P37THN/Yarkoni and Westfall - 2017 - Choosing prediction over explanation in psychology.pdf},
  journal = {Perspectives on psychological science : a journal of the Association for Psychological Science},
  number = {6},
  pmcid = {PMC6603289},
  pmid = {28841086}
}

@article{zhangCrossvalidationSelectingModel2015,
  title = {Cross-Validation for Selecting a Model Selection Procedure},
  author = {Zhang, Yongli and Yang, Yuhong},
  year = {2015},
  month = jul,
  volume = {187},
  pages = {95--112},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2015.02.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0304407615000305},
  urldate = {2020-06-03},
  abstract = {While there are various model selection methods, an unanswered but important question is how to select one of them for data at hand. The difficulty is due to that the targeted behaviors of the model selection procedures depend heavily on uncheckable or difficult-to-check assumptions on the data generating process. Fortunately, cross-validation (CV) provides a general tool to solve this problem. In this work, results are provided on how to apply CV to consistently choose the best method, yielding new insights and guidance for potentially vast amount of application. In addition, we address several seemingly widely spread misconceptions on CV.},
  file = {/Users/solomonkurz/Zotero/storage/W4YYPU24/S0304407615000305.html},
  journal = {Journal of Econometrics},
  keywords = {Adaptive procedure selection,Cross-validation,Cross-validation paradox,Data splitting ratio,Information criterion,LASSO,MCP,SCAD},
  language = {en},
  number = {1}
}


