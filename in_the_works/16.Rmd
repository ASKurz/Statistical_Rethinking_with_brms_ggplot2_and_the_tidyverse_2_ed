---
title: "Ch. 16 Generalized Linear Madness"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output: github_document
bibliography: bib.bib
biblio-style: apalike
csl: apa.csl
link-citations: yes
---

```{r, echo = F, cache = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 110)
```

# Generalized Linear Madness

> Applied statistics has to apply to all the sciences, and so it is often much vaguer about models. Instead it focuses on average performance, regardless of the model. The generalized linear models in the preceding chapters are not credible scientific models of most natural processes. They are powerful, geocentric ([Chapter 4][Geocentric Models]) descriptions of associations. In combination with a logic of causal inference, for example DAGs and *do*-calculus, generalized linear models can nevertheless be unreasonably powerful.
>
> But there are problems with this GLMs-plus-DAGs approach. Not everything can be modeled as a GLM—a linear combination of variables mapped onto a non-linear outcome. But if it is the only approach you know, then you have to use it....
>
> In this chapter, I will go **beyond generalized linear madness**. I'll work through examples in which the scientific context provides a causal model that will breathe life into the statistical model. I've chosen examples which are individually distinct and highlight different challenges in developing and translating causal models into *bespoke* (see the Rethinking box below) statistical models. You won't require any specialized scientific expertise to grasp these examples. And the basic strategy is the same as it has been from the start: Define a generative model of a phenomenon and then use that model to design strategies for causal inference and statistical estimation. [@mcelreathStatisticalRethinkingBayesian2020, p. 525, *emphasis* in the original]

#### Rethinking: Bespoken for.

> Mass production has some advantages, but it also makes our clothes fit badly. Garments bought off-the-shelf are not manufactured with you in mind. They are not *bespoke* products, designed for any particular person with a particular body. Unless you are lucky to have a perfectly average body shape, you will need a tailor to get better.
> 
> Statistical analyses are similar. Generalized linear models are off-the-shelf products, mass produced for a consumer market of impatient researchers with diverse goals. Science asked statisticians for tools that could be used anywhere. And so they delivered. But the clothes don't always fit. (p. 526, *emphasis* in the original)

## 16.1. Geometric people

> Back in [Chapter 4][Geocentric Models], you met linear regression in the context of building a predictive model of height using weight. You even saw how to measure non-linear associations between the two variables. But nothing in that example was scientifically satisfying. The height-weight model was just a statistical device. It contains no biological information and tells us nothing about how the association between height and weight arises. Consider for example that weight obviously does not *cause* height, at least not in humans. If anything, the causal relationship is the reverse.
>
> So now let's try to do better. Why? Because when the model is scientifically inspired, rather than just statistically required, disagreements between model and data are informative of real causal relationships.
>
> Suppose for example that a person is shaped like a cylinder. Of course a person isn't exactly shaped like a cylinder. There are arms and a head. But let's see how far this cylinder model gets us. The weight of the cylinder is a consequence of the volume of the cylinder. And the volume of the cylinder is a consequence of growth in the height and width of the cylinder. So if we can relate the height to the volume, then we’d have a model to predict weight from height. (p. 526, *emphasis* in the original)

### 16.1.1. The scientific model.

If we let $V$ stand for volume, $r$ stand for a radius, and $h$ stand for height, we can solve for volume by

$$V = \pi r^2 h.$$
If we further presume a person's radius is unknown, but some proportion ($p$) of height ($ph$), we can rewrite the formula as

$$
\begin{align*}
V & = \pi (ph)^2 h \\
  & = \pi p^2 h^3.
\end{align*}
$$

Though we're not interested in volume per se, we might presume weight is some proportion of volume. Thus we could include a final parameter $k$ to stand for the conversion form weight to volume, leaving us with the formula

$$W = kV = k \pi p^2 h^3,$$

where $W$ denotes weight.

### 16.1.2. The statistical model. 

```{r, warning = F, message = F}
library(tidyverse)
data(Howell1, package = "rethinking")
d <- Howell1
rm(Howell1)

# scale observed variables
d <-
  d %>% 
  mutate(w = weight / mean(weight),
         h = height / mean(height))
```

McElreath's proposed statistical model follows the form

$$
\begin{align*}
\text{w}_i  & \sim \operatorname{Log-Normal}(\mu_i, \sigma) \\
\exp(\mu_i) & = k \pi p^2 \text{h}_i^3 \\
k      & \sim \operatorname{Exponential}(0.5) \\
p      & \sim \operatorname{Beta}(2, 18) \\
\sigma & \sim \operatorname{Exponential}(1), && \text{where} \\
\text w_i & = \text{weight}_i / \overline{\text{weight}}, && \text{and} \\
\text h_i & = \text{height}_i / \overline{\text{height}}.
\end{align*}
$$

The Log-Normal likelihood ensures the predictions for $\text{weight}_i$ will always be non-negative. Because our parameter $p$ is the ratio of radius to height, $p = r / h$, it must be positive. Since people are typically taller than their width, it should also be less than one, and probably substantially less than that. Here's a look at our priors.

For the plots in this chapter, we'll give a nod the minimalistic plots in the authoritative text by @gelman2013bayesian, [*Bayesian data analysis: Third edition*](https://stat.columbia.edu/~gelman/book/). Just to be a little kick, we'll set the font to `family = "Times"`. Most of the adjustments will come from `ggthemes::theme_base()`.

```{r, message = F, warninf = F}
library(ggthemes)

theme_set(
  theme_base(base_size = 12) +
    theme(text = element_text(family = "Times"),
          axis.text = element_text(family = "Times"),
          axis.ticks = element_line(size = 0.25),
          axis.ticks.length = unit(0.1, "cm"),
          panel.background = element_rect(size = 0.1),
          plot.background = element_blank(),
          )
  )
```

Now we have our theme, let's get a sense of our priors.

```{r, message = F, warning = F, fig.width = 8, fig.height = 2.75}
library(tidybayes)
library(brms)

c(prior(beta(2, 18), nlpar = p, coef = italic(p)),
  prior(exponential(0.5), nlpar = p, coef = italic(k)),
  prior(exponential(1), class = sigma, coef = sigma)) %>% 
  
  parse_dist(prior) %>%
  
  ggplot(aes(y = 0, dist = .dist, args = .args)) +
  stat_dist_halfeye(.width = .5, size = 1, p_limits = c(0, 0.9995),
                    n = 2e3, normalize = "xy") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(theta)) +
  facet_wrap(~coef, scales = "free_x", labeller = label_parsed)
```

Here the points are the posterior medians and the horizontal lines the quantile-based 50% intervals. Turns out that $\operatorname{Beta}(2, 18)$ prior for $p$ pushes the bulk of the prior mass down near zero. The beta distribution also forces the parameter space for $p$ to range between 0 and 1. If we denote the two parameters of the beta distribution as $\alpha$ and $beta$, we can compute the mean for any beta distribution as $\alpha / (\alpha + \beta)$. Thus the mean for our $\operatorname{Beta}(2, 18)$ prior is $2 / (2 + 18) = 2 / 20 = 0.1$. 

Because we computed our weight and height variables, `w` and `h`, by dividing the original variables by their respective means, each now has a mean of 1.

```{r, message = F}
d %>% 
  pivot_longer(w:h) %>% 
  group_by(name) %>% 
  summarise(mean = mean(value))
```

Here's their bivariate distribution in a scatter plot.

```{r, fig.width = 5, fig.height = 3}
d %>% 
  ggplot(aes(x = h, y = w)) +
  geom_vline(xintercept = 1, linetype = 2, size = 1/4, color = "grey50") +
  geom_hline(yintercept = 1, linetype = 2, size = 1/4, color = "grey50") +
  geom_point(size = 1/4)
```

With this scaling, here is the formula for an individual with average weight and height:

$$
\begin{align*}
1 & = k \pi p^2 1^3 \\
  & = k \pi p^2.
\end{align*}
$$

If you assume $p < .5$, $k$ must be greater than 1. $k$ also has to be positive. To get a sense of this, we can further work the algebra:

$$
\begin{align*}
1 & = k \pi p^2 \\
1/k  & = \pi p^2 \\
k  & = 1 / \pi p^2.
\end{align*}
$$

To get a better sense of that relation, we might plot.

```{r, fig.width = 5, fig.height = 3}
tibble(p = seq(from = 0.001, to = 0.499, by = 0.001)) %>% 
  mutate(k = 1 / (pi * p^2)) %>% 
  ggplot(aes(x = p, y = k)) +
  geom_line() +
  labs(x = expression(italic(p)),
       y = expression(italic(k))) +
  coord_cartesian(ylim = c(0, 500))
```

McElreath's quick and dirty solution was to set $k \sim \operatorname{Exponential}(0.5)$, which has a prior predictive mean of 2.

By setting up his model formula as `exp(mu) = ...`, McElreath effectively used the log link. It turns out that **brms** only supports the `identity` and `inverse` links for `family = lognormal`. However, we can sneak in the log link by nesting the right-hand side of the formula within `log()`.

```{r b16.1}
b16.1 <- 
  brm(data = d,
      family = lognormal,
      bf(w ~ log(3.141593 * k * p^2 * h^3),
         k + p ~ 1,
         nl = TRUE),
      prior = c(prior(beta(2, 18), nlpar = p, lb = 0, ub = 1),
                prior(exponential(0.5), nlpar = k, lb = 0),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 16,
      file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/b16.01")
```

Check the parameter summary.

```{r}
print(b16.1)
```

McElreath didn't show the parameter summary for his `m16.1` in the text. If you fit the model with both **rethinking** and **brms**, you'll see our `b16.1` matches up quite well. To make our version of Figure 16.2, we'll use a `GGally::ggpairs()` workflow. First we'll save our customizes settings for the three subplot types.

```{r}
my_lower <- function(data, mapping, ...) {
  
  # get the x and y data to use the other code
  x <- eval_data_col(data, mapping$x)
  y <- eval_data_col(data, mapping$y)
  
  # compute the correlations
  corr <- cor(x, y, method = "p", use = "pairwise")
  abs_corr <- abs(corr)
  
  # plot the cor value
  ggally_text(
    label = formatC(corr, digits = 2, format = "f") %>% str_replace(., "0.", "."),
    mapping = aes(),
    size = 3.5, 
    color = "black",
    family = "Times") +
    scale_x_continuous(NULL, breaks = NULL) +
    scale_y_continuous(NULL, breaks = NULL)
}

my_diag <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
    geom_histogram(size = 1/4, color = "white", fill = "grey67", bins = 20) +
    scale_x_continuous(NULL, breaks = NULL) +
    scale_y_continuous(NULL, breaks = NULL)
}

my_upper <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) + 
    geom_point(size = 1/4, alpha = 1/4) +
    scale_x_continuous(NULL, breaks = NULL) +
    scale_y_continuous(NULL, breaks = NULL)
}
```

Now we make our version of Figure 16.2.a.

```{r, warning = F, message = F, fig.width = 4.5, fig.height = 4}
library(GGally)

posterior_samples(b16.1) %>% 
  select(-lp__) %>% 
  set_names(c("italic(k)", "italic(p)", "sigma")) %>% 
  ggpairs(upper = list(continuous = my_upper),
          diag = list(continuous = my_diag),
          lower = list(continuous = my_lower),
          labeller = label_parsed) +
  theme(strip.text = element_text(size = 8),
        strip.text.y = element_text(angle = 0))
```

We see the lack of identifiability of $k$ and $p$ resulted in a strong inverse relation between them. Now here's how we might make Figure 16.2.b.

```{r, fig.width = 5, fig.height = 3}
nd <- 
  tibble(h = seq(from = 0, to = 1.5, length.out = 50))

p <-
  predict(b16.1,
          newdata = nd) %>% 
  data.frame() %>% 
  bind_cols(nd)

d %>% 
  ggplot(aes(x = h)) +
  geom_smooth(data = p,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey76", color = "black", size = 1/4) +
  geom_point(aes(y = w),
             size = 1/3) +
  coord_cartesian(xlim = c(0, max(d$h)),
                  ylim = c(0, max(d$w))) +
  labs(x = "height (scaled)",
       y = "weight (scaled)")
```

Overall the model did okay, but the poor fit for the cases with lower values of height and weight suggests we might be missing important differences between children and adults.

### 16.1.3. GLM in disguise.

Recall that because **brms** does not support the log link for the Log-Normal likelihood, we recast our `b16.1` likelihood as

$$
\begin{align*}
\text{w}_i & \sim \operatorname{Log-Normal}(\mu_i, \sigma) \\
\mu_i      & = \log(k \pi p^2 \text{h}_i^3).
\end{align*}
$$

Because multiplication becomes addition on the log scale, we can also express this as

$$
\begin{align*}
\text{w}_i & \sim \operatorname{Log-Normal}(\mu_i, \sigma) \\
\mu_i      & = \log(k) + \log(\pi) + 2 \log(p) + 3 \log(\text{h}_i),
\end{align*}
$$

which means our fancy non-linear model is just linear regression on the log scale. McElreath pointed this out

> to highlight one of the reasons that generalized linear models are so powerful. Lots of natural relationships are GLM relationships, on a specific scale of measurement. At the same time, the GLM approach wants to simply estimate parameters which may be informed by a proper theory, as in this case. (p. 531)

## 16.2. Hidden minds and observed behavior

```{r}
data(Boxes, package = "rethinking")
d <- Boxes
rm(Boxes)

rethinking::precis(d)
```



### 16.2.1. The scientific model.

### 16.2.2. The statistical model.

### 16.2.3. Coding the statistical model.

```{r, eval = F, echo = F}
b16.2 <-
  brm(data = d,
      family = cumulative,
      y ~ 1,
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 16)
```

```{r, eval = F, echo = F}
print(b16.2)
```

```{r, eval = F, echo = F}
get_prior(data = d,
      family = mixture(cumulative, cumulative, cumulative, cumulative, cumulative),
      y ~ 1)
```

I don't think the `constant()` prior trick will work, here. Rather, I suspect this will require a custom likelihood. If you look at McElreath's Stan code at the top of page 536, I think that's him defining a custom likelihood.

```{r, eval = F, echo = F}
b16.2 <-
  brm(data = d,
      family = mixture(cumulative, cumulative, cumulative, cumulative, cumulative),
      y ~ 1,
      prior= c(prior(dirichlet(4, 4, 4, 4, 4), theta),
               prior(constant(), class = Intercept, coef = , dpar = mu1),
               prior(constant(), class = Intercept, coef = , dpar = mu1),
               prior(constant(), class = Intercept, coef = , dpar = mu2),
               prior(constant(), class = Intercept, coef = , dpar = mu2),
               prior(constant(), class = Intercept, coef = , dpar = mu3),
               prior(constant(), class = Intercept, coef = , dpar = mu3),
               prior(constant(), class = Intercept, coef = , dpar = mu4),
               prior(constant(), class = Intercept, coef = , dpar = mu4),
               prior(constant(), class = Intercept, coef = , dpar = mu5),
               prior(constant(), class = Intercept, coef = , dpar = mu5)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 16)
```

```{r, eval = F, echo = F}
print(b16.2)
```

```{r, warning = F, message = F}
library(rethinking)
data(Boxes_model) 
cat(Boxes_model)
```

Now fit the model with `rstan::stan()`.

```{r m16.2, echo = F}
# save(list = c("dat_list", "m16.2"), file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/m16.2.rda")

load(file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/m16.2.rda")
```

```{r, eval = F}
# prep data 
dat_list <- list(
  N = nrow(d),
  y = d$y,
  majority_first = d$majority_first )

# run the sampler
m16.2 <-
  stan(model_code = Boxes_model, 
       data = dat_list, 
       chains = 3, cores = 3)
```

Check the summary.

```{r}
precis(m16.2, depth = 2)
```

Here we show marginal posterior for $p_s$.

```{r, fig.width = 5, fig.height = 1.75}
label <- c("Majority~(italic(s)[1])", 
           "Minority~(italic(s)[2])",
           "Maverick~(italic(s)[3])", 
           "Random~(italic(s)[4])", 
           "Follow~First~(italic(s)[5])")

precis(m16.2, depth = 2) %>% 
  data.frame() %>% 
  mutate(name = factor(label, levels = label)) %>% 
  mutate(name = fct_rev(name)) %>% 
  
  ggplot(aes(x = mean, xmin = X5.5., xmax = X94.5., y = name)) +
  geom_vline(xintercept = .2, size = 1/4, linetype = 2) +
  geom_pointrange(size = 1/4, fatten = 6/4) +
  scale_x_continuous(expression(italic(p[s])), limits = c(0, 1),
                     breaks = 0:5 / 5) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  theme(axis.text.y = element_text(hjust = 0))
```

As an alternative, this might be a good time to revisit the `tidybayes::stat_ccdfinterval()` approach (see [Section 12.3.3][Adding predictor variables.]), which will depict those posteriors with a bar plot where the top parts of the bars depict our uncertainty in terms of cumulative density curves.

```{r, fig.width = 5, fig.height = 2.5}
extract.samples(m16.2) %>% 
  data.frame() %>% 
  select(-lp__) %>% 
  set_names(label) %>% 
  pivot_longer(everything()) %>% 
  mutate(name = factor(name, levels = label)) %>% 
  mutate(name = fct_rev(name)) %>% 
  
  ggplot(aes(x = value, y = name)) +
  geom_vline(xintercept = .2, size = 1/4, linetype = 2) +
  stat_ccdfinterval(.width = .95, size = 1/4, alpha = .8) +
  scale_x_continuous(expression(italic(p[s])), expand = c(0, 0), limits = c(0, 1),
                     breaks = 0:5 / 5) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  theme(axis.text.y = element_text(hjust = 0),
        axis.ticks.y = element_blank())
```

### 16.2.4. State space models.

## 16.3. Ordinary differential nut cracking

```{r}
data(Panda_nuts, package = "rethinking")
d <- Panda_nuts
rm(Panda_nuts)
```

Anticipating McElreath's **R** code 16.11, we'll wrangle a little.

```{r}
d <-
  d %>% 
  mutate(n     = nuts_opened,
         age_s = age / max(age))

glimpse(d)
```

### 16.3.1. Scientific model.

Figure 16.4

```{r, fig.width = 6, fig.height = 3.5}
n <- 1e4
at <- 0:6 / 4
sample_n <- 50

set.seed(16)

prior <-
  tibble(index = 1:n,
         phi   = rlnorm(n, meanlog = log(1), sdlog = 0.1),
         k     = rlnorm(n, meanlog = log(2), sdlog = 0.25),
         theta = rlnorm(n, meanlog = log(5), sdlog = 0.25)) %>% 
  sample_n(size = sample_n) %>% 
  expand(nesting(index, phi, k, theta),
         age = seq(from = 0, to = 1.5, length.out = 1e2)) %>% 
  mutate(bm = 1 - exp(-k * age),
         ns = phi * (1 - exp(-k * age))^theta) 

p1 <-
  prior %>% 
  ggplot(aes(x = age, y = bm, group = index)) +
  geom_line(size = 1/4, alpha = 1/2) +
  scale_x_continuous(breaks = at, labels = round(at * max(d$age))) +
  ylab("body mass")

p2 <-
  prior %>% 
  ggplot(aes(x = age, y = ns, group = index)) +
  geom_line(size = 1/4, alpha = 1/2) +
  scale_x_continuous(breaks = at, labels = round(at * max(d$age))) +
  ylab("nuts per second")

library(patchwork)
p1 + p2 + 
  plot_annotation(title = "Prior predictive simulation for the nut opening model",
                  subtitle = "Each panel shows the results from 50 prior draws.")
```



```{r b16.4}
b16.4 <- 
  brm(data = d,
      family = poisson(link = identity),
      bf(n ~ seconds * phi * (1 - exp(-k * age_s))^theta,
         phi + k + theta ~ 1,
         nl = TRUE),
      prior = c(prior(lognormal(log(1), 0.1), nlpar = phi, lb = 0),
                prior(lognormal(log(2), 0.25), nlpar = k, lb = 0),
                prior(lognormal(log(5), 0.25), nlpar = theta, lb = 0)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 16,
      file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/b16.04")
```

Check the parameter summary.

```{r}
print(b16.4)
```

Now make our version of Figure 16.5.

```{r, fig.width = 4, fig.height = 2.75}
posterior_samples(b16.4) %>% 
  mutate(iter = 1:n()) %>% 
  sample_n(sample_n) %>% 
  expand(nesting(iter, b_phi_Intercept, b_k_Intercept, b_theta_Intercept),
         age = seq(from = 0, to = 1.5, length.out = 1e2)) %>% 
  mutate(ns = b_phi_Intercept * (1 - exp(-b_k_Intercept * age))^b_theta_Intercept) %>% 
  
  ggplot() +
  geom_line(aes(x = age, y = ns, group = iter),
            size = 1/4, alpha = 1/2) +
  geom_jitter(data = d,
              aes(x = age_s, y = n / seconds, size = seconds),
              shape = 1, width = 0.01, color = "grey50") +
  scale_size_continuous(breaks = c(1, 50, 100), limits = c(1, NA)) +
  scale_x_continuous(breaks = at, labels = round(at * max(d$age))) +
  ylab("nuts per second") +
  theme(legend.background = element_blank(),
        legend.position = c(.9, .25))
```

## 16.4. Population dynamics

Load the `Lynx_Hare` population dynamics data [@hewittTheConservation1921].

```{r}
data(Lynx_Hare, package = "rethinking")

glimpse(Lynx_Hare)
```

Figure 6.6 will give us a sense of how the lynx and hare populations ebbed and flowed.

```{r, fig.width = 6.5, fig.height = 3}
# for annotation
text <-
  tibble(name  = c("Hare", "Lynx"),
         label = c("Lepus", "Lynx"),
         Year  = c(1913.5, 1915.5),
         value = c(78, 52))

# wrangle
Lynx_Hare %>% 
  pivot_longer(-Year) %>% 
  
  # plot!
  ggplot(aes(x = Year, y = value)) +
  geom_line(aes(color = name),
            size = 1/4) +
  geom_point(aes(fill = name),
             size = 3, shape = 21, color = "white") +
  geom_text(data = text,
            aes(label = label, color = name),
            hjust = 0, family = "Times") +
  scale_fill_grey(start = 0, end = .5) +
  scale_color_grey(start = 0, end = .5) +
  scale_y_continuous("thousands of pelts", breaks = 0:4 * 20, limits = c(0, 90)) +
  theme(legend.position = "none")
```

Note, however, that these are numbers of pelts, not of actual animals.

A typical way to model evenly-spaced time series data like this would be with an autoregressive model with the basic structure

$$\operatorname{E}(y_t) = \alpha + \beta_1 y_{t-1},$$

where $t$ indexes time and $t - 1$ is the time point immediately before $t$. Models following this form are called first-order autoregressive models (AR1), meaning that the current time point is only influenced by the previous time point, but none of the earlier ones. You can build on this format by adding other predictors. A natural way would be to use a predictor from $t - 1$ to predict $y_t$, following the form

$$\operatorname{E}(y_t) = \alpha + \beta_1 y_{t-1} + \beta_2 x_{t-1}.$$

But that's still a first-order model. A second order model, AR2, would include a term for $x_{t - 2}$, such as

$$\operatorname{E}(y_t) = \alpha + \beta_1 y_{t-1} + \beta_2 x_{t-1} + \beta_3 y_{t-2}.$$

McElreath isn't a huge fan of these models, particularly from the scientific modeling perspective he developed in this chapter. But **brms** can fit them and we'll practice a little in a bonus section, later on. In the mean time, we'll follow along and learn about **ordinary differential equations** (ODEs).

### 16.4.1. The scientific model.

If we let $H_t$ be the number of hares at time $t$, we can express the **rate of change** in the hare population as

$$\frac{\mathrm{d} H}{\mathrm{d} t} = H_t \times (\text{birth rate}) - H_t \times (\text{death rate}).$$

If we presume both birth rates and death rates (*mortality* rates) are constants, we might denote them $b_H$ and $m_H$, respectively, and re-express the formula as

$$\frac{\mathrm{d} H}{\mathrm{d} t} = H_t b_H - H_t m_H = H_t (b_H - m_H).$$

If we let $L_t$ stand for the number of lynx present at time $t$, we can allow the mortality rate depend on that variable with the expanded formula

$$\frac{\mathrm{d} H}{\mathrm{d} t} = H_t (b_H - L_t m_H).$$

We can expand this even further to model how the number of hares at a given time influence the birth rate for lynx ($b_L$) to help us model the rate of change in the lynx population as

$$\frac{\mathrm{d} L}{\mathrm{d} t} = L_t (H_t b_L - m_L),$$

where the lynx mortality rate ($m_l$) is now constant. This is called the **Lotka-Volterra model** [@lotkaPrinciplesOfPhysicalBiology1925; @volterraFluctuationsAbundanceSpecies1926]. You may have noticed how the above equations shifted our focus from what were were originally interested in, $\operatorname{E}(H_t)$, to a rate of change, $\mathrm{d} H / \mathrm{d} t$. Happily, our equation for $\mathrm{d} H / \mathrm{d} t$, "tells us how to update $H$ after each tiny unit of passing time $\mathrm d t$" (p. 544). You update by

$$H_{t +\mathrm d t} = H_t + \mathrm d t \frac{\mathrm d H}{\mathrm d t} = H_t + \mathrm d t H_t (b_H - L_t m_H).$$
Here we'll use the custom `sim_lynx_hare()` function to simulate how this can work. Our version of the function is very similar to the one McElreath displayed in his **R** code 16.14, but we changed it so it returns a tibble.

```{r}
sim_lynx_hare <- function(n_steps, init, theta, dt = 0.002) { 
  
  L <- rep(NA, n_steps)
  H <- rep(NA, n_steps)
  L[1] <- init[1]
  H[1] <- init[2]
  
  for (i in 2:n_steps) {
    H[i] <- H[i - 1] + dt * H[i - 1] * (theta[1] - theta[2] * L[i - 1])
    L[i] <- L[i - 1] + dt * L[i - 1] * (theta[3] * H[i - 1] - theta[4])
  }
  
  # return a tibble
  tibble(H = H,
         L = L)
  
}
```

Now we simulate.

```{r}
theta <- c(0.5, 0.05, 0.025, 0.5)

# simulate
z <- sim_lynx_hare(n_steps = 1e4, 
                   init = as.numeric(Lynx_Hare[1, 2:3]), 
                   theta)

# what did we do?
glimpse(z)
```

Each row is a stand-in index for time. Here we'll explicitly add a `time` column and them plot the results in our version of Figure 16.7.

```{r, fig.width = 4, fig.height = 3}
z %>% 
  mutate(time = 1:n()) %>% 
  pivot_longer(-time) %>% 
  
  ggplot(aes(x = time, y = value)) +
  geom_line(aes(color = name),
            size = 1/4) +
  scale_color_grey(start = 0, end = .5) +
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous("number (thousands)", breaks = 0:4 * 10, limits = c(0, 45)) +
  theme(legend.position = "none")
```

"This model produces cycles, similar to what we see in the data. The model behaves this way, because lynx eat hares. Once the hares are eaten, the lynx begin to die off. Then the cycle repeats (p. 545)."

### 16.4.2. The statistical model.

If we continue to let $H_t$ and $L_t$ be the number of hares and lynx at time $t$, we might also want to acknowledge the distinction between those numbers and our observations by letting $h_t$ and $l_t$ stand for the observed numbers of hares and lynx. These observed numbers, recall, are from counts of pelts. We want a statistical model that can connect $h_t$ to $H_t$ and connect $l_t$ to $L_t$. Part of that model would include the probability a hare was trapped on a given year, $p_h$, and a similar probability for a lynx getting trapped, $p_l$. To make things worse, further imagine the number of pelts for each, in a given year, was  rounded to the nearest $100$ and divided by $1{,}000$. Those are our values.

We practice simulating all this in Figure 16.8. Here we propose a population of $H_t = 10^4$ hares and an average trapping rate of about $10\%$, as expressed by $p_t \sim \operatorname{Beta}(2, 18)$. As described above, we then divide the number of observed pelts by $1{,}000$ and round the results, yielding $h_t$.

```{r, fig.width = 4, fig.height = 3}
n <- 1e4
Ht <- 1e4

set.seed(16)

# simulate
tibble(pt = rbeta(n, shape1 = 2, shape2 = 18)) %>% 
  mutate(ht = rbinom(n, size = Ht, prob = pt)) %>% 
  mutate(ht = round(ht / 1000, digits = 2)) %>% 
  
  # plot
  ggplot(aes(x = ht)) +
  geom_histogram(size = 1/4, binwidth = 0.1,
                 color = "white", fill = "grey67") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(thousand~of~pelts~(italic(h[t]))))
```

On page 546, McElreath encouraged us to try the simulation with different values of $H_t$ and $p_t$. Here we'll do so with a $3 \times 3$ grid of $H_t = \{5{,}000, 10{,}000, 15{,}000\}$ and $p_t \sim \{ \operatorname{Beta}(2, 18), \operatorname{Beta}(10, 10), \operatorname{Beta}(18, 2) \}$.

```{r, fig.width = 7, fig.height = 5}
set.seed(16)

# define the 3X3 grid
tibble(shape1 = c(2, 10, 18),
       shape2 = c(18, 10, 2)) %>% 
  expand(nesting(shape1, shape2),
         Ht = c(5e3, 1e4, 15e3)) %>% 
  # simulate
  mutate(pt = purrr::map2(shape1, shape2, ~rbeta(n, shape1 = .x, shape2 = .y))) %>% 
  mutate(ht = purrr::map2(Ht, pt, ~rbinom(n, size = .x, prob = .y))) %>% 
  unnest(c(pt, ht)) %>% 
  # wrangle
  mutate(ht    = round(ht / 1000, digits = 2),
         beta  = str_c("italic(p[t])%~%'Beta '(", shape1, ", ", shape2, ")"),
         Htlab = str_c("italic(H[t])==", Ht)) %>%
  mutate(beta  = factor(beta,
                        levels = c("italic(p[t])%~%'Beta '(2, 18)", "italic(p[t])%~%'Beta '(10, 10)", "italic(p[t])%~%'Beta '(18, 2)")),
         Htlab = factor(Htlab,
                        levels = c("italic(H[t])==15000", "italic(H[t])==10000", "italic(H[t])==5000"))) %>% 
  
  # plot!
  ggplot(aes(x = ht)) +
  geom_histogram(aes(fill = beta == "italic(p[t])%~%'Beta '(2, 18)" & Htlab == "italic(H[t])==10000"),
                 size = 1/10, binwidth = 0.25) +
  geom_vline(aes(xintercept = Ht / 1000), 
             size = 1/4, linetype = 2) +
  scale_fill_grey(start = .67, end = 0, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(thousand~of~pelts~(italic(h[t])))) +
  facet_grid(Htlab~beta, labeller = label_parsed, scales = "free_y")
```

The vertical dashed lines mark off the maximum values in each panel. The histogram in black is of the simulation parameters based on our version of Figure 16.8, above.

McElreath's proposed model is

$$
\begin{align*}
h_t & \sim \operatorname{Log-Normal} \big (\log(p_H H_t), \sigma_H \big) \\
l_t & \sim \operatorname{Log-Normal} \big (\log(p_L L_t), \sigma_L \big) \\
H_1 & \sim \operatorname{Log-Normal}(\log 10, 1) \\
L_1 & \sim \operatorname{Log-Normal}(\log 10, 1) \\
H_{T >1} & = H_1 + \int_1^T H_t (b_H - m_H L_t) \mathrm{d} t \\
L_{T >1} & = L_1 + \int_1^T L_t (b_L H_T - m_L) \mathrm{d} t \\
\sigma_H & \sim \operatorname{Exponential}(1) \\
\sigma_L & \sim \operatorname{Exponential}(1) \\
p_H & \sim \operatorname{Beta}(\alpha_H, \beta_H) \\
p_L & \sim \operatorname{Beta}(\alpha_L, \beta_L) \\
b_H & \sim \operatorname{Half-Normal}(1, 0.5) \\
b_L & \sim \operatorname{Half-Normal}(0.5, 0.5) \\
m_H & \sim \operatorname{Half-Normal}(0.5, 0.5) \\
m_L & \sim \operatorname{Half-Normal}(1, 0.5).
\end{align*}
$$

It's not immediately clear from the text, but if you look closely at the output from `cat(Lynx_Hare_model)` (see below), you'll see $\alpha_H = \alpha_L = 40$ and $\beta_H = \beta_L = 200$.

Happily, Stan has built-in functions for solving differential equations [@standevelopmentteamStanReferenceManual2020, Chapter 13] and doing is, in principle, possible with **brms**, For an example of an ODE model with **brms**, see [Markus Gesmann](https://twitter.com/MarkusGesmann)'s blog post, [*PK/PD reserving models*](https://magesblog.com/post/2018-01-30-pkpd-reserving-models/). However, this model is beyond my current skill set. Instead of attempting to fit the model with `brms::brm()`, we'll follow McElreath's example and load his pre-written Stan code.

```{r}
data(Lynx_Hare_model) 
cat(Lynx_Hare_model)
```

Fit the model directly with `rstan::stan()`.

```{r m16.5, echo = F}
# save(list = c("dat_list", "m16.5"), file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/m16.5.rda")

load(file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/m16.5.rda")
```

```{r, eval = F}
dat_list <- list(
  N     = nrow(Lynx_Hare), 
  pelts = Lynx_Hare[, 2:3])

m16.5 <- 
  stan(model_code = Lynx_Hare_model, 
       data = dat_list,
       chains = 3, cores = 3,
       seed = 16,
       control = list(adapt_delta = .95))
```

Check the model summary.

```{r, warning = F, message = F}
precis(m16.5, depth = 2)
```

```{r, eval = F, echo = F}
precis(m16.5, depth = 3)
```

For a more elaborate summary, execute `precis(m16.5, depth = 3)`. Here we extract the posterior samples with `extract.samples()`. Since this differs a bit from our typical **brms** workflow, we'll want to inspect the structure of its contents.

```{r}
post <- extract.samples(m16.5) 

post %>% 
  glimpse()
```

To make our version of the top portion of Figure 16.9, we'll want to draw from the contents of `post$pelts_pred`.

```{r}
# for annotation
text <-
  tibble(name  = c("Hare", "Lynx"),
         label = c("Lepus", "Lynx"),
         year  = c(1914, 1916.5),
         value = c(92, 54))

# subset the relevant posterior draws
p1 <-
  rbind(post$pelts_pred[1:21, 1:21, 2],
        post$pelts_pred[1:21, 1:21, 1])%>% 
  # wrangle
  data.frame() %>% 
  set_names(1900:1920) %>% 
  mutate(iter = rep(1:21, times = 2),
         name = rep(c("Hare", "Lynx"), each = 21)) %>% 
  pivot_longer(-c(iter, name),
               names_to = "year") %>% 
  mutate(year = as.numeric(year)) %>% 
  
  # plot
  ggplot(aes(x = year, y = value)) +
  geom_line(aes(group = interaction(iter, name), color = name),
            size = 1/3, alpha = 1/2) +
  geom_point(data = Lynx_Hare %>% pivot_longer(Lynx:Hare),
             aes(x = Year, color = name),
             size = 2) +
  geom_text(data = text,
            aes(label = label, color = name),
            hjust = 0, family = "Times") +
  scale_color_grey(start = 0, end = .5, breaks = NULL) +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_y_continuous("thousands of pelts", breaks = 0:6 * 20) +
  coord_cartesian(ylim = c(0, 120))
```

The workflow for the bottom portion of Figure 16.9 will be very similar. The main difference is this time we're subsetting the posterior draws from `post$pop`.

```{r, fig.width = 6.5, fig.height = 6}
# subset the relevant posterior draws
p2 <-
  rbind(post$pop[1:21, 1:21, 2],
        post$pop[1:21, 1:21, 1])%>% 
  # wrangle
  data.frame() %>% 
  set_names(1900:1920) %>% 
  mutate(iter = rep(1:21, times = 2),
         name = rep(c("Hare", "Lynx"), each = 21)) %>% 
  pivot_longer(-c(iter, name),
               names_to = "year") %>% 
  mutate(year = as.numeric(year)) %>% 
  
  # plot
  ggplot(aes(x = year, y = value)) +
  geom_line(aes(group = interaction(iter, name), color = name),
            size = 1/3, alpha = 1/2) +
  scale_color_grey(start = 0, end = .5, breaks = NULL) +
  scale_y_continuous("thousands of animals", breaks = 0:5 * 100) +
  coord_cartesian(ylim = c(0, 500))

# combine and add a title
p1 / p2 + 
  plot_annotation(title = "Posterior predictions for the lynx-hare model")
```

### 16.4.3. ~~Lynx lessons~~ Bonus: Practice with the autoregressive model.

Back in [Section 16.4][Population dynamics], we briefly discussed how autoregressive models are a typical way to explore processes like those in lynx-hare data. In this bonus section, we'll practice fitting a few of these. To start off, we'll restrict ourselves to focusing on just one of the criteria, `Hare`. Our basic autoregressive model will follow the form

$$
\begin{align*}
\text{Hare}_t & \sim \operatorname{Normal}(\mu_t, \sigma) \\
\mu_t & = \alpha + \beta_1 \text{Hare}_{t - 1} \\
\alpha  & \sim \; ? \\
\beta_1 & \sim \; ? \\
\sigma  & \sim \operatorname{Exponential}(1),
\end{align*}
$$

were $\beta_1$ is the first-order autoregressive coefficient and the question marks in the third and fourth lines indicate we're not wedding ourselves to specific priors, at the moment. Also, note the $t$ subscripts, which denote which time period the observation is drawn from, which in these data is $\text{Year} = 1900, 1901, \dots, 1920$. Conceptually, $t$ is *now* and $t - 1$ the time point just before now. So if we were particularly interested in $\operatorname E (\text{Hare}_{t = 1920})$, $\text{Hare}_{t - 1}$ would be the same as $\text{Hare}_{t = 1919}$.

With **brms**, you can fit a model like this using the `ar()` function. By default, `ar()` presumes the criterion variable (`Hare`, in this case) is ordered chronologically. If you're unsure or just want to be on the safe side, you can enter your *time* variable in the `time` argument. Also, though the `ar()` function presumes a first-order autoregressive structure by default, it is capable of fitting models with higher-order autoregressive structures. You can manually specify this with the `p` argument. Here's how to fit our model with explicit `ar()` syntax.

```{r b16.6}
b16.6 <-
  brm(data = Lynx_Hare,
      family = gaussian,
      Hare ~ 1 + ar(time = Year, p = 1),
      prior(exponential(0.04669846), class = sigma),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 16,
      file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/b16.06")
```

For simplicity, we just went with the default **brms** priors for $\alpha$ and $\beta_1$. We got the value for the exponential prior for $\sigma$ by executing the following.

```{r}
1 / sd(Lynx_Hare$Hare)
```

Here's the model summary.

```{r}
print(b16.6)
```

Our autoregressive $\beta_1$ parameter is summarized in the 'Correlation Structures,' in which it's called 'ar[1].' Another more old-school way to fit a autoregressive model is by manually computing a lagged version of your criterion variable. In **R**, you can do this with the `lag()` function.

```{r}
Lynx_Hare <-
  Lynx_Hare %>% 
  mutate(Hare_1 = lag(Hare))

head(Lynx_Hare)
```

Look closely at the relation between the values in the `Hare` and `Hare_1` columns. They are set up such that $\text{Hare}_{\text{Year} = 1901} = \text{Hare_1}_{\text{Year} = 1900}$, $\text{Hare}_{\text{Year} = 1902} = \text{Hare_1}_{\text{Year} = 1901}$, and so on. Unfortunately, this approach does produce a single missing value in the first time point for the lagged variable, `Hare_1`. Here's how you might use such a variable to manually fit an autoregressive model with `brms::brm()`.

```{r b16.7}
b16.7 <-
  brm(data = Lynx_Hare,
      family = gaussian,
      Hare ~ 1 + Hare_1,
      prior = c(prior(normal(0, 1), class = b),
                prior(exponential(0.04669846), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 16,
      file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/b16.07")
```

```{r}
print(b16.7)
```

Did you notice how the third line in the output read 'Number of observations: 20?' That's because we had that one missing value for `Hare_1`. One quick and dirty hack might be to use the missing data syntax we learned from [Chapter 15][Missing Data and Other Opportunities]. Here's how that might look like.

```{r b16.8}
b16.8 <-
  brm(data = Lynx_Hare,
      family = gaussian,
      bf(Hare ~ 1 + mi(Hare_1)) +
        bf(Hare_1 | mi() ~ 1) +
        set_rescor(FALSE),
      prior = c(prior(normal(0, 1), class = b),
                prior(exponential(0.04669846), class = sigma, resp = Hare),
                prior(exponential(0.04669846), class = sigma, resp = Hare1)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 16,
      file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/b16.08")
```

Check the model summary.

```{r}
print(b16.8)
```

Now we have a model based on all 21 observations, again. I'm still not in love with this fix, because it presumes that value in `Hare_1` was missing at random, with no accounting for the autoregressive structure. This is why, when you can, it's probably better to use the `ar()` syntax.

Anyway, here's how we might use `fitted()` to get a sense of $\operatorname{E}(\text{Hare}_t)$ from our first autoregressive model, `b16.6`.

```{r, fig.width = 6.5, fig.height = 3}
set.seed(16)

fitted(b16.6,
       summary = F,
       nsamples = 21) %>% 
  data.frame() %>% 
  set_names(1900:1920) %>% 
  mutate(iter = 1:n()) %>% 
  pivot_longer(-iter) %>% 
  mutate(Year = as.integer(name)) %>% 
  
  ggplot(aes(x = Year)) +
  geom_line(aes(y = value, group = iter),
            size = 1/3, alpha = 1/2) +
  geom_point(data = Lynx_Hare,
             aes(y = Hare)) +
  annotate(geom = "text",
           x = 1913.5, y = 85,
           label = "Lepus", family = "Times") +
  scale_y_continuous("thousands of hare pelts", breaks = 0:6 * 20, limits = c(0, 120))
```

So far we've been fitting the autoregressive models with the Gaussian likelihood, which is typical. If you look at McElreath's practice problem 16H3, you'll see he proposed a bivariate autoregressive model using the Log-Normal likelihood. His approach used hand-made lagged predictors and ignored the missing value problem by dropping the first case. That model followed the form

$$
\begin{align*}
L_t & \sim \operatorname{Log-Normal}(\log \mu_{L, t}, \sigma_L) \\
H_t & \sim \operatorname{Log-Normal}(\log \mu_{H, t}, \sigma_H) \\
\mu_{L, t} & = \alpha_L + \beta_{L1} L_{t - 1} + \beta_{L2} H_{t - 1} \\
\mu_{H, t} & = \alpha_H + \beta_{H1} H_{t - 1} + \beta_{H2} L_{t - 1},
\end{align*}
$$

where $\beta_{L1}$ and $\beta_{H1}$ are the autoregressive parameters and $\beta_{L2}$ and $\beta_{H2}$ are what are sometimes called the cross-lag parameters. McElreath left the priors up to us. I propose something like this:

$$
\begin{align*}
\alpha_L & \sim \operatorname{Normal}(\log 10, 1) \\
\alpha_H & \sim \operatorname{Normal}(\log 10, 1) \\
\beta_{L1}, \dots, \beta_{H2} & \sim \operatorname{Normal}(0, 0.5) \\
\sigma_L & \sim \operatorname{Exponential}(1) \\
\sigma_H & \sim \operatorname{Exponential}(1).
\end{align*}
$$

Before we fit the model, we'll need to make a lagged version of `Lynx`.

```{r}
Lynx_Hare <-
  Lynx_Hare %>% 
  mutate(Lynx_1 = lag(Lynx))
```

Because the predictor variables are not centered at zero, we'll want to use the `0 + Intercept...` syntax. Now fit the bivariate autoregressive model.

```{r b16.9}
b16.9 <-
  brm(data = Lynx_Hare,
      family = lognormal,
      bf(Hare ~ 0 + Intercept + Hare_1 + Lynx_1) +
        bf(Lynx ~ 0 + Intercept + Lynx_1 + Hare_1) +
        set_rescor(FALSE),
      prior = c(prior(normal(log(10), 1), class = b, resp = Hare, coef = Intercept),
                prior(normal(log(10), 1), class = b, resp = Lynx, coef = Intercept),
                prior(normal(0, 0.5), class = b, resp = Hare),
                prior(normal(0, 0.5), class = b, resp = Lynx),
                prior(exponential(1), class = sigma, resp = Hare),
                prior(exponential(1), class = sigma, resp = Lynx)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 16,
      file = "/Users/solomonkurz/Dropbox/Recoding Statistical Rethinking 2nd ed/fits/b16.09")
```

Check the summary.

```{r}
print(b16.9)
```

Here we'll use `fitted()` to make a variant of the posterior predictions from the top portion of Figure 16.9.

```{r, fig.width = 6.5, fig.height = 3}
rbind(fitted(b16.9, resp = "Hare"),
      fitted(b16.9, resp = "Lynx")) %>% 
  data.frame() %>% 
  mutate(name = rep(c("Hare", "Lynx"), each = n() / 2),
         year = rep(1901:1920, times = 2)) %>% 
  
  ggplot(aes(x = year)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5, group = name, fill = name),
              alpha = 1/3) +
  geom_line(aes(y = Estimate, group = name, color = name)) +
  geom_point(data = Lynx_Hare %>% pivot_longer(Lynx:Hare),
             aes(x = Year, y = value, color = name),
             size = 2) +
  scale_fill_grey(start = 0, end = .5, breaks = NULL) +
  scale_color_grey(start = 0, end = .5, breaks = NULL) +
  scale_x_continuous(limits = c(1900, 1920)) +
  scale_y_continuous("thousands of pelts", breaks = 0:6 * 20)
```

In the next practice problem (16H4), McElreath suggested we "adapt the autoregressive model to use a *two-step* lag variable" (p. 551, *emphasis* added). Using the verbiage from above, we might also refer to that as second-order autoregressive model, AR(2). That would be a straight generalization of the approach we just took. I'll leave the exercise to the interested reader.

The kinds autoregressive models we fit in this section are special cases of what are called **autoregressive moving average** (ARMA) models. Those are available in **brms** with help from the `arma()` function. However, if you want to dive deep into models of this kind,  you might want to check out the [**varstan** package](https://github.com/asael697/varstan) [@matamorosVarstanPackageBayesian2020], which is designed to fit a variety of Bayesian structured time series models.

## Session info {-}

```{r}
sessionInfo()
```

```{r, warning = F, echo = F, eval = F}
rm()
```

```{r, echo = F, message = F, warning = F, results = "hide", eval = F}
ggplot2::theme_set(ggplot2::theme_grey())
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

