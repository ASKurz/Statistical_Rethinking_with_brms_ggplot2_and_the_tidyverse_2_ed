---
output: github_document
---

```{r, echo = F, cache = F}
knitr::opts_chunk$set(fig.retina = 2.5)
knitr::opts_chunk$set(fig.align = "center")
options(width = 100)
```

# The Haunted DAG & The Causal Terror

#### Overthinking: Simulated science distortion.

```{r, warning = F, message = F}
library(tidyverse)
library(brms)
library(bayesplot)
library(tidybayes)
library(GGally)
library(ggdag)
library(dagitty)


theme_set(theme_minimal())
color_scheme_set("orange")
```

## Multicollinearity

### Multicollinear legs.

### Multicollinear `milk`.

#### Rethinking: Identification guaranteed; comprehension up to you.

#### Overthinking: Simulating collinearity.

## Post-treatment bias

### A prior is born.

### Blocked by consequence.

### Fungus and $d$-separation.

```{r, fig.width = 4, fig.height = 1.5, message = F, warning = F}
gg_simple_dag <- function(d) {
  
  d %>% 
    ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
    geom_dag_point(color = "steelblue", alpha = 1/2, size = 6.5) +
    geom_dag_text(color = "black") +
    geom_dag_edges() + 
    theme_dag()
  
}
```

#### Rethinking: Model selection doesn't help.

## Collider bias

```{r, fig.width = 3, fig.height = 1}
dag_coords <-
  tibble(name = c("T", "S", "N"),
         x    = 1:3,
         y    = 1)

dagify(S ~ T + N,
       coords = dag_coords) %>%
  gg_simple_dag()
```

### Collider of false sorrow.

All it takes is a  single `mutate()` line in the `dagify()` function to amend our previous DAG.

```{r, fig.width = 3, fig.height = 1}
dagify(M ~ H + A,
       coords = dag_coords %>%
         mutate(name = c("H", "M", "A"))) %>%
  gg_simple_dag()
```

In this made-up example,

> happiness ($H$) and age ($A$) both cause marriage ($M$). Marriage is therefore a collider. Even though there is no causal association between happiness and age, if we condition on marriage--which means here, if we include it as a predictor in a regression--then it will induce a statistical association between age and happiness. And this can mislead us to think that happiness changes with age, when in fact it is constant.
>
> To convince you of this, let's do another simulation. (pp. 176--177)

```{r, eval = F, echo = F}
rethinking::sim_happiness
```

McElreath simulated the data for this section using his custom `rethinking::sim_happiness()` function. If you'd like to see the guts of the function, execute `rethinking::sim_happiness`. Our approach will be to simulate the data from the ground up. The workflow to follow is based on help from the great [Randall Pruim](https://github.com/rpruim); I was initially stumped and he [lent a helping hand](https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/issues/26). The first step is to make a simple `new_borns()` function, which returns a tibble with `n` unmarried one-year-old's who have different levels of happiness. We'll set the default for `n` at `20`.

```{r}
new_borns <- function(n = 20) {
  tibble(a = 1,                                       # 1 year old
         m = 0,                                       # not married
         h = seq(from = -2, to = 2, length.out = n))  # range of happiness scores
}
```

Here's how it works.

```{r}
new_borns()
```

The second step is to make another custom function, `update_population()`, which takes the input from `new_borns()`. This function will age up the simulated one-year-old's from `new_borns()`, add another cohort of `new_borns()`, and append the cohorts. As you iterate, the initial cohort of `new_borns()` will eventually hit the age of 18, which is also the age they're first eligible to marry (`aom = 18`). At that age and up, the happier people are more likely to get married than the less happy folks. You'll also see that our simulation follows McElreath's in that we remove people from the population after the age of 65. `r emo::ji("shrug")`

```{r}
update_population <- function(pop, n_births = 20, aom = 18, max_age = 65) {
  
  pop %>%
    mutate(a = a + 1,  # everyone gets one year older
           # some people get married
           m = ifelse(m >= 1, 1, (a >= aom) * rbinom(n(), 1, rethinking::inv_logit(h - 4)))) %>%
    filter(a <= max_age) %>%        # old people die
    bind_rows(new_borns(n_births))  # new people are born
  
}
```

Here's what it looks like if we start with an initial `new_borns()` and pump them into `update_population()`.

```{r}
new_borns() %>% 
  update_population()
```

For our final step, we run the population simulation for 1,000 years.

```{r}
# this was McElreath's seed
set.seed(1977)

# year 1
d <- new_borns(n = 20)

# years 2 through 1000
for(i in 2:1000) {
  d <- update_population(d, n_births = 20, aom = 18, max_age = 65)
}

# now rename()
d <- 
  d %>% 
  rename(age = a, married = m, happiness = h)

# take a look
glimpse(d)
```

Summarize the variables.

```{r}
d %>% 
  pivot_longer(everything()) %>% 
  group_by(name) %>% 
  mean_qi(value) %>% 
  mutate_if(is.double, round, digits = 2)
```

Here's our version of Figure 6.5.

```{r, fig.width = 8, fig.height = 2.5}
d %>% 
  mutate(married = factor(married, labels = c("unmarried", "married"))) %>% 
  
  ggplot(aes(x = age, y = happiness, color = married)) +
  geom_point(size = 1.75) +
  scale_color_manual(NULL, values = c("grey85", "forestgreen")) +
  scale_x_continuous(expand = c(.015, .015)) +
  theme(panel.grid = element_blank())
```

Here's the likelihood for the simple Gaussian multivariable model predicting happiness:

\begin{align*}
\text{happiness}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i              & = \alpha_{\text{married} [i]} + \beta_1 \text{age}_i ,\\
\end{align*}

where $\text{married}[i]$ is the marriage status of individual $i$. Here we make `d2`, the subset of `d` containing only those 18 and up. We then make a new `age` variable, `a`, which is scaled such that $18 = 0$, $65 = 1$, and so on.

```{r}
d2 <-
  d %>% 
  filter(age > 17) %>% 
  mutate(a = (age - 18) / (65 - 18))

head(d2)
```

With respect to priors,

> happiness is on an arbitrary scale, in these data, from $-2$ to $+2$. So our imaginary strongest relationship, taking happiness from maximum to minimum, has a slope with rise over run of $(2 - (-2))/1 = 4$. Remember that 95% of the mass of a normal distribution is contained within 2 standard deviations. So if we set the standard deviation of the prior to half of 4, we are saying that we expect 95% of plausible slopes to be less than maximally strong. That isn't a very strong prior, but again, it at least helps bound inference to realistic ranges. Now for the intercepts. Each $\alpha$ is the value of $\mu_i$ when $A_i = 0$. In this case, that means at age 18. So we need to allow $\alpha$ to cover the full range of happiness scores. $\operatorname{Normal}(0, 1)$ will put 95% of the mass in the $-2$ to $+2$ interval. (p. 178)

Here we'll take one last step before fitting our model with **brms**. Saving the `mid` index variable as a factor will make it easier to interpret the model results. To see what I mean, skip this step, fit the model, and compare your results with mine, below.

```{r}
d2 <-
  d2 %>% 
  mutate(mid = factor(married + 1, labels = c("single", "married")))

head(d2)
```

<<refit the models. the data is slightly different>>

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F}
rm(list = ls())
```

```{r, echo = F, message = F, warning = F, results = "hide"}
ggplot2::theme_set(ggplot2::theme_grey())
bayesplot::color_scheme_set("blue")
# pacman::p_unload(pacman::p_loaded(), character.only = TRUE)
```

