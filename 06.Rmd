---
title: "Ch. 6 The Haunted DAG & The Causal Terror"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output:
  github_document
---

# The Haunted DAG & The Causal Terror

Read this opening and cry:

> It seems like the most newsworthy scientific studies are the least trustworthy. The more likely it is to kill you, if true, the less likely it is to be true. The more boring the topic, the more rigorous the results. How could this widely believed negative correlation exist? There doesn’t seem to be any reason for studies of topics that people care about to produce less reliable results. Maybe popular topics attract more and worse researchers, like flies drawn to the smell of honey?
>
> Actually all that is necessary for such a negative correlation to arise is that peer reviewers care about both newsworthiness and trustworthiness. Whether it is grant review or journal review, if editors and reviewers care about both, then the act of selection itself is enough to make the most newsworthy studies the least trustworthy....
>
> Strong selection induces a negative correlation among the criteria used in selection. Why? If the only way to cross the threshold is to score high, it is more common to score high on one item than on both. Therefore among funded proposals, the most newsworthy studies can actually have less than average trustworthiness (less than 0 in the figure). Similarly the most trustworthy studies can actually be less newsworthy than average.
>
> This general phenomenon has been recognized for a long time. It is sometimes called Berkson’s paradox. But it is easier to remember if we call it the *selection-distortion effect*. Once you appreciate this effect, you’ll see it everywhere....
>
> The selection-distortion effect can happen inside of a multiple regression, because the act of adding a predictor in- duces statistical selection within the model, a phenomenon that goes by the unhelpful name collider bias. This can mislead us into believing, for example, that there is a negative association between newsworthiness and trustworthiness in general, when in fact it is just a consequence of conditioning on some variable. This is both a deeply confusing fact and one that is important to understand in order to regress responsibly.
>
> This chapter and the next are both about terrible things that can happen when we simply add variables to a regression, without a clear idea of a causal model. (pp. 159--160, *emphasis* in the original)

The three hazards we'll explore are:

1. multicollinearity
2. post-treatment bias
3. collider bias

#### Overthinking: Simulated science distortion.

First let's run the simulation.

```{r, warning = F, message = F}
library(tidyverse)

set.seed(1914)
n <- 200 # num grant proposals
p <- 0.1 # proportion to select

d <-
  # uncorrelated newsworthiness and trustworthiness
  tibble(newsworthiness  = rnorm(n, mean = 0, sd = 1),
         trustworthiness = rnorm(n, mean = 0, sd = 1)) %>% 
  # total_score
  mutate(total_score = newsworthiness + trustworthiness) %>% 
  # select top 10% of combined scores
  mutate(selected = ifelse(total_score >= quantile(total_score, 1 - p), TRUE, FALSE))

head(d)
```

Here's the correlation among those cases for which `selected == TRUE`.

```{r}
d %>% 
  filter(selected == TRUE) %>% 
  select(newsworthiness, trustworthiness) %>% 
  cor()
```

For the plots in this chapter, we'll take some aesthetic cues from Aki Vehtari's great [*Bayesian Data Analysis demos for R*](https://github.com/avehtari/BDA_R_demos).

```{r}
theme_set(theme_minimal())
```

Okay, let's make Figure 6.1.

```{r, fig.width = 3.5, fig.height = 3.25}
# we'll need this for the annotation
text <-
  tibble(newsworthiness  = c(2, 1), 
         trustworthiness = c(2.25, -2.5),
         selected = c(TRUE, FALSE),
         label    = c("selected", "rejected"))

d %>% 
  ggplot(aes(x = newsworthiness, y = trustworthiness, color = selected)) +
  geom_point(alpha = 2/3) +
  geom_text(data = text,
            aes(label = label)) +
  geom_smooth(data = d %>% filter(selected == TRUE),
              method = "lm", fullrange = T,
              fill = "orange", color = "orange", alpha = 1/4, size = 1/5) +
  scale_color_manual(values = c("black", "orange")) +
  scale_x_continuous(limits = c(-3, 3.9), expand = c(0, 0)) +
  coord_cartesian(ylim = range(d$trustworthiness)) +
  theme(legend.position = "none")
```

## Multicollinearity

> Multicollinearity means very strong correlation between two or more predictor variables. The consequence of it is that the posterior distribution will seem to suggest that none of the variables is reliably associated with the outcome, even if all of the variables are in reality strongly associated with the outcome. This frustrating phenomenon arises from the details of how multiple regression works. So once you understand multicollinearity, you will better understand regression models in general. (p. 161)

### Multicollinear legs.

Let's simulate some leg data.

```{r}
n <- 100
set.seed(909)

d <- 
  tibble(height    = rnorm(n, mean = 10, sd = 2),
         leg_prop  = runif(n, min = 0.4, max = 0.5)) %>% 
  mutate(leg_left  = leg_prop * height + rnorm(n, mean = 0, sd = 0.02),
         leg_right = leg_prop * height + rnorm(n, mean = 0, sd = 0.02))
```

`leg_left` and `leg_right` are **highly** correlated.

```{r}
d %>%
  select(leg_left:leg_right) %>%
  cor() %>%
  round(digits = 4)
```

Have you ever even seen a $\rho = .9995$ correlation, before? Here it is in a plot.

```{r, fig.width = 3, fig.height = 3}
d %>%
  ggplot(aes(x = leg_left, y = leg_right)) +
  geom_point(alpha = 1/2, color = "darkgreen")
```

Load brms.

```{r, message = F, warning = F}
library(brms)
```

Here's our attempt to predict `height` with both legs.

```{r b6.1, cache = T, message = F, warning = F}
b6.1 <- 
  brm(data = d, family = gaussian,
      height ~ 1 + leg_left + leg_right,
      prior = c(prior(normal(10, 100), class = Intercept),
                prior(normal(2, 10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      seed = 6)
```

Let's inspect the damage.

```{r}
print(b6.1)
```

That 'Est.Error' column isn't looking too good. But it's easy to miss that, which is why McElreath suggested "a graphical view of the [output] is more useful because it displays the posterior means and [intervals] in a way that allows us with a glance to see that something has gone wrong here" (p. 143).

Here's our coefficient plot using `brms::stanplot()` with a little help from `bayesplot::color_scheme_set()`.

```{r, message = F, warning = F, fig.width = 6.5, fig.height = 1.75}
library(bayesplot)

color_scheme_set("orange")

stanplot(b6.1, 
         type = "intervals", 
         prob = .5, 
         prob_outer = .95,
         point_est = "mean") +
  labs(title = "The coefficient plot for the two-leg model",
       subtitle = "Holy smokes; look at the widths of those betas!") +
  theme_bw() +
  theme(text = element_text(size = 14),
        panel.grid   = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))
```

Now you can use the `brms::stanplot()` function without explicitly loading the bayesplot package. But loading bayesplot allows you to set the color scheme with `color_scheme_set()`.

This is perhaps the simplest way to plot the bivariate posterior of our two predictor coefficients, Figure 5.8.a.

```{r, fig.width = 3, fig.height = 3}
pairs(b6.1, pars = parnames(b6.1)[2:3])
```

If you'd like a nicer and more focused attempt, you might have to revert to the `posterior_samples()` function and a little ggplot2 code.

```{r, fig.width = 3, fig.height = 3}
post <- posterior_samples(b6.1)
  
post %>% 
  ggplot(aes(x = b_leg_left, y = b_leg_right)) +
  geom_point(color = "darkgreen", alpha = 1/10, size = 1/2)
```

While we're at it, you can make a similar plot with the [`mcmc_scatter()` function](https://cran.r-project.org/web/packages/bayesplot/vignettes/plotting-mcmc-draws.html).

```{r, fig.width = 3, fig.height = 3}
color_scheme_set("green")

post %>% 
  mcmc_scatter(pars = c("b_leg_left", "b_leg_right"),
               size = 1/2, 
               alpha = 1/10)
```

But wow, those coefficients look about as highly correlated as the predictors, just with the reversed sign.

```{r}
post %>% 
  select(b_leg_left:b_leg_right) %>% 
  cor()
```

On page 165, McElreath clarified that "from the computer's perspective, this model is simply:"

$$
\begin{eqnarray}
y_i & \sim & \text{Normal}(\mu_i, \sigma) \\
\mu_i & = & \alpha + (\beta_1 + \beta_2) x_i
\end{eqnarray}
$$

Accordingly, here's the posterior of the sum of the two regression coefficients, Figure 6.2.b. We'll use `tidybayes::geom_halfeyeh()` to both plot the density and mark off the posterior median and percentile-based 95% probability intervals at its base.

```{r, fig.width = 3, fig.height = 3, warning = F, message = F}
library(tidybayes)

post %>% 
  ggplot(aes(x = b_leg_left + b_leg_right, y = 0)) +
  geom_halfeyeh(fill = "steelblue", 
                point_interval = median_qi, .width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title    = "Sum the multicollinear coefficients",
       subtitle = "Marked by the median and 95% PIs")
```

Now we fit the model after ditching one of the leg lengths.

```{r b6.2, cache = T, message = F, warning = F}
b6.2 <- 
  brm(data = d, family = gaussian,
      height ~ 1 + leg_left,
      prior = c(prior(normal(10, 100), class = Intercept),
                prior(normal(2, 10), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      seed = 6)
```

```{r}
print(b6.2)
```

That posterior $SD$ looks much better. Compare this density to the one in Figure 6.1.b.

```{r, fig.width = 3, fig.height = 3}
posterior_samples(b6.2) %>% 
  
  ggplot(aes(x = b_leg_left, y = 0)) +
  geom_halfeyeh(fill = "steelblue", 
                point_interval = median_qi, .width = .95) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title    = "Just one coefficient needed",
       subtitle = "Marked by the median and 95% PIs",
       x        = "only b_leg_left, this time")
```

> *When two predictor variables are very strongly correlated, including both in a model may lead to confusion.* The posterior distribution isn’t wrong, in such a case. It’s telling you that the question you asked cannot be answered with these data. And that’s a great thing for a model to say, that it cannot answer your question. (p. 165, *emphasis* in the original)

### Multicollinear `milk`.

Multicollinearity arises in real data, too.

```{r, message = F}
library(rethinking)
data(milk)
d <- milk
```

Unload rethinking and load brms.

```{r, message = F}
rm(milk)
detach(package:rethinking, unload = T)
library(brms)
```

Here we standardize our three focal variables.

```{r}
d <-
  d %>% 
  mutate(k = (kcal.per.g   - mean(kcal.per.g))   / sd(kcal.per.g),
         f = (perc.fat     - mean(perc.fat))     / sd(perc.fat),
         l = (perc.lactose - mean(perc.lactose)) / sd(perc.lactose))
```

We'll follow the text and fit the two univariable models, first. Note our use of `update()`.

```{r b6.3_and_b6.4, cache = T, message = F, warning = F, results = 'hide'}
# k regressed on f
b6.3 <- 
  brm(data = d, family = gaussian,
      k ~ 1 + f,
      prior = c(prior(normal(0, 0.2), class = Intercept),
                prior(normal(0, 0.5), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      seed = 6)

# k regressed on l
b6.4 <- 
  update(b6.3,
         newdata = d,
         formula = k ~ 1 + l)
```

```{r}
posterior_summary(b6.3) %>% round(digits = 2)
posterior_summary(b6.4) %>% round(digits = 2)
```

Now "watch what happens when we place both predictor varaibles in the same regression model" (p. 166).

```{r b6.5, cache = T, message = F, warning = F, results = 'hide'}
b6.5 <- 
  update(b6.4,
         newdata = d,
         formula = k ~ 1 + f + l)
```

```{r}
posterior_summary(b6.5) %>% round(digits = 2)
```

You can make custom pairs plots with [GGalley](https://cran.r-project.org/web/packages/GGally/index.html), which will also compute the point estimates for the bivariate correlations. Here's a default plot.

```{r, fig.width = 3, fig.height = 3, warning = F, message = F}
#install.packages("GGally", dependencies = T)
library(GGally)

ggpairs(data = d, columns = c(3:4, 6))
```

But you can customize [these](http://ggobi.github.io/ggally/), too. E.g.,

```{r, fig.width = 3, fig.height = 3}
my_diag <- function(data, mapping, ...){
  ggplot(data = data, mapping = mapping) + 
    geom_density(fill = "steelblue", color = "black")
}

my_lower <- function(data, mapping, ...){
  ggplot(data = data, mapping = mapping) + 
    geom_smooth(method = "lm", color = "orange", size = 1/3, 
                fill = "orange", alpha = 1/4) +
    geom_point(alpha = .8, size = 1/4)
  }

# Then plug those custom functions into `ggpairs()`
ggpairs(data  = d, columns = c(3:4, 6),
        diag  = list(continuous = my_diag),
        lower = list(continuous = my_lower)) + 
  theme_bw() +
  theme(strip.background = element_rect(fill = "white", color = "white"),
        axis.text        = element_blank(),
        axis.ticks       = element_blank())
```

Our two predictor "variables are negatively correlated, and so strongly so that they are nearly redundant. Either helps in predicting `kcal.per.g`, but neither helps much *once you already know the other*" (p. 168, *emphasis* in the original). You can really see that on the lower two scatter plots. You'll note the `ggpairs()` plot also showed the Pearson's correlation coefficients.

Making a DAG might help us make sense of this.

```{r, fig.width = 3, fig.height = 1.5, message = F, warning = F}
library(ggdag)

dag_coords <-
  tibble(name = c("L", "D", "F", "K"),
         x    = c(1, 2, 3, 2),
         y    = c(2, 2, 2, 1))

dagify(L ~ D,
       F ~ D,
       K ~ L + F,
       coords = dag_coords) %>%
  
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(color = "steelblue", alpha = 1/2, size = 10) +
  geom_dag_text(color = "black") +
  geom_dag_edges() +
  scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1))
```

> The central tradeoff decides how dense, D, the milk needs to be. Then fat, F, and lactose, L, are determined. Finally, the composition of F and L determines the kilocalories, K. If we could measure D, or had an evolutionary and economic model to predict it based upon other aspects of a species, that would be better than stumbling through regressions. (p. 167)

### How bad is correlation? 

```{r}
d %>% 
  select(perc.fat, perc.lactose) %>% 
  cor()
```

> What can be done about multicollinearity? The best thing to do is be aware of it. You can anticipate this problem by checking the predictor variables against one another in a pairs plot. Any pair or cluster of variables with very large correlations, over about 0.9, may be problematic, once included in the same model. However, it isn’t always true that highly correlated variables are completely redundant—other predictors might be correlated with only one of the pair, and so help extract the unique information each predictor provides. So you can’t know just from a table of correlations nor from a matrix of scatterplots whether multicollinearity will prevent you from including sets of variables in the same model. We always need conceptual models, based upon scientific background, to do useful statistics. The data themselves just aren’t enough. (p. 168)

#### Overthinking: Simulating collinearity.

First we'll get the data and define the functions. You'll note I've defined my `sim_coll()` a little differently from `sim.coll()` in the text. I've omitted `rep.sim.coll()` as an independent function altogether, but computed similar summary information with the `summarise()` code at the bottom of the block.

```{r, warning = F, message = F}
sim_coll <- function(seed, rho){
  set.seed(seed)
  d <-
    d %>% 
    mutate(x = rnorm(n(), 
                     mean = perc.fat * rho,
                     sd   = sqrt((1 - rho^2) * var(perc.fat))))
    
  m <- lm(kcal.per.g ~ perc.fat + x, data = d)
  
  sqrt(diag(vcov(m)))[2]  # parameter SD
}

# how many simulations per `rho`-value would you like?
n_seed <- 100
# how many `rho`-values from 0 to .99 would you like to evaluate the process over?
n_rho  <- 30

d <-
  tibble(seed = 1:n_seed) %>% 
  expand(seed, rho = seq(from = 0, to = .99, length.out = n_rho)) %>% 
  mutate(parameter_sd = purrr::map2_dbl(seed, rho, sim_coll)) %>% 
  group_by(rho) %>% 
  # we'll `summarise()` our output by the mean and 95% intervals
  summarise(mean = mean(parameter_sd),
            ll   = quantile(parameter_sd, prob = .025),
            ul   = quantile(parameter_sd, prob = .975))
```

We've added 95% interval bands to our version of Figure 5.10.

```{r, fig.width = 3.25, fig.height = 2.75}
d %>% 
  ggplot(aes(x = rho, y = mean)) +
  geom_smooth(aes(ymin = ll, ymax = ul),
              stat = "identity",
              fill = "orange", color = "orange", alpha = 1/3, size = 2/3) +
  labs(x = expression(rho),
       y = "parameter SD") +
  coord_cartesian(ylim = c(0, .0072))
```

Did you notice we used the base R `lm()` function to fit the models? As McElreath rightly pointed out, `lm()` presumes flat priors. Proper Bayesian modeling could improve on that. But then we’d have to wait for a whole lot of HMC chains to run and until our personal computers or the algorithms we use to fit our Bayesian models become orders of magnitude faster, we just don’t have time for that.

## Post-treatment bias.

It helped me understand the next example by mapping out the sequence of events McElreath described in the second paragraph:

* seed and sprout plants
* measure heights
* apply different antifungal soil treatments (i.e., the experimental manipulation)
* measure (a) the heights and (b) the presence of fungus

Based on the design, let's simulate our data.

```{r}
# how many plants would you like?
n <- 100

set.seed(71)
d <- 
  tibble(h0        = rnorm(n, mean = 10, sd = 2), 
         treatment = rep(0:1, each = n / 2),
         fungus    = rbinom(n, size = 1, prob = .5 - treatment * 0.4),
         h1        = h0 + rnorm(n, mean = 5 - 3 * fungus, sd = 1))
```

We'll use `head()` to peek at the data.

```{r}
d %>%
  head()
```

And here's a quick summary with `tidybayes::mean_qi()`.

```{r}
d %>% 
  gather() %>% 
  group_by(key) %>% 
  mean_qi(.width = .89) %>% 
  mutate_if(is.double, round, digits = 2)
```

### A prior is born.

Let's take a look at the $p \sim \text{Log-Normal}(0, 0.25)$ prior distribution.

```{r, fig.width = 6, fig.height = 3.25}
set.seed(6)

# simulate
sim_p <-
  tibble(sim_p = rlnorm(1e4, meanlog = 0, sdlog = 0.25)) 

# wrangle
sim_p %>% 
  mutate(`exp(sim_p)` = exp(sim_p)) %>%
  gather() %>% 
  
  # plot
  ggplot(aes(x = value)) +
  geom_density(fill = "steelblue") +
  scale_x_continuous(breaks = c(0, .5, 1, 1.5, 2, 3, 5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:6) +
  theme(panel.grid.minor.x = element_blank()) +
  facet_wrap(~key, scale = "free_y", ncol = 1)
```

```{r}
sim_p %>% 
  mutate(`exp(sim_p)` = exp(sim_p)) %>%
  gather() %>%
  group_by(key) %>% 
  mean_qi(.width = .89) %>% 
  mutate_if(is.double, round, digits = 2)
```

So then, our initial statistical model follows the form

$$
\begin{eqnarray}
h_{1i} & \sim & \text{Normal} (\mu_i, \sigma)\\
\mu_i & = & h_{0i} \times p\\
p & \sim & \text{Log-Normal}(0, 0.25) \\
\sigma & \sim & \text {Exponential} (1)
\end{eqnarray}
$$

Let's fit the model.

```{r b6.6, cache = T, message = F, warning = F}
b6.6 <- 
  brm(data = d, family = gaussian,
      h1 ~ 0 + h0,
      prior = c(prior(lognormal(0, 0.25), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 6)
```

Behold the summary.

```{r}
print(b6.6)
```

So then, the expectation is an increase of about `r round((fixef(b6.6)[1] - 1) * 100, 0)` percent relative to $h_0$. But this isn't the best model. We're leaving important predictors on the table. Our updated model follows the form

$$
\begin{eqnarray}
h_{1i} & \sim & \text{Normal} (\mu_i, \sigma)\\
\mu_i & = & h_{0i} \times p\\
p & = & \alpha + \beta_1 \text{treatment}_i + \beta_2 \text{fungus}_i \\
\alpha & \sim & \text{Log-Normal}(0, 0.25) \\
\beta_1 & \sim & \text{Normal} (0, 0.5) \\
\beta_2 & \sim & \text{Normal} (0, 0.5) \\
\sigma & \sim & \text {Exponential} (1)
\end{eqnarray}
$$

That is, now the "proportion of growth $p$ is now a function of the predictor variables" (p. 172). 

Although we will fit the equivalent of McElreath's model in brms, I'm not aware that we can translate it directly into brms syntax. But take a look at the critical two lines from above.

$$
\begin{eqnarray}
\mu_i & = & h_{0i} \times p\\
p & = & \alpha + \beta_1 \text{treatment}_i + \beta_2 \text{fungus}_i \\
\end{eqnarray}
$$

With just a little algebra, we can re-express that as

$$
\mu_i = h_{0i} \times (\alpha + \beta_1 \text{treatment}_i + \beta_2 \text{fungus}_i)
$$

And that’s something we can do within brms if we’re willing to use the [nonlinear syntax](https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html). Here it is.

```{r b6.7, cache = T, message = F, warning = F}
b6.7 <- 
  brm(data = d, family = gaussian,
      bf(h1 ~ h0 * (a + t * treatment + f * fungus),
         a + t + f ~ 1,
         nl = TRUE),
      prior = c(prior(lognormal(0, 0.2), nlpar = a),
                prior(normal(0, 0.5), nlpar = t),
                prior(normal(0, 0.5), nlpar = f),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      seed = 6)
```

To explain what's going on with our `formula `syntax, it's probably best to quote [Bürkner's vignette](https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html) at length:

> When looking at the above code, the first thing that becomes obvious is that we changed the `formula` syntax to display the non-linear formula including predictors (i.e., [`h0`, `treatment`, and `fungus`]) and parameters (i.e., [`a`, `t`, and `f`]) wrapped in a call to [the `bf()` function]. This stands in contrast to classical **R** formulas, where only predictors are given and parameters are implicit. The argument [`a + t + f ~ 1`] serves two purposes. First, it provides information, which variables in `formula` are parameters, and second, it specifies the linear predictor terms for each parameter. In fact, we should think of non-linear parameters as placeholders for linear predictor terms rather than as parameters themselves (see also the following examples). In the present case, we have no further variables to predict [`a`,  `t`, and `f`] and thus we just fit intercepts that represent our estimates of [$\alpha$, $t$, and  $f$]. The formula [`a + t + f ~ 1`] is a short form of [`a ~ 1, t ~ 1, f ~ 1`] that can be used if multiple non-linear parameters share the same formula. Setting `nl = TRUE` tells **brms** that the formula should be treated as non-linear.
>
> In contrast to generalized linear models, priors on population-level parameters (i.e., 'fixed effects') are often mandatory to identify a non-linear model. Thus, **brms** requires the user to explicitely specify these priors. In the present example, we used a [`lognormal(0, 0.2)` prior on (the population-level intercept of) `a`, while we used a `normal(0, 0.5)` prior on both (population-level intercepts of) `t` and `f`]. Setting priors is a non-trivial task in all kinds of models, especially in non-linear models, so you should always invest some time to think of appropriate priors. Quite often, you may be forced to change your priors after fitting a non-linear model for the first time, when you observe different MCMC chains converging to different posterior regions. This is a clear sign of an idenfication problem and one solution is to set stronger (i.e., more narrow) priors. (**emphasis** in the original)

Let's see what we've done.

```{r}
print(b6.7)
```

All in all, it looks like we did a good job matching up McElreath's results. The posterior doesn't, however, match up well with the way we generated the data...

### Blocked by consequence.

To measure the treatment effect properly, we should omit `fungus` from the model. This leaves us with the equation

$$
\begin{eqnarray}
h_{1i} & \sim & \text{Normal} (\mu_i, \sigma)\\
\mu_i & = & h_{0i} \times (\alpha + \beta_1 \text{treatment}_i) \\
\alpha & \sim & \text{Log-Normal}(0, 0.25) \\
\beta_1 & \sim & \text{Normal} (0, 0.5) \\
\sigma & \sim & \text {Exponential} (1)
\end{eqnarray}
$$

Fit the model.

```{r b6.8, cache = T, message = F, warning = F}
b6.8 <- 
  brm(data = d, family = gaussian,
      bf(h1 ~ h0 * (a + t * treatment),
         a + t ~ 1,
         nl = TRUE),
      prior = c(prior(lognormal(0, 0.2), nlpar = a),
                prior(normal(0, 0.5), nlpar = t),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      seed = 6)
```

Did we do better?

```{r}
print(b6.8)
```

Yes, now we have a positive treatment effect.

### Fungus and $d$-separation.

Let's make a DAG.

```{r, fig.width = 4, fig.height = 1.5, message = F, warning = F}
# define our coordinates
dag_coords <-
  tibble(name = c("H0", "T", "F", "H1"),
         x    = c(1, 5, 4, 3),
         y    = c(2, 2, 1.5, 1))

# save our DAG
dag <-
  dagify(F ~ T,
         H1 ~ H0 + F,
         coords = dag_coords)

# plot 
dag %>%
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(color = "steelblue", alpha = 1/2, size = 10) +
  geom_dag_text(color = "black") +
  geom_dag_edges() +
  scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1))
```

The DAG clarifies 

> that learning the treatment tells us nothing about the outcome, once we know the fungus status.
>
> An even more DAG way to say this is that conditioning on **F** induces d-separation. The "d" stands for *dependence*. $d$-separation means that some variables are independent of others, given that we condition on some other set of variables. In this case, **H1** is $d$-separated from **T** when we condition on **F**. If we do not condition on **F**, then they are not $d$-separated. (p. 174, *emphasis* and **emphasis** in the original)

Note that our ggdag object, `dag`, will also work with the `dagitty::dseparated()` function.

```{r, message = F, warning = F}
library(dagitty)

dag %>% 
  dseparated("T", "H1")

dag %>% 
  dseparated("T", "H1", "F")
```

The descriptively-named `dagitty::mpliedConditionalIndependencies()` function will work, too.

```{r}
impliedConditionalIndependencies(dag)
```

Notice that last line. "Final height is independent of treatment, when conditioning on fungus" (p. 175)

#### Rethinking: Model selection doesn’t help. 

> In the next chapter, you’ll learn about model selection using information criteria. Like other model comparison and selection schemes, these criteria help in contrasting and choosing model structure. But such approaches are no help in the example presented just above, since the model that includes `fungus` both fits the sample better and would make better out-of-sample predictions. Model [`b6.7`] misleads because it asks the wrong question, not because it would make poor predictions. No statistical procedure can substitute for scientific knowledge and attention to it. We need multiple models because they help us understand causal paths, not just so we can choose one or another for prediction. (p. 175)

Brutal.

## Collider bias

Make the collider bias DAG of the trustworthiness/newsworthiness example.

```{r, fig.width = 3, fig.height = 1}
dag_coords <-
  tibble(name = c("T", "S", "N"),
         x    = 1:3,
         y    = 1)

dagify(S ~ T + N,
       coords = dag_coords) %>%
  
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(color = "steelblue", alpha = 1/2, size = 10) +
  geom_dag_text(color = "black") +
  geom_dag_edges() +
  scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1))
```

> The fact that two arrows enter S means it is a collider. This is perhaps an unhelpful label. But the core concept is easy to understand: When you condition on a collider, it creates statistical—but not necessarily causal—associations among its causes. In this case, once you learn that a proposal has been selected (S), then learning its trustworthiness (T) also provides information about its newsworthiness (N). Why? Because if, for example, a selected proposal has low trustworthiness, then it must have high newsworthiness. Otherwise it wouldn’t have been funded. The same works in reverse: If a proposal has low newsworthiness, we’d infer that it must have higher than average trustworthiness. Otherwise it would not have been selected for funding. (p. 175)

### Collider of false sorrow.

All it takes is a  single `mutate()` line in the `dagify()` function to amend our previous DAG.

```{r, fig.width = 3, fig.height = 1}
dagify(M ~ H + A,
       coords = dag_coords %>%
         mutate(name = c("H", "M", "A"))) %>%
  
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(color = "steelblue", alpha = 1/2, size = 10) +
  geom_dag_text(color = "black") +
  geom_dag_edges() +
  scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1))
```

In this made-up example,

> happiness (H) and age (A) both cause marriage (M). Marriage is therefore a collider. Even though there is no causal association between happiness and age, if we condition on marriage— which means here, if we include it as a predictor in a regression—then it will induce a statis- tical association between age and happiness. And this can mislead us to think that happiness changes with age, when in fact it is constant.
>
> To convince you of this, let’s do another simulation. (p. 177)

Here's the code for McElreath's `rethinking::sim_happiness()` function.

```{r}
rethinking::sim_happiness
```

Quite frankly, I can’t make sense of it. So we’ll just have to move forward and use the convenience function rather than practicing a tidyverse alternative. If you have a handle on what's going on and have a tidyverse alternative, [please share your code](https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/issues).

```{r}
d <- rethinking::sim_happiness(seed = 1977, N_years = 1000)

head(d)
```

Here's our version of Figure 6.5.

```{r, fig.width = 8, fig.height = 2.5}
d %>% 
  mutate(married = factor(married,
                          labels = c("unmarried", "married"))) %>% 
  
  ggplot(aes(x = age, y = happiness)) +
  geom_point(aes(color = married), size = 1.75) +
  scale_color_manual(NULL, values = c("grey80", "darkgreen")) +
  scale_x_continuous(expand = c(.015, .015)) +
  theme(panel.grid = element_blank())
```

Here's the simple multivariable model predicting happiness

$$
\begin{eqnarray}
\text{happiness}_i & \sim & \text{Normal} (\mu_i, \sigma)\\
\mu_i & = & \alpha_{\text{married} [i]} + \beta_1 \text{age}_i \\
\end{eqnarray}
$$

where $\text{married} [i]$ is the marriage status of individual $i$.

Here we make `d2`, the subset of `d` containing only those 18 and up. We then make a new `age` variable, `a`, which is scaled such that $18 = 0$, $65 = 1$, and so on. 

```{r}
d2 <-
  d %>% 
  filter(age > 17) %>% 
  mutate(a = (age - 18) / (65 - 18))

head(d2)
```

With respect to priors,

> Happiness is on an arbitrary scale, in these data, from −2 to +2. So our imaginary strongest relationship, taking happiness from maximum to minimum, has a slope with rise over run of $(2 − (−2))/1 = 4$. Remember that 95% of the mass of a normal distribution is contained within 2 standard deviations. So if we set the standard deviation of the prior to half of 4, we are saying that we expect 95% of plausible slopes to be less than maximally strong. That isn’t a very strong prior, but again, it at least helps bound inference to realistic ranges. Now for the intercepts. Each $\alpha$ is the value of $\mu_i$ when $\text a_i = 0$. In this case, that means at age 18. So we need to allow α to cover the full range of happiness scores. $\text{Normal} (0, 1)$ will put 95% of the mass in the −2 to +2 interval. (p. 177)

```{r}
d2 <-
  d2 %>% 
  mutate(mid = factor(married + 1, labels = c("single", "married")))

head(d2)
```

Fit the model.

```{r b6.9, cache = T, message = F, warning = F}
b6.9 <- 
  brm(data = d2, family = gaussian,
      happiness ~ 0 + mid + a,
      prior = c(prior(normal(0, 1), class = b, coef = midmarried),
                prior(normal(0, 1), class = b, coef = midsingle),
                prior(normal(0, 2), class = b, coef = a),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      seed = 6)
```

```{r}
print(b6.9)
```

Now drop marriage status, `mid`.

```{r b6.10, cache = T, message = F, warning = F}
b6.10 <- 
  brm(data = d2, family = gaussian,
      happiness ~ 0 + intercept + a,
      prior = c(prior(normal(0, 1), class = b, coef = intercept),
                prior(normal(0, 2), class = b, coef = a),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      seed = 6)
```

```{r}
print(b6.10)
```

Wow. So when we take out `mid`, the coefficient for `a` drops to zero.

> The pattern above is exactly what we should expect when we condition on a collider. The collider is marriage status. It a common consequence of age and happiness. As a result, when we condition on it, we induce a spurious association between the two causes. So it looks like, to model `b6.9`, that age is negatively associated with happiness. But this is just a statistical association, not a causal association. Once we know whether someone is married or not, then their age does provide information about how happy they are. (p. 178)

### The haunted DAG.

"I’m sorry to say that we also have to consider the possibility that our DAG may be haunted" (p. 179).

Here's the unhaunted DAG.

```{r, fig.width = 2.5, fig.height = 2}
dag_coords <-
  tibble(name = c("G", "P", "C"),
         x    = c(1, 2, 2),
         y    = c(2, 2, 1))

dagify(P ~ G,
       C ~ P + G,
       coords = dag_coords) %>%
  
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(color = "steelblue", alpha = 1/2, size = 10) +
  geom_dag_text(color = "black") +
  geom_dag_edges() +
  scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1))
```

Now we add the haunting variable, `U`.

```{r, fig.width = 3.25, fig.height = 2}
dag_coords <-
  tibble(name = c("G", "P", "C", "U"),
         x    = c(1, 2, 2, 2.5),
         y    = c(2, 2, 1, 1.5))

dagify(P ~ G + U,
       C ~ P + G + U,
       coords = dag_coords) %>%
  
  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_point(color = "steelblue", alpha = 1/2, size = 10) +
  geom_dag_text(color = "black") +
  geom_dag_edges() +
  scale_x_continuous(NULL, breaks = NULL, expand = c(.1, .1)) +
  scale_y_continuous(NULL, breaks = NULL, expand = c(.1, .1))
```

Let's simulate our data.

```{r}
# How many grandparent-parent-child triads would you like?
n    <- 200 
b_gp <- 1  # direct effect of G on P
b_gc <- 0  # direct effect of G on C
b_pc <- 1  # direct effect of P on C
b_u  <- 2  # direct effect of U on P and C

# simulate triads
set.seed(1)
d <-
  tibble(U = 2 * rbinom(n, size = 1, prob = .5) - 1,
         G = rnorm(n, mean = 0, sd = 1)) %>% 
  mutate(P = rnorm(n, mean = b_gp * G + b_u * U, sd = 1)) %>% 
  mutate(C = rnorm(n, mean = b_pc * P + b_gc * G + b_u * U, sd = 1)) %>%
  mutate_at(vars(G, P, C), funs(as.vector(scale(.))))

head(d)
```

"Now P is a common consequence of G and U, so if we condition on P, it will bias inference about G $\rightarrow$ C, *even if we never get to measure* U" (p. 179, *emphasis* in the original).

Fit the model without `U`.

```{r b0, cache = T, message = F, warning = F}
b0 <- 
  brm(data = d, family = gaussian,
      C ~ 1 + P + G,
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(normal(0, 1), class = b),
                prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 500, chains = 4, cores = 4,
      seed = 6)
```

```{r}
print(b0)
```

Now fit the model including `U`.

```{r b1, cache = T, message = F, warning = F, results = "hide"}
b1 <- 
  update(b0,
         newdata = d,
         formula = C ~ 1 + P + G + U)
```



```{r}
print(b1)
```

Now the posterior for $\beta_\text G$ is hovering around 0, where it belongs.

### Primate collider. `x`

## 6.4. Instrumental variables

## Reference {-}

[McElreath, R. (2016). *Statistical rethinking: A Bayesian course with examples in R and Stan.* Chapman & Hall/CRC Press.](https://xcelab.net/rm/statistical-rethinking/)

## Session info {-}

```{r}
sessionInfo()
```

```{r, echo = F, eval = F}
rm()
```

```{r, echo = F, message = F, warning = F, results = "hide"}
pacman::p_unload(pacman::p_loaded(), character.only = TRUE)

ggplot2::theme_set(ggplot2::theme_grey())

bayesplot::color_scheme_set("blue")
```